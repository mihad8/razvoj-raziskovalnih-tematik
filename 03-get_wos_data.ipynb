{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import gzip\n",
    "import mechanicalsoup\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wos_data(file):\n",
    "    soup = BeautifulSoup(open(file, encoding='utf-8'), 'html.parser')\n",
    "    \n",
    "    # Title\n",
    "    title = str(soup.find(\"div\", {\"class\": \"title\"}))\n",
    "    title = title[title.find('<value>') + 7:title.find('</value>')]\n",
    "    \n",
    "    # Authors\n",
    "    authors = []\n",
    "    authors_array = soup.findAll(\"a\", {\"title\": \"Find more records by this author\"})\n",
    "\n",
    "    for author in authors_array:\n",
    "        author = str(author)\n",
    "        authors.append(author[author.find('>', 400)+1:author.rfind('</a>')] + '.')\n",
    "    \n",
    "    # Keywords\n",
    "    a_array = soup.findAll(\"a\", {\"class\": \"snowplow-author-keyword-link\"})\n",
    "    keywords = []\n",
    "\n",
    "    for a in a_array:\n",
    "        a = str(a)\n",
    "        keywords.append(a[a.find('>', 450) + 1:-4])\n",
    "    \n",
    "    # Abstract and Date\n",
    "    p_array = soup.findAll(\"p\", {\"class\": \"FR_field\"})\n",
    "    for p in p_array:\n",
    "        p = str(p)\n",
    "        if p[19] == '>' and (p[20] != '\\n' and p[20] != ' '):\n",
    "            abstract = p[p.find('>') + 1:-4]\n",
    "        elif p.startswith('<p class=\"FR_field\">\\n<span class=\"FR_label\">Published:</span>'):\n",
    "            date = p[p.find('<value>') + 7:p.find('</value>')]\n",
    "    \n",
    "    # No. of citations\n",
    "    h2 = str(soup.find('h2'))\n",
    "    citations = h2[h2.find(':') + 2:-5]\n",
    "    \n",
    "    # Funding agency\n",
    "    tr = soup.find(\"tr\", {\"class\": \"fr_data_row\"})\n",
    "    funding = tr.find(\"td\").string[:-1]\n",
    "    \n",
    "    data = {}\n",
    "    data['title'] = title\n",
    "    data['authors'] = authors\n",
    "    data['keywords'] = keywords\n",
    "    data['date'] = date\n",
    "    data['abstract'] = abstract\n",
    "    data['language'] = detect(abstract)\n",
    "    data['citations'] = citations\n",
    "    data['funding_agency'] = funding\n",
    "    \n",
    "    json_data = json.dumps(data)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def get_newest_file(path):\n",
    "    files = os.listdir(path)\n",
    "    paths = [os.path.join(path, basename) for basename in files]\n",
    "    return max(paths, key=os.path.getctime)\n",
    "\n",
    "def get_url(DOI):\n",
    "    start_url = 'http://apps.webofknowledge.com/WOS_GeneralSearch_input.do?product=WOS&search_mode=GeneralSearch&SID=E4nAHEnmQkjgeXGHVKC&preferencesSaved='\n",
    "    \n",
    "    browser = mechanicalsoup.StatefulBrowser()\n",
    "    browser.open(start_url)\n",
    "    browser.select_form('form[action=\"/WOS_GeneralSearch.do\"]')\n",
    "    # browser.get_current_form().print_summary()\n",
    "    browser['value(input1)'] = DOI\n",
    "    response = browser.submit_selected()\n",
    "\n",
    "    start = response.text.find('href=\\\"/full_record') + 6\n",
    "    end = response.text.find('>', start) - 1\n",
    "    \n",
    "    url = ''\n",
    "    if response.text[start:end].startswith('/full_record'):\n",
    "        url = 'http://apps.webofknowledge.com' + response.text[start:end]\n",
    "        url = url.replace('amp;', '')\n",
    "    print(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iz XML datoteke, ki smo jo dobili iz Sicrisa, izvlečemo DOI člankov, ki ga bomo uporabili za iskanje člankov po portalu Web of Scence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xmldoc = minidom.parse('downloads/cobiss-org/2019-06-24-144940_cobiss.xml')\n",
    "listDOI = xmldoc.getElementsByTagName('DOI')\n",
    "listWOS = xmldoc.getElementsByTagName('WoS')\n",
    "DOIs = []\n",
    "for doi, wos in zip(listDOI, listWOS):\n",
    "    if len(doi.childNodes) != 0 and len(wos.childNodes) != 0:\n",
    "        DOIs.append([doi.childNodes[0].nodeValue, wos.childNodes[0].nodeValue[-15:]])\n",
    "        \n",
    "DOIs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DOI in DOIs:\n",
    "    if not os.path.exists('downloads/wos/{:}'.format(DOI[1])):\n",
    "        os.mkdir('downloads/wos/{:}'.format(DOI[1]))\n",
    "\n",
    "    if len(os.listdir('downloads/wos/{:}'.format(DOI[1]))) != 0:\n",
    "        file = get_newest_file('downloads/wos/{:}'.format(DOI[1]))\n",
    "        file_date = datetime.strptime(file[file.find('\\\\')+1:-5], '%Y-%m-%d')\n",
    "        if file_date < now - timedelta(days = 14):\n",
    "            url = get_url(DOI[0])\n",
    "            if len(url):\n",
    "                response = requests.get(url)\n",
    "                file = open('downloads/wos/{:}/{:}.html'.format(DOI[1], now.strftime('%Y-%m-%d')), \"w\", encoding='utf-8')\n",
    "                file.write(response.text)\n",
    "                file.close()\n",
    "    else:\n",
    "        url = get_url(DOI[0])\n",
    "        if len(url):\n",
    "            response = requests.get(url)\n",
    "            file = open('downloads/wos/{:}/{:}.html'.format(DOI[1], now.strftime('%Y-%m-%d')), \"w\", encoding='utf-8')\n",
    "            file.write(response.text)\n",
    "            file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
