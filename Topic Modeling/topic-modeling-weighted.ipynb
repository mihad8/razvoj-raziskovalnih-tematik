{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights k-Means\n",
    "\n",
    "This is the same script as topic-modeling, but considers weights. Currently, weights are set to 'CitedBy' column, which is a count of citations as sources by Scopus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pickle.load(open('../Preprocessing/tokens.pkl', 'rb'))\n",
    "data = pickle.load(open('../Preprocessing/topic-corpus.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(tokens):\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cited_by = data['CitedBy'].to_numpy()\n",
    "weights = cited_by.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_texts(texts, clusters=3, weight=weights):\n",
    "    \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "    vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                            tokenizer=dummy_fun,\n",
    "                            preprocessor=dummy_fun,\n",
    "                            max_df=0.9,\n",
    "                            min_df=0.1,\n",
    "                            token_pattern=None) \n",
    "    X = vectorizer.fit_transform(tokens)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    km_model = KMeans(n_clusters=clusters)\n",
    "    km_model.fit(X, sample_weight=weights)\n",
    " \n",
    "    clustering = collections.defaultdict(list)\n",
    " \n",
    "    for idx, label in enumerate(km_model.labels_):\n",
    "        clustering[label].append(idx)\n",
    " \n",
    "    return clustering, X, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_CLUSTERS = 7\n",
    "\n",
    "clusters, X, features = cluster_texts(data, NUMBER_OF_CLUSTERS, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "items = [[key, i] for key, value in dict(clusters).items() for i in value]\n",
    "df2 = pd.DataFrame(items, columns=['Cluster', 'Index'])\n",
    "df2.set_index('Index', inplace=True)\n",
    "df2 = df2.sort_index()\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = pd.concat([data, df2], axis=1)\n",
    "conc.to_csv('/Users/ajda/Desktop/weighted-clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of clusters found: {}\".format(len(clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Orange.statistics.util import FDR\n",
    "from orangecontrib.text.stats import hypergeom_p_values\n",
    "import numpy as np\n",
    "\n",
    "filter_p_value = 0.01\n",
    "filter_fdr_value = 1\n",
    "\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    mask = []\n",
    "    for cl, ind in items:\n",
    "        if cl == i:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "\n",
    "    data = X.toarray()\n",
    "    selected_data = np.compress(mask, data, axis=0)\n",
    "\n",
    "    p_values = hypergeom_p_values(data, selected_data)\n",
    "    fdr_values = FDR(p_values)\n",
    "    \n",
    "    fp = lambda score: \"%0.5f\" % score if score > 10e-3 else \"%0.1e\" % score\n",
    "    fpt = lambda score: \"%0.9f\" % score if score > 10e-3 else \"%0.5e\" % score\n",
    "    \n",
    "    print(\"Enrichment of cluster {}\".format(i+1))\n",
    "    \n",
    "    result = []\n",
    "    for word, pval, fval in zip(features, p_values, fdr_values):\n",
    "        if pval <= filter_p_value and fval <= filter_fdr_value:\n",
    "            result.append((word, fp(pval), fpt(fval)))\n",
    "    result = sorted(result, key=getKey)\n",
    "    for w, p, f in result:\n",
    "        print(\"    \", w, p, f)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
