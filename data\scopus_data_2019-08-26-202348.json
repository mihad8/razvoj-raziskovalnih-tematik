[
{"Title": "An improved edge profile based method for text detection in images of natural scenes", "Authors": ["Ikica A.", "Peer P."], "Keywords": ["computer vision", "edge profiles", "natural scenes", "text detection"], "Date": "2011", "Abstract": "Text detection in natural images has gained much attention in the last years as it is a primary step towards fully autonomous text recognition. Understanding the visual text content is of a vital importance in many applicative areas from the internet search engines to the PDA signboard translators. Images of natural scenes, however, pose numerous difficulties comPared to the traditional scanned documents. They mainly contain diverse complex text of different sizes, styles and colors with complex backgrounds. Furthermore, such images are captured under variable lighting conditions and are often affected by the skew distortion and perspective projections. In this article an improved edge profile based text detection method is presented. It uses a set of heuristic rules to eliminate detection of non-text areas. The method is evaluated on CVL OCR DB, an annotated image database of text in natural scenes. \u00a9 2011 IEEE.", "Language": "en", "Citations": "11"},
{"Title": "Improving mobile operator information system efficiency through EAI", "Authors": ["Rozman I.", "Juric M.B.", "Hericko M.", "Vezocnik I.", "Krisper M."], "Keywords": ["CORBA", "EAI", "Integration", "J2EE", "Methods", "Web services"], "Date": "2004", "Abstract": "Ability to integrate legacy assets and reusing their functionality in modern web-enabled information systems are important features of new-generation technologies. In the paper organisational challenges are discussed, a multiphase integration procedure is proposed, which enables step-by-step integration of legacy applications and extension of their functionality with new generation technologies. A decision model for selecting the most appropriate technology infrastructure is presented and several technologies are evaluated. The paper, which also presents a integration case study of a major mobile phone operator company, contributes to the general understanding of legacy systems integration, gives a solid basis for making a decision about the underlying technology and presents an integration procedure that provably works in practice.", "Language": "en", "Citations": "1"},
{},
{"Title": "Fuzzy user modeling for adaptation in educational hypermedia", "Authors": ["Kavcic A."], "Keywords": [], "Date": "2004", "Abstract": "Education is a dominating application area for adaptive hypermedia. Web-based adaptive educational systems incorporate complex intelligent tutoring techniques, which enable the system to recognize an individual user and their needs, and consequently adapt the instructional sequence. The personalization is done through the user model, which collects information about the user. Since the description of user knowledge and features also involves imprecision and vagueness, a user model has to be designed that is able to deal with this uncertainty. This paper presents a way of describing the uncertainty of user knowledge, which is used for user knowledge modeling in an adaptive educational system. The system builds on the concept domain model. A fuzzy user model is proposed to deal with vagueness in the user's knowledge description. The model uses fuzzy sets for knowledge representation and linguistic rules for model updating. The data from the fuzzy user model form the basis for the system adaptation, which implements various navigation support techniques. The evaluation of the presented educational system has shown that the system and its adaptation techniques provide a valuable, easy-to-use tool, which positively affects user knowledge acquisition and, therefore, leads to better learning results. \u00a9 2004 IEEE.", "Language": "en", "Citations": "77"},
{"Title": "Scalability of learning impact on complex parameters in recurrent neural networks", "Authors": ["Ster B.", "Dobnikar A."], "Keywords": ["Learning automata", "Network size", "Problem extent", "Recurrent neural networks", "Scalability", "Structural parameters"], "Date": "2009", "Abstract": "The impact of problem extents and network sizes on learning in recurrent neural networks is analysed in terms of structural parameters of related graphs. In previous work the influence of learning on the changes of the typical parameters such as characteristic path length, clustering coefficient, degree distribution and entropy, was investigated. In the present work the focus is enlarged to the scaling problem of the learning paradigm. The results prove the scalability of learning procedures due to the retained dynamics of the parameters during learning with different problem extents and network sizes. \u00a9 Springer-Verlag 2009.", "Language": "en", "Citations": "0"},
{"Title": "The EthnoMuse digital library: Conceptual representation and annotation of ethnomusicological materials", "Authors": ["Strle G.", "Marolt M."], "Keywords": ["CIDOC CRM", "Digital libraries", "Folk song and music", "FRBRoo", "Music information retrieval"], "Date": "2012", "Abstract": "The paper presents two vital aspects of the EthnoMuse digital library. We first present the development of a flexible data model through FRBRoo and CIDOC CRM conceptualization of processes and relations in folk song and music realizations. The approach is novel in that it conceptualizes and integrates various folkloristic and ethnomusicological materials, and also standardizes the workflow of production and post-production processes related to recording and documenting of folk song and music. We also present how novel music information retrieval techniques were integrated into the library to provide support for annotation of its contents. Two case studies are presented: automatic segmentation and labeling of field recordings, and transcription of bell chiming recordings. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "3"},
{"Title": "Robust localization using panoramic view-based recognition", "Authors": ["Jogan M.", "Leonardis A."], "Keywords": [], "Date": "2000", "Abstract": "The results of recent studies on the possibility of spatial localization from panoramic images have shown good prospects for view-based methods. The major advantages of these methods are a wide field-of-view, capability of modelling cluttered environments, and flexibility in the learning phase. The redundant information captured in similar views is efficiently handled by the eigenspace approach. However, the standard approaches are sensitive to noise and occlusion. In this paper, we present a method of view-based localization in a robust framework that solves these problems to a large degree. Experimental results on a large set of real panoramic images demonstrate the effectiveness of the approach and the level of achieved robustness. \u00a9 2000 IEEE.", "Language": "en", "Citations": "47"},
{"Title": "Qualitative assessment dynamics - Complementing trust methods for decision making", "Authors": ["Trcek D."], "Keywords": ["Decision making", "Multi-agent systems", "Simulation", "Trust management", "User modeling"], "Date": "2014", "Abstract": "Trust is not only one key ingredient of prosperous organizations and societies, but also an essential factor in decision-making processes. And when it comes to trust, the latest advances in computing sciences area are increasingly supporting the related processes by deployment of so-called trust management systems. These systems are slowly advancing from their early stages of evolution toward more sophisticated and already operationally deployable solutions. As there seems to be no Swiss-army knife like methodology for trust management, it is reasonable to assume that not only one, but a few of them will be deployed in the future, depending on their basic principles of functioning,purposes and contexts of use. Therefore there still exists a gap in this area with unaddressed issues where humans (or humans-like agents) would be in focus. Quality Assessment Dynamics, QAD, which is presented in this paper, is taking these issues into account. It is based on operands and operators that model human ways of reasoning as described in many natural languages. Further, it is a formal system and therefore enabled for deployment in computing environments. This way QAD complements existing trust management methods and provides additional means for decision making through deployment in simulations and in trust management engines, while being understandable to ordinary users without requiring sophisticated expert knowledge. \u00a9 2014 World Scientific Publishing Company.", "Language": "en", "Citations": "1"},
{"Title": "Universal takagi-sugeno fuzzy controller core implemented in a PLD device", "Authors": ["Oseli D.", "Mraz M.", "Zimic N."], "Keywords": [], "Date": "2004", "Abstract": "Fuzzy logic controllers are nowadays mostly implemented as software code and run on conventional microprocessors. If there is a need for high speed processing, the controller must be implemented in hardware. One of the solutions is implementing a fuzzy logic controller in a programmable logic device. Taking into consideration some initial limitations, a universal fuzzy controller core can be constructed. Such controller can be quickly adapted to various system transfer functions, even while the controller is operating. This paper outlines some important design issues that we came across while constructing such fuzzy controller cores.", "Language": "en", "Citations": "0"},
{"Title": "A methodology for provision of sustainable information systems security", "Authors": ["Likar B.", "Trcek D."], "Keywords": ["company culture", "human factors", "innovation", "IS management", "policy", "security"], "Date": "2012", "Abstract": "Information represents one of the most important factors in the success of any enterprise today. Moreover, confidential information is becoming increasingly integrated into complex info-innovation solutions and is accordingly exposed to novel means of manipulation and theft. The legal requirements concerning information security (IS) policies in organizations are mainly based on reactive approaches that follow the standards applied in this area and are regularly updated every few years. However, a complementary approach that takes into account a fast-changing information/innovation security threats landscape and that is of proactive nature is required. Such an approach is presented in this article by linking the information security field with the field of innovation management. \u00a9 2012 Taylor and Francis Group, LLC.", "Language": "en", "Citations": "1"},
{"Title": "Sliding suffix tree", "Authors": ["Brodnik A.", "Jekovec M."], "Keywords": ["Online pattern matching", "Sliding window", "Suffix tree"], "Date": "2018", "Abstract": "We consider a sliding windowW over a stream of characters from some alphabet of constant size. We want to look up a pattern in the current sliding window content and obtain all positions of the matches. We present an indexed version of the sliding window, based on a suffix tree. The data structure of size \u0398(|W|) has optimal time queries \u0398(m + occ) and amortized constant time updates, where m is the length of the query string and occ is its number of occurrences.", "Language": "en", "Citations": "0"},
{"Title": "A data mining library for miRNA annotation and analysis", "Authors": ["Nuzzo A.", "Beretta R.", "Mulas F.", "Roobrouck V.", "Verfaillie C.", "Zupan B.", "Bellazzi R."], "Keywords": ["annotation tools", "Data Mining", "miRNA analysis"], "Date": "2011", "Abstract": "Understanding the key role that miRNAs play in the regulation of gene expression is one of the most important challenges in modern molecular biology. Standard gene set enrichment analysis (GSEA) is not appropriate in this context, due to the low specificity of the relation between miRNAs and their target genes. We developed alternative strategies to gain better insights in the differences in biological processes involved in different experimental conditions. We here describe a novel method to analyze and interpret miRNA expression data correctly, and demonstrate that annotating miRNA directly to biological processes through their target genes (which is nevertheless the only way possible) is a non-trivial task. We are currently employing the same strategy to relate miRNA expression patterns directly to pathway information, to generate new hypotheses, which may be relevant for the interpretation of their role in the gene expression regulatory processes. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "Brain metastases in lung adenocarcinoma: Impact of EGFR mutation status on incidence and survival", "Authors": ["Stanic K.", "Zwitter M.", "Hitij N.T.", "Kern I.", "Sadikov A.", "Cufer T."], "Keywords": ["Brain metastases", "EGFR mutations", "Lung adenocarcinoma"], "Date": "2014", "Abstract": "Background. The brain represents a frequent progression site in lung adenocarcinoma. This study was designed to analyse the association between the epidermal growth factor receptor (EGFR) mutation status and the frequency of brain metastases (BM) and survival in routine clinical practice. Patients and methods. We retrospectively analysed the medical records of 629 patients with adenocarcinoma in Slovenia who were tested for EGFR mutations in order to analyse the cumulative incidence of BM, the time from the diagnosis to the development of BM (TDBM), the time from BM to death (TTD) and the median survival. Results. Out of 629 patients, 168 (27%) had BM, 90 patients already at the time of diagnosis. Additional 78 patients developed BM after a median interval of 14.3 months; 25.8 months in EGFR positive and 11.8 months in EGFR negative patients, respectively (p = 0.002). EGFR mutations were present in 47 (28%) patients with BM. The curves for cumulative incidence of BM in EGFR positive and negative patients demonstrate a trend for a higher incidence of BM in EGFR mutant patients at diagnosis (19% vs. 13%, p = 0.078), but no difference later during the course of the disease. The patients with BM at diagnosis had a statistically longer TTD (7.3 months) than patients who developed BM later (3.1 months). The TTD in EGFR positive patients with BM at diagnosis was longer than in EGFR negative patients (12.6 vs. 6.8, p = 0.005), while there was no impact of EGFR status on the TTD of patients who developed BM later. Conclusions. Except for a non-significant increase of frequency of BM at diagnosis in EGFR positive patients, EGFR status had no influence upon the cumulative incidence of BM. EGFR positive patients had a longer time to CNS progression. While EGFR positive patients with BM at diagnosis had a longer survival, EGFR status had no influence on TTD in patients who developed BM later during the course of disease.", "Language": "en", "Citations": "26"},
{"Title": "Boids with a fuzzy way of thinking", "Authors": ["Lebar Bajec I.", "Mraz M.", "Zimic N."], "Keywords": ["Animat", "Behavioral animation", "Bird", "Boid", "Flock", "Fuzzy logic"], "Date": "2003", "Abstract": "The increase of the processing power of personal computers in the last decade resulted in a notable progress of the artificial animal (animat) construction and simulation field. Regardless of the achieved results the coding of an animal's behavior can, to someone unfamiliar with physics of motion and robotics, seem like pure witchcraft. Not to mention the wealth of ethological knowledge required regarding the behavior of the animal that is being modeled. In this article we suggest the use of fuzzy logic as the basis of an animat's decision about its next step. We hypothesize that by using linguistic programming based on common sense unclear and even partially contradictory knowledge of the animal's behavior, we can achieve comparable if not even better simulation results than with the classical crisp implementation. The following article presents an investigation of our theories on the case of a boid - a special type of animat - limiting itself on the boid's urge of alignment with its flockmates.", "Language": "en", "Citations": "11"},
{"Title": "Pairwise saturations in inductive logic programming", "Authors": ["Drole M.", "Kononenko I."], "Keywords": ["Bottom-up", "Inductive logic programming", "Machine learning", "Saturation"], "Date": "2017", "Abstract": "One of the main issues when using inductive logic programming (ILP) in practice remain the long running times that are needed by ILP systems to induce the hypothesis. We explore the possibility of reducing the induction running times of systems that use asymmetric relative minimal generalisation (ARMG) by analysing the bottom clauses of examples that serve as inputs into the generalisation operator. Using the fact that the ARMG covers all of the examples and that it is a subset of the variabilization of one of the examples, we identify literals that cannot appear in the ARMG and remove them prior to computing the generalisation. We apply this procedure to the ProGolem ILP system and test its performance on several real world data sets. The experimental results show an average speedup of 36% compared to the base ProGolem system and 12% compared to ProGolem extended with caching, both without a decrease in the accuracy of the produced hypotheses. We also observe that the gain from using the proposed method varies greatly, depending on the structure of the data set.", "Language": "en", "Citations": "0"},
{"Title": "1-Homogeneous graphs with cocktail party \u03bc-graphs", "Authors": ["Jurisic A.", "Koolen J."], "Keywords": ["1-homogeneous", "Cocktail party graph", "Distance-regular graph", "Johnson graph"], "Date": "2003", "Abstract": "Let \u0393 be a graph with diameter d \u2265 2. Recall \u0393 is 1-homogeneous (in the sense of Nomura) whenever for every edge xy of \u0393 the distance partition {{z \u03b5 V(\u0393) | \u2202(z, y) = i, \u2202(x,z) = j} | 0 \u2264 i, j \u2264 d} is equitable and its parameters do not depend on the edge xy. Let \u0393 be 1-homogeneous. The \u0393 is distance-regular and also locally strongly regular with parameters (v\u2032, k\u2032, \u03bb\u2032, \u03bc\u2032), where v\u2032 = k, k\u2032 = a", "Language": "en", "Citations": "14"},
{"Title": "Graph grammar induction as a parser-controlled heuristic search process", "Authors": ["Furst L.", "Mernik M.", "Mahnic V."], "Keywords": ["graph grammar induction", "graph grammar parsing", "Graph grammars", "heuristic search"], "Date": "2012", "Abstract": "A graph grammar is a generative description of a graph language (a possibly infinite set of graphs). In this paper, we present a novel algorithm for inducing a graph grammar from a given set of 'positive' and 'negative' graphs. The algorithm is guaranteed to produce a grammar that can generate all of the positive and none of the negative input graphs. Driven by a heuristic specific-to-general search process, the algorithm tries to find a small grammar that generalizes beyond the positive input set. During the search, the algorithm employs a graph grammar parser to eliminate the candidate grammars that can generate at least one negative input graph. We validate our method by inducing grammars for chemical structural formulas and flowcharts and thereby show its potential applicability to chemical engineering and visual programming. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "5"},
{"Title": "Learning and teaching numerical methods with a system for automatic assessment", "Authors": ["Jerse G.", "Lokar M."], "Keywords": [], "Date": "2017", "Abstract": "The emphasis in several courses at technical facilities is on using a computer to perform numerical methods. Instead of focusing on mathematical rigorousness such courses usually concentrate on demonstrating the practical usage of numerical mathematical methods to the students. The practical usage of numerical methods is best learned by exposing the students to a large set of exercises, which they have tn solve The process of solving problems has to be supervised in order to provide the students with a swift feedback about the quality of their solutions. The following paper presents a web system for automatic assessment called Projekt Tomo, which was developed as a support tool for leaching programming and numerical methods oriented classes.", "Language": "en", "Citations": "1"},
{"Title": "Operator Positivstellens\u00e4tze for noncommutative polynomials positive on matrix convex sets", "Authors": ["Zalar A."], "Keywords": ["Completely positive", "Free convexity", "Free real algebraic geometry", "Linear matrix inequality (LMI)", "Positivstellensatz", "Spectrahedron"], "Date": "2017", "Abstract": "This article studies algebraic certificates of positivity for noncommutative (nc) operator-valued polynomials on matrix convex sets, such as the solution set D", "Language": "en", "Citations": "10"},
{"Title": "Anticipation mobile digital health: Towards personalized proactive therapies and prevention strategies", "Authors": ["Pejovic V.", "Mehrotra A.", "Musolesi M."], "Keywords": ["Anticipatory mobile computing", "Anticipatory mobile digital health", "Machine learning", "Mobile sensing", "Ubiquitous computing"], "Date": "2017", "Abstract": "Recent advances in healthcare illuminated the role that individual traits and behaviors play in a person\u2019s health. Consequently, a need has arisen for, currently expensive and non-scalable, continuous long-term patient monitoring and individually tailored therapies. Equipped with an array of sensors, high-performance computing power, and carried by their owners at all times, mobile computing devices promise to enable continuous patient monitoring, and, with the help of machine learning, build predictive models of patient\u2019s health and behavior. Moreover, through their close integration with a user\u2019s lifestyle, mobiles can be used to deliver personalized proactive therapies. In our work we develop the concept of anticipatory mobile-based healthcare (anticipatory mobile digital health) and examine the opportunities and challenges associated with its practical realization.", "Language": "en", "Citations": "2"},
{"Title": "Gathering a dataset of multi-modal mood-dependent perceptual responses to music", "Authors": ["Pesek M.", "Godec P.", "Poredos M.", "Strle G.", "Guna J.", "Stojmenova E.", "Pogacnik M.", "Marolt M."], "Keywords": ["Color perception", "Human computer interaction", "Mood estimation", "Music information retrieval"], "Date": "2014", "Abstract": "The paper presents a new dataset that captures the effect of mood on visual and auditory perception of music. With an online survey, we have collected a dataset of over 6600 responses capturing users' mood, emotions evoked and expressed by music and the perception of color with regard to emotions and music. We describe the methodology of gathering the responses and present two new models for capturing users' emotional states: the MoodGraph and MoodStripe. Also, general research questions and goals, as well as possible future applications of the collected dataset, are being discussed.", "Language": "en", "Citations": "3"},
{"Title": "Clustering-based typology and analysis of private small-scale forest owners in Slovenia", "Authors": ["Kumer P.", "Strumbelj E."], "Keywords": ["Forest owner typology", "k-Medoids", "Management objectives", "Non-industrial forest owners", "Policy instruments", "Private owners", "Values"], "Date": "2017", "Abstract": "Small-scale private forest owners (SPFO) have been recognized as a relatively heterogeneous social group; therefore typology and classification have become key to describe their characteristics and differences. Most of Slovenian forest is owned by SPFOs. To understand why these forest estates are relatively poorly managed, the owners' values and objectives were analysed. We conducted a questionnaire-based survey (n=387) and based our typology on three values and four management variables. The typology was constructed automatically, using the k-medoids clustering algorithm. Clustering resulted in two clusters, which were our basis for two types of owners: \u201cengaged\u201d and \u201cdetached\u201d. We analysed these two types through socio-economic and broader geo-spatial perspectives. We found that multi-objective orientation and high valuation of production function are positively related to active forest management and to the likelihood that the forest will be managed in the future. Conversely, higher value to environmental and social function corresponds to lower management levels. Spatial patterns of owners residencies and forest estates influence managing decisions. Results confirm the importance of spatial factors and owner values and objectives for understanding forest management.", "Language": "en", "Citations": "5"},
{"Title": "Using reverse engineering to construct the platform independent model of a web application for student information systems", "Authors": ["Rozanc I.", "Slivnik B."], "Keywords": ["PL/SQL", "Platform independent model", "Reverse engineering", "Web application"], "Date": "2013", "Abstract": "A methodology for extracting the domain knowledge from an existing three-tier web application and subsequent formulation of the platform independent model (PIM) is described. As it was devised during a reverse engineering process of an existing web application which needed to be reimplemented on a new platform using new technology, it focuses on the domain knowledge and business functions. It produces the business model and the hypertext model leaving the presentation model aside. The methodology is semi-automated-the generation of the activity diagrams and parts of the hypertext model must be in part performed by an analyst, preferably the one with some domain knowledge. As the paper is primarily aimed at practitioners, a case study illustrating the application of the presented method is included.", "Language": "en", "Citations": "3"},
{"Title": "Image processing and machine learning for fully automated probabilistic evaluation of medical images", "Authors": ["Sajn L.", "Kukar M."], "Keywords": ["Association rules", "Coronary artery disease", "Machine learning", "Medical diagnostics", "Multi-resolution image parameterization", "Principal component analysis"], "Date": "2011", "Abstract": "The paper presents results of our long-term study on using image processing and data mining methods in a medical imaging. Since evaluation of modern medical images is becoming increasingly complex, advanced analytical and decision support tools are involved in integration of partial diagnostic results. Such partial results, frequently obtained from tests with substantial imperfections, are integrated into ultimate diagnostic conclusion about the probability of disease for a given patient. We study various topics such as improving the predictive power of clinical tests by utilizing pre-test and post-test probabilities, texture representation, multi-resolution feature extraction, feature construction and data mining algorithms that significantly outperform medical practice. Our long-term study reveals three significant milestones. The first improvement was achieved by significantly increasing post-test diagnostic probabilities with respect to expert physicians. The second, even more significant improvement utilizes multi-resolution image parametrization. Machine learning methods in conjunction with the feature subset selection on these parameters significantly improve diagnostic performance. However, further feature construction with the principle component analysis on these features elevates results to an even higher accuracy level that represents the third milestone. With the proposed approach clinical results are significantly improved throughout the study. The most significant result of our study is improvement in the diagnostic power of the whole diagnostic process. Our compound approach aids, but does not replace, the physician's judgment and may assist in decisions on cost effectiveness of tests. \u00a9 2010 Elsevier Ireland Ltd.", "Language": "en", "Citations": "19"},
{"Title": "Exploring the relation between learning style models and preferred multimedia types", "Authors": ["Ocepek U.", "Bosnic Z.", "Nancovska Serbec I.", "Rugelj J."], "Keywords": ["Interactive learning environments", "Media in education", "Multimedia/hypermedia systems", "Teaching/learning strategies"], "Date": "2013", "Abstract": "There are many adaptive learning systems that adapt learning materials to student properties, preferences, and activities. This study is focused on designing such a learning system by relating combinations of different learning styles to preferred types of multimedia materials. We explore a decision model aimed at proposing learning material of an appropriate multimedia type. This study includes 272 student participants. The resulting decision model shows that students prefer well-structured learning texts with color discrimination, and that the hemispheric learning style model is the most important criterion in deciding student preferences for different multimedia learning materials. To provide a more accurate and reliable model for recommending different multimedia types more learning style models must be combined. Kolb's classification and the VAK classification allow us to learn if students prefer an active role in the learning process, and what multimedia type they prefer. \u00a9 2013 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "43"},
{"Title": "A Nordhaus\u2013Gaddum conjecture for the minimum number of distinct eigenvalues of a graph", "Authors": ["Levene R.H.", "Oblak P.", "Smigoc H."], "Keywords": ["Inverse eigenvalue problem for graphs", "Minimum number of distinct eigenvalues", "Minimum rank", "Nordhaus\u2013Gaddum inequality", "Orthogonal matrices"], "Date": "2019", "Abstract": "We propose a Nordhaus\u2013Gaddum conjecture for q(G), the minimum number of distinct eigenvalues of a symmetric matrix corresponding to a graph G: for every graph G excluding four exceptions, we conjecture that q(G)+q(G", "Language": "en", "Citations": "0"},
{"Title": "Solving the k-center problem efficiently with a dominating set algorithm", "Authors": ["Mihelic J.", "Robic B."], "Keywords": ["Combinatorial problems", "Dominating set", "Facility location", "Graph algorithms", "K-center", "Performance evaluation"], "Date": "2005", "Abstract": "We present a polynomial time heuristic algorithm for the minimum dominating set problem. The algorithm can readily be used for solving the minimum a -all-neighbor dominating set problem and the minimum set cover problem. We apply the algorithm in heuristic solving the minimum k-center problem in polynomial time. Using a standard set of 40 test problems we experimentally show that our k-center algorithm performs much better than other well-known heuristics and is competitive with the best known (non-polynomial time) algorithms for solving the k-center problem in terms of average quality and deviation of the results as well as the execution time.", "Language": "en", "Citations": "22"},
{"Title": "GenePath: A computer program for genetic pathway discovery from mutant data", "Authors": ["Demsar J.", "Zupan B.", "Bratko I.", "Kuspa A.", "Halter J.A.", "Beck R.J.", "Shaulsky G."], "Keywords": ["Artificial intelligence", "Bioinformatics", "Functional genomics", "Genetic networks"], "Date": "2001", "Abstract": "The sequencing of the human genome and the genomes of several model organisms is the first step toward the long-term objective of genetic research: the identification of all genes, and the discovery of their functions and mutual interactions. This article presents a methodology and a computer program called GenePath to support the discovery of gene function. GenePath uses mutant data and available genetic knowledge to identify potential genetic pathways. Several pilot applications based on experimental results from Dictyostelium and C. elegans confirmed the usefulness of the proposed schema. Our results suggest that GenePath is a valuable tool that can be used as an intelligent assistant to support genetic reasoning. \u00a9 2001 IMIA. All right reserved.", "Language": "en", "Citations": "5"},
{"Title": "Agent oriented approach for integration of BI systems", "Authors": ["Lavbic D.", "Rupnik R.", "Bajec M.", "Krisper M."], "Keywords": ["Decision support", "Intelligent agent", "MAS", "Ontology"], "Date": "2007", "Abstract": "Decision Support in enterprises is gaining its importance in the age of Internet and e-business. While dealing with the need for fast response in the dynamic competitive environments and rapid increase of information available on various networks we propose Multi-Agent System approach to backend Decision Support. Research case study presented in the paper is from the domain of mobile communications with system's main goal of delivering right information at the right time to the right users. We integrate several BI systems such as Data Mining and Data Warehousing with information available on the Internet. Ontologies are used to store derived knowledge, to support knowledge exchange between our MAS and other systems, to support agent-to-agent communication and enable seamless extending of MAS capabilities.", "Language": "en", "Citations": "1"},
{"Title": "SkipCor: Skip-mention coreference resolution using linear-chain conditional random fields", "Authors": ["Zitnik S.", "Subelj L.", "Bajec M."], "Keywords": [], "Date": "2014", "Abstract": "Coreference resolution tries to identify all expressions (called mentions) in observed text that refer to the same entity. Beside entity extraction and relation extraction, it represents one of the three complementary tasks in Information Extraction. In this paper we describe a novel coreference resolution system SkipCor that reformulates the problem as a sequence labeling task. None of the existing supervised, unsupervised, pairwise or sequence-based models are similar to our approach, which only uses linear-chain conditional random fields and supports high scalability with fast model training and inference, and a straightforward parallelization. We evaluate the proposed system against the ACE 2004, CoNLL 2012 and SemEval 2010 benchmark datasets. SkipCor clearly outperforms two baseline systems that detect coreferentiality using the same features as SkipCor. The obtained results are at least comparable to the current state-of-the-art in coreference resolution. \u00a9 2014 \u017ditnik et al.", "Language": "en", "Citations": "7"},
{"Title": "An O(1) solution to the Prefix Sum problem on a specialized memory architecture", "Authors": ["Brodnik A.", "Karlsson J.", "Munro J.I.", "Nilsson A."], "Keywords": [], "Date": "2006", "Abstract": "In this paper we study the Prefix Sum problem introduced by Fredman. We show that it is possible to perform both update and retrieval in O(1) time simultaneously under a memory model in which individual bits may be shared by several words. We also show that two variants (generalizations) of the problem can be solved optimally in \u03b8(lg N) time under the comparison based model of computation. \u00a9 2006 International Federation for Information Processing.", "Language": "en", "Citations": "4"},
{"Title": "Prediction intervals in supervised learning for model evaluation and discrimination", "Authors": ["Pevec D.", "Kononenko I."], "Keywords": ["Machine learning", "Prediction aggregation", "Prediction intervals", "Reliability and robustness", "Statistical computing", "Visualization techniques and methodology"], "Date": "2015", "Abstract": "In this paper we explore prediction intervals and how they can be used for model evaluation and discrimination in the supervised regression setting of medium sized datasets. We review three different methods for making prediction intervals and the statistics used for their evaluation. How the prediction intervals look like, how different methods behave and how the prediction intervals can be utilized for the graphical evaluation of models is illustrated with the help of simple datasets. Afterwards we propose a combined method for making prediction intervals and explore its performance with two voting schemes for combining predictions of a diverse ensemble of models. All methods are tested on a large set of datasets on which we evaluate individual methods and aggregated variants for their abilities of selecting the best predictions. The analysis of correlations between the root mean squared error and our evaluation statistic show that both stability and reliability of the results increase as the techniques get more elaborate. We confirm that the methodology is suitable for the graphical comparison of individual models and is a viable way of discriminating among model candidates.", "Language": "en", "Citations": "4"},
{"Title": "Qualitatively faithful quantitative prediction", "Authors": ["Suc D.", "Vladusic D.", "Bratko I."], "Keywords": ["Automated model building", "Inductive learning", "Learning qualitative models", "Machine learning", "Numerical regression", "Qualitative reasoning", "System identification"], "Date": "2004", "Abstract": "We describe an approach to machine learning from numerical data that combines both qualitative and numerical learning. This approach is carried out in two stages: (1) induction of a qualitative model from numerical examples of the behaviour of a physical system, and (2) induction of a numerical regression function that both respects the qualitative constraints and fits the training data numerically. We call this approach Q", "Language": "en", "Citations": "34"},
{"Title": "Attributed context-sensitive graph grammars", "Authors": ["Furst L."], "Keywords": ["Attributed grammar", "Graph grammar", "Graphical language", "Semantics", "Visual language"], "Date": "2014", "Abstract": "The paper introduces a concept of attributed context-sensitive graph grammars. The graph grammars are a graphical generalization of the textual grammars and can thus be used to specify the syntax of graphical programming or modeling languages. The attributed graph grammars extend the basic graph grammars with definitions of attributes and the associated attribute evaluation rules. By analogy to the attributed textual grammars, the purpose of the attributes and the rules is to define the semantic elements of a graphical language. The introduced concept is illustrated by an example of a grammar-driven conversion of flowcharts to the equivalent C code. The presented example might find its use in introductory programming courses.", "Language": "en", "Citations": "0"},
{"Title": "Feasibility of spirography features for objective assessment of motor symptoms in parkinson\u2019s disease", "Authors": ["Sadikov A.", "Zabkar J.", "Mozina M.", "Groznik V.", "Nyholm D.", "Memedi M."], "Keywords": ["Movement disorder", "Objective monitoring", "Parkinson\u2019s disease", "Spirography", "Spirography features"], "Date": "2015", "Abstract": "Parkinson\u2019s disease (PD) is currently incurable, however the proper treatment can ease the symptoms and significantly improve the quality of patient\u2019s life. Since PD is a chronic disease, its efficient monitoring and management is very important. The objective of this paper is to investigate the feasibility of using the features and methodology of a spirography device, originally designed to measure early Parkinson\u2019s disease (PD) symptoms, for assessing motor symptoms of advanced PD patients suffering from motor fluctuations. More specifically, the aim is to objectively assess motor symptoms related to bradykinesias (slowness of movements occurring as a result of under-medication) and dyskinesias (involuntary movements occurring as a result of over-medication). The work combines spirography data and clinical assessments from a longitudinal clinical study in Sweden with the features and pre-processing methodology of a Slovenian spirography application. The target outcome was to learn to predict the \u201ccause\u201d of upper limb motor dysfunctions as assessed by a clinician who observed animated spirals in a web interface. Using the machine learning methods with feature descriptions from the Slovenian application resulted in 86% classification accuracy and over 90% AUC, demonstrating the usefulness of this approach for objective monitoring of PD patients.", "Language": "en", "Citations": "0"},
{"Title": "Number of instances for reliable feature ranking in a given problem", "Authors": ["Bohanec M.", "Borstnar M.K.", "Robnik-Sikonja M."], "Keywords": ["Feature evaluation", "Feature ranking", "Machine learning"], "Date": "2018", "Abstract": "Background: In practical use of machine learning models, users may add new features to an existing classification model, reflecting their (changed) empirical understanding of a field. New features potentially increase classification accuracy of the model or improve its interpretability. Objectives: We have introduced a guideline for determination of the sample size needed to reliably estimate the impact of a new feature. Methods/Approach: Our approach is based on the feature evaluation measure ReliefF and the bootstrap-based estimation of confidence intervals for feature ranks. Results: We test our approach using real world qualitative business-to-business sales forecasting data and two UCI data sets, one with missing values. The results show that new features with a high or a low rank can be detected using a relatively small number of instances, but features ranked near the border of useful features need larger samples to determine their impact. Conclusions: A combination of the feature evaluation measure ReliefF and the bootstrap-based estimation of confidence intervals can be used to reliably estimate the impact of a new feature in a given problem.", "Language": "en", "Citations": "0"},
{"Title": "\"Atlas 2012\" augmented reality: A case study in the domain of fine arts", "Authors": ["Bovcon N.", "Vaupotic A.", "Klemenc B.", "Solina F."], "Keywords": ["augmented reality", "case study", "emblem", "fine art projects", "new media art", "virtual objects in physical environment"], "Date": "2013", "Abstract": "The article presents a case study of artistic use of augmented reality built with the Layar augmented reality application. Members of ArtNetLab, a group of new media artists, have conceptualized a series of projects for geolocated virtual objects (augments), that can be perceived by means of smart-phone or tablet-computer applications in the urban space of a city. The user experience in art is not limited to practical and efficient use of a gadget directed towards a predetermined set of actions, instead it has to involve the user in the art experience, and this opens up a broad field of conceivable contexts. Our case study presents an art project that proposes and tests a concrete solution to the latent question proposed by an existing technology, and we describe how the artists encoded meanings by using a ubiquitous mobile platform for augmented reality. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "3"},
{"Title": "An adaptive bic approach for robust speaker change detection in continuous audio streams", "Authors": ["Zibert J.", "Brodnik A.", "Mihelic F."], "Keywords": [], "Date": "2009", "Abstract": "In this paper we focus on an audio segmentation. We present a novel method for robust and accurate detection of acoustic change points in continuous audio streams. The presented segmentation procedure was developed as a part of an audio diarization system for broadcast news audio indexing. In the presented approach, we tried to remove a need for using pre-determined decision-thresholds for detecting of segment boundaries, which are usually the case in the standard segmentation procedures. The proposed segmentation aims to estimate decision-thresholds directly from the currently processed audio data and thus reduces a need for additional threshold tuning from development data. It employs change-detection methods from two well-established audio segmentation approaches based on the Bayesian Information Criterion. Combining methods from both approaches enabled us to adaptively tune boundary-detection thresholds from the underlying processing data. All three segmentation procedures are tested and compared on a broadcast news audio database, where our proposed audio segmentation procedure shows its potential. \u00a9 2009 Springer Berlin Heidelberg.", "Language": "en", "Citations": "1"},
{"Title": "Usability evaluation of input devices for navigation and interaction in 3D visualisation", "Authors": ["Skrlj P.", "Guna J.", "Bohak C.", "Marolt M."], "Keywords": ["3D mouse", "3D navigation", "Touchless interaction", "Usability evaluation", "User experience", "User study"], "Date": "2014", "Abstract": "We present an assessment study of user experience and usability of different kinds of input devices for view manipulation in a 3D data visualisation application. Three input devices were compared: a computer mouse, a 3D mouse with six degrees of freedom, and the Leap Motion Controller - a device for touchless interaction. Assessment of these devices was conducted using the System Usability Scale (SUS) methodology, with addition of application specific questions. To gain further insight into users' behaviour, the users' performance and feedback on the given tasks was recorded and analysed. The best results were achieved by using the 3D mouse (SUS score 88.7), followed by the regular mouse (SUS score 72.4). The Leap Motion Controller (SUS score 56.5) was the least preferred mode of interaction, nevertheless it was described as natural and intuitive, showing great potential.", "Language": "en", "Citations": "1"},
{"Title": "An open-source tool to evaluate performance of transient ST segment episode detection algorithms", "Authors": ["Jager F.", "Smrdel A.", "Mark R.G."], "Keywords": [], "Date": "2004", "Abstract": "Performance measures and evaluation protocols for evaluating the performance and robustness of transient ST segment episode detection algorithms are specific, complex and not trivial to realize. We developed an open-source tool (EVAL_ST) to evaluate and compare performance and robustness of ST episode detection algorithms. The tool supports all standard and other relevant performance measures, aggregate gross and average statistics, and bootstrap statistical procedure to predict real-world clinical performance. The tool (written in C) is compilable an a wide variety of platforms and contains an additional graphic user interface module (LessTif/Motif environment) for use on the LINUX/UNIX operating systems. \u00a9 2004 IEEE.", "Language": "en", "Citations": "5"},
{"Title": "Quality assessment of individual classifications in machine learning and data mining", "Authors": ["Kukar M."], "Keywords": ["Data mining", "Machine learning", "Quality assessment", "Transduction", "Typicalness"], "Date": "2006", "Abstract": "Although in the past machine learning algorithms have been successfully used in many problems, their serious practical use is affected by the fact that often they cannot produce reliable and unbiased assessments of their predictions' quality. In last few years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: Either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper we describe typicalness and transductive reliability estimation frameworks and propose a joint approach that compensates the above-mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into a joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense. We perform series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary method as well as with kernel density estimation. We show that the proposed method performs as well as proprietary methods and significantly outperforms density estimation methods. \u00a9 Springer-Verlag London Ltd. 2005.", "Language": "en", "Citations": "22"},
{"Title": "Fixing missing eye-contact in video conferencing systems", "Authors": ["Solina F.", "Ravnik R."], "Keywords": ["Eye-contact", "Human computer interface", "Videoconferencing"], "Date": "2010", "Abstract": "A novel but simple and experimentally motivated method related to the Mona Lisa effect for establishing eye contact in video conferences using big screens and cameras mounted above the screens is proposed.", "Language": "en", "Citations": "6"},
{"Title": "Trust management in the pervasive computing era", "Authors": ["Trcek D."], "Keywords": ["IT", "pervasive computing", "security and privacy", "trust management"], "Date": "2011", "Abstract": "The study of trust should be multidisciplinary. This primarily means including computing and information science on one hand, and psychology on the other. Although some research projects have already employed multidisciplinary approaches, they've rarely included all the necessary ingredients. Furthermore, the core of the trust phenomenon is often overlooked. In addition, to complement current quantitative methodologies, we should develop methodologies that support a quantitative treatment by using qualitative assessments of trust. \u00a9 2006 IEEE.", "Language": "en", "Citations": "13"},
{"Title": "Transformation based walking speed normalization for gait recognition", "Authors": ["Kovac J.", "Peer P."], "Keywords": ["Biometrics", "Computer vision", "Gait recognition", "Identification of persons", "Pattern recognition"], "Date": "2013", "Abstract": "Humans are able to recognize small number of people they know well by the way they walk. This ability represents basic motivation for using human gait as the means for biometric identification. Such biometric can be captured at public places from a distance without subject's collaboration, awareness or even consent. Although current approaches give encouraging results, we are still far from effective use in practical applications. In general, methods set various constraints to circumvent the influence factors like changes of view, walking speed, capture environment, clothing, footwear, object carrying, that have negative impact on recognition results. In this paper we investigate the influence of walking speed variation to different visual based gait recognition approaches and propose normalization based on geometric transformations, which mitigates its influence on recognition results. With the evaluation on MoBo gait dataset we demonstrate the benefits of using such normalization in combination with different types of gait recognition approaches. \u00a9 2013 KSII.", "Language": "en", "Citations": "4"},
{"Title": "Multilaterals in configurations", "Authors": ["Boben M.", "Grunbaum B.", "Pisanski T."], "Keywords": ["Configurations", "Cycles", "Incidence graphs", "Multilaterals"], "Date": "2013", "Abstract": "We investigate the existence of g-laterals in geometric and combinatorial configurations. First we can show that within a special family of configurations any of the eight possible combinations of the existence or non-existence of g-laterals for 3 \u2264 g \u2264 5 may arise. Moreover, this is true for arbitrarily large configurations belonging to this family. We also present geometric realizations of the two smallest trilateral-, quadrilateral- and pentalateral-free (v", "Language": "en", "Citations": "2"},
{"Title": "Towards empirical evaluation of test-driven development in a university environment", "Authors": ["Pancur M.", "Ciglaric M.", "Trampus M.", "Vidmar T."], "Keywords": ["Agile methodologies", "Empirical experimental evaluation", "Test-driven development", "University environment"], "Date": "2003", "Abstract": "Test Driven Development (TDD) is an agile software development technique and it is one of the core development practices of Extreme Programming (XP). In TDD, developers write automatically executable tests prior to writing the code they test. We ran a set of experiments to empirically assess different parameters of the TDD, We compared TDD to a more \"traditionally\" oriented iterative test-last development process (ITL). Our preliminary results show that TDD is not substantially different from ITL and our qualitative findings about a development processes are different from results obtained from other researchers.", "Language": "en", "Citations": "45"},
{"Title": "Population-based methods as a form of metaheuristic combinatorial optimization Populacijske metode kot oblika metahevristi\u010dne kombinatori\u010dne optimizacije", "Authors": ["Korosec P.", "Silc J.", "Robic B."], "Keywords": ["Ant colony optimization", "Combinatorial optimization", "Evolutionary computation", "Metaheuristics"], "Date": "2005", "Abstract": "The field of metaheuristics for the application to combinatorial optimization problems is a rapidly growing field of research. This is due to the importance of combinatorial optimization problems for the scientific as well as the industrial world. Population-based metaheuristic methods deal in every iteration of the algorithm with a set (i.e. a population) of solutions rather than with a single solution. As they deal with a population of solutions, population-based algorithms provide a natural, intrinsic way for the exploration of the search space. Yet, the final performance depends strongly on the way the population is manipulated. The most studied population-based methods in combinatorial optimization are Evolutionary Computation (EC) and Ant Colony Optimization (ACO). In EC algorithms, a population of individuals is modified by recombination and mutation operators, and in ACO a colony of artificial ants is used to construct solutions guided by the pheromone trails and heuristic information.", "Language": "en", "Citations": "2"},
{"Title": "Conversion demands on TV archive", "Authors": ["Lukicic T.", "Grgic S.", "Peer P."], "Keywords": ["Aspect ratio", "Picture format", "Telecine", "TV archive", "Video signal"], "Date": "2007", "Abstract": "As the TV world is going digital, new solutions to some conversion demands on TV archive materials need to be specified, and at the same time change of format need to be taken into consideration. Telecine was and still is main conversion tool used to get electronic video signal of film picture, and still protect projects closest possible artistic atmosphere of film image, or documentary content of archived material, bur film is not what TV archive is all about. Differences appear because of different TV screen aspect ratio, as well as different resolution. Therefore different balance of picture elements, because of the format conversion processes results with different content or atmosphere of the TV projected picture. In this article some possible combinations of transmitted signal and TV aspect ratio are compared.", "Language": "en", "Citations": "0"},
{"Title": "CVL OCR DB, an annotated image database of text in natural scenes, and its usability", "Authors": ["Ikica A.", "Peer P."], "Keywords": ["Computer vision", "Natural scenes", "Optical character recognition", "Text detection"], "Date": "2011", "Abstract": "Text detection and optical character recognition (OCR) in images of natural scenes is a fairly new computer vision area but yet very useful in numerous applicative areas. Although many implementations gain promising results, they are evaluated mostly on the private image collections that are very hard or even impossible to get. Therefore, it is very difficult to compare them objectively. Since our aim is to help the research community in standardizing the evaluation of the text detection and OCR methods, we present CVL OCR DB, a public database of annotated images of text in diverse natural scenes, captured at varying weather and lighting conditions. All the images in the database are annotated with the text region and single character location information, making CVL OCR DB suitable for testing and evaluating both text detection and OCR methods. Moreover, all the single characters are also cropped from the original images and stored individually, turning our database into a huge collection of characters suitable for training and testing OCR classifiers.", "Language": "en", "Citations": "3"},
{"Title": "OpenSPA - An open and extensible protocol for single packet authorization", "Authors": ["Krmelj G.R.", "Pancur M.", "Grohar M.", "Ciglaric M."], "Keywords": ["Firewall", "Hidden services", "Network protocol design", "Network security", "SPA"], "Date": "2018", "Abstract": "Applications are vulnerable. Opening such applications to the Internet creates a big attack surface for potential exploit. The use of common network defenses such asfi rewalls helps mitigate the risks, however possibility of a secure scalable system that assigns network access to a service purely by identifying a device by a static IP address is a delusion. Firewalls need to improve to support dynamic allocation of device access. Such a techniquewould allowservices to be hidden to the general public, unauthorized to access them, but would at the same time allow authorized users global connectivity. Single Packet Authorization (SPA) is an approach, building on firewall functionality which hides services from unauthorized users and helps mitigate common network attacks such as Distributed Denial of Service (DDoS) attacks by stopping them earlier in the network stack. In this paper we introduce OpenSPA, a SPA protocol suitable for deployment in various complex networking environments and enabling flexibility to support different network policies. With support for IPv6 aswell as extensible support for custom user programmable authentication, authorization and rewall logic.", "Language": "en", "Citations": "0"},
{"Title": "An algorithm for classification of ambulatory ECG leads according to type of transient ischemic episodes", "Authors": ["Smrdel A.", "Jager F."], "Keywords": ["Ambulatory electrocardiogram", "ECG lead classification", "Long-Term ST Database", "Transient ischemia"], "Date": "2007", "Abstract": "We developed and evaluated an algorithm for classification of ECG leads in ambulatory records according to type of transient ischemic ST segment episodes using the Long- Term ST Database. The algorithm robustly generates ST level function in each ECG lead, tracks non-ischemic ST changes to construct the ST reference function and subtracts it from the ST level function to obtain the ST deviation function. Then the algorithm using statistical moment of the histogram of the ST deviation function given lead classifies the lead according to type of transient ischemic ST episodes (elevations, depressions). The algorithm correctly classified all 9 ECG leads with elevated and 89 out of 90 ECG leads with depressed transient ischemic ST episodes.", "Language": "en", "Citations": "0"},
{"Title": "IS audit checklist for router management performed by third-party", "Authors": ["Mahnic V.", "Klepec B.", "Zabkar N."], "Keywords": [], "Date": "2001", "Abstract": "Network management is an important part of business management. Router management is one component of network management. Because of its complexity, router management is often performed by a third-party. In this paper, the risks involved with such a solution are described as well as guide lines for establishing controls to mitigate these risks. Standard COBIT (control objectives for information technology) has been used for this purpose and the result has been presented in the form of an IS (information system) audit checklist. Elements of this checklist are described and conclusions given.", "Language": "en", "Citations": "1"},
{"Title": "CW decompositions of equivariant CW complexes", "Authors": ["Cencelj M.", "Mramor Kosta N."], "Keywords": [], "Date": "2002", "Abstract": "We discuss conditions which ensure that a G-CW complex is G-homotopy equivalent to a CW complex with cellular action with respect to some CW decomposition of the compact Lie group G. For G = SU(2), we prove that for every G-CW complex X, there exists a CW complex Y which is G-homotopy equivalent to X, such that the action G \u00d7 Y \u2192 Y is a cellular map.", "Language": "en", "Citations": "2"},
{"Title": "Self-supervised cross-modal online learning of basic object affordances for developmental robotic systems", "Authors": ["Ridge B.", "Skocaj D.", "Leonardis A."], "Keywords": [], "Date": "2010", "Abstract": "For a developmental robotic system to function successfully in the real world, it is important that it be able to form its own internal representations of affordance classes based on observable regularities in sensory data. Usually successful classifiers are built using labeled training data, but it is not always realistic to assume that labels are available in a developmental robotics setting. There does, however, exist an advantage in this setting that can help circumvent the absence of labels: co-occurrence of correlated data across separate sensory modalities over time. The main contribution of this paper is an online classifier training algorithm based on Kohonen's learning vector quantization (LVQ) that, by taking advantage of this co-occurrence information, does not require labels during training, either dynamically generated or otherwise. We evaluate the algorithm in experiments involving a robotic arm that interacts with various household objects on a table surface where camera systems extract features for two separate visual modalities. It is shown to improve its ability to classify the affordances of novel objects over time, coming close to the performance of equivalent fully-supervised algorithms. \u00a92010 IEEE.", "Language": "en", "Citations": "31"},
{"Title": "VizRank: Data visualization guided by machine learning", "Authors": ["Leban G.", "Zupan B.", "Vidmar G.", "Bratko I."], "Keywords": ["Data mining", "Data visualization", "Exploratory data analysis", "Machine learning", "Visual data mining"], "Date": "2006", "Abstract": "Data visualization plays a crucial role in identifying interesting patterns in exploratory data analysis. Its use is, however, made difficult by the large number of possible data projections showing different attribute subsets that must be evaluated by the data analyst. In this paper, we introduce a method called VizRank, which is applied on classified data to automatically select the most useful data projections. VizRank can be used with any visualization method that maps attribute values to points in a two-dimensional visualization space. It assesses possible data projections and ranks them by their ability to visually discriminate between classes. The quality of class separation is estimated by computing the predictive accuracy of k-nearest neighbor classifier on the data set consisting of x and y positions of the projected data points and their class information. The paper introduces the method and presents experimental results which show that VizRank's ranking of projections highly agrees with subjective rankings by data analysts. The practical use of VizRank is also demonstrated by an application in the field of functional genomics. \u00a9 2006 Springer Science + Business Media, LLC.", "Language": "en", "Citations": "50"},
{"Title": "A function-decomposition method for development of hierarchical multi-attribute decision models", "Authors": ["Bohanec M.", "Zupan B."], "Keywords": ["Data mining", "Data-driven modeling", "Function decomposition", "Hierarchical models", "Multi-attribute decision making"], "Date": "2004", "Abstract": "Function decomposition is a recent machine learning method that develops a hierarchical structure from class-labeled data by discovering new aggregate attributes and their descriptions. Each new aggregate attribute is described by an example set whose complexity is lower than the complexity of the initial set. We show that function decomposition can be used to develop a hierarchical multi-attribute decision model from a given unstructured set of decision examples. The method implemented in a system called HINT is experimentally evaluated on a real-world housing loans allocation problem and on the rediscovery of three hierarchical decision models. The experimentation demonstrates that the decomposition can discover meaningful and transparent decision models of high classification accuracy. We specifically study the effects of human interaction through either assistance or provision of background knowledge for function decomposition, and show that this has a positive effect on both the comprehensibility and classification accuracy. \u00a9 2002 Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "41"},
{"Title": "Artificial intelligence techniques applied to design of logic circuits based on ternary quantum-dot cellular automata", "Authors": ["Janez M.", "Lebar Bajec I.", "Pecar P.", "Zimic N.", "Mraz M."], "Keywords": [], "Date": "2007", "Abstract": "We developed the tool which finds a minimal logic circuit composed of given basic structures that realizes an arbitrary three-valued logic function. It can easily be extended to consider arbitrary QCA structure which is constructed based on simulation of the interaction of cells. However the problem is combinatorially complex, so it is important to use an appropriate heuristic to find a (sub)optimal logic circuits.", "Language": "en", "Citations": "0"},
{"Title": "Acoustic modeling for speech recognition in telephone based dialog system using limited audio resources", "Authors": ["Gajsek R.", "Zibert J.", "Mihelic F."], "Keywords": ["Acoustic modeling", "Environment adaptation", "Robust speech recognition"], "Date": "2008", "Abstract": "In the article we evaluate different techniques of acoustic modeling for speech recognition in the case of limited audio resources. The objective was to build different sets of acoustic models, the first was trained on a small set of telephone speech recordings and the other was trained on a bigger database with broadband speech recordings and later adapted to a different audio environment. Different adaptation methods (MLLR, MAP) were examined in combination with different parameterization features (MFCC, PLP, RPLP). We show that using adaptation methods, which are mainly used for speaker adaptation purposes, can increase the robustness of speech recognition in cases of mismatched training and working acoustic environment conditions. \u00a9 2008 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "1"},
{"Title": "Stochastic simulation algorithm for gene regulatory networks with multiple binding sites", "Authors": ["Petroni M.", "Zimic N.", "Mraz M.", "Moskon M."], "Keywords": ["computational modeling", "gene regulatory networks", "multiple binding sites", "multiscale stochastic simulation algorithm", "systems biology"], "Date": "2015", "Abstract": "Promoters with multiple binding sites present a regulatory mechanism of several natural biological systems. It has been shown that such systems reflect a higher stability in comparison to the systems with small numbers of binding sites. Regulatory mechanisms with multiple binding sites are therefore used more frequently in artificially designed biological systems in recent years. While the number of possible promoter states increases exponentially with the number of binding sites, it is extremely hard to model such systems accurately. Here we present an adaptation of stochastic simulation algorithm for accurate modeling of gene regulatory networks with multiple binding sites. Small computational complexity of adapted algorithm allows us to model any feasible number of binding sites per promoter. The approach introduced in this work is demonstrated on the model of switching mechanism in Epstein-Barr virus, where 20 binding sites are observed on one of the promoters. We show that the presented approach is easy to adapt to any biological systems based on the regulatory mechanisms with multiple binding sites in order to obtain and analyze their behavior.", "Language": "en", "Citations": "1"},
{"Title": "Rule- and context-based dynamic business process modelling and simulation", "Authors": ["Vasilecas O.", "Kalibatiene D.", "Lavbic D."], "Keywords": ["Business process modelling", "Business rules", "Context", "Dynamic business process", "Simulation"], "Date": "2016", "Abstract": "The traditional approach used to implement a business process (BP) in today's information systems (IS) no longer covers the actual needs of the dynamically changing business. Therefore, a necessity for a new approach of dynamic business process (DBP) modelling and simulation has arisen. To date, existing approaches to DBP modelling and simulation have been incomplete, i.e. they lack theory or a case study or both. Furthermore, there is no commonly accepted definition of BDP. Current BP modelling tools are suitable almost solely for the modelling and simulation of a static BP that strictly prescribes which activities, and in which sequence, to execute. Usually, a DBP is not defined strictly at the beginning of its execution, and it changes under new conditions at runtime. In our paper, we propose six requirements of DBP and an approach for rule- and context-based DBP modelling and simulation. The approach is based on changing BP rules, BP actions and their sequences at process instance runtime, according to the new business system context. Based on the proposed approach, a reference architecture and prototype of a DBP simulation tool were developed. Modelling and simulation were carried out using this prototype, and the case study shows correspondence to the needs of dynamically changing business, as well as possibilities for modelling and simulating DBP.", "Language": "en", "Citations": "7"},
{"Title": "Comparison of protocols to annotate ischaemic electrocardiogram episodes in the reference ESC DB and LTST DB databases Primerjava protokolov ozna\u010devanja ishemi\u010dnih epizod elektrokardiograma v referen\u010dnih podatkovnih bazah ESC BD in LTST DB", "Authors": ["Amon M.", "Jager F."], "Keywords": ["Ambulatory electrocardiogram", "Annotation protocols", "ESC DB database", "LTST DB database", "Performance evaluation", "Transient ischaemic ST segment change analysers"], "Date": "2005", "Abstract": "In the field of development and performance evaluation of algorithms and transient ischaemic electrocardiogram ST segment episode analysers, two standardized international reference databases are currently in use: the European Society of Cardiology ST-T database (ESC DB) [1] and the Long-Term ST Database (LTST DB) [2]. The aim of the study was to categorize possible discrepancies between the databases, evaluate these discrepancies, compare both expert protocols to annotate ischaemic episodes (Figure 1), and study how comparable the performance evaluation results, given transient ischaemic ST episode analyser, could be if using these two databases. We compared the databases and protocols on the basis of ten equivalent records (C DB, Table 1) of which source signals are the same for both databases. To effectively synchronize these equivalent records which differ in length, we developed a graphic visualization tool LE (LTST DB - ESC DB) [3] which allows visual identification of significant reference pairs of heart beats in each pair of equivalent records to determine matching intervals. For quantitative evaluation of matching of the expert annotations, we defined specific annotation-matching metrics (Equations 1 and 2) [3]. Besides, the LE tool (Figure 2) allows synchronous display and comparison of episode annotations, display of separately derived [4] time series of diagnostic and morphology feature vectors, navigation along the signals and time series, and computation of the annotation-matching measures. Evaluation of the differences in sampling frequency and in QRS complex amplitude (Table 2) suggested that these differences are due to mechanisms for tape movement and to sampling equipment. The annotation-matching measures showed that the expert annotations of both databases are comparable but moderate divergences in the evaluation results can be expected (Table 3). Slightly higher sensitivities and lower positive predictivities in detecting transient ischaemic ST segment episodes are expected if using the LTST DB as the test database. The experimental results using our own developed system for analysis of transient ST segment changes [5] and standard performance measures to evaluate ischaemia detectors [6, 7] confirmed this hypothesis. The performance results showed (Table 4) higher sensitivity and lower positive predictivity in ischaemic ST episode and ischaemic ST episode duration detection if using the LTST DB. We conclude that these differences are mostly due to discrepancies in the annotation protocols. Results of the study are consistent with findings of other researchers which reported lower sensitivities of their recognition algorithms in detecting transient ischaemic episodes [8, 9] if using the ESC DB.", "Language": "en", "Citations": "0"},
{"Title": "A Bayesian approach to forecasting daily air-pollutant levels", "Authors": ["FaganeliPucer J.", "Pirs G.", "Strumbelj E."], "Keywords": ["Air pollutants", "Bayesian statistics", "Cost matrix", "Forecasting", "Gaussian processes", "Machine learning"], "Date": "2018", "Abstract": "Forecasting air-pollutant levels is an important issue, due to their adverse effects on public health, and often a legislative necessity. The advantage of Bayesian methods is their ability to provide density predictions which can easily be transformed into ordinal or binary predictions given a set of thresholds. We develop a Bayesian approach to forecasting PM", "Language": "en", "Citations": "1"},
{"Title": "Java 2 distributed object middleware performance analysis and optimization", "Authors": ["Juric M.B.", "Rozman I.", "Nash S."], "Keywords": ["CORBA", "IDL", "IIOP", "Java", "Performance analysis and optimization", "RMI"], "Date": "2000", "Abstract": "This paper is focused on the performance analysis, comparison and optimization of distributed object middleware for Java 2: RMI (Remote Method Invocation), CORBA IDL (Interface Definition Language) and RMI-IIOP (Remote Method Invocation on Internet Inter-ORB Protocol). The paper presents the following contributions to the research on distributed object performance. First, a detailed performance analysis is provided with the comparison. These results help to understand how the models perform. Second, an overhead analysis has been done, which explains why there are differences in performance. Third, optimizations and improved performance for RMI-IIOP and CORBA IDL are presented. These show considerably better performance in all areas compared to the original versions.", "Language": "en", "Citations": "13"},
{"Title": "Ask, but don't interrupt: The case for interruptibility-aware mobile experience sampling", "Authors": ["Mehrotra A.", "Vermeulen J.", "Pejovic V.", "Musolesi M."], "Keywords": ["Context-aware computing", "Experience sampling method (ESM)", "Interruption", "Mobile sensing", "User surveys"], "Date": "2015", "Abstract": "The mobile phone-based Experience Sampling Method (ESM) enables in situ recording of human behaviour and experience by querying users, via their smartphones, anywhere and anytime. Sampling can happen on a previously unimaginable scale, and across a diverse pool of participants. Therefore, mobile ESM is not limited to capturing users' manual responses, as the surrounding context can be automatically captured by mobile sensors. However, obtaining high quality data with ESM is challenging, as users may fail to respond honestly, or may even ignore the questionnaire prompts if they perceive the study as too burdensome. In this paper, we discuss the potential of using interruptibility prediction models to deliver mobile ESM questionnaires at opportune moments, and thus improve the effectiveness of a study. We examine context prediction and interruptibility inference, which are fundamental challenges that need we need to overcome in order to make mobile ESMs better aligned with a user's lifestyle, and consequently paint a truthful picture of a user's behaviour.", "Language": "en", "Citations": "10"},
{"Title": "Closed world specialisation inside the induction process", "Authors": ["Drole M.", "Kononenko I."], "Keywords": ["bottom-up inductive logic programming", "Negation", "nonmonotonic inductive logic programming"], "Date": "2016", "Abstract": "This paper explores the idea of closed world specialisation (CWS). While traditional CWS is performed as a postprocessing step, we propose two different approaches to incorporating it into the induction process of a bottom-up inductive logic programming system. The motivation comes from the fact that using CWS as a postprocessing step is incapable of solving problems in which the negated part of the hypothesis is crucial. We apply the proposed approaches to the ProGolem bottom-up ILP system. We give examples of problems, where classical CWS fails to find a complete and consistent solution, whereas the proposed approaches succeed. Tests on real-world datasets show that the proposed approaches perform at least as well as regular CWS, while being better in terms of predictive accuracy in some cases. We also point out some weaknesses of different CWS approaches.", "Language": "en", "Citations": "0"},
{"Title": "Information systems integration process model", "Authors": ["Juric M.B.", "Tekavc M.", "Hericko M."], "Keywords": ["B2B", "EAI", "Information systems", "Integration", "Process"], "Date": "2004", "Abstract": "Integration of information systems is a complex field where major challenges are semantic, process and technology related. Integration must be performed using methods, disciplines and activities that enable it to be effective in terms of costs and time - thus it should be supported by a well defined integration process. This article presents an information systems integration process model proposal with the goal to guarantee the quality of the integrated solution. The article focuses particularly on the integration specific disciplines: analysis of existing applications and integration design.", "Language": "en", "Citations": "0"},
{"Title": "Multi-view approach to parkinson\u2019s disease quality of life data analysis", "Authors": ["Valmarska A.", "Miljkovic D.", "Robnik-Sikonja M.", "Lavrac N."], "Keywords": ["Multi-view learning", "Parkinson\u2019s disease", "Personalized medicine", "Rule learning", "Subgroup discovery"], "Date": "2017", "Abstract": "Parkinson\u2019s disease is a neurodegenerative disorder that affects people worldwide. While the motor symptoms such as tremor, rigidity, bradykinesia and postural instability are predominant, patients experience also non-motor symptoms, such as decline of cognitive abilities, behavioural problems, sleep disturbances, and other symptoms that greatly affect their quality of life. Careful management of patient\u2019s condition is crucial to ensure the patient\u2019s independence and the best possible quality of life. This is achieved by personalized medication treatment based on individual patient\u2019s symptoms and medical history. This paper explores the utility of machine learning to help development of decision models, aimed to support clinicians\u2019 decisions regarding patients\u2019 therapies. We propose a new multi-view methodology for determining groups of patients with similar symptoms and detecting patterns of medications changes that lead to the improvement or decline of patients\u2019 quality of life. We identify groups of patients ordered in accordance to their quality of life assessment and find examples of therapy modifications which induce positive or negative change of patients\u2019 symptoms. The results demonstrate that motor and autonomic symptoms are the most informative for evaluating the quality of life of Parkinson\u2019s disease patients.", "Language": "en", "Citations": "2"},
{"Title": "Selecting a methodology for business information systems development: Decision model and tool support", "Authors": ["Vavpotic D.", "Vasilecas O."], "Keywords": ["Business information systems development", "Decision model", "Development methodology"], "Date": "2012", "Abstract": "The paper presents a decision model and a tool that helps to find an information systems development methodology (ISDM) for a computer-based business information system (IS) that is suitable to a certain IS development project or an organisation dealing with IS development. The intention of the model is not only to suggest a certain ISDM, but also to propose the properties an ISDM should have to suite the project or the organisation. It is designed in a way that facilitates experimentation with different project, organisation and ISDM properties. Based on the model we created a tool that has been applied on several cases in which we validated the correctness of its recommendations and established that it can have a significant positive contribution in the process of ISDM selection and in the process of improvement of existing ISDM.", "Language": "en", "Citations": "3"},
{"Title": "Cholesky decomposition of matrices over commutative semirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Cholesky decomposition", "positive semidefiniteness", "Rajesh Pereira", "Semiring"], "Date": "2018", "Abstract": "We prove that over a commutative semiring every symmetric strongly invertible matrix with nonnegative numerical range has a Cholesky decomposition.", "Language": "en", "Citations": "0"},
{"Title": "Attribute visualisation for computer-aided diagnosis: A case study", "Authors": ["Groznik V.", "Sadikov A.", "Moina M.", "Abkar J.", "Georgiev D.", "Bratko I."], "Keywords": ["attribute visualisation", "time series", "tremor"], "Date": "2014", "Abstract": "Digitalised spirography is a relatively new computer-assisted method for detection and evaluation of tremors. The task of the patient is to draw an Archimedean spiral on the tablet, and different quantitative parameters (attributes) of the spiral are provided by the computer. The neurologists or a computer decision support system (DSS) use these parameters to assess whether the spiral exhibits signs of Parkinsonian or essential tremor. The goal of the present pilot study is to provide the user (physician or a DSS developer) with a meaningful visualisation of the most important attributes for the particular case. The purpose of such visualisation is threefold: (a) it provides the physician with immediate visual clues to be aware of when assessing the spiral, (b) it can serve as a 'visual debugging aid' for the developers of a DSS, and (c) it can trigger generation of new domain knowledge. In the paper we demonstrate a visualisation method and its application to all three cases using the Parkinson Check application.", "Language": "en", "Citations": "4"},
{"Title": "Feature subset selection for B2B sales forecasting", "Authors": ["Bohanec M.", "Borstnar M.K.", "Robnik-Sikonja M."], "Keywords": ["Business case", "Feature subset selection", "Knowledge engineering", "Machine learning"], "Date": "2015", "Abstract": "Although the use of information technology in business to business (B2B) sales is high, and generates lots of data, sales forecasting relies more on the personal judgement and established mental models. Business domain data sets often list a high number of descriptive features, however only a few have some value for machine learning techniques. Small number of features is important for understanding and interpretation in the context of organizational learning. We present an operational guideline for selection of small number of the most relevant features while maintaining high classification accuracy. Promising results of introduced three-step operational guideline are presented in this paper.", "Language": "en", "Citations": "5"},
{"Title": "Identification and characteristic descriptions of procedural chunks", "Authors": ["Krivec J.", "Guid M.", "Bratko I."], "Keywords": ["Chess", "Chunk", "Information processing", "Machine learning", "Procedural knowledge"], "Date": "2009", "Abstract": "When dealing with cognitive architecture and behavior, chunks are one of the most well known and accepted constructs. Despite that, the nature of chunks still remains very elusive, especially with understanding chunks in procedural knowledge. Our attempt is to show the existence of chunks in procedural knowledge, define them, and describe their characteristics. With this purpose in mind, we use data mining techniques. We chose the game of chess as an experimental domain, due to its complexity, well defined rules, and a standardized measure of chess-players' knowledge. Results could contribute to the understanding of human information processing and cognitive architecture. They could be beneficial for tutoring and student modeling, and may serve as a framework for knowledge-based driven AI agents. \u00a9 2009 IEEE.", "Language": "en", "Citations": "1"},
{"Title": "Data-driven program synthesis for hint generation in programming tutors", "Authors": ["Lazar T.", "Bratko I."], "Keywords": ["hint generation", "program synthesis", "programming tutors"], "Date": "2014", "Abstract": "One of the main functions of intelligent tutoring systems is providing feedback to help students solve problems. We present a novel approach to program synthesis that can be used as a basis for automatic hint generation in programming tutors. Instead of using a state-space representation of the problem-solving process, our method finds a set of textual edits commonly used by students on program code. Given an incorrect program it then synthesizes new programs by applying sequences of edits until a solution is found. The edit sequence can be used to provide hints with varying levels of detail. Experimental results confirm the feasibility of our approach. \u00a9 2014 Springer International Publishing Switzerland.", "Language": "en", "Citations": "16"},
{"Title": "Conserved developmental transcriptomes in evolutionarily divergent species", "Authors": ["Parikh A.", "Miranda E.R.", "Katoh-Kurasawa M.", "Fuller D.", "Rot G.", "Zagar L.", "Curk T.", "Sucgang R.", "Chen R.", "Zupan B.", "Loomis W.F.", "Kuspa A.", "Shaulsky G."], "Keywords": [], "Date": "2010", "Abstract": "Background: Evolutionarily divergent organisms often share developmental anatomies despite vast differences between their genome sequences. The social amoebae Dictyostelium discoideum and Dictyostelium purpureum have similar developmental morphologies although their genomes are as divergent as those of man and jawed fish.Results: Here we show that the anatomical similarities are accompanied by extensive transcriptome conservation. Using RNA sequencing we compared the abundance and developmental regulation of all the transcripts in the two species. In both species, most genes are developmentally regulated and the greatest expression changes occur during the transition from unicellularity to multicellularity. The developmental regulation of transcription is highly conserved between orthologs in the two species. In addition to timing of expression, the level of mRNA production is also conserved between orthologs and is consistent with the intuitive notion that transcript abundance correlates with the amount of protein required. Furthermore, the conservation of transcriptomes extends to cell-type specific expression.Conclusions: These findings suggest that developmental programs are remarkably conserved at the transcriptome level, considering the great evolutionary distance between the genomes. Moreover, this transcriptional conservation may be responsible for the similar developmental anatomies of Dictyostelium discoideum and Dictyostelium purpureum. \u00a9 2010 Parikh et al.; licensee BioMed Central Ltd.", "Language": "en", "Citations": "109"},
{"Title": "Parallel implementations of recurrent neural network learning", "Authors": ["Lotric U.", "Dobnikar A."], "Keywords": [], "Date": "2009", "Abstract": "Neural networks have proved to be effective in solving a wide range of problems. As problems become more and more demanding, they require larger neural networks, and the time used for learning is consequently greater. Parallel implementations of learning algorithms are therefore vital for a useful application. Implementation, however, strongly depends on the features of the learning algorithm and the underlying hardware architecture. For this experimental work a dynamic problem was chosen which implicates the use of recurrent neural networks and a learning algorithm based on the paradigm of learning automata. Two parallel implementations of the algorithm were applied - one on a computing cluster using MPI and OpenMP libraries and one on a graphics processing unit using the CUDA library. The performance of both parallel implementations justifies the development of parallel algorithms. \u00a9 Springer-Verlag 2009.", "Language": "en", "Citations": "11"},
{"Title": "On the appropriateness of domain-specific languages derived from different metamodels", "Authors": ["Rozanc I.", "Slivnik B."], "Keywords": ["domain-specific languages", "metamodel quality", "model-driven development", "quality metrics"], "Date": "2014", "Abstract": "In model-driven development domain-specific languages (DSL) are often considered models while the description of DSLs are expressed using various metamodels. To estimate the influence of a metamodel on the quality of DSLs derived from it, it is appropriate to measure functional suitability. As defined by the standard ISO/IEC 25010 (SQuaRE), functional suitability consists of completeness, correctness, and appropriateness. Among these issues, only appropriateness can be evaluated without specifying the domain. This paper is a study of a relationship between (a) the metamodel's expressive power regarding the syntax of the DSLs derived from the metamodel and (b) the appropriateness of those DSLs. In this regard two metrics are defined. The first metric evaluates a metamodel and produces the estimation of the derived DSLs' appropriateness. The second metric incorporates the domain and further assesses the quality of a DSL in terms of appropriateness. Both metrics are based on abstract syntax trees of programs written in the derived DSLs, and demonstrated using examples on two different domains.", "Language": "en", "Citations": "3"},
{"Title": "Distributed web service for geospatial applications", "Authors": ["Cibej U.", "Mihelic J.", "Slivnik B."], "Keywords": [], "Date": "2009", "Abstract": "Geospatial data has gained a lot of importance in the last decade and many products have been deployed providing functionality for this type of applications. In this article we present an architecture for combining distributed geospatial data into a homogeneous service. We present the challenges when implementing such a service and benefits which the distributed implementation provides. We also give an overview of the existing solutions and advantages of our system. Our current design gives the possibility to easily connect distributed data into hierarchies, but other topologies of interconnected servers are also possible. Our implementation is based on the well-known GeoTools library, which makes it not only a proof of a concept but also a fully usable, standard compliant, and easily pluggable into standardized GIS solutions, such as GeoServer or MapServer. \u00a9 2009 by MIPRO.", "Language": "en", "Citations": "0"},
{"Title": "Chronic subthalamic nucleus stimulation in Parkinson's disease: Optimal frequency for gait depends on stimulation site and axial symptoms", "Authors": ["Giulio I.D.", "Kalliolia E.", "Georgiev D.", "Peters A.L.", "Voyce D.C.", "Akram H.", "Foltynie T.", "Limousin P.", "Day B.L."], "Keywords": ["Axial symptoms", "Deep brain stimulation", "Gait", "Parkinson's disease", "Subthalamic nucleus"], "Date": "2019", "Abstract": "Axial symptoms emerge in a significant proportion of patients with Parkinson's disease (PD) within 5 years of deep brain stimulation (STN-DBS). Lowering the stimulation frequency may reduce these symptoms. The objectives of the current study were to establish the relationship between gait performance and STN-DBS frequency in chronically stimulated patients with PD, and to identify factors underlying variability in this relationship. Twenty-four patients treated chronically with STN-DBS (>4 years) were studied off-medication. The effect of stimulation frequency (40\u2013140 Hz, 20 Hz-steps, constant energy) on gait was assessed in 6 sessions spread over 1 day. Half of the trials/session involved walking through a narrow doorway. The influence of stimulation voltage was investigated separately in 10 patients. Gait was measured using 3D motion capture and axial symptoms severity was assessed clinically. A novel statistical method established the optimal frequency(ies) for each patient by operating on frequency-tuning curves for multiple gait parameters. Narrowly-tuned optimal frequencies (20 Hz bandwidth) were found in 79% of patients. Frequency change produced a larger effect on gait performance than voltage change. Optimal frequency varied between patients (between 60 and 140 Hz). Contact site in the right STN and severity of axial symptoms were independent predictors of optimal frequency (P = 0.009), with lower frequencies associated with more dorsal contacts and worse axial symptoms. We conclude that gait performance is sensitive to small changes in STN-DBS frequency. The optimal frequency varies considerably between patients and is associated with electrode contact site and severity of axial symptoms. Between-subject variability of optimal frequency may stem from variable pathology outside the basal ganglia.", "Language": "en", "Citations": "0"},
{"Title": "The degree of maps of free G-manifolds", "Authors": ["Jaworowski J.", "Kosta N.M."], "Keywords": ["Classifying map", "Degree of a map", "Free G-manifold", "Power map", "Torus action", "Transfer"], "Date": "2007", "Abstract": "Suppose that M and N are orientable, closed, connected manifolds with free actions of compact Lie groups G and H of the same dimension, and suppose that u : G \u2192 H is a homomorphism. We study the degree of maps f : M \u2192 N that are \"equivariant up to u\". For abelian actions and for a power map u : \u03bb \u03bb", "Language": "en", "Citations": "0"},
{"Title": "Transduction and typicalness for quality assessment of individual classifications in machine learning and data mining", "Authors": ["Kukar M."], "Keywords": ["Confidence estimation", "Machine learning", "Quality assessment", "Transduction", "Typicalness"], "Date": "2004", "Abstract": "In the past machine learning algorithms have been successfully used in many problems, and are emerging as valuable data analysis tools. However, their serious practical use is affected by the fact, that more often than not, they cannot produce reliable and unbiased assessments of their predictions' quality. In last years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper we propose a joint approach that compensates the mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense (e.g., a confidence level of 95% means that in 95% the predicted class is also a true class), as well as provides us with a general principle that is independent of to the particular underlying classifier We perform a series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary TCM-NN method as well as with kernel density estimation. We show that the proposed method significantly outperforms density estimation methods, and how it may be used to improve their performance. \u00a9 2004 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "E-learning best practices in health care", "Authors": ["Zvanut B.", "Vavpotic D.", "Pucer P."], "Keywords": ["Best practices", "E-learning", "Health care"], "Date": "2015", "Abstract": "In health care study programmes the importance of e-learning is increasing. This teaching and learning approach is nowadays included in different health care curricula. In this paper some e-learning best practices in health care, which were validated in our previous projects/studies, are presented: e-learning contents for the development of critical thinking, inclusion of the health care students in the development of e-contents, and evaluation and comparison of e-learning and traditional pedagogical elements value by health care students and teachers. Our experience show that properly developed e-contents, which vividly presents situation from the clinical practice and promote students' reflection are a prerequisite for the successful e-learning promotion in health care educational institutions. The students' participation in the cocreation of e-learning environment makes it more student-oriented and has several positive effects on entire e-learning courses. Finally, the continuous evaluation and comparison of e-learning and traditional pedagogical elements value helps the management to identify whether previous e-learning initiatives and activities were successful. Best practices, presented in this paper, represents an evidence that the use of modern information and communication technologies should be considered in the future as a serious requisite in health care study programmes.", "Language": "en", "Citations": "0"},
{"Title": "Applicability of approximate multipliers in hardware neural networks", "Authors": ["Lotric U.", "Bulic P."], "Keywords": ["Computer arithmetic", "Digital design", "FPGA", "Hardware neural network", "Iterative logarithmic multiplier"], "Date": "2012", "Abstract": "In recent years there has been a growing interest in hardware neural networks, which express many benefits over conventional software models, mainly in applications where speed, cost, reliability, or energy efficiency are of great importance. These hardware neural networks require many resource-, power- and time-consuming multiplication operations, thus special care must be taken during their design. Since the neural network processing can be performed in parallel, there is usually a requirement for designs with as many concurrent multiplication circuits as possible.One option to achieve this goal is to replace the complex exact multiplying circuits with simpler, approximate ones. The present work demonstrates the application of approximate multiplying circuits in the design of a feed-forward neural network model with on-chip learning ability. The experiments performed on a heterogeneous P. roben1 benchmark dataset show that the adaptive nature of the neural network model successfully compensates for the calculation errors of the approximate multiplying circuits. At the same time, the proposed designs also profit from more computing power and increased energy efficiency. \u00a9 2012 Elsevier B.V.", "Language": "en", "Citations": "25"},
{"Title": "A survey of parallel and distributed algorithms for the STEINER TREE PROBLEM", "Authors": ["Bezensek M.", "Robic B."], "Keywords": ["Applications", "Distributed computing", "Optimization", "Parallel computing", "STEINER TREE", "Survey"], "Date": "2014", "Abstract": "Given a set of input points, the STEINER TREE PROBLEM (STP) is to find a minimum-length tree that connects the input points, where it is possible to add new points to minimize the length of the tree. Solving the STP is of great importance since it is one of the fundamental problems in network design, very large scale integration routing, multicast routing, wire length estimation, computational biology, and many other areas. However, the STP is NP-hard, which shatters any hopes of finding a polynomial-time algorithm to solve the problem exactly. This is why the majority of research has looked at finding efficient heuristic algorithms. Additionally, many authors focused their work on utilizing the ever-increasing computational power and developed many parallel and distributed methods for solving the problem. In this way we are able to obtain better results in less time than ever before. Here, we present a survey of the parallel and distributed methods for solving the STP and discuss some of their applications. \u00a9 Springer Science+Business Media New York 2013.", "Language": "en", "Citations": "5"},
{"Title": "On detecting note onsets in piano music", "Authors": ["Marolt M.", "Kavcic A.", "Privosnik M.", "Divjak S."], "Keywords": ["Music transcription", "Neural networks", "Onset detection"], "Date": "2002", "Abstract": "This paper presents a brief overview of our researches in the use of connectionist systems for transcription of polyphonic piano music and concentrates on the issue of onset detection in musical signals. We propose a new technique for detecting onsets in a piano performance. The technique is based on a combination of a bank of auditory filters, a network of integrate- and-fire neurons and a multilayer perceptron. Such structure introduces several advantages over the standard peak-picking onset detection approach and we present its performance on several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing onset detection algorithms.", "Language": "en", "Citations": "11"},
{"Title": "Odd complete minors in even embeddings on surfaces", "Authors": ["Fijavz G.", "Nakamoto A."], "Keywords": ["Even embedding", "Graph embedding", "Graph minor", "Odd minor"], "Date": "2016", "Abstract": "In this paper, we study the odd <sup>Km</sup>-minor problem in even embeddings on surfaces. We first establish a general theory for even embeddings with odd <sup>Km</sup>-minors. Given an integer m we show that for every surface <sup>F2</sup> of sufficiently high genus there exists a constant N=N(<sup>F2</sup>) so that every non-bipartite even embedding on <sup>F2</sup> with representativity at least N contains an odd <sup>Km</sup> as a minor. In the second part we prove that every 19-representative non-bipartite even embedding in an arbitrary orientable surface of genus 1 has an odd <sup>K5</sup>-minor.", "Language": "en", "Citations": "1"},
{"Title": "The effect of voluntariness on the acceptance of e-learning by nursing students", "Authors": ["Zvanut B.", "Pucer P.", "Licen S.", "Trobec I.", "Plazar N.", "Vavpotic D."], "Keywords": ["Acceptance", "E-learning", "Nursing students", "Voluntariness"], "Date": "2011", "Abstract": "Although e-learning is an innovation that is worth making generally available, it is not always accepted by nursing students. Many researchers state that voluntariness is closely related to the individual level of adoption of innovations. Hence, we hypothesized that voluntariness moderates the effect of perceived attributes of innovations (e.g. relative advantage, compatibility, complexity, trialability, and observability), which determines the acceptance of e-learning. To test the hypothesis a survey involving two groups of nursing students was carried out. For the first group the usage of e-learning was mandatory, for the second group it was optional. The results confirm our hypothesis. Institutions, interested in e-learning initiatives, should consider the effect of voluntariness when implementing e-learning. This paper provides a useful reference that can help e-learning providers to develop guidelines that can improve the acceptance of e-learning. \u00a9 2010 Elsevier Ltd.", "Language": "en", "Citations": "18"},
{"Title": "Calculating the cryptographic currencies using GPUs Ra\u010dunanje kriptografskih valut z GPE Ra\u010dunanje kriptografskih valut z GPE", "Authors": ["Sedmak L.", "Dobravec T."], "Keywords": [], "Date": "2015", "Abstract": "In the field of modern finance, a concentration has developed in a handful of financial institutions. As an alternative to such centralization, deregulated and fully distributed synthetic currencies have been introduced. They are designed as a peer-to-peer network, where by using cryptographic functions, transactions and creation of the currency are controlled, which allows for full transparency along with an anonymity and security. To obtain some cryptographic currencies, we assembled a device that uses a large amount of the fast memory on video cards by running a special software which confirms transactions in the network. Using this device, we collected a small amount of the currency as a reward. In the paper we describe characteristics of various cryptocurrencies, explain the methods for maximizing the profit, describe the hardware used, present characteristics of various cryptocurrencies, introduce methods for maximizing the profit and describe the hardware used. We also explain the algorithm used to validate the transactions and show the impact of the configuration parameters on the resulting hashing speed.", "Language": "en", "Citations": "0"},
{"Title": "Audio melody extraction based on timbral similarity of melodic fragments", "Authors": ["Marolt M."], "Keywords": ["Audio melody extraction", "Melodic fragments", "Music information retrieval"], "Date": "2005", "Abstract": "The presented study deals with extraction of melodic line(s) from polyphonic audio recordings. Our approach is based on finding significant melodic fragments throughout the analyzed piece of music and clustering these fragments according to their timbral similarity. Fragments within clusters are taken to represent fragments belonging to different melodic lines. Holes between significant fragments within each cluster are filled ill by a shortest-path approach over all melodic fragments. The paper presents our study in more detail and provides results on real recordings. \u00a9 2005 IEEE.", "Language": "en", "Citations": "9"},
{"Title": "Restrictions on classical distance-regular graphs", "Authors": ["Jurisic A.", "Vidali J."], "Keywords": ["Classical parameters", "Distance-regular graphs", "Formally self-dual", "Locally strongly regular", "Tight graphs"], "Date": "2017", "Abstract": "Let \u0393 be a distance-regular graph with diameter d\u2265 2. It is said to have classical parameters(d, b, \u03b1, \u03b2) when its intersection array { b", "Language": "en", "Citations": "1"},
{"Title": "Correction of regression predictions using the secondary learner on the sensitivity analysis outputs", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": ["Correction of predictions", "Prediction accuracy", "Prediction error", "Predictions", "Regression", "Sensitivity analysis"], "Date": "2010", "Abstract": "For a given regression model, each individual prediction may be more or less accurate. The average accuracy of the system cannot provide the error estimate for a single particular prediction, which could be used to correct the prediction to a more accurate value. We propose a method for correction of the regression predictions that is based on the sensitivity analysis approach. Using predictions, gained in sensitivity analysis procedure, we build a secondary regression predictor whose task is to predict the signed error of the prediction which was made using the original regression model. We test the proposed methodology using four regression models: locally weighted regression, linear regression, regression trees and neural networks. The results of our experiments indicate significant increase of prediction accuracy in more than 20 % of experiments. The favorable results prevale especially with the regression trees and neural networks, where locally weighted regression was used as a model for predicting the prediction error. In these experiments the prediction accuracy increased in 60 % of experiments with regression trees and in 50 % of experiments with neural networks, while the increase of the prediction error did not occur in any experiment.", "Language": "en", "Citations": "5"},
{"Title": "Verifying epistemic properties of multi-agent systems via action-based temporal logic", "Authors": ["Bagic M.", "Babac A.", "Ciglari c M."], "Keywords": [], "Date": "2008", "Abstract": "This paper provides a specifying and verifying framework of a multi-agent system, with the emphasis on their epistemic features. We use an epistemic transition system to specify the agents and an epistemic synchronous product to specify the multi-agent system.We verify the system by means of a special action-based temporal logic - ACTLW for Epistemic Reasoning (ACTLW stands for Action Computation Tree Logic with Unless Operator). Using temporal and epistemic operators we create the appropriate formulae to perform model checking for the system. We test our method by the example of security communication protocol called Dining Cryptographers. \u00a9 2008 IEEE.", "Language": "en", "Citations": "1"},
{"Title": "Separation of interleaved web sessions with heuristic search", "Authors": ["Pozenel M.", "Mahnic V.", "Kukar M."], "Keywords": ["Clickstream", "Data quality", "HTTP session", "Interleaved session", "Markov model", "Session separation process", "Sessionization", "User behavior"], "Date": "2011", "Abstract": "We describe a heuristic search-based method for interleaved HTTP (Web) session reconstruction building upon first order Markov models. An interleaved session is generated by a user who is concurrently browsing the same web site in two or more web sessions (browser tabs or windows). In order to assure data quality for subsequent phases in analyzing user's browsing behavior, such sessions need to be separated in advance. We propose a separating process based on best-first search and trained first order Markov chains. We develop a testing method based on various measures of reconstructed sessions similarity to original ones. We evaluate the developed method on two real world clickstream data sources: a web shop and a university student records information system. Preliminary results show that the proposed method performs well. \u00a9 2010 IEEE.", "Language": "en", "Citations": "5"},
{"Title": "A combinatorial approach to graphlet counting", "Authors": ["Hocevar T.", "Demsar J."], "Keywords": [], "Date": "2014", "Abstract": "Motivation: Small-induced subgraphs called graphlets are emerging as a possible tool for exploration of global and local structure of networks and for analysis of roles of individual nodes. One of the obstacles to their wider use is the computational complexity of algorithms for their discovery and counting.Results: We propose a new combinatorial method for counting graphlets and orbit signatures of network nodes. The algorithm builds a system of equations that connect counts of orbits from graphlets with up to five nodes, which allows to compute all orbit counts by enumerating just a single one. This reduces its practical time complexity in sparse graphs by an order of magnitude as compared with the existing pure enumeration-based algorithms.Availability and implementation: Source code is available freely at http://www.biolab.si/supp/ orca/orca.html.Contact: Supplementary information: Supplementary data are available at Bioinformatics online. \u00a9 2013 The Author.", "Language": "en", "Citations": "59"},
{"Title": "GDV measures vitality?", "Authors": ["Kononenko I.", "Sedej M.", "Sadikov A."], "Keywords": [], "Date": "2005", "Abstract": "The study verifies a hypothesis that GDV in fact measures vitality. For that purpose a limited study was performed and the findings support the hypothesis. However, further investigation is needed in order to more reliably confirm it. \u00a9 2005 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "Multi-agent system for decision support in enterprises", "Authors": ["Lavbic D.", "Rupnik R."], "Keywords": ["Business rules", "Data Warehouse", "Information retrieval", "Intelligent agent", "Ontology"], "Date": "2009", "Abstract": "Business decisions must rely not only on organisation's internal data but also on external data from competitors or relevant events. This information can be obtained from the Web but must be integrated with the data in an organisation's Data Warehouse (DW). In this paper we discuss the agent-based integration approach using ontologies. To enable common understanding of a domain between people and application systems we introduce business rules approach towards ontology management. Because knowledge in organisation's ontologies is acquired from business users without technical knowledge simple user interface based on ontology restrictions and predefined templates are used. After data from internal DW, Web and business rules are acquired; agent can deduce new knowledge and therefore facilitate decision making process. Tasks like information retrieval from competitors, creating and reviewing OLAP reports are autonomously performed by agents, while business users have control over their execution through knowledge base in ontology. The approach presented in the paper was verified on the case study from the domain of mobile communications with the emphasis on supply and demand of mobile phones and its accessories.", "Language": "en", "Citations": "13"},
{"Title": "Transductive machine learning for reliable medical diagnostics", "Authors": ["Kukar M.", "Groselj C."], "Keywords": ["Coronary artery disease", "Machine learning", "Medical diagnosis", "Reliability", "Transduction"], "Date": "2005", "Abstract": "In the past decades Machine Learning tools have been successfully used in several medical diagnostic problems. While they often significantly outperform expert physicians (in terms of diagnostic accuracy, sensitivity, and specificity), they are mostly not being used in practice. One reason for this is that it is difficult to obtain an unbiased estimation of diagnose's reliability. We discuss how reliability of diagnoses is assessed in medical decision making and propose a general framework for reliability estimation in Machine Learning, based on transductive inference. We compare our approach with a usual (Machine Learning) probabilistic approach as well as with classical stepwise diagnostic process where reliability of diagnose is presented as its posttest probability. The proposed transductive approach is evaluated on several medical data sets from the UCI (University of California, Irvine) repository as well as on a practical problem of clinical diagnosis of the coronary artery disease. In all cases significant improvements over existing techniques are achieved. \u00a9 2005 Springer Science+Business Media, Inc.", "Language": "en", "Citations": "6"},
{"Title": "Robustness and visualization of decision models", "Authors": ["Bregar A.", "Gyorkos J.", "Jurie M."], "Keywords": ["Decision support", "Electre", "Mathematical optimization", "Multi-criteria decision analysis", "Principal components analysis", "Promethee", "Robustness metrics", "Utility theory"], "Date": "2009", "Abstract": "Robustness analysis and visualization are two of key concepts of multi-criteria decision support. They enable the decision-maker to improve his understanding of both the model and the problem domain. A class of original mathematical optimization based robustness metrics is hence defined in this paper. In addition, several efficient existing techniques that have been successfully used in various ICT projects are presented. They include the stability intervals/regions and the principal components analysis. All approaches are applied to the multi-attribute utility function, and to the PROMETHEE II and ELECTRE TRI methods. Their benefits are discussed and demonstrated on real life cases.", "Language": "en", "Citations": "4"},
{"Title": "SenSocial: A middleware for integrating online social networks and mobile sensing data streams", "Authors": ["Mehrotra A.", "Pejovic V.", "Musolesi M."], "Keywords": ["Mobile middleware", "Mobile sensing", "Social sensing", "Ubiquitous computing"], "Date": "2014", "Abstract": "Smartphone sensing enables inference of physical context, while online social networks (OSNs) allow mobile applications to harness users' interpersonal relationships. However, OSNs and smartphone sensing remain disconnected, since obstacles, including the synchronization of mobile sensing and OSN monitoring, inefficiency of smartphone sensors, and privacy concerns, stand in the way of merging the information from these two sources. In this paper we present the design, implementation and evaluation of SenSocial, a middleware that automates the process of obtaining and joining OSN and physical context data streams for the development of ubiquitous computing applications. SenSocial enables instantiation, management and aggregation of context streams from multiple remote devices. Through micro-benchmarks we show that SenSocial successfully and efficiently captures OSN and mobile sensed data streams. We developed two prototype applications in order to evaluate our middleware and we demonstrate that SenSocial significantly reduces the amount of programming effort needed for building social sensing applications.", "Language": "en", "Citations": "20"},
{"Title": "Towards scalable representations of object categories: Learning a hierarchy of parts", "Authors": ["Fidler S.", "Leonardis A."], "Keywords": [], "Date": "2007", "Abstract": "This paper proposes a novel approach to constructing a hierarchical representation of visual input that aims to enable recognition and detection of a large number of object categories. Inspired by the principles of efficient indexing (bottom-up), robust matching (top-down), and ideas of compositionality, our approach learns a hierarchy of spatially flexible compositions, i.e. parts, in an unsupervised, statistics-driven manner. Starting with simple, frequent features, we learn the statistically most significant compositions (parts composed of parts), which consequently define the next layer. Parts are learned sequentially, layer after layer, optimally adjusting to the visual data. Lower layers are learned in a category-independent way to obtain complex, yet shamble visual building blocks, which is a crucial step towards a scalable representation. Higher layers of the hierarchy, on the other hand, are constructed by using specific categories, achieving a category representation with a small number of highly generalizable parts that gained their structural flexibility through composition within the hierarchy. Built in this way, new categories can be efficiently and continuously added to the system by adding a small number of parts only in the higher layers. The approach is demonstrated on a large collection of images and a variety of object categories. Detection results confirm the effectiveness and robustness of the learned parts. \u00a9 2007 IEEE.", "Language": "en", "Citations": "115"},
{"Title": "An iterative logarithmicmultiplier", "Authors": ["Babic Z.", "Avramovic A.", "Bulic P."], "Keywords": ["Computer arithmetic", "Digital signal processing", "Logarithmic number system", "Multiplier"], "Date": "2010", "Abstract": "The paper presents a new multiplier enabling achievement of an arbitrary accuracy. It follows the same idea of number representation as the Mitchell's algorithm, but does not use logarithm approximation. The proposed iterative algorithm is simple and efficient and its error percentage is as small as required. As its hardware solution involves adders and shifters, it is not gate and power consuming. Parallel circuits are used for error correction. The error summary for operands ranging from 8-bit to 16-bit operands indicates a very low error percentage with only two parallel correction circuits.", "Language": "en", "Citations": "0"},
{"Title": "Security models: Refocusing on the human factor", "Authors": ["Trcek D."], "Keywords": ["Agent technologies", "IT Systems Perspectives", "Security"], "Date": "2006", "Abstract": "As IS security refocuses on the human factor, complex models based on system dynamics and agent technologies promise to play an important role in improving and strengthening future information systems. Using the two methodologies independently can yield unique insights into the soundness of a given approach, although in some cases only agents or system dynamics can be applied. \u00a9 2006 IEEE.", "Language": "en", "Citations": "5"},
{"Title": "Shortest paths in semi-directed circulant graphs Najkraj\u0161e poti v delno usmerjenih kro\u017enih grafih", "Authors": ["Dobravec T."], "Keywords": ["Algorithms", "Circulant graphs", "Restricted shortest paths", "Routing", "Semi-directed graphs"], "Date": "2005", "Abstract": "A k-circulant graph G(n; \u00b1h", "Language": "en", "Citations": "0"},
{"Title": "Generating discrete morse functions from point data", "Authors": ["King H.", "Knudson K.", "Mramor N."], "Keywords": ["Discrete Morse theory", "Persistence"], "Date": "2005", "Abstract": "If K is a finite simplicial complex and h is an injective map from the vertices of K to \u211d, we show how to extend h to a discrete Morse function in the sense of Forman [Forman 02] in a reasonably efficient manner so that the resulting discrete Morse function mirrors the large-scale behavior of h. A concrete algorithm is given for the case where K is a subcomplex of \u211d", "Language": "en", "Citations": "56"},
{"Title": "Learning from depth sensor data using inductive logic programming", "Authors": ["Drole M.", "Vracar P.", "Panjkota A.", "Stancic I.", "Music J.", "Kononenko I.", "Kukar M."], "Keywords": ["assistive devices", "context awareness", "Knowlegde discovery", "supervised learning"], "Date": "2015", "Abstract": "The problem of detecting objects and their movements in sensor data is of crucial importance in providing safe navigation through both indoor and outdoor environments for the visually impaired. In our setting we use depth-sensor data obtained from a simulator and use inductive logic programming (ILP), a subfield of machine learning that deals with learning concept descriptions, to learn how to detect borders, find the border that is nearest to some point of interest, and border correspondence through time. We demonstrate how ILP can be used to tackle this problem in an incremental manner by using previously learned predicates to construct more complex ones. The learned concept descriptions show high (> 90%) accuracy and their natural language interpretation closely matches an intuitive understanding of their meaning.", "Language": "en", "Citations": "0"},
{"Title": "Feasibility of an eHealth service to support collaborative depression care: Results of a pilot study", "Authors": ["Meglic M.", "Furlan M.", "Kuzmanic M.", "Kozel D.", "Baraga D.", "Kuhar I.", "Kosir B.", "Iljaz R.", "Sarotar B.N.", "Dernovsek M.Z.", "Marusic A.", "Eysenbach G.", "Brodnik A."], "Keywords": ["Collaborative care", "Depression", "Feasibility study", "Information systems", "Internet", "Medication adherence", "Patient care management", "Pilot study", "Treatment outcome"], "Date": "2010", "Abstract": "Background: Treatments and organizational changes supported by eHealth are beginning to play an important role in improving disease treatment outcome and providing cost-efficient care management. \"Improvehealth.eu\" is a novel eHealth service to support the treatment of patients with depressive disorder. It offers active patient engagement and collaborative care management by combining Web-and mobile-based information and communication technology systems and access to care managers. Objectives: Our objective was to assess the feasibility of a novel eHealth service. Methods: The intervention-the \"Improvehealth.eu\" service-was explored in the course of a pilot study comparing two groups of patients receiving treatment as usual and treatment as usual with eHealth intervention. We compared patients' medication adherence and outcome measures between both groups and additionally explored usage and overall perceptions of the intervention in intervention group. Results: The intervention was successfully implemented in a pilot with 46 patients, of whom 40 were female. Of the 46 patients, 25 received treatment as usual, and 21 received the intervention in addition to treatment as usual. A total of 55% (12/25) of patients in the former group and 45% (10/21) in the latter group finished the 6-month pilot. Available case analysis indicated an improvement of adherence in the intervention group (odds ratio [OR] = 10.0, P =.03). Intention-to-treat analysis indicated an improvement of outcome in the intervention group (ORs ranging from 0.35 to 18; P values ranging from.003 to.20), but confidence intervals were large due to small sample sizes. Average duration of use of the intervention was 107 days. The intervention was well received by 81% (17/21) of patients who reported feeling actively engaged, in control of their disease, and that they had access to a high level of information. In all, 33% (7/21) of the patients also described drawbacks of the intervention, mostly related to usability issues. Conclusions: The results of this pilot study indicate that the intervention was well accepted and helped the patients in the course of treatment. The results also suggest the potential of the intervention to improve both medication adherence and outcome measures of treatment, including reduction of depression severity and patients becoming \"healthy\".", "Language": "en", "Citations": "34"},
{"Title": "An efficient way to filter out data dependences with a sufficiently large distance between memory references", "Authors": ["Bulic P.", "Gustin V."], "Keywords": ["Data dependence analysis", "SIMD microprocessors", "Vectorizing compilers"], "Date": "2005", "Abstract": "There are a number of data dependence tests that have been proposed in the literature. The most widely used approximate data dependence tests are the Banerjee inequality and the GCD test. In this paper we consider parallelization for microprocessors with the multimedia extensions. For the short SIMD parallelism extraction it is essential that, if dependency exists, then the distance between memory references is greater than or equal to the number of data processed in the SIMD register. This implies that some loops that could not be vectorized on traditional vector processors can still be parallelized for the short SIMD execution. In this paper we present an accurate and simple method that can filter out data dependences with a sufficiently large distance between memory references for linear array references within a. nested loop. The presented method is suitable for use in a dependence analyzer that is organized as a series of tests, progressively increasing in accuracy, as a replacement for the GCD or Banerjee tests.", "Language": "en", "Citations": "0"},
{"Title": "Interactive network exploration with Orange", "Authors": ["Stajdohar M.", "Demsar J."], "Keywords": ["Data mining", "Networks", "Python", "Visualization"], "Date": "2013", "Abstract": "Network analysis is one of the most widely used techniques in many areas of modern science. Most existing tools for that purpose are limited to drawing networks and computing their basic general characteristics. The user is not able to interactively and graphically manipulate the networks, select and explore subgraphs using other statistical and data mining techniques, add and plot various other data within the graph, and so on. In this paper we present a tool that addresses these challenges, an add-on for exploration of networks within the general component-based environment Orange.", "Language": "en", "Citations": "4"},
{"Title": "Towards multistate nanocomputing: The implementation of a primitive fuzzy controller", "Authors": ["Mraz M.", "Magdevski Z.", "Ficzko J.", "Zimic N.", "Moskon M.", "Janez M.", "Bajec I.L."], "Keywords": [], "Date": "2008", "Abstract": "In the following article we present the generalization of the basic building blocks of Quantum-dot Cellular Automata (QCA). The bistable QCA cell, which can be used to represent two well defined states, is enhanced in a way that allows the representation of three logic values. The significance of such structures is presented through the study case of a simplified control system. \u00a9 2008 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "A tool for IT process construction", "Authors": ["Zvanut B.", "Bajec M."], "Keywords": ["IT process construction", "Method engineering", "Organisation-specific process", "Process engineering"], "Date": "2010", "Abstract": "Context: The field of IT processes lacks a scientifically-based tool that constructs organisation-specific IT processes according to the organisation's socio-technical characteristics. Objective: In this paper we propose a solution to this problem in the form of IT process engineering (ITPE). ITPE is based on established method engineering principles which we have adapted to IT process construction. Method: The tool was demonstrated by having three organisations use ITPE to each construct two IT processes. Results: ITPE provided useful guidance in all three cases. Conclusions: The study demonstrates that method engineering principles can be applied in research fields other than information system development. \u00a9 2009 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "9"},
{"Title": "Mesh partitioning with ant colonies Razdelitev mre\u017ee s kolonijami mravelj", "Authors": ["Korosec P.", "Silc J.", "Robic B."], "Keywords": ["Ant colony optimization", "Mesh partitioning", "Multilevel algorithm", "Vector quantization"], "Date": "2006", "Abstract": "Many real-world engineering problems can be expressed in terms of partial differential equations and solved by using the finite-element method, which is usually parallelized, i.e. the mesh is divided among several processors. To achieve high parallel efficiency it is important that the mesh is partitioned in such a way that workloads are well balanced and interprocessor communication is minimized. In this paper we present an enhancement of a technique that uses a nature-inspired metaheuristic approach to achieve higher-quality partitions. We present two heuristic mesh-partitioning methods, both of which build on the multiple ant-colony algorithm in order to improve the quality of the mesh partitions. The first method augments the multiple ant-colony algorithm with a multilevel paradigm, whereas the second uses the multiple ant-colony algorithm as a refinement to the initial partition obtained by vector quantization. The two methods are experimentally compared with the well-known mesh-partitioning programs, p-METIS and Chaco.", "Language": "en", "Citations": "1"},
{"Title": "Constructivist learning environment in a cloud", "Authors": ["Rugelj J.", "Ciglaric M.", "Krevl A.", "Pancur M.", "Brodnik A."], "Keywords": [], "Date": "2012", "Abstract": "The paper presents a development of web-based learning environment for constructivist learning in higher education. The main focus in the design was to take into account recent findings of pedagogical research and availability of new technologies in order to create efficient and effective learning support for the engineering students. The central component of the environment is a virtual laboratory, which is defined as a service that can be used in a cloud - LaaS (Laboratory as a Service). The paper also presents our experience with the environment used in Computer Science classes with over 700 students who experienced active forms of learning, collaboration and appropriate feedback. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "4"},
{"Title": "Strong traces model of self-assembly polypeptide structures", "Authors": ["Fijavz G.", "Pisanski T.", "Rus J."], "Keywords": [], "Date": "2014", "Abstract": "A novel self-assembly strategy for polypeptide nanostructure design was presented in [Design of a single-chain polypeptide tetrahedron assembled from coiled-coil segments, Nature Chemical Biology 9 (2013) 362-366]. The first mathematical model (polypeptide nanostructure can naturally be presented as a skeleton graph of a polyhedron) from [Stable traces as a model for self-assembly of polypeptide nanoscale polyhedrons, MATCH Commun. Math. Comput. Chem. 70 (2013) 317-330] introduced stable traces as the appropriate mathematical description, yet we find them deficient in modeling graphs with either very small (\u2264 2) or large (\u2265 6) degree vertices. We introduce strong traces which remedy both of the above mentioned drawbacks. We show that every connected graph admits a strong trace by studying a connection between strong traces and graph embeddings. Further we also characterize graphs which admit parallel (resp. antiparallel) strong.", "Language": "en", "Citations": "16"},
{"Title": "Knowledge-based bioinformatics for the study of mammalian oocytes", "Authors": ["Mulas F.", "Sacchi L.", "Zagar L.", "Garagna S.", "Zuccotti M.", "Zupan B.", "Bellazzi R."], "Keywords": ["Database", "Knowledge extraction", "Oocyte", "Stem cell"], "Date": "2012", "Abstract": "Bioinformatics tools have been recently applied to study the differentiation of the mammalian oocyte during folliculogenesis. In this review, we will summarize our knowledge of 1) the use of biological databases for the extraction of relevant information, 2) bioinformatics methods for knowledge extraction and representation, 3) the application of these methods to the study of mammalian oocyte differentiation and 4) state-of the-art prediction approaches for the assessment and estimation of the cell differentiation status. \u00a9 2013 UBC Press.", "Language": "en", "Citations": "1"},
{"Title": "Impact of learning on the structural properties of neural networks", "Authors": ["Ster B.", "Gabrijel I.", "Dobnikar A."], "Keywords": [], "Date": "2007", "Abstract": "We research the impact of the learning process of neural networks (NN) on the structural properties of the derived graphs. A type of recurrent neural network is used (GARNN). A graph is derived from a NN by defining a connection between any pair od nodes having weights in both directions above a certain threshold. We measured structural properties of graphs such as characteristic path lengths (L), clustering coefficients (C) and degree distributions (P). We found that well trained networks differ from badly trained ones in both L and C. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "3"},
{"Title": "The minor crossing number", "Authors": ["Bokal D.", "Fijavz G.", "Mohar B."], "Keywords": ["Crossing number", "Graph minor"], "Date": "2006", "Abstract": "The minor crossing number of a graph G is defined as the minimum crossing number of all graphs that contain G as a minor. Basic properties of this new invariant are presented. We study topological structure of graphs with bounded minor crossing number and obtain a new strong version of a lower bound based on the genus. We also give a. generalization of an inequality of Moreno and Salazar crossing numbers of a graph and its minors. \u00a9 2006 Society for Industrial and Applied Mathematics.", "Language": "en", "Citations": "29"},
{"Title": "Evaluating comparisons and evaluations of learning management systems", "Authors": ["Kljun M.", "Vicic J.", "Kavsek B.", "Kavcic A."], "Keywords": ["Comparison", "Evaluating criteria", "Evaluation methods", "LMS"], "Date": "2007", "Abstract": "In the last years, numerous papers were published comparing different learning management systems (LMS). Some of them dealt with only few comparison criteria, while others included almost every imaginable feature. When faced to do a comparison ourselves, we came across many of such papers and did a research of what authors considered relevant in an LMS. By comparing papers written in different years, we tried to find out if there is a pattern of features linked to a certain time period, how a demand for new features was evolving through time, and how did LMS developers respond to this demand. We also tried to figure out the present demands and which new features will be included in future versions of LMSs.", "Language": "en", "Citations": "20"},
{"Title": "Implementation of a training-model simulator with free tools Izdelava simulatorja u nega sistema s prosto dostopnimi orodji", "Authors": ["Ilc N.", "Lotri U."], "Keywords": [], "Date": "2018", "Abstract": "One of the important modern concepts in production optimization is implementation and usage of digital twins - virtual copies of physical setups. There are many tools on the market targeting at the digital twin concept, but are payable and for small projects it is di cult to justify the initial investment. For the needs of a faculty course, we developed our own simulator with a 3D visualization of physical training models based on the Unity game engine. The simulator imitates the physical devices well enough to enable students developing e cient programs for industrial controllers and practicing integration with higher-level systems. The simulator greatly alleviates the work on project tasks and contributes to more accomplished final projects. The tools and procedures used to create the simulator present a good base for the development of more complex simulators of other training models or even industrial systems.", "Language": "en", "Citations": "0"},
{"Title": "Improving job scheduling in GRID environments with use of simple machine learning methods", "Authors": ["Vladusic D.", "Cernivec A.", "Slivnik B."], "Keywords": [], "Date": "2009", "Abstract": "This paper presents an attempt to improve job scheduling over heterogeneous GRID nodes by employing machine learning methods. Our proposed architecture takes into account the fact that GRID frameworks and their modules are not easy to modify or re-implement. It is therefore our aim to provide a plug-in which can be easily added to existing frameworks, thus avoiding significant and time-consuming modifications. Furthermore, we assume that existing scheduling algorithm in the framework should not be completely overridden, but rather modified only if there are chances, based on historical data, that the modification will yield a better result. Finally, we focus on use of off-the-shelf simple machine learning methods in a black-box manner with internal parameter optimization. We present three experiments within a simulated environment, performed with synthetic data aimed at congestion of the system. The results show that improvements over the simple scheduling algorithms can be made. \u00a9 2009 IEEE.", "Language": "en", "Citations": "7"},
{"Title": "VillageLink: Wide-area wireless coverage", "Authors": ["Pejovic V.", "Johnson D.L.", "Zheleva M.", "Belding E.M.", "Lysko A."], "Keywords": [], "Date": "2014", "Abstract": "White spaces promise to revolutionize the way wireless connectivity is delivered over wide areas. However, large-scale white space networks face the problem of allocating channels to multiple contending users in the wide white space band. To tackle the issue, we first examine wireless propagation in a long-distance outdoor white space testbed and find that a complex combination of free-space loss and antenna effects impacts transmission in white spaces. Thus, a need arises for a strategy that goes beyond simple channel utilization balancing, and uses frequency probing to profile channels according to their propagation properties. We devise VillageLink, a Gibbs sampling-based method that optimizes channel allocation in a distributed manner with a minimum number of channel switching events. Through extensive simulations we demonstrate that VillageLink results in a significant capacity improvement over alternative solutions. \u00a9 2014 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "A new application model for mobile technologies", "Authors": ["Rupnik R.", "Krisper M.", "Bajec M."], "Keywords": ["Context-aware mobile applications", "Context-awareness", "Mobile applications", "Mobile business", "Mobility", "Workflow"], "Date": "2004", "Abstract": "The information society demands higher and higher productivity from people, which they indeed can achieve by using services and access information in the state of mobility. Mobile applications represent new application models that enable information support in the state of mobility. First, we discuss mobile applications in the context of information society, define their scope and limitations. Then we introduce a classical mobile application model, context, context-awareness, and context-aware mobile application model. In order to show the potential of information support in the state of mobility we then discuss types of mobile applications and emphasise the significance of workflow concept for mobile applications.", "Language": "en", "Citations": "4"},
{"Title": "Facilitating ontology development with continuous evaluation", "Authors": ["Lavbic D.", "Krisper M."], "Keywords": ["ontology completeness", "ontology development methodology", "ontology evaluation", "rapid ontology development", "semantic web"], "Date": "2010", "Abstract": "In this paper we propose facilitating ontology development by constant evaluation of steps in the process of ontology development. Existing methodologies for ontology development are complex and they require technical knowledge that business users and developers don't poses. By introducing ontology completeness indicator developer is guided throughout the development process and constantly aided by recommendations to progress to next step and improve the quality of ontology. In evaluating the ontology, several aspects are considered; from description, partition, consistency, redundancy and to anomaly. The applicability of the approach was demonstrated on Financial Instruments and Trading Strategies (FITS) ontology with comparison to other approaches. \u00a9 2010 Vilnius University.", "Language": "en", "Citations": "9"},
{"Title": "Modelling impacts of cropping systems: Demands and solutions for DEX methodology", "Authors": ["Znidarsic M.", "Bohanec M.", "Zupan B."], "Keywords": ["Decision analysis", "Multiple criteria analysis", "OR in agriculture", "Software", "Uncertainty modelling"], "Date": "2007", "Abstract": "Decision modelling of diverse groups of problems makes different requirements to the modelling methodologies and software. We present an actual decision problem and the required characteristics of corresponding decision models. The problem is from agronomy and addresses the ecological and economic impacts of cropping systems, with the focus on the differences between cropping systems with conventional crops and the ones with genetically modified crops. We describe the extensions of an existing DEX qualitative multi-attribute modelling methodology, which were made to cope with the challenges of the problem. The extensions address general hierarchical structures, probabilistic utility functions and numerical values of basic attributes. A new, freely available software tool called proDEX was implemented to support the extended methodology. In this paper we describe the problem of cropping system assessment, propose methodological extensions to DEX, and present the implementation of proDEX. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "21"},
{"Title": "Leaps and lulls in the developmental transcriptome of Dictyostelium discoideum", "Authors": ["Rosengarten R.D.", "Santhanam B.", "Fuller D.", "Katoh-Kurasawa M.", "Loomis W.F.", "Zupan B.", "Shaulsky G."], "Keywords": ["Development", "Dictyostelium discoideum", "Differential expression", "Principal component analysis", "Slime mold", "Synchrony", "Time course", "Transcriptome"], "Date": "2015", "Abstract": "Background: Development of the soil amoeba Dictyostelium discoideum is triggered by starvation. When placed on a solid substrate, the starving solitary amoebae cease growth, communicate via extracellular cAMP, aggregate by tens of thousands and develop into multicellular organisms. Early phases of the developmental program are often studied in cells starved in suspension while cAMP is provided exogenously. Previous studies revealed massive shifts in the transcriptome under both developmental conditions and a close relationship between gene expression and morphogenesis, but were limited by the sampling frequency and the resolution of the methods. Results: Here, we combine the superior depth and specificity of RNA-seq-based analysis of mRNA abundance with high frequency sampling during filter development and cAMP pulsing in suspension. We found that the developmental transcriptome exhibits mostly gradual changes interspersed by a few instances of large shifts. For each time point we treated the entire transcriptome as single phenotype, and were able to characterize development as groups of similar time points separated by gaps. The grouped time points represented gradual changes in mRNA abundance, or molecular phenotype, and the gaps represented times during which many genes are differentially expressed rapidly, and thus the phenotype changes dramatically. Comparing developmental experiments revealed that gene expression in filter developed cells lagged behind those treated with exogenous cAMP in suspension. The high sampling frequency revealed many genes whose regulation is reproducibly more complex than indicated by previous studies. Gene Ontology enrichment analysis suggested that the transition to multicellularity coincided with rapid accumulation of transcripts associated with DNA processes and mitosis. Later development included the up-regulation of organic signaling molecules and co-factor biosynthesis. Our analysis also demonstrated a high level of synchrony among the developing structures throughout development. Conclusions: Our data describe D. discoideum development as a series of coordinated cellular and multicellular activities. Coordination occurred within fields of aggregating cells and among multicellular bodies, such as mounds or migratory slugs that experience both cell-cell contact and various soluble signaling regimes. These time courses, sampled at the highest temporal resolution to date in this system, provide a comprehensive resource for studies of developmental gene expression.", "Language": "en", "Citations": "24"},
{"Title": "Human skeleton model based dynamic features for walking speed invariant gait recognition", "Authors": ["Kovac J.", "Peer P."], "Keywords": [], "Date": "2014", "Abstract": "Humans are able to recognize small number of people they know well by the way they walk. This ability represents basic motivation for using human gait as the means for biometric identification. Such biometrics can be captured at public places from a distance without subject's collaboration, awareness, and even consent. Although current approaches give encouraging results, we are still far from effective use in real-life applications. In general, methods set various constraints to circumvent the influence of covariate factors like changes of walking speed, view, clothing, footwear, and object carrying, that have negative impact on recognition performance. In this paper we propose a skeleton model based gait recognition system focusing on modelling gait dynamics and eliminating the influence of subjects appearance on recognition. Furthermore, we tackle the problem of walking speed variation and propose space transformation and feature fusion that mitigates its influence on recognition performance. With the evaluation on OU-ISIR gait dataset, we demonstrate state of the art performance of proposed methods. \u00a9 2014 Jure Kova\u010d and Peter Peer.", "Language": "en", "Citations": "20"},
{"Title": "Wide-angle camera distortions and non-uniform illumination in mobile robot tracking", "Authors": ["Klancar G.", "Kristan M.", "Karba R."], "Keywords": ["Camera calibration", "Computer vision", "Mobile robots tracking", "Non-uniform illumination correction"], "Date": "2003", "Abstract": "In this paper some fundamentals and solutions to accompanying problems in vision system design for mobile robot tracking are presented. The main topics are correction of camera lens distortion and compensation of non-uniform illumination. Both correction methods contribute to vision system performance if implemented in the appropriate manner. Their applicability is demonstrated by applying them to vision for robot soccer. The lens correction method successfully corrects the distortion caused by the camera lens, thus achieving a more accurate and precise estimation of object position. The illumination compensation improves robustness to irregular and non-uniform illumination that is nearly always present in real conditions. \u00a9 2003 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "39"},
{"Title": "Efficiently explaining decisions of probabilistic RBF classification networks", "Authors": ["Robnik-Sikonja M.", "Likas A.", "Constantinopoulos C.", "Kononenko I.", "Strumbelj E."], "Keywords": ["classification explanation", "comprehensibility", "game theory", "model explanation", "model visualization", "probabilistic RBF networks"], "Date": "2011", "Abstract": "For many important practical applications model transparency is an important requirement. A probabilistic radial basis function (PRBF) network is an effective non-linear classifier, but similarly to most other neural network models it is not straightforward to obtain explanations for its decisions. Recently two general methods for explaining of a model's decisions for individual instances have been introduced which are based on the decomposition of a model's prediction into contributions of each attribute. By exploiting the marginalization property of the Gaussian distribution, we show that PRBF is especially suitable for these explanation techniques. By explaining the PRBF's decisions for new unlabeled cases we demonstrate resulting methods and accompany presentation with visualization technique that works both for single instances as well as for the attributes and their values, thus providing a valuable tool for inspection of the otherwise opaque models. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "2"},
{"Title": "Forgetting early estimates in Monte Carlo control methods", "Authors": ["Vodopivec T.", "Ster B."], "Keywords": ["Decision problem", "Monte Carlo control", "Monte Carlo tree search", "On-line learning", "On-policy Monte Carlo control", "Reinforcement learning", "Upper confidence bounds for trees"], "Date": "2015", "Abstract": "Monte Carlo algorithms are one of the three main reinforcement learning paradigms that are capable of efficiently solving control and decision problems in dynamic environments. Through sampling they shape the values of states in the search space. Based on these values they develop an exploration policy that is in turn used to guide the future direction of sampling. Studies confirm the convergence of this interleaving iterative approach to an optimal solution; however, when a learning agent lacks prior knowledge of the problem domain, the convergence rate may be extremely slow in case of an erroneous staring policy that causes far-from-optimal value estimates. In this paper we present a brief overview of Monte Carlo control algorithms in the scope of reinforcement learning and propose a method to improve the convergence by gradually forgetting early estimates. Our method keeps track of the state values with a moving average that gives a higher weight to the recent rewards and discounts the weight of the previous rewards, while assuming that the policy is improving over time. We apply it to the general on-policy Monte Carlo control algorithm and to the popular upper confidence bounds for trees algorithm in the Monte Carlo tree search framework. The evaluation on several decision problems confirms that our method regularly improves the convergence rate of both algorithms and in some cases also their final policy.", "Language": "en", "Citations": "0"},
{"Title": "A graphical model for rapid obstacle Image-Map estimation from unmanned surface vehicles", "Authors": ["Kristan M.", "Pers J.", "Sulic V.", "Kovacic S."], "Keywords": [], "Date": "2015", "Abstract": "Obstacle detection plays an important role in unmanned surface vehicles (USV). Continuous detection from images taken onboard the vessel poses a particular challenge due to the diversity of the environment and the obstacle appearance. An obstacle may be a floating piece of wood, a scuba diver, a pier, or some other part of a shoreline. In this paper we tackle this problem by proposing a new graphical model that affords a fast and continuous obstacle image-map estimation from a single video stream captured onboard a USV. The model accounts for the semantic structure of marine environment as observed from USV by imposing weak structural constraints. A Markov random field framework is adopted and a highly efficient algorithm for simultaneous optimization of model parameters and segmentation mask estimation is derived. Our approach does not require computationally intensive extraction of texture features and runs faster than real-time. We also present a new, challenging, dataset for segmentation and obstacle detection in marine environments, which is the largest annotated dataset of its kind. Results on this dataset show that our model compares favorably in accuracy to the related approaches, requiring a fraction of computational effort.", "Language": "en", "Citations": "6"},
{"Title": "A system for speaker detection and tracking in audio broadcast news", "Authors": ["Zibert J.", "Vesnicer B.", "Mihelic F."], "Keywords": ["Audio indexing", "Speaker clustering", "Speaker diarization", "Speaker recognition", "Speaker tracking", "Speech detection"], "Date": "2008", "Abstract": "A system for speaker-based audio-indexing and an application for speaker-tracking in broadcast news audio are presented. The process of producing an indexing information in continuous audio streams based on detected speakers is composed of several tasks and is therefore treated as a multistage process. The main building blocks of such an indexing system include components for an audio segmentation, a speech detection, a speaker clustering and a speaker identification. We give an overview of each component of the system with emphasis to the approaches that are followed in each stage of building of our speaker-diarization and tracking system. The proposed system is evaluated on the audio data from the broadcast news domain, whereas we test each of the system's component and measure their impacts to the overall system's performance. The evaluation results indicate the importance of an audio segmentation and a speech detection module to the reliable performance of the whole system. Based on an indexing information produced by our system we also developed an application for searching target speakers in broadcast news. The application is designed in a way to be user-friendly and can be easily integrated in various computer environments.", "Language": "en", "Citations": "6"},
{"Title": "Computer analysis of world chess champions", "Authors": ["Guid M.", "Bratko I."], "Keywords": [], "Date": "2006", "Abstract": "Who is the best chess player of all time? Chess players are often interested in this question that has never been answered authoritatively, because it requires a comparison between chess players of different eras who never met across the board. In this contribution, we attempt to make such a comparison. It is based on the evaluation of the games played by the World Chess Champions in their championship matches. The evaluation is performed by the chess-playing program CRAFTY. For this purpose we slightly adapted CRAFTY. Our analysis takes into account the differences in players' styles to compensate the fact that calm positional players in their typical games have less chance to commit gross tactical errors than aggressive tactical players. Therefore, we designed a method to assess the difficulty of positions. Some of the results of this computer analysis might be quite surprising. Overall, the results can be nicely interpreted by a chess expert.", "Language": "en", "Citations": "26"},
{"Title": "A mid-level representation for melody-based retrieval in audio collections", "Authors": ["Marolt M."], "Keywords": ["Audio collections", "Information retrieval", "Melody", "Music"], "Date": "2008", "Abstract": "Searching audio collections using high-level musical descriptors is a difficult problem, due to the lack of reliable methods for extracting melody, harmony, rhythm, and other such descriptors from unstructured audio signals. In this paper, we present a novel approach to melody-based retrieval in audio collections. Our approach supports audio, as well as symbolic queries and ranks results according to melodic similarity to the query. We introduce a beat-synchronous melodic representation consisting of salient melodic lines, which are extracted from the analyzed audio signal. We propose the use of a 2-D shift-invariant transform to extract shift-invariant melodic fragments from the melodic representation and demonstrate how such fragments can be indexed and stored in a song database. An efficient search algorithm based on locality-sensitive hashing is used to perform retrieval according to similarity of melodic fragments. On the cover song detection task, good results are achieved for audio, as well as for symbolic queries, while fast retrieval performance makes the proposed system suitable for retrieval in large databases. \u00a9 2006 IEEE.", "Language": "en", "Citations": "31"},
{"Title": "Assessing teamwork in a software engineering capstone course", "Authors": ["Pozenel M."], "Keywords": [], "Date": "2013", "Abstract": "The quality of teamwork within a software engineering capstone course requiring students to develop a quasi-real project following the Scrum agile software development methodology was analysed in this study. The analysis is based on an instrument recommended in the literature that addresses key characteristics of teamwork and presents them along five dimensions: shared leadership, team orientation, redundancy, learning and autonomy. The results of 33 student teams who attended the courses Software Engineering (Bachelor level) and Modern Methods of Software Development (Master level) in the academic year 2011/2012 are presented and compared to results from industry reported in the literature. The comparison revealed a fair degree of similarity between students and professional developers. Both rated the highest, learning and shared leadership, while autonomy received the lowest grade. However, professional developers were much more uniform in their opinions than were students \u00a9 2013 WIETE.", "Language": "en", "Citations": "2"},
{"Title": "Discriminative Correlation Filter Tracker with Channel and Spatial Reliability", "Authors": ["Lukezic A.", "Vojir T.", "CehovinZajc L.", "Matas J.", "Kristan M."], "Keywords": ["Channel reliability", "Constrained optimization", "Correlation filters", "Visual tracking"], "Date": "2018", "Abstract": "Short-term tracking is an open and challenging problem for which discriminative correlation filters (DCF) have shown excellent performance. We introduce the channel and spatial reliability concepts to DCF tracking and provide a learning algorithm for its efficient and seamless integration in the filter update and the tracking process. The spatial reliability map adjusts the filter support to the part of the object suitable for tracking. This both allows to enlarge the search region and improves tracking of non-rectangular objects. Reliability scores reflect channel-wise quality of the learned filters and are used as feature weighting coefficients in localization. Experimentally, with only two simple standard feature sets, HoGs and colornames, the novel CSR-DCF method\u2014DCF with channel and spatial reliability\u2014achieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF runs close to real-time on a CPU.", "Language": "en", "Citations": "17"},
{"Title": "SWT voting-based color reduction method for detecting text in natural scene images", "Authors": ["Ikica A."], "Keywords": [], "Date": "2014", "Abstract": "In our PhD thesis [1] we give a very detailed and in-depth survey of natural scene text detection methods and propose two novel methods, namely SWT (Stroke Width Transform) voting-based color reduction method [2] and SWT direction determination method [2]. SWT voting-based color reduction method (to which we will refer also as SWT-V) is a novel text detection method that - opposed to many other text detection methods - combines both structural and color information in order to detect text. The proposed method upgrades the text detection oriented color reduction method (to which we will refer to as TOCR) [3] with the additional SWT voting stage and substantially outperforms other state-of-the-art text detection methods. All the image colors rich with SWT pixels [4] that most likely belong to text characters are blocked from being mean-shifted away in the color reduction process. One of the disadvantages of the SWT method [4], however, is the problem of 'light text on the dark background' described in the following paragraphs. To cope with the problem and in order to provide true SWT values to the SWT voting stage we propose an adaptive SWT direction determination method. The method uses SWT profiles to partition an image into subblocks and analyses their SWT histograms of both SWT search directions [2]. Text detection literature does not explicitly address the SWT direction issue, therefore, the proposed method represents a unique scientific contribution to the research field. All text detection methods were evaluated on the CVL OCR DB text detection evaluation dataset [2, 5].", "Language": "en", "Citations": "1"},
{"Title": "Visual information abstraction for interactive robot learning", "Authors": ["Zhou K.", "Richtsfeld A.", "Zillich M.", "Vincze M.", "Vreclen A.", "Skocaj D."], "Keywords": [], "Date": "2011", "Abstract": "Semantic visual perception for knowledge acquisition plays an important role in human cognition, as well as in the learning process of any cognitive robot. In this paper, we present a visual information abstraction mechanism designed for continuously learning robotic systems. We generate spatial information in the scene by considering plane estimation and stereo line detection coherently within a unified probabilistic framework, and show how spaces of interest (SOIs) are generated and segmented using the spatial information. We also demonstrate how the existence of SOIs is validated in the long-term learning process. The proposed mechanism facilitates robust visual information abstraction which is a requirement for continuous interactive learning. Experiments demonstrate that with the refined spatial information, our approach provides accurate and plausible representation of visual objects. \u00a9 2011 IEEE.", "Language": "en", "Citations": "9"},
{"Title": "Load-balanced parallel solver for the minimum k-center and related problems", "Authors": ["Cernivec A.", "Vladusic D.", "Slivnik B."], "Keywords": ["Generating combinations", "Load balancing", "Minimum k-center"], "Date": "2009", "Abstract": "A load-balanced exact solver for computing the exact solutions of minimum k-center and related facility locations problems, is described. To achieve the load balance on a dedicated multiprocessor system, i.e., a cluster or a supercomputer, a new algorithm for parallel generation of a set of all k-combinations of n-things (without repetitions) is introduced and analysed. We demonstrate that the new algorithm can also be used in a resource competitive environment if used or supplemented with a simple adaptive job scheduler. The solver is tested by producing the benchmarks for the minimum k-center problem.", "Language": "en", "Citations": "1"},
{"Title": "TACO: a novel method for trust rating subjectivity elimination based on Trust Attitudes COmparison", "Authors": ["Zupancic E.", "Juric M.B."], "Keywords": ["E-commerce", "Personalization", "Rating systems", "Reputation", "Subjectivity", "Trust"], "Date": "2015", "Abstract": "Trust ratings shared by users in electronic commerce environments are subjective as trust evaluation depends on evaluators\u2019 personal disposition to trust. As such, aggregation of shared trust ratings to compute a user\u2019s reputation may be questionable without proper consideration of rating subjectivity. Although the problem of subjectivity in trust opinions has already been recognized, it has not been adequately resolved so far. In this paper, we address the problem of proper trust rating analysis and aggregation, which includes elimination of subjectivity. We propose a novel method based on Trust Attitudes COmparison (TACO method), which derives adjusted reputations compliant with the behavioral patterns of the evaluators and eliminates the subjectivity from the trust ratings. With the TACO method, all participants have comparable opportunities to choose trustworthy transaction partners, regardless of their trust dispositions. The TACO method finds the users with similar trust attitudes, taking advantage of nonparametric statistical methods. After that, it computes the personalized reputation scores of other users with the aggregation of trust values shared by users with similar trust attitudes. The method derives the characteristics of participants\u2019 trust dispositions implicitly from their past ratings and does not request them to disclose any part of their trust evaluation process, such as motivating criteria for trust assessments, underlying beliefs, or criteria preferences. We have evaluated the performance of our method with extensive simulations with varying numbers of users, different numbers of available trust ratings, and with different distributions of users\u2019 personalities. The results showed significant improvements using our TACO method with an average improvement of 50.0\u00a0% over the Abdul-Rahman and 72.9\u00a0% over the Hasan method.", "Language": "en", "Citations": "4"},
{"Title": "Matrix Fej\u00e9r\u2013Riesz theorem with gaps", "Authors": ["Zalar A."], "Keywords": ["13J30", "14P10", "47A56"], "Date": "2016", "Abstract": "The matrix Fej\u00e9r\u2013Riesz theorem characterizes positive semidefinite matrix polynomials on the real line R. We extend a characterization to arbitrary closed semialgebraic sets K\u2286R by the use of matrix preorderings from real algebraic geometry. In the compact case a denominator-free characterization exists, while in the non-compact case there are counterexamples. However, there is a weaker characterization with denominators in the non-compact case. At the end we extend the results to algebraic curves.", "Language": "en", "Citations": "0"},
{"Title": "Learning process termination criteria", "Authors": ["Brumen B.", "Holbl M.", "Harej Pulko K.", "Welzer T.", "Hericko M.", "Juric M.B.", "Jaakkola H."], "Keywords": ["Accuracy", "Assessment", "Classification", "Data mining", "Learning curve", "Learning process"], "Date": "2012", "Abstract": "In a supervised learning, the relationship between the available data and the performance (what is learnt) is not well understood. How much data to use, or when to stop the learning process, are the key questions. In the paper, we present an approach for an early assessment of the extracted knowledge (classification models) in the terms of performance (accuracy). The key questions are answered by detecting the point of convergence, i.e., where the classification model's performance does not improve any more even when adding more data items to the learning set. For the learning process termination criteria we developed a set of equations for detection of the convergence that follow the basic principles of the learning curve. The developed solution was evaluated on real datasets. The results of the experiment prove that the solution is well-designed: the learning process stopping criteria are not subjected to local variance and the convergence is detected where it actually has occurred. \u00a9 2012 Vilnius Universit.", "Language": "en", "Citations": "8"},
{"Title": "Statistical comparisons of classifiers in machine learning Statisti\u010dne primerjave klasifikatorjev pri strojnem u\u010denju", "Authors": ["Demsar J."], "Keywords": ["Artificial intelligence", "Machine learning", "Statistical tests"], "Date": "2006", "Abstract": "Comparisons between classifiers are a crucial element in most studies that introduce new machine learning algorithms or modifications of the existing ones. Despite their importance, there is no consensus in the community regarding which test should be applied in a certain situation, and excellent machine learning papers quite often conclude with statistical tests that are conceptually or statistically inappropriate. The situation is especially bad in comparisons of multiple classifiers, where the tests designed for comparisons of two samples are often used on each pair of classifiers instead of using omnibus tests like ANOVA or at least applying the appropriate corrections for multiple hypotheses testing. We analyzed the tests which are or which should (in our opinion) be used in machine learning studies: the paired t-test, the Wilcoxon signed-ranks test and the sign test for comparison of two classifiers, and ANOVA and the Friedman test with appropriate post-hoc tests for comparisons of multiple classifiers [10]. We checked what the tests really measure and what assumptions they make about the data; specifically, the parametric tests require commensurability of the results accross different domains and assume that the results of classifiers are distributed normally. Since both of these conditions for the use of parametric tests are most probably violated, we dissuade from using the parametric tests. On the other hand, the described non-parametric tests suffer from none of these deficiencies. The same conclusion in favour of non-parametric tests is reached in the experimental part of the paper where we compare the tests on a selection of standard machine learning algorithms using the data sets from the UCI machine learning repository [1]. The non-parametric tests seem to have more power and be more replicable than the parametric ones both in comparisons between two (Fig. 1) and between multiple classifiers (Fig. 3), with the only exception of the Dunnet test for post-hoc comparisons of one classifier against all others, which rejects a somewhat larger number of hypotheses than the corresponding non-parametric alternative. Altogether, we recommend the use of non-parametric tests for comparisons of classifiers, but warn that other criteria beyond the grasp of statistics should be considered and possibly even favoured over the pure improvements in predictive power of classifiers.", "Language": "en", "Citations": "0"},
{"Title": "Parallelization of ant system for GPU under the PRAM model", "Authors": ["Brodnik A.", "Grgurovic M."], "Keywords": ["Ant system", "Combinatorial optimization", "Graphics processing unit", "Metaheuristics", "Parallel random access machine", "Traveling salesman problem"], "Date": "2018", "Abstract": "\n                                                         We study the parallelized ant system algorithm solving the traveling salesman problem on n cities. First, following the series of recent results for the graphics processing unit, we show that they translate to the PRAM (parallel random access machine) model. In addition, we develop a novel pheromone matrix update method under the PRAM CREW (concurrent-read exclusive-write) model and translate it to the graphics processing unit without atomic instructions. As a consequence, we give new asymptotic bounds for the parallel ant system, resulting in step complexities 0(n lglgn) on CRCW (concurrent-read concurrent-write) and O(nlgn) on CREW variants of PRAM using n                             \n                            ", "Language": "en", "Citations": "0"},
{"Title": "An integrated system for interactive continuous learning of categorical knowledge", "Authors": ["Skocaj D.", "Vrecko A.", "Mahnic M.", "Janicek M.", "Kruijff G.-J.M.", "Hanheide M.", "Hawes N.", "Wyatt J.L.", "Keller T.", "Zhou K.", "Zillich M.", "Kristan M."], "Keywords": ["Cognitive system", "extrospection", "interactive learning", "introspection", "knowledge gap detection", "motive management"], "Date": "2016", "Abstract": "This article presents an integrated robot system capable of interactive learning in dialogue with a human. Such a system needs to have several competencies and must be able to process different types of representations. In this article, we describe a collection of mechanisms that enable integration of heterogeneous competencies in a principled way. Central to our design is the creation of beliefs from visual and linguistic information, and the use of these beliefs for planning system behaviour to satisfy internal drives. The system is able to detect gaps in its knowledge and to plan and execute actions that provide information needed to fill these gaps. We propose a hierarchy of mechanisms which are capable of engaging in different kinds of learning interactions, e.g. those initiated by a tutor or by the system itself. We present the theory these mechanisms are build upon and an instantiation of this theory in the form of an integrated robot system. We demonstrate the operation of the system in the case of learning conceptual models of objects and their visual properties.", "Language": "en", "Citations": "1"},
{"Title": "Integration of machine learning insights into organizational learning: A case of B2B sales forecasting", "Authors": ["Bohanec M.", "Borstnar M.K.", "Robnik-Sikonja M."], "Keywords": ["B2B sales modeling", "Forecasting error reduction", "Knowledge engineering", "Machine learning", "Organizational learning", "Visual data mining"], "Date": "2015", "Abstract": "Business to Business (B2B) sales forecast can be described as a decision-making process, which is based on past data (internal and external), formalized rules, subjective judgment, and tacit organizational knowledge. Its consequences are measured in profit and loss. The research focus of this paper is aimed to narrow the gap between planned and realized performance, introducing a novel model based on machine learning techniques. Preliminary results of machine learning model performance are presented, with focus on distilled visualizations that create powerful, yet human comprehensible and actionable insights, enabling positive climate for reflection and contributing to continuous organizational learning.", "Language": "en", "Citations": "0"},
{"Title": "G-complexes with a compatible CW structure", "Authors": ["Cencelj M.", "Kosta N.M.", "Vavpetic A."], "Keywords": [], "Date": "2003", "Abstract": "If G is a toral group, i.e. an extension of a torus by a finite group, and X is a G-CW complex we prove that there exists a G-homotopy equivalent CW complex Y with the property that the action map \u03c1: G \u00d7 Y \u2192 Y is a cellular map.", "Language": "en", "Citations": "1"},
{"Title": "Tracking business rule evolution to support is maintenance", "Authors": ["Bajec M.", "Krisper M.", "Rupnik R."], "Keywords": ["Business rules", "Enterprise modelling"], "Date": "2003", "Abstract": "Business rules are an important business category as they describe how enterprises are doing business. Their ability to make applications flexible and amendable to change makes them attractive also within the information system domain. In the paper we argue that if business rules are tracked from their source to their implementation this can help in IS maintenance. We introduce the business rule management scenario for managing business rules in enterprises. The scenario advocates using business rule management as an interface between business modelling and IS development and maintenance.", "Language": "en", "Citations": "1"},
{"Title": "Logarithmic Arithmetic for Low-Power Adaptive Control Systems", "Authors": ["Lotric U.", "Bulic P."], "Keywords": ["Adaptive control systems", "Approximate arithmetic", "FPGA", "Kalman filter", "Low-power design"], "Date": "2017", "Abstract": "To reduce the power dissipation in adaptive control systems, we propose replacing the exact arithmetic hardware units with approximate ones. As a case study, an adaptive control system for object tracking based on the Kalman filter is implemented in FPGA. A thorough analysis of the Kalman filter\u2019s circuitry for real-world object tracks acquired by an aviation radar system proved that adaptive control systems can successfully compensate for the calculation errors introduced by the approximate arithmetic units. The main contributions of this paper are that the introduction of the approximate arithmetic circuits to the adaptive control system (1) preserves the required accuracy and (2) significantly reduces the power dissipation and the size of the adaptive system\u2019s circuitry.", "Language": "en", "Citations": "0"},
{"Title": "Testing reliability of medical software", "Authors": ["Podgorelec V.", "Hericko M.", "Juric M.B.", "Rozman I."], "Keywords": [], "Date": "2002", "Abstract": "In the paper we present some important aspects of software testing. We explain why the software reliability is important, especially for the medical software products. The automation of testing procedures is important to achieve the desired reliability level. For this purpose intelligent systems can be used to identify the potentially dangerous software modules. We present evolutionary decision trees approach to the identification of dangerous software modules in a real-world medical application.", "Language": "en", "Citations": "1"},
{"Title": "Learning qualitative models from numerical data", "Authors": ["Zabkar J.", "Mozina M.", "Bratko I.", "Demsar J."], "Keywords": [], "Date": "2013", "Abstract": "Qualitative models are predictive models that describe how changes in values of input variables affect the output variable in qualitative terms, e.g. increasing or decreasing. We describe Pad\u00e9, a new method for qualitative learning which estimates partial derivatives of the target function from training data and uses them to induce qualitative models of the target function. We formulated three methods for computation of derivatives, all based on using linear regression on local neighbourhoods. The methods were empirically tested on artificial and real-world data. We also provide a case study which shows how the developed methods can be used in practice.", "Language": "en", "Citations": "1"},
{"Title": "Rotary polygons in configurations", "Authors": ["Boben M.", "Miklavic S.", "Potocnik P."], "Keywords": [], "Date": "2011", "Abstract": "A polygon A in a configuration C is called rotary if C admits an automorphism which acts upon A as a one-step rotation. We study rotary polygons and their orbits under the group of automorphisms (and antimorphisms) of C. We determine the number of such orbits for several symmetry types of rotary polygons in the case when C is flag-transitive. As an example, we provide tables of flag-transitive (v", "Language": "en", "Citations": "1"},
{"Title": "Evaluation on home storage performance of table grape based on sensory quality and consumers\u2019 satisfaction", "Authors": ["Ma C.", "Fu Z.", "Xu M.", "Trebar M.", "Zhang X."], "Keywords": ["Consumer satisfaction", "Home storage", "Quality control", "Sensory evaluation", "Table grape"], "Date": "2016", "Abstract": "With continuous rise of table grapes consumption and increased public awareness of food safety, the quality control of grapes in storage after purchase is not sufficiently examined. Home storage constitutes the last and important stage in grape supply chain. Literature review shows that few researches on grape quality focus on the home storage stage compared with numerous researches reported on the quality control during postharvest and transportation process. This paper reports the performance evaluation of grape quality at home storage and consumers\u2019 satisfaction using integrated sensory evaluations. The internal attributes, including Texture, Taste and Odor of the table grapes and the appearance indices, Color and Cleanliness are examined. Key results show that during home storage, all the internal attributes decrease rapidly as time goes on, and cleanliness and color appear to be deteriorating in a lower speed. A comprehensive quality index was created to measure the quality of table grape which has high correlation with the Overall acceptability perceived by consumers.", "Language": "en", "Citations": "8"},
{"Title": "Explaining classifications for individual instances", "Authors": ["Robnik-Sikonja M.", "Kononenko I."], "Keywords": ["Classification", "Decision support", "Decision visualization", "Information visualization", "Knowledge modeling", "Machine learning", "Model comprehensibility", "Model explanation", "Nearest eighbor", "Neural nets", "Prediction models", "Support vector machines"], "Date": "2008", "Abstract": "We present a method for explaining predictions for individual instances. The presented approach is general and can be used with all classification models that output probabilities. It is based on the decomposition of a model's predictions on individual contributions of each attribute. Our method works for the so-called black box models such as support vector machines, neural networks, and nearest neighbor algorithms, as well as for ensemble methods such as boosting and random forests. We demonstrate that the generated explanations closely follow the learned models and present a visualization technique that shows the utility of our approach and enables the comparison of different prediction methods. \u00a9 2007 IEEE.", "Language": "en", "Citations": "67"},
{"Title": "Speeding-up text categorization in a GRID computing environment", "Authors": ["Silva C.", "Ribeiro B.", "Lotric U."], "Keywords": [], "Date": "2005", "Abstract": "The amount of texts available in digital form has dramatically increased, giving rise to the need of fast text classifiers. The tasks involved can be parallelized and distributed in a GRID environment. This paper reports a study conducted on Reuters-21578 corpus, using a SVM learning machine. The task of text categorization is distributed in several platforms. The results achieved are very promising for speeding-up text categorization tasks and are valid independently of the learning machine. \u00a9 2005 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Parkinsonian signs in patients with cervical dystonia treated with pallidal deep brain stimulation", "Authors": ["Mahlknecht P.", "Georgiev D.", "Akram H.", "Brugger F.", "Vinke S.", "Zrinzo L.", "Hariz M.", "Bhatia K.P.", "Hariz G.-M.", "Willeit P.", "Rothwell J.C.", "Foltynie T.", "Limousin P."], "Keywords": ["bradykinesia", "deep brain stimulation", "dystonia", "gait", "movement disorders: imaging"], "Date": "2018", "Abstract": "Pallidal deep brain stimulation is an established treatment in patients with dystonia. However, evidence from case series or uncontrolled studies suggests that it may lead in some patients to specific parkinsonian symptoms such as freezing of gait, micrographia, and bradykinesia. We investigated parkinsonian signs using the Movement Disorder Society Unified Parkinson's Disease Rating Scale motor score by means of observer-blinded video ratings in a group of 29 patients treated with pallidal stimulation and a non-surgical control group of 22 patients, both with predominant cervical dystonia. Additional assessments included MRI-based models of volume of neural tissue activated to investigate areas of stimulation related to dystonic symptom control and those likely to induce parkinsonian signs as well as an EMG analysis to investigate functional vicinity of stimulation fields to the pyramidal tract. Compared with controls, stimulated patients had significantly higher motor scores (median, 25th-75th percentile: 14.0, 8.0-19.5 versus 3.0, 2.0-8.0; P < 0.0001), as well as bradykinesia (8.0, 6.0-14.0 versus 2.0, 0.0-3.0; P < 0.0001) and axial motor subscores (2.0, 1.0-4.0 versus 0.0, 0.0-1.0; P = 0.0002), while rigidity and tremor subscores were not different between groups. Parkinsonian signs were partially reversible upon switching stimulation off for a median of 90 min in a subset of 19 patients tolerating this condition. Furthermore, the stimulation group reported more features of freezing of gait on a questionnaire basis. Quality of life was better in stimulated patients compared with control patients, but parkinsonian signs were negatively associated with quality of life. In the descriptive imaging analysis maximum efficacy for dystonia improvement projected to the posteroventrolateral internal pallidum with overlapping clusters driving severity of bradykinesia and axial motor symptoms. The severities of parkinsonian signs were not correlated with functional vicinity to the pyramidal tract as assessed by EMG. In conclusion, parkinsonian signs, particularly bradykinesia and axial motor signs, due to pallidal stimulation in dystonic patients are frequent and negatively impact on motor functioning and quality of life. Therefore, patients with pallidal stimulation should be monitored closely for such signs both in clinical routine and future clinical trials. Spread of current outside the internal pallidum is an unlikely explanation for this phenomenon, which seems to be caused by stimulation of neural elements within the stimulation target volume.", "Language": "en", "Citations": "0"},
{"Title": "RFID from Farm to Fork: Traceability along the complete food chain", "Authors": ["Cuinas I.", "Catarinucci L.", "Trebar M."], "Keywords": [], "Date": "2011", "Abstract": "The project \"RFID from Farm to Fork\" looks for the extension of RFID technologies along the complete food chain: from the farms where cows, fishes, sheep, grapes, etc. grow; to the final consumer at the supermarkets, including all intermediate stages: transports, factory processes, storage. The paper is intended to show the project objectives and concerns, as well as it highlights the main radio propagation problems detected within a RFID system installed in a food factory. The paper also shows a proposal of using RFID traceability in different study cases.", "Language": "en", "Citations": "13"},
{"Title": "Graphs that allow all the eigenvalue multiplicities to be even", "Authors": ["Oblak P.", "Smigoc H."], "Keywords": ["Eigenvalue", "Graph", "Maximum multiplicity", "Symmetric matrix"], "Date": "2014", "Abstract": "Let G be an undirected graph on n vertices and let S(G) be the set of all n\u00d7n real symmetric matrices whose nonzero off-diagonal entries occur in exactly the positions corresponding to the edges of G. The Inverse Eigenvalue Problem for a graph G is a problem of determining all possible lists that can occur as the lists of eigenvalues of matrices in S(G). This question is, in general, hard to answer and several variations were studied, most notably the minimum rank problem. In this paper we introduce the problem of determining for which graphs G there exists a matrix in S(G) whose characteristic polynomial is a square, i.e. the multiplicities of all its eigenvalues are even. We solve this question for several families of graphs. \u00a9 2014 Elsevier Inc.", "Language": "en", "Citations": "1"},
{"Title": "Comparison of approaches for estimating reliability of individual regression predictions", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": ["Prediction accuracy", "Prediction error", "Regression", "Reliability estimate", "Sensitivity analysis"], "Date": "2008", "Abstract": "The paper compares different approaches to estimate the reliability of individual predictions in regression. We compare the sensitivity-based reliability estimates developed in our previous work with four approaches found in the literature: variance of bagged models, local cross-validation, density estimation, and local modeling. By combining pairs of individual estimates, we compose a combined estimate that performs better than the individual estimates. We tested the estimates by running data from 28 domains through eight regression models: regression trees, linear regression, neural networks, bagging, support vector machines, locally weighted regression, random forests, and generalized additive model. The results demonstrate the potential of a sensitivity-based estimate, as well as the local modeling of prediction error with regression trees. Among the tested approaches, the best average performance was achieved by estimation using the bagging variance approach, which achieved the best performance with neural networks, bagging and locally weighted regression. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "44"},
{"Title": "Identification of RNA-binding domains of RNA-binding proteins in cultured cells on a system-wide scale with RBDmap", "Authors": ["Castello A.", "Frese C.K.", "Fischer B.", "Jarvelin A.I.", "Horos R.", "Alleaume A.-M.", "Foehr S.", "Curk T.", "Krijgsveld J.", "Hentze M.W."], "Keywords": [], "Date": "2017", "Abstract": "RBDmap is a method for identifying, in a proteome-wide manner, the regions of RNA-binding proteins (RBPs) engaged in native interactions with RNA. In brief, cells are irradiated with UV light to induce proteinRNA cross-links. Following stringent denaturing washes, the resulting covalently linked proteinRNA complexes are purified with oligo(dT) magnetic beads. After elution, RBPs are subjected to partial proteolysis, in which the protein regions still bound to the RNA and those released to the supernatant are separated by a second oligo(dT) selection. After sample preparation and mass-spectrometric analysis, peptide intensity ratios between the RNA-bound and released fractions are used to determine the RNA-binding regions. As a Protocol Extension, this article describes an adaptation of an existing Protocol and offers additional applications. The earlier protocol (for the RNA interactome capture method) describes how to identify the active RBPs in cultured cells, whereas this Protocol Extension also enables the identification of the RNA-binding domains of RBPs. The experimental workflow takes 1 week plus 2 additional weeks for proteomics and data analysis. Notably, RBDmap presents numerous advantages over classic methods for determining RNA-binding domains: it produces proteome-wide, high-resolution maps of the protein regions contacting the RNA in a physiological context and can be adapted to different biological systems and conditions. Because RBDmap relies on the isolation of polyadenylated RNA via oligo(dT), it will not provide RNA-binding information on proteins interacting exclusively with nonpolyadenylated transcripts. Applied to HeLa cells, RBDmap uncovered 1,174 RNA-binding sites in 529 proteins, many of which were previously unknown.", "Language": "en", "Citations": "8"},
{"Title": "Extremal graphs with respect to vertex betweenness centrality for certain graph families", "Authors": ["Klisara J.", "Hurajova J.C.", "Madaras T.", "Skrekovski R."], "Keywords": ["Betweenness centrality", "Connectivity", "Degree", "Diameter", "Extremal value"], "Date": "2016", "Abstract": "The betweenness centrality of a vertex in a graph is the sum of relative numbers of shortest paths that pass through that vertex. We study extremal values of vertex betweenness within various families of graphs. We prove that, in the family of 2-connected (resp. 3-connected) graphs on n vertices, the maximum betweenness value is reached for the maximum degree vertex of the fan graph F", "Language": "en", "Citations": "3"},
{"Title": "Event-related potentials and cognition in Parkinson's disease: An integrative review", "Authors": ["Seer C.", "Lange F.", "Georgiev D.", "Jahanshahi M.", "Kopp B."], "Keywords": ["Basal ganglia", "Cognition", "Dementia", "Dopamine", "Event-related potentials (ERPs)", "Executive function", "MMN", "N2", "Ne/ERN", "NoGo-P3", "P3", "P3a", "P3b", "Parkinson's disease"], "Date": "2016", "Abstract": "Cognitive impairment is a common non-motor symptom of Parkinson's disease (PD), but the nature of cognitive changes varies considerably between individuals. According to the dual-syndrome hypothesis, one cluster of patients is characterized by deficits in executive function that may be related to fronto-striatal dysfunction. Other patients primarily show non-frontal cognitive impairments that progress rapidly to PD dementia (PDD). We provide a comprehensive review of event-related potential (ERP) studies to identify ERP measures substantiating the heterogeneity of cognitive impairment in PD. Our review revealed evidence for P3b and mismatch-negativity alterations in PDD, but not in non-demented PD, indicating that alterations of these ERPs constitute electrophysiological markers for PDD. In contrast, ERP correlates of executive functions, such as NoGo-P3, N2, and error(-related) negativity (N", "Language": "en", "Citations": "24"},
{"Title": "Decision making matters: A better way to evaluate trust models", "Authors": ["Jelenc D.", "Hermoso R.", "Sabater-Mir J.", "Trcek D."], "Keywords": ["Evaluation", "Multi-agent system", "Reputation", "Testbed", "Trust"], "Date": "2013", "Abstract": "Trust models are mechanisms that predict behavior of potential interaction partners. They have been proposed in several domains and many advances in trust formation have been made recently. The question of comparing trust models, however, is still without a clear answer. Traditionally, authors set up ad hoc experiments and present evaluation results that are difficult to compare - sometimes even interpret - in the context of other trust models. As a solution, the community came up with common evaluation platforms, called trust testbeds. In this paper we expose shortcomings of evaluation models that existing testbeds use; they evaluate trust models by combining them with some ad hoc decision making mechanism and then evaluate the quality of trust-based decisions. They assume that if all trust models use the same decision making mechanism, the mechanism itself becomes irrelevant for the evaluation. We hypothesized that the choice of decision making mechanism is in fact relevant. To test our claim we built a testbed, called Alpha testbed, that can evaluate trust models either with or without decision making mechanism. With it we evaluated five well-known trust models using two different decision making mechanisms. The results confirm our hypothesis; the choice of decision making mechanisms influences the performance of trust models. Based on our findings, we recommend to evaluate trust models independently of the decision making mechanism - and we also provide a method (and a tool) to do so. \u00a9 2013 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "24"},
{"Title": "Nonexistence of some Antipodal Distance-regular Graphs of Diameter Four", "Authors": ["Jurisic A.", "Koolen J."], "Keywords": [], "Date": "2000", "Abstract": "We find an inequality involving the eigenvalues of a regular graph; equality holds if and only if the graph is strongly regular. We apply this inequality to the first subconstituents of a distance-regular graph and obtain a simple proof of the fundamental bound for distance-regular graphs, discovered by Juri\u0161i\u0107, Koolen and Terwilliger. Using this we show that for distance-regular graphs with certain intersection arrays, the first subconstituent graphs are strongly regular. From these results we prove the nonexistence of distance-regular graphs associated to 20 feasible intersection arrays from the book Distance-Regular Graphs by Brouwer, Cohen and Neumaier [3]. \u00a9 2000 Academic Press.", "Language": "en", "Citations": "15"},
{"Title": "Separating sets of term and pre-term uterine EMG records", "Authors": ["Smrdel A.", "Jager F."], "Keywords": ["Adaptive autoregressive method", "Pre-term labor prediction", "Sample entropy", "Term-Preterm EHG database", "Uterine electromyogram"], "Date": "2015", "Abstract": "The analysis of uterine EMG (electrohysterogram - EHG) records may help solve the problem of predicting pre-term labor. We investigated the adaptive autoregressive (AAR) method to estimate the EHG signal spectrograms and sample entropy, to separate and classify sets of term and pre-term delivery records, using the Term-Preterm EHG Database. The database contains four sets of records divided according to the time of delivery (term or preterm: \u226437 or < 37 weeks of gestation, respectively) and according to the time of recording (early or later: before or after the 26", "Language": "en", "Citations": "10"},
{"Title": "An FPGA-based integrated environment for computer architecture", "Authors": ["Bulic P.", "Gustin V.", "Sonc D.", "Strancar A."], "Keywords": ["computer architecture", "computer architecture education", "computer organization", "development board", "FPGAs"], "Date": "2013", "Abstract": "We present a new, integrated environment used in computer-architecture education. The environment consists of a hardware platform and GUI software running on a PC. The hardware platform is entirely implemented in Xilinx Spartan-3 FPGA. The main part of the hardware platform is a 32-bit pipelined RISC processor with a trace/debug unit. This trace/debug unit is a hardware unit that enables debugging and transfers the pipeline contents to the PC. It also enables communication between the GUI application on the PC and the microprocessor core. Such a system makes it possible to download the students' programs to the FPGA-based microprocessor and graphically depicts the processor's internal state on the PC. \u00a9 2010 Wiley Periodicals, Inc. Comput Appl Eng Educ 21: 26-35, 2013 Copyright \u00a9 2010 Wiley Periodicals, Inc.", "Language": "en", "Citations": "14"},
{"Title": "Symmetry-Compressible Graphs", "Authors": ["Cibej U.", "Mihelic J."], "Keywords": ["graph automorphisms", "graph compression", "graphlet compression", "symmetries"], "Date": "2017", "Abstract": "This article describes an alternative representation of graphs, using symmetries. We define the class of graphs that are compressed using this representation as symmetry-compressible graphs. This class of graphs is extended into the class of near symmetry-compressible graphs, which includes many more graphs arising in practical applications. To demonstrate the practical potential of the proposed concepts, an empirical evaluation of two algorithms is given.", "Language": "en", "Citations": "0"},
{"Title": "Planar graphs without cycles of specific lengths", "Authors": ["Fijavz G.", "Juvan M.", "Mohar B.", "Skrekovski R."], "Keywords": [], "Date": "2002", "Abstract": "It is easy to see that planar graphs without 3-cycles are 3-degenerate. Recently, it was proved that planar graphs without 5-cycles are also 3-degenerate. In this paper it is shown, more surprisingly, that the same holds for planar graphs without 6-cycles. \u00a9 2002 Elsevier Science Ltd. All rights reserved.", "Language": "en", "Citations": "29"},
{"Title": "Decision-making framework with double-loop learning through interpretable black-box machine learning models", "Authors": ["Bohanec M.", "Robnik-Sikonja M.", "Borstnar M.K."], "Keywords": ["B2B sales forecasting", "Double-loop learning", "Explanation of black-box models", "Machine learning"], "Date": "2017", "Abstract": "Purpose - The purpose of this paper is to address the problem of weak acceptance of machine learning (ML) models in business. The proposed framework of top-performing ML models coupled with general explanation methods provides additional information to the decision-making process. This builds a foundation for sustainable organizational learning. Design/methodology/approach - To address user acceptance, participatory approach of action design research (ADR) was chosen. The proposed framework is demonstrated on a B2B sales forecasting process in an organizational setting, following cross-industry standard process for data mining (CRISP-DM) methodology. Findings - The provided ML model explanations efficiently support business decision makers, reduce forecasting error for new sales opportunities, and facilitate discussion about the context of opportunities in the sales team. Research limitations/implications - The quality and quantity of available data affect the performance of models and explanations. Practical implications - The application in the real-world company demonstrates the utility of the approach and provides evidence that transparent explanations of ML models contribute to individual and organizational learning. Social implications - All used methods are available as an open-source software and can improve the acceptance of ML in data-driven decision making. Originality/value - The proposed framework incorporates existing ML models and general explanation methodology into a decision-making process. To the authors' knowledge, this is the first attempt to support organizational learning with a framework combining ML explanations, ADR, and data mining methodology based on the CRISP-DM industry standard.", "Language": "en", "Citations": "4"},
{"Title": "Formal Quality of Service assurances, ranking and verification of cloud deployment options with a probabilistic model checking method", "Authors": ["Kochovski P.", "Drobintsev P.D.", "Stankovski V."], "Keywords": ["Cloud", "Decision-making", "Edge", "Equivalence classes", "Fog", "Probabilistic model checking", "Software engineering"], "Date": "2019", "Abstract": "Context: Existing software workbenches allow for the deployment of cloud applications across a variety of Infrastructure-as-a-Service (IaaS) providers. The expected workload, Quality of Service (QoS) and Non-Functional Requirements (NFRs) must be considered before an appropriate infrastructure is selected. However, this decision-making process is complex and time-consuming. Moreover, the software engineer needs assurances that the selected infrastructure will lead to an adequate QoS of the application. Objective: The goal is to develop a new method for selection of an optimal cloud deployment option, that is, an infrastructure and configuration for deployment and to verify that all hard and as many soft QoS requirements as possible will be met at runtime. Method: A new Formal QoS Assurances Method (FoQoSAM), which relies on stochastic Markov models is introduced to facilitate an automated decision-making process. For a given workload, it uses QoS monitoring data and a user-related metric in order to automatically generate a probabilistic model. The probabilistic model takes the form of a finite automaton. It is further used to produce a rank list of cloud deployment options. As a result, any of the cloud deployment options can be verified by applying a probabilistic model checking approach. Results: Testing was performed by ranking deployment options for two cloud applications, File Upload and Video-conferencing. The FoQoSAM method was compared to a baseline Analytic Hierarchy Process (AHP). The results show that the first ranked cloud deployment options satisfy all hard and at least one of the soft requirements for both methods, however, the FoQoSAM method always satisfies at least an additional QoS requirement compared to the baseline AHP method. Conclusions: The proposed new FoQoSAM method is appropriate and can be used in decision-making when ranking and verifying cloud deployment options. Due to its practical utility it was integrated into the SWITCH workbench.", "Language": "en", "Citations": "2"},
{"Title": "Online discriminative kernel density estimation", "Authors": ["Kristan M.", "Leonardis A."], "Keywords": ["Discriminative models", "Kernel density estimation", "Online estimation"], "Date": "2010", "Abstract": "We propose a new method for online estimation of probabilistic discriminative models. The method is based on the recently proposed online Kernel Density Estimation (oKDE) framework which produces Gaussian mixture models and allows adaptation using only a single data point at a time. The oKDE builds reconstructive models from the data, and we extend it to take into account the interclass discrimination through a new distance function between the classifiers. We arrive at an online discriminative Kernel Density Estimator (odKDE). We compare the odKDE to oKDE, batch state-of-the-art KDEs and support vector machine (SVM) on a standard database. The odKDE achieves comparable classification performance to that of best batch KDEs and SVM, while allowing online adaptation, and produces models of lower complexity than the oKDE. \u00a9 2010 IEEE.", "Language": "en", "Citations": "9"},
{"Title": "Network monitoring applications based on IoT system", "Authors": ["Kos A.", "Sedlar U.", "Sterle J.", "Volk M.", "Bester J.", "Bajec M."], "Keywords": ["correlation", "DSL", "IoT", "IPTV", "lightning", "network monitoring", "visualisation"], "Date": "2013", "Abstract": "We present applications for network monitoring based on intelligent communication platform that can also be used to support various usage scenarios related to future internet of things. Applications presented include real time DSL access line monitoring and IPTV monitoring, correlated with lightning reports. The solution is used in the field of proactive monitoring, enabling the operator's helpdesk and field technical teams to pinpoint the cause of service degradations. \u00a9 2013 IEEE.", "Language": "en", "Citations": "4"},
{"Title": "Characterization and automatic classification of preterm and term uterine records", "Authors": ["Jager F.", "Libensek S.", "Gersak K."], "Keywords": [], "Date": "2018", "Abstract": "Predicting preterm birth is uncertain, and numerous scientists are searching for noninvasive methods to improve its predictability. Current researches are based on the analysis of ElectroHysteroGram (EHG) records, which contain information about the electrophysiological properties of the uterine muscle and uterine contractions. Since pregnancy is a long process, we decided to also characterize, for the first time, non-contraction intervals (dummy intervals) of the uterine records, i.e., EHG signals accompanied by a simultaneously recorded external tocogram measuring mechanical uterine activity (TOCO signal). For this purpose, we developed a new set of uterine records, TPEHGT DS, containing preterm and term uterine records of pregnant women, and uterine records of non-pregnant women. We quantitatively characterized contraction intervals (contractions) and dummy intervals of the uterine records of the TPEHGT DS in terms of the normalized power spectra of the EHG and TOCO signals, and developed a new method for predicting preterm birth. The results on the characterization revealed that the peak amplitudes of the normalized power spectra of the EHG and TOCO signals of the contraction and dummy intervals in the frequency band 1.0-2.2 Hz, describing the electrical and mechanical activity of the uterus due to the maternal heart (maternal heart rate), are high only during term pregnancies, when the delivery is still far away; and they are low when the delivery is close. However, these peak amplitudes are also low during preterm pregnancies, when the delivery is still supposed to be far away (thus suggesting the danger of preterm birth); and they are also low or barely present for non-pregnant women. We propose the values of the peak amplitudes of the normalized power spectra due to the influence of the maternal heart, in an elec-tro-mechanical sense, in the frequency band 1.0-2.2 Hz as a new biophysical marker for the preliminary, or early, assessment of the danger of preterm birth. The classification of preterm and term, contraction and dummy intervals of the TPEHGT DS, for the task of the automatic prediction of preterm birth, using sample entropy, the median frequency of the power spectra, and the peak amplitude of the normalized power spectra, revealed that the dummy intervals provide quite comparable and slightly higher classification performances than these features obtained from the contraction intervals. This result suggests a novel and simple clinical technique, not necessarily to seek contraction intervals but using the dummy intervals, for the early assessment of the danger of preterm birth. Using the publicly available TPEHG DB database to predict preterm birth in terms of classifying between preterm and term EHG records, the proposed method outperformed all currently existing methods. The achieved classification accuracy was 100% for early records, recorded around the 23rd week of pregnancy; and 96.33%, the area under the curve of 99.44%, for all records of the database. Since the proposed method is capable of using the dummy intervals with high classification accuracy, it is also suitable for clinical use very early during pregnancy, around the 23rd week of pregnancy, when contractions may or may not be present.", "Language": "en", "Citations": "1"},
{"Title": "Teaching scrum through team-project work: Students' perceptions and teacher's observations", "Authors": ["Mahnic V."], "Keywords": ["Agile methods", "Capstone project", "Scrum", "Software engineering education"], "Date": "2010", "Abstract": "In order to prepare students for the increasing use of agile methods in industry, teaching these methods is becoming an important part of the Computer Science and Software Engineering curricula. So far most of the attention has been devoted to Extreme Programming and its practices, but there is not much reported about teaching Scrum, in spile of the fact that Scrum is one of the most widespread agile methods. To fill this gap, a course was developed at the University of Ljubljana that not only teaches Scrum through a capstone project, but also serves as a study about the learnability and applicability of Scrum. This paper describes the course details and analyses students' perceptions and teachers' observations after running the course for the first time in the Spring semester of the Academic Year 2008109. The student surveys showed that students were overwhelmingly positive about the course and confirmed the anecdotal evidence of Scrum's benefits as reported in the literature. \u00a9 2010 TEMPUS Publications.", "Language": "en", "Citations": "43"},
{"Title": "Reservoir sampling techniques in modern data analysis", "Authors": ["Pecar A.", "Zidar M.", "Kukar M."], "Keywords": ["AUC maximization", "Data streams", "Frequent itemsets", "Online learning", "Reservoir sampling"], "Date": "2012", "Abstract": "Reservoir sampling is an interesting statistical sampling technique, developed almost 40 years ago in order to enable analysis of large scale data (for that time) while utilizing limited computer memory resources. We present an overview of frequently used reservoir sampling techniques and discuss how they can be used for learning from data streams. While they are not perfect for all scenarios, they can easily be modified for many purpose, and also find place in surprisingly useful modern data analysis approaches. Copyright 2012 ACM.", "Language": "en", "Citations": "0"},
{"Title": "Cold chain and shelf life prediction of refrigerated fish \u2013 From farm to table", "Authors": ["Trebar M."], "Keywords": ["Cold chain", "Fish supply chain", "Prediction", "Shelf life"], "Date": "2018", "Abstract": "Fresh perishables are normally stored and distributed with a proper cold chain control in the supply chain from farm to retail. Usually, the consumers break the cold chain after the point of sale. The question is whether consumers are aware of requirements during the transport to and storage at home. The handling conditions and temperature changes can significantly decrease the shelf life and cause faster spoilage of food. The study presents two examples of shelf life prediction. The first one is based on temperature measurements of fish covered with ice in a Styrofoam box with supported information of environment temperatures in the cold store, uncooled car and refrigerator. In the second, measurements from first phase of storage on temperatures (0 \u00b0C\u20134 \u00b0C) were used with assumption of fish stored later on higher temperatures without ice. The results show important shortening of shelf life after the point of sale.", "Language": "en", "Citations": "1"},
{"Title": "Robust visual tracking using template anchors", "Authors": ["Cehovin L.", "Leonardis A.", "Kristan M."], "Keywords": [], "Date": "2016", "Abstract": "Deformable part models exhibit excellent performance in tracking non-rigidly deforming targets, but are usually outperformed by holistic models when the target does not deform or in the presence of uncertain visual data. The reason is that part-based models require estimation of a larger number of parameters compared to holistic models and since the updating process is self-supervised, the errors in parameter estimation are amplified with time, leading to a faster accuracy reduction than in holistic models. On the other hand, the robustness of part-based trackers is generally greater than in holistic trackers. We address the problem of self-supervised estimation of a large number of parameters by introducing controlled graduation in estimation of the free parameters. We propose decomposing the visual model into several sub-models, each describing the target at a different level of detail. The sub-models interact during target localization and, depending on the visual uncertainty, serve for cross-sub-model supervised updating. A new tracker is proposed based on this model which exhibits the qualities of part-based as well as holistic models. The tracker is tested on the highly-challenging VOT2013 and VOT2014 benchmarks, outperforming the state-of-the-art.", "Language": "en", "Citations": "15"},
{"Title": "Hyperlinking reality via camera phones", "Authors": ["Omercevic D.", "Leonardis A."], "Keywords": ["Augmented reality", "Image matching using local invariant features", "Image-based localization", "Wide baseline stereo matching"], "Date": "2011", "Abstract": "A novel user interface concept for camera phones, called \"Hyperlinking Reality via Camera Phones\", that we present in this article, provides a solution to one of the main challenges facing mobile user interfaces, that is, the problem of selection and visualization of actions that are relevant to the user in her current context. Instead of typing keywords on a small and inconvenient keypad of a mobile device, a user of our system just snaps a photo of her surroundings and objects in the image become hyperlinks to information. Our method commences by matching a query image to reference panoramas depicting the same scene that were collected and annotated with information beforehand. Once the query image is related to the reference panoramas, we transfer the relevant information from the reference panoramas to the query image. By visualizing the information on the query image and displaying it on the camera phone's (multi-)touch screen, the query image augmented with hyperlinks allows the user intuitive access to information. \u00a9 Springer-Verlag 2010.", "Language": "en", "Citations": "2"},
{"Title": "Fast spot hypothesizer for 2-DE research", "Authors": ["Peer P.", "Corzo L.G."], "Keywords": ["Image analysis", "Segmentation", "Two-Dimensional gel electrophoresis"], "Date": "2007", "Abstract": "Two-dimensional gel electrophoresis (2-DE) images show the expression levels of several hundred of proteins where each protein is represented as a blob shaped spot of grey level values. The spot detection, i.e. segmentation process has to be efficient as it is the first step in the gel processing. Such extraction of information is a very complex task. In this paper we propose a real time spot detector that is basically a morphology based method with use of seeded region growing as a central paradigm and which relies on the spot correlation information. The method is tested on gels with human samples in SWISS-2DPAGE (two-dimensional poly-acrylamide gel electrophoresis) database. The average time to process the image is less than a second, while the results are very intuitive for human perception and as such they help the user to focus on important parts of the gel in the subsequent processing. In gels with less than 50 identified spots as proteins (proteins that compose a proteome) in the mentioned database, the algorithm detects all obvious spots.", "Language": "en", "Citations": "0"},
{"Title": "The Total Graphs of Finite Rings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Finite ring", "Total graph", "Zero-divisor"], "Date": "2015", "Abstract": "In this paper we extend the study of total graphs \u03c4(R) to noncommutative finite rings R. We prove that \u03c4(R) is connected if and only if R is not local, and we see that in that case \u03c4(R) is always Hamiltonian. We also find an upper bound for the domination number of \u03c4(R) for all finite rings R.", "Language": "en", "Citations": "3"},
{"Title": "Towards the optimal minimization of a pronunciation dictionary model", "Authors": ["Dobrisek S.", "Zibert J.", "Mihelic F."], "Keywords": [], "Date": "2010", "Abstract": "This paper presents the results of our efforts to obtain the minimum possible finite-state representation of a pronunciation dictionary. Finite-state transducers are widely used to encode word pronunciations and our experiments revealed that the conventional redundancy-reduction algorithms developed within this framework yield suboptimal solutions. We found that the incremental construction and redundancy reduction of acyclic finite-state transducers creates considerably smaller models (up to 60%) than the conventional, non-incremental (batch) algorithms implemented in the OpenFST toolkit. \u00a9 2010 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "2"},
{"Title": "Physics based modelling of human motion using kalman filter and collision avoidance algorithm", "Authors": ["Perse M.", "Pers J.", "Kristan M.", "Kovacic S.", "Vuckovic G."], "Keywords": [], "Date": "2005", "Abstract": "The paper deals with the problem of computer vision based multi-person motion tracking, which in many cases suffers from lack of discriminating features of observed persons. To solve this problem, a physics based model of human motion is proposed, which includes intertial forces of the persons by the means of the Kalman filter, and the cylindrical envelopes, which produce collision avoiding forces when observed persons come to close proximity. We tested the proposed method on two sequences, one from squash match, and the other from the basketball play and found out that the number of tracker mistakes significantly decreased.", "Language": "en", "Citations": "10"},
{"Title": "Towards run-time verification of adaptive security for IoT in eHealth", "Authors": ["Torjusen A.B.", "Abie H.", "Paintsil E.", "Trcek D.", "Skomedal A."], "Keywords": ["Adaptive Security", "eHealth", "Formal Run-time Verification", "IoT"], "Date": "2014", "Abstract": "This paper integrates run-time verification enablers in the feedback adaptation loop of the ASSET adaptive security framework for Internet of Things (IoT) in the eHealth settings and instantiates the resulting framework with Colored Petri Nets. The run-time enablers make machine-readable formal models of a system state and context available at run-time. In addition, they make requirements that define the objectives of verification available at run-time as formal specifications and enable dynamic context monitoring and adaptation. Run-time adaptive behavior that deviates from the normal mode of operation of the system represents a major threat to the sustainability of critical eHealth services. Therefore, the integration of run-time enablers into the ASSET adaptive framework could lead to a sustainable security framework for IoT in eHealth. \u00a9 2014 ACM.", "Language": "en", "Citations": "13"},
{"Title": "Optimising the storage of multimedia educational material using genetic algorithm Optimizacija shranjevanja multimedijskega ucnega gradiva na magnetni trak z genetskim algoritmom", "Authors": ["Lesjak I.", "Leonardis A.", "Leban M.", "Kosir A."], "Keywords": [], "Date": "2000", "Abstract": "The multimedia educational material contains a large amount of data stored in various test and multimedia files. To store the material, the Massive Storage Device (MSD) can be employed. This article discusses how to achieve a minimal access time of MSD while reading the stored material.", "Language": "en", "Citations": "0"},
{"Title": "Latched recurrent neural network", "Authors": ["Ster B."], "Keywords": ["Finite state automata", "Latch", "Long-term dependencies", "Recurrent neural networks", "Temporal processing"], "Date": "2003", "Abstract": "An extended architecture of recurrent neural networks is proposed. It is based on ignoring unimportant input information using a register of latches as the input layer of the network. The latch is implemented with a multiplexer 2/1 whose output is differentiable with respect to all of its inputs, thus enabling the derivatives to be propagated through the network. The relevance of input vectors is learned together with the weights of the network using a gradient-based algorithm.", "Language": "en", "Citations": "5"},
{"Title": "Thesaurus of modern Slovene: By the community for the community", "Authors": ["Holdt S.A.", "Cibej J.", "Dobrovoljc K.", "Gantar P.", "Gorjanc V.", "Klemenc B.", "Kosem I.", "Krek S.", "Laskowski C.", "Robnik-Sikonja M."], "Keywords": ["Community", "Crowdsourcing", "Digital lexicography", "Responsive dictionary", "Slovene", "Thesaurus"], "Date": "2018", "Abstract": "By presenting the Thesaurus of Modern Slovene, the largest open-access collection of Slovene synonyms, this paper describes the concept of a responsive dictionary, a dictionary that allows its data to continuously respond to the changes in language and the feedback from the language community. We begin by briefly summarizing the method of its construction and its technical aspects. A great deal of deliberation and work has been put into interface design, with the aim to make the Thesaurus as user-friendly as possible for all digital media. This is followed by a more detailed description of the types of user input (e.g. synonym suggestions, synonym votes) and feedback (interface improvement suggestions) collected as part of development, as well as the methodology for their implementation. We also touch upon a series of dissemination activities aimed specifically at community building and user involvement. In conclusion, we describe our plans for the future, such as updates to be implemented in version 1.1 of the Thesaurus.", "Language": "en", "Citations": "0"},
{"Title": "Using smart cards as a secure storage for digitally signed documents", "Authors": ["Trampus M.", "Ciglaric M.", "Pancur M.", "Vidmar T.", "Krevl A.", "Rome P.", "Aksentic Z.", "Berginc G."], "Keywords": ["Digital signature", "Java Card", "Security", "Smart card"], "Date": "2003", "Abstract": "The paper presents an open application framework for secure storage and management of digitally signed documents on Java Cards. Its purpose is to encourage similar solutions in other target environments.", "Language": "en", "Citations": "1"},
{"Title": "The capstone course as a means for teaching agile software development through project-based learning", "Authors": ["Mahnic V."], "Keywords": [], "Date": "2015", "Abstract": "Since most of the core software engineering courses do not pay enough attention to agile software development, a software engineering capstone course represents an appropriate place for more in-depth treatment of this topic. In this article, the evolution of the software engineering capstone course at the University of Ljubljana is described since its conception in the 2008/09 academic year till now. The course requires students to develop a quasi-real project strictly following Scrum, which is the most widespread agile method. Additionally, the course design enables the conduct of studies that contribute to empirical evidence regarding agile processes. The article explains the reasons for introducing Scrum, presents the course design, describes some examples of empirical studies that were conducted within the course, and outlines the course upgrade with lean approaches to software development. Students' opinions about the course are overwhelmingly positive, indicating that the course is interesting and beneficial to their employability and professional career.", "Language": "en", "Citations": "6"},
{"Title": "Implementation and evaluation of algorithms with ALGATOr", "Authors": ["Dobravec T."], "Keywords": ["Automatic algorithm testing", "Empirical analysis", "Quality evaluation"], "Date": "2019", "Abstract": "In this paper we present an automatic algorithm evaluation system called ALGATOR, which was developed to facilitate the algorithm design and evaluation process. The system enables unbiased tests of the correctness of the algorithm\u2019s results on given test cases and comparisons of the quality of implemented algorithms for solving various kinds of problems (e.g. sorting data, matrix multiplication, traveler salesman problem, shortest path problem, and the like). Within the ALGATOR one can define a problem by specifying the problem descriptors, test sets with corresponding test cases, input parameters and output indicators, algorithm specifications and criteria for measuring the quality of algorithms. When a user of the system submits an algorithm for solving a given problem, ALGATOR automatically executes this algorithm on predefined tests, measures the quality indicators and prepares the results to be compared with the results of other algorithms in the system. The ALGATOR is meant to be used by algorithm developers to perform independent quality tests for their solutions.", "Language": "en", "Citations": "0"},
{"Title": "Evolutionary construction of emergent properties in multi-agent systems", "Authors": ["Privosnik M.", "Marolt M.", "Kavcic A.", "Divjak S."], "Keywords": ["Decentralization", "Emergence", "Evolutionary computation", "Multi-agent systems"], "Date": "2002", "Abstract": "Emergent behavior results in complex systems. Emergent behavior can be observed in various systems - a colony of ants, an economy, a brain or a large networks of computers - these are all systems with complex emergent behavior. The behavior of a complex system as a whole emerges in a highly nonlinear manner from the behaviors of the low level constituent elementary units. This makes traditional linear analysis difficult, if not impossible. In this paper we will focus on emergent properties of massive multi-agent systems. We will present how these properties can be utilized to model global behavior of the system.", "Language": "en", "Citations": "3"},
{"Title": "Is real-valued minimax pathological?", "Authors": ["Lustrek M.", "Gams M.", "Bratko I."], "Keywords": ["Chess", "Game playing", "Game tree", "Minimax", "Pathology", "Real value"], "Date": "2006", "Abstract": "Deeper searches in game-playing programs relying on the minimax principle generally produce better results. Theoretical analyses, however, suggest that in many cases minimaxing amplifies the noise introduced by the heuristic function used to evaluate the leaves of the game tree, leading to what is known as pathological behavior, where deeper searches produce worse results. In most minimax models analyzed in previous research, positions' true values and sometimes also heuristic values were only losses and wins. In contrast to this, a model is proposed in this paper that uses real numbers for both true and heuristic values. This model did not behave pathologically in the experiments performed. The mechanism that causes deeper searches to produce better evaluations is explained. A comparison with chess is made, indicating that the model realistically reflects position evaluations in chess-playing programs. Conditions under which the pathology might appear in a real-value model are also examined. The essential difference between our real-value model and the common two-value model, which causes the pathology in the two-value model, is identified. Most previous research reports that the pathology tends to disappear when there are dependences between the values of sibling nodes in a game tree. In this paper, another explanation is presented which indicates that in the two-value models the error of the heuristic evaluation was not modeled realistically. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "14"},
{"Title": "Irreducible (v3) configurations and graphs", "Authors": ["Boben M."], "Keywords": ["Configuration", "Cubic graph", "Incidence structure"], "Date": "2007", "Abstract": "Cubic bipartite graphs with girth at least 6 correspond to symmetric combinatorial (v", "Language": "en", "Citations": "4"},
{"Title": "Advanced traceability system in aquaculture supply chain", "Authors": ["Parreno-Marchante A.", "Alvarez-Melcon A.", "Trebar M.", "Filippin P."], "Keywords": ["Aquaculture", "Farmed fish", "RFID", "Supply chain", "Traceability", "WSN"], "Date": "2013", "Abstract": "The paper presents a novel traceability system architecture based on web services, which are used to integrate traceability data captured through Radio Frequency Identification (RFID) systems with environmental data collected with Wireless Sensor Networks (WSN) infrastructure. The solution, suitable to be deployed in Small to Medium Enterprises (SMEs), is provided by integrating information collected along the entire food supply chain, tracking the products from the farm to the consumer. The results of the deployment of the novel system in two pilots in the aquaculture business are also presented, showcasing how business processes in the aquaculture supply chain can be improved by the architecture and flexibility of the new system, since the two companies involved in the project are of very different sizes. Additionally, we present an analysis of the benefits obtained by the introduction of the new system in the companies based on predefined objectives and the evaluation of KPIs. The evaluation of KPIs is presented as the time reduction of activities and can improve the Efficiency of the companies in 89-95%. \u00a9 2013 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "50"},
{"Title": "The minor crossing number of graphs with an excluded minor", "Authors": ["Bokal D.", "Fijavz G.", "Wood D.R."], "Keywords": [], "Date": "2008", "Abstract": "The minor crossing number of a graph G is the minimum crossing number of a graph that contains G as a minor. It is proved that for every graph H there is a constant c, such that every graph G with no H-minor has minor crossing number at most c|V(G)|.", "Language": "en", "Citations": "7"},
{"Title": "A high-availability Bebras competition system", "Authors": ["Kristan N.", "Gostisa D.", "Fele-Zorz G.", "Brodnik A.", "Brodnik A."], "Keywords": [], "Date": "2014", "Abstract": "In this paper we present a new system that can be used for the Bebras and related competitions. The system supports both noninteractive tasks consisting of a question and multiple choice answers and standardized interactive tasks. It also contains a highly versatile administration interface permitting individual teachers/mentors to organize their own competitions (class-wide, school-wide,\u2026) that can be used in teaching of informatics. Furthermore, the system is also highly scalable and can be distributed across multiple servers. We have successfully evaluated the system in multiple competitions, some with over 10,000 students. Because it is designed to support i18n, it was easily localized and used in Slovenia and in Serbia.", "Language": "en", "Citations": "3"},
{"Title": "Information systems security and human behaviour", "Authors": ["Trcek D.", "Trobec R.", "Pavesic N.", "Tasic J.F."], "Keywords": ["Business dynamics", "Human behaviour", "Information systems", "Modeling and simulation", "Security policy"], "Date": "2007", "Abstract": "Until recently, most of the effort for providing security in information systems has been focused on technology. However, it turned out during the last years that human factors have played a central role. Therefore, to ensure appropriate security in contemporary information systems, it is necessary to address not only technology-related issues, but also human behaviour and organisation-related issues that are usually embodied in security policies. This paper presents a template model, which is intended to support risk management for information systems, and which is concentrated on human factors. The model is based on business dynamics that provide the means for qualitative and quantitative treatment of the above-mentioned issues.", "Language": "en", "Citations": "27"},
{"Title": "15 seconds of fame", "Authors": ["Solina F."], "Keywords": [], "Date": "2004", "Abstract": "15 seconds of fame is an interactive installation that every 15 seconds generates a new pop-art portrait of a randomly selected viewer. The installation was inspired by Andy Warhol's ironical statement that \"in the future everybody will be famous for 15 minutes.\" The installation detects human faces and crops them from the wide-angle view of people standing before the installation. Pop-art portraits are then generated by applying randomly selected filters to a randomly chosen face from the audience. These portraits are then shown in 15-second intervals on the flat-panel computer monitor, which is framed as a painting. \u00a92004 ISAST.", "Language": "en", "Citations": "11"},
{"Title": "Complete and reusable description of message structural constraints in web service interfaces", "Authors": ["Frece A.", "Juric M.B."], "Keywords": ["Reuse", "Service description", "Use case-specific structural constraints", "XML Schema (XSD)"], "Date": "2013", "Abstract": "Existing specifications for describing message structure as a part of web service description do not support use case-specific definition of structural constraints. We propose a solution to describe a complete set of structural constraints for a particular business object in all its use cases. To implement our solution we use XML Schema (XSD), de facto standard for description of web service message structure. We propose XSD extensions that realize two distinct and complementary approaches. Measurements have shown that by using our extensions the average complexity of real world schemas (XSD documents) comparing to expressional equivalent alternatives is smaller by ~ 29%. \u00a9 2012 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "1"},
{"Title": "Secure and interoperable communication infrastructures for PPDR organisations", "Authors": ["Muller W.", "Marques H.", "Pereira L.", "Rodriguez J.", "Brouwer F.", "Bouwers B.", "Politis I.", "Lykourgiotis A.", "Ladas A.", "Adigun O.", "Jelenc D."], "Keywords": ["Communication Infrastructures", "Enterprise Architecture", "Long Term Evolution (LTE)", "PPDR", "Public Safety", "Rivate Mobile Radio (PMR)", "Security", "TETRA", "TETRAPOL"], "Date": "2016", "Abstract": "The growing number of events affecting public safety and security (PS&S) on a regional scale with potential to grow up to large scale cross border disasters puts an increased pressure on agencies and organisation responsible for PS&S. In order to respond timely and in an adequate manner to such events, Public Protection and Disaster Relief (PPDR) organisations need to cooperate, align their procedures and activities, share the needed information and be interoperable. Existing PPDR/PMR technologies such as TETRA, TETRAPOL or P25, do not currently provide broadband capability nor is expected such technologies to be upgraded in the future. This presents a major limitation in supporting new services and information flows. Furthermore, there is no known standard that addresses interoperability of these technologies. In this contribution the design of a next generation communication infrastructure for PPDR organisations which fulfills the requirements of secure and seamless end-to-end communication and interoperable information exchange within the deployed communication networks is presented. Based on Enterprise Architecture of PPDR organisations, a next generation PPDR network that is backward compatible with legacy communication technologies is designed and implemented, capable of providing security, privacy, seamless mobility, QoS and reliability support for mission-critical Private Mobile Radio (PMR) voice and broadband data services. The designed solution provides a robust, reliable, and secure mobile broadband communications system for a wide variety of PMR applications and services on PPDR broadband networks, including the ability of inter-system, interagency and cross-border operations with emphasis on interoperability between users in PMR and LTE.", "Language": "en", "Citations": "2"},
{"Title": "Learning and explaining the impact of enterprises' organizational quality on their economic results", "Authors": ["Pregeljc M.", "Strumbelj E.", "Mihelcic M.", "Kononenko I."], "Keywords": [], "Date": "2012", "Abstract": "The authors employed traditional and novel machine learning to improve insight into the connections between the quality of an organization of enterprises as a type of formal social units and the results of enterprises' performance in this chapter. The analyzed data set contains 72 Slovenian enterprises' economic results across four years and indicators of their organizational quality. The authors hypothesize that a causal relationship exists between the latter and the former. In the first part of a two-part process, they use several classification algorithms to study these relationships and to evaluate how accurately they predict the target economic results. However, the most successful models were often very complex and difficult to interpret, especially for non-technical users. Therefore, in the second part, the authors take advantage of a novel general explanation method that can be used to explain the influence of individual features on the model's prediction. Results show that traditional machine-learning approaches are successful at modeling the dependency relationship. Furthermore, the explanation of the influence of the input features on the predicted economic results provides insights that have a meaningful economic interpretation. \u00a9 2012, IGI Global.", "Language": "en", "Citations": "4"},
{"Title": "Drifting concepts as hidden factors in clinical studies", "Authors": ["Kukar M."], "Keywords": ["Clinical studies", "Concept drift", "Data mining", "Gradual forgetting", "Machine learning", "Partial memory learning", "Windowing"], "Date": "2003", "Abstract": "Most statistical, Machine Learning and Data Mining algorithms assume that the data they use is a random sample drawn from a stationary distribution. Unfortunately, many of the databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them may have changed during this time, sometimes radically (this is also known as a concept drift). In clinical institutions, where the patients' data are regularly stored in a central computer databases, similar situations may occur. Expert physicians may easily, even unconsciously, adapt to the changed environment, whereas Machine Learning and Data Mining tools may fail due to their underlaying assumptions. It is therefore important to detect and adapt to the changed situation. In the paper we review several techniques for dealing with concept drift in Machine Learning and Data Mining frameworks and evaluate their use in clinical studies with a case study of coronary artery disease diagnostics. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "10"},
{"Title": "Detecting fortresses in chess", "Authors": ["Guid M.", "Bratko I."], "Keywords": ["Chess", "Computer chess", "Fortress", "Game playing", "Heuristic search"], "Date": "2012", "Abstract": "We introduce a computational method for semi-automatical detecting fortresses in the game of chess. It is based on computer heuristic search and can be easily used with any state-of-the-art chess program. We also demonstrate a method for avoiding fortresses and show how to find a break-through plan when one exists. Although the paper is not concerned with the question whether it is practical or not to implement the method within the state-of-the-art chess programs, the method can be useful, for example, in correspondence chess or in composing chess studies, where a human-computer interaction is of great importance, and the time available is significantly longer than in ordinary chess competitions.", "Language": "en", "Citations": "2"},
{"Title": "Building cloud-based biometric services", "Authors": ["Peer P.", "Bule J.", "Gros J.Z.", "Struc V."], "Keywords": ["Biometrics", "Cloud computing", "Cloud integration", "Fingerprint recognition", "SaaS"], "Date": "2013", "Abstract": "Over the next few years the amount of biometric data being at the disposal of various agencies and authentication service providers is expected to grow significantly. Such quantities of data require not only enormous amounts of storage but unprecedented processing power as well. To be able to face this future challenges more and more people are looking towards cloud computing, which can address these challenges quite effectively with its seemingly unlimited storage capacity, rapid data distribution and parallel processing capabilities. Since the available literature on how to implement cloud-based biometric services is extremely scarce, this paper capitalizes on the most important challenges encountered during the development work on biometric services, presents the most important standards and recommendations pertaining to biometric services in the cloud and ultimately, elaborates on the potential value of cloud-based biometric solutions by presenting a few existing (commercial) examples. In the final part of the paper, a case study on fingerprint recognition in the cloud and its integration into the e-learning environment Moodle is presented.", "Language": "en", "Citations": "26"},
{"Title": "Estimation of minimum sample size for identification of the most important features: A case study providing a qualitative B2B sales data set", "Authors": ["Bohanec M.", "Borstnar M.K.", "Robnik-Sikonja M."], "Keywords": ["B2B sales forecasting", "Data set reduction", "Machine learning", "Sample size"], "Date": "2017", "Abstract": "An important task in machine learning is to reduce data set dimensionality, which in turn contributes to reducing computational load and data collection costs, while improving human understanding and interpretation of models. We introduce an operational guideline for determining the minimum number of instances sufficient to identify correct ranks of features with the highest impact. We conduct tests based on qualitative B2B sales forecasting data. The results show that a relatively small instance subset is sufficient for identifying the most important features when rank is not important.", "Language": "en", "Citations": "0"},
{"Title": "Calculating similarity of folk song variants with melody-based features", "Authors": ["Bohak C.", "Marolt M."], "Keywords": [], "Date": "2009", "Abstract": "As folk songs live largely through oral transmission, there usually is no standard form of a song - each performance of a folk song may be unique. Different interpretations of the same song are called song variants, all variants of a song belong to the same variant type. In the paper, we explore how various melody-based features relate to folk song variants. Specifically, we explore whether we can derive a melodic similarity measure that would correlate to variant types in the sense that it would measure songs belonging to the same variant type as more similar, in contrast to songs from different variant types. The measure would be useful for folk song retrieval based on variant types, classification of unknown tunes, as well as a measure of similarity between variant types. We experimented with a number of melodic features calculated from symbolic representations of folk song melodies and combined them into a melodybased folk song similarity measure. We evaluated the measure on the task of classifying an unknown melody into a set of existing variant types. We show that the proposed measure gives the correct variant type in the top 10 list for 68% of queries in our data set. \u00a9 2009 International Society for Music Information Retrieval.", "Language": "en", "Citations": "7"},
{"Title": "Mining data from hemodynamic simulations for generating prediction and explanation models", "Authors": ["Bosnic Z.", "Vracar P.", "Radovi M.D.", "Devedzic G.", "Filipovic N.D.", "Kononenko I."], "Keywords": ["Arterial stenosis", "data mining", "machine learning", "medical expert system"], "Date": "2012", "Abstract": "One of the most common causes of human death is stroke, which can be caused by carotid bifurcation stenosis. In our work, we aim at proposing a prototype of a medical expert system that could significantly aid medical experts to detect hemodynamic abnormalities (increased artery wall shear stress). Based on the acquired simulated data, we apply several methodologies for1) predicting magnitudes and locations of maximum wall shear stress in the artery, 2) estimating reliability of computed predictions, and 3) providing user-friendly explanation of the models decision. The obtained results indicate that the evaluated methodologies can provide a useful tool for the given problem domain. \u00a9 2012 IEEE.", "Language": "en", "Citations": "12"},
{},
{"Title": "Fuzzy c-means in an MDL-framework", "Authors": ["Selb A.", "Bischof H.", "Leonardis A."], "Keywords": [], "Date": "2000", "Abstract": "In this paper we present a Minimum Description Length (MDL) framework for fuzzy clustering algorithms. This framework enables us to find an optimal number of cluster centers. We applied our approach to the fuzzy c-means algorithm for which we designed a computationally efficient procedure. We report the results of our approach on a 2D clustering problem and on RGB color image segmentation. \u00a9 2000 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "Cots products to trace method enactment: Review and selection", "Authors": ["Khodabandelou G.", "Hug C.", "Deneckere R.", "Salinesi C.", "Bajec M.", "Kornyshova E.", "Jankovic M."], "Keywords": ["Decision-making", "Method engineering", "Tool review", "Trace based tool"], "Date": "2013", "Abstract": "Observing information systems projects shows that Information Systems Engineering (ISE) methods are underused. The iAMF project aims at (a) tracing stakeholders' activities to identify whether this statement is true and (b) proposing more efficient ISE methods. To trace stakeholders' activities, we need a tool able to record any computerized actions -as opening applications, modifying documents, compiling programs, etc. This paper presents a review of trace-based tools that was undertaken to address the issue of recording information systems engineering methods enactment. We followed the MADISE decision making approach to select the most appropriate trace-based tool for the iAMF project.", "Language": "en", "Citations": "2"},
{"Title": "Multi-resolution image parametrization in stepwise diagnostics of coronary artery disease", "Authors": ["Kukar M.", "Sajn L.", "Groselj C.", "Groselj J."], "Keywords": ["Association rules", "Coronary artery disease", "Image parametrization", "Machine learning", "Medical diagnosis", "Stepwise diagnostic process"], "Date": "2007", "Abstract": "Coronary artery disease is one of the world's most important causes of early mortality, so any improvements of diagnostic procedures are highly appreciated. In the clinical setting, coronary artery disease diagnostics is typically performed in a sequential manner. The four diagnostic levels consist of evaluation of (1) signs and symptoms of the disease and ECG (electrocardiogram) at rest, (2) ECG testing during a controlled exercise, (3) myocardial perfusion scintigraphy, and (4) finally coronary angiography (which is considered as the \"gold standard\" reference method). In our study we focus on improving diagnostic performance of the third diagnostic level (myocardial perfusion scintigraphy). This diagnostic level consists of series of medical images that are easily obtained and the imaging procedure represents only a minor threat to patients' health. In clinical practice, these images are manually described (parameterized) and subsequently evaluated by expert physicians. In our paper we present an innovative alternative to manual image evaluation - an automatic image parametrization on multiple resolutions, based on texture description with specialized association rules, and image evaluation with machine learning methods. Our results show that multi-resolution image parameterizations equals the physicians in terms of quality of image parameters. However, by using both manual and automatic image description parameters at the same time, diagnostic performance can be significantly improved with respect to the results of clinical practice. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "7"},
{"Title": "Computer analysis of chess champions", "Authors": ["Guid M.", "Bratko I."], "Keywords": [], "Date": "2007", "Abstract": "Who is the best chess player of all time? Chess players are often interested in this question that has never been answered authoritar tively, because it requires a comparison between chess players of different eras who never met across the board. In this contribution, we attempt to make such a comparison.It is based on the evaluation of the games played by the World Chess Champions in their championship matches. The evaluation is performed by the chess-playing program CRAFTY. For this purpose we slightly adapted CRAFTY. Our analysis takes into account the differences in players' styles to compensate the fact that calm positional players in their typical games have less chance to commit gross tactical errors than aggressive tactical players. Therefore, we designed a method to assess the difficulty of positions. Some of the results of this computer analysis might be quite surprising. Overall, the results can be nicely interpreted by a chess expert. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "0"},
{"Title": "WSDL and BPEL extensions for Event Driven Architecture", "Authors": ["Juric M.B."], "Keywords": ["BPEL (Business Process Execution Language)", "EDA (Event Driven Architecture)", "Event", "SOA (Service Oriented Architecture)", "WSDL (Web Service Description Language)", "WSPA (Web Services Platform Architecture)"], "Date": "2010", "Abstract": "Context: Service Oriented Architecture (SOA) and Event Driven Architecture (EDA) are two acknowledged architectures for the development of business applications and information systems, which have evolved separately over the years. Objective: This paper proposes a solution for extending the SOA/Web Services Platform Architecture (WSPA) with support for business events and EDA concepts. Our solution enables services to act as event producers and event consumers. It also enables event-driven service orchestrations in business processes. Method: Based on a comparison of SOA and EDA, we have identified and designed the required extensions to enable support for events and event-driven process orchestration in WSPA. Results: We propose specific extensions to WSDL and BPEL, and a flexible XML representation of the event payload data. We introduce event sinks, sources, and triggers to WSDL. We extend BPEL with new activities to trigger and catch events, and extend fault and event handlers, variables, and correlation properties to accommodate events. Conclusion: As a proof-of-concept, we have developed a prototype implementation and assessed the extensions on three pilot projects. We have shown that our proposed extensions work on real projects and that combining event-driven and service-oriented semantics makes sense in many business applications and can considerably reduce the development effort. \u00a9 2010 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "34"},
{"Title": "Breaking ground in cross-cultural research on the fear of being laughed at (gelotophobia): A multi-national study involving 73 countries", "Authors": ["Proyer R.T.", "Ruch W.", "Ali N.S.", "Al-Olimat H.S.", "Amemiya T.", "Adal T.A.", "Ansari S.A.", "Arhar P.", "Asem G.", "Baudin N.", "Bawab S.", "Bergen D.", "Brdar I.", "Brites R.", "Brunner-Sciarra M.", "Carrell A.", "Dios H.C.", "Celik M.", "Ceschi G.", "Chang K.", "Guo-Hai C.", "Cheryomukhin A.", "Chik M.P.Y.", "Chlopicki W.", "Cranney J.", "Dahourou D.", "Doosje S.", "Dore M.", "El-Arousy N.", "Fickova E.", "Fuhr M.", "Gallivan J.", "Geling H.", "Germikova L.", "Giedraityte M.", "Goh A.", "Gonzalez R.D.", "Ho S.K.", "Hrebickova M.", "Jaime B.", "Kaare B.H.", "Kamble S.", "Kazarian S.", "Kerkkanen P.", "Klementova M.", "Kobozeva I.M.", "Kovjanic S.", "Kumaraswamy N.", "Lampert M.", "Liao C.-C.", "Levesque M.", "Loizou E.", "Diaz Loving R.", "Lyttle J.", "MacHline V.C.", "McGoldrick S.", "McRorie M.", "Min L.", "Mottus R.", "Munyae M.M.", "Navia C.E.", "Nkhalamba M.", "Pedrini P.P.", "Petkova M.", "Platt T.", "Popa D.-E.", "Radomska A.", "Rashid T.", "Rawlings D.", "Rubio V.J.", "Samson A.C.", "Sarid O.", "Shams S.", "Sisokohm S.", "Smari J.", "Sneddon I.", "Snikhovska I.", "Stephanenko E.A.", "Stokenberga I.", "Stuer H.", "Tanoto Y.S.R.", "Tapia L.", "Taylor J.", "Thibault P.", "Thompson A.", "Thorn H.", "Toyota H.", "Ujlaky J.", "Vanno V.", "Wang J.", "Van Der Westhuizen B.", "Wijayathilake D.", "Wong P.S.O.", "Wycoff E.B.", "Yeun E.J."], "Keywords": ["Cross-cultural comparisons", "Gelotophobia", "Humor", "Laughter", "Multi-national study"], "Date": "2009", "Abstract": "The current study examines whether the fear of being laughed at (gelotophobia) can be assessed reliably and validly by means of a self-report instrument in different countries of the world. All items of the GELOPH (Ruch and Titze 1998; Ruch and Proyer 2008b) were translated to the local language of the collaborator (42 languages in total). In total, 22,610 participants in 93 samples from 73 countries completed the GELOPH. Across all samples the reliability of the 15-item questionnaire was high (mean alpha of .85) and in all samples the scales appeared to be unidimensional. The endorsement rates for the items ranged from 1.31% through 80.00% to a single item. Variations in the mean scores of the items were more strongly related to the culture in a country and not to the language in which the data were collected. This was also supported by a multidimensional scaling analysis with standardized mean scores of the items from the GELOPH\u300815\u3009. This analysis identified two dimensions that further helped explaining the data (i.e., insecure vs. intense avoidant-restrictive and low vs. high suspicious tendencies towards the laughter of others). Furthermore, multiple samples derived from one country tended to be (with a few exceptions) highly similar. The study shows that gelotophobia can be assessed reliably by means of a self-report instrument in cross-cultural research. This study enables further studies of the fear of being laughed at with regard to differences in the prevalence and putative causes of gelotophobia in comparisons to different cultures. \u00a9 2009 by Walter de Gruyter.", "Language": "en", "Citations": "34"},
{"Title": "Development of a speaker diarization system for speaker tracking in audio broadcast news: A case study", "Authors": ["Zibert J.", "Vesnicer B.", "Mihelic F."], "Keywords": ["Audio indexing", "Audio segmentation", "Speaker clustering", "Speaker diarization", "Speaker recognition", "Speaker tracking", "Speech detection"], "Date": "2008", "Abstract": "A system for speaker tracking in broadcast-news audio data is presented and the impacts of the main components of the system to the overall speaker-tracking performance are evaluated. The process of speaker tracking in continuous audio streams involves several processing tasks and is therefore treated as a multistage process. The main building blocks of such system include the components for audio segmentation, speech detection, speaker clustering and speaker identification. The aim of the first three processes is to find homogeneous regions in continuous audio streams that belong to one speaker and to join each region of the same speaker together. The task of organizing the audio data in this way is known as a speaker diarization and plays an important role in various speech-processing applications. In our case the impact of speaker diarization was assessed in a speaker-tracking system by performing a comparative study of how each of the component influenced the overall speaker-detection results. The evaluation experiments were performed on broadcast-news audio data with a speaker-tracking system, which was capable of detecting 41 target speakers. We implemented several different approaches in each component of the system and compared their performances by inspecting the final speaker-tracking results. The evaluation results indicate the importance of the audio-segmentation and speech-detection components, while no significant improvement of the overall results was achieved by additionally including a speaker-clustering component to the speaker-tracking system.", "Language": "en", "Citations": "1"},
{"Title": "A quality management model based on the \"deep quality concept\"", "Authors": ["Srdoc A.", "Sluga A.", "Bratko I."], "Keywords": ["Artificial intelligence", "Knowledge management", "Quality management", "Quality management techniques"], "Date": "2005", "Abstract": "Purpose - According to many authors, differences in firm performances are increasingly attributed to tacit knowledge that cannot easily be transmitted or imitated. On the other hand, current quality management models knowledge typically relates only to people. Situations, in which knowledge that is related to people is not available, sufficient, reliable or lucrative for application, are not considered. This paper aims to investigate how to overcome this gap. Design/methodology/approach - Based on the adopted classification, types of knowledge typically present in an organisation are identified, and are discussed. Techniques for acquiring and formalising tacit knowledge are explored, and related criteria are defined. Particular attention is shown to knowledge management and artificial intelligence techniques. Findings - A new approach to quality management called deep quality concept (DQC) is conceptualised, and mechanisms and concepts needed to acquire and integrate formalised knowledge into quality systems are identified. Other concepts that need to be incorporated are also identified. Finally, a new quality management model based on the DQC is developed. Research limitations/implications - In further research the main points of the presented theoretical framework need to be validated through real examples from practice, and the resulting quality standard, i.e. award criteria, as well as the related handbooks completed and formalised. Practical implications - Knowledge-related and other relevant concepts need to be incorporated into contemporary quality management systems, as systematically and carefully as conventional quality management concepts. Knowledge of methods and tools suitable for that also needs to be assimilated. Originality/value - In the paper a novel knowledge-focused approach to quality management is presented. For this reason the paper is of great value for quality management theory and practice. \u00a9 Emerald Group Publishing Limited.", "Language": "en", "Citations": "18"},
{"Title": "Ethnomuse: Archiving folk music and dance culture", "Authors": ["Marolt M.", "Vratanar J.F.", "Strle G."], "Keywords": ["CIDOC CRM", "Digital libraries", "Folk music", "FRBR"], "Date": "2009", "Abstract": "The paper presents the development of EthnoMuse: multimedia digital library of Slovenian folk music and dance culture. The main scope of the project concerns the digitization of production and post-production processes that relate to collecting, documenting and archiving of folk heritage and development of multimedia applications for various content types (folk song, music and dance) and formats (image, audio, video, notation, MIDI etc.). The main objective of this paper is to discuss the former, focusing on the conceptual design of a flexible data model the archive is based on. We also briefly describe the tools developed to support the workflow of researchers involved in collecting and archiving of folk music related contents. \u00a9 2009 AACC.", "Language": "en", "Citations": "6"},
{"Title": "Prosodic events recognition in evaluation of speech-synthesis system performance", "Authors": ["Mihelic F.", "Vesnicer B.", "Zibert J.", "Noth E."], "Keywords": ["Prosody", "Speech synthesis", "System evaluation"], "Date": "2008", "Abstract": "We present an objective-evaluation method of the prosody modeling in an HMM-based Slovene speech-synthesis system. Method is based on the results of the automatic recognition of syntactic-prosodic boundary positions and accented words in the synthetic speech. We have shown that the recognition results represent a close match with the prosodic notations, labeled by the human expert on the natural-speech counterpart that was used to train the speech-synthesis system. The recognition rate of the prosodic events is proposed as an objective evaluation measure for the quality of the prosodic modeling in the speech-synthesis system. The results of the proposed evaluation method are also in accordance with previous subjective-listening assesment evaluations, where high scores for the naturalness for such a type of speech synthesis were observed. \u00a9 2008 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "0"},
{"Title": "Towards knowledge-based gene expression data mining", "Authors": ["Bellazzi R.", "Zupan B."], "Keywords": ["Classification", "Data mining", "Gene association", "Gene expression data analysis", "Gene networks", "Knowledge-based data mining"], "Date": "2007", "Abstract": "The field of gene expression data analysis has grown in the past few years from being purely data-centric to integrative, aiming at complementing microarray analysis with data and knowledge from diverse available sources. In this review, we report on the plethora of gene expression data mining techniques and focus on their evolution toward knowledge-based data analysis approaches. In particular, we discuss recent developments in gene expression-based analysis methods used in association and classification studies, phenotyping and reverse engineering of gene networks. \u00a9 2007 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "50"},
{"Title": "Robust subspace visual learning Robustno vizualno u\u010denje na podlagi podprostorov", "Authors": ["Skocaj D."], "Keywords": ["Appearance-based modeling", "Computer vision", "Incremental learning", "Principal component analysis", "Robust learning"], "Date": "2004", "Abstract": "In the real world, visual learning is supposed to be a robust and continuous process. All available visual data is not equally important; in the case of occlusions or other undesirable intrusions in the field of view some visual data can even be misleading. Human visual system treats visual data selectively and builds efficient representations of observed objects and scenes even under non-ideal conditions. Furthermore, these representations can afterwards be updated with newly acquired information. In this paper we present several methods, which introduce similar principles in the machine visual learning and recognition as well. We approach visual learning by the appearance-based modeling of objects and scenes. Models are built using principal component analysis (PCA), which has several shortcomings with respect to the premises mentioned above. In order to overcome them, we propose several extensions of the standard PCA. PCA-based learning is traditionally performed in a batch mode, thus requiring all training images to be given in advance. Since this is not admissible in the framework of continuous learning, we propose an incremental method that processes images sequentially one by one and updates the representation at each step accordingly. Each image can be discarded immediately after the model is updated, which makes the method perfectly well suited for real on-line scenarios. In addition, in the standard PCA approach all pixels of an image receive equal treatment. Also, all training images have equal influence on the estimation of principal subspace. In this paper, we present a generalized PCA approach, which estimates principal axes and principal components considering weighted pixels and images. We further extend this weighted approach into a method for learning from incomplete data, which builds the model of an object even when the part of input data is missing. Images of objects and scenes are not always ideal; they may contain various deceptive additions like reflections or occlusions. PCA in its standard form is intrinsically non-robust to such non-gaussian noise. Several methods for robust recognition have already been proposed, however robust learning has been tackled very rarely. In the paper we present a novel approach to the robust subspace learning. The proposed batch and incremental methods detect inconsistencies in the training images and build the representations from consistent data only. As a result, the obtained models are more robust and efficient enabling more reliable visual learning and recognition even when the learning conditions are not ideal. In the paper we briefly describe all the methods mentioned above. We also present evaluation results of the proposed algorithms on different image domains (Fig. 1) and determine the applicability of the methods in different scenarios. A detailed derivation and analysis of all the methods can be found in [7].", "Language": "en", "Citations": "0"},
{"Title": "A web-based virtual reality environment for medical visualization", "Authors": ["Kokelj Z.", "Bohak C.", "Marolt M."], "Keywords": [], "Date": "2018", "Abstract": "In this paper, we present a novel approach to integrating virtual reality (VR) into a web-based medical visualization framework. The framework supports visualization of volumetric data, such as 3D scalar fields acquired by a CT, MRI or PET scanners. To improve users' perception, understanding and manipulation of 3D volumes, we adapted the traditional 2D screen representation with support for visualization of data in a VR environment. By providing complete visual immersion, VR can help users to gain better insights and understanding of the visualized data. Our main goal was to allow users to view the medical data in VR and interact with it with hand-held controllers for better immersion and spatial perception. In the paper, we present a novel approach to implementation of VR for medical imaging, which combines WebGL-based hardware accelerated web visualization with VR. This allows users to use the visualization framework with or without a VR headset by switching between 'standard' and 'VR' modes. Since visualization runs in a web browser, it is portable, easy to use on different devices and therefore accessible to a broad number of users. The visualization system was tested with real medical scans to assess its performance and usability.", "Language": "en", "Citations": "2"},
{"Title": "The RNA-binding protein HuR is essential for the B cell antibody response", "Authors": ["Diaz-Munoz M.D.", "Bell S.E.", "Fairfax K.", "Monzon-Casanova E.", "Cunningham A.F.", "Gonzalez-Porta M.", "Andrews S.R.", "Bunik V.I.", "Zarnack K.", "Curk T.", "Heggermont W.A.", "Heymans S.", "Gibson G.E.", "Kontoyiannis D.L.", "Ule J.", "Turner M."], "Keywords": [], "Date": "2015", "Abstract": "Post-transcriptional regulation of mRNA by the RNA-binding protein HuR (encoded by Elavl1) is required in B cells for the germinal center reaction and for the production of class-switched antibodies in response to thymus-independent antigens. Transcriptome-wide examination of RNA isoforms and their abundance and translation in HuR-deficient B cells, together with direct measurements of HuR-RNA interactions, revealed that HuR-dependent splicing of mRNA affected hundreds of transcripts, including that encoding dihydrolipoamide S-succinyltransferase (Dlst), a subunit of the 2-oxoglutarate dehydrogenase (\u03b1-KGDH) complex. In the absence of HuR, defective mitochondrial metabolism resulted in large amounts of reactive oxygen species and B cell death. Our study shows how post-transcriptional processes control the balance of energy metabolism required for the proliferation and differentiation of B cells.", "Language": "en", "Citations": "53"},
{"Title": "Interactive aggregation/disaggregation dichotomic sorting procedure for group decision analysis based on the threshold model", "Authors": ["Bregar A.", "Gyorkos J.", "Juric M.B."], "Keywords": ["Agents", "Decision-making", "Fuzzy sets", "Group decisions and negotiations", "Nonlinear optimization", "Outranking relation", "Preference aggregation/disaggregation", "Pseudo-criterion"], "Date": "2008", "Abstract": "In this paper, a new multi-criteria decision-making procedure is presented, which captures preferential information in the form of the threshold model. It is based on the ELECTRE-like sorting analysis restricted by the localization principle, which enables high adaptability of the decision model and reduces the cognitive load imposed on the decision-makers. It lays the foundation for the introduction of three concepts that have been previously insufficiently supported by outranking methods - semiautomatic derivation of criteria weights according to the selective effects of discordance and veto thresholds, convergent group consensus seeking, and autonomous multi-agent negotiation. The interdependent principles are justified, and the methodological solutions underlying their implementation are provided. \u00a9 2008 Institute of Mathematics and Informatics.", "Language": "en", "Citations": "21"},
{"Title": "Birth and death in discrete Morse theory", "Authors": ["King H.", "Knudson K.", "Mramor Kosta N."], "Keywords": ["Birth\u2013death point", "Discrete Morse theory"], "Date": "2017", "Abstract": "Suppose M is a finite cell decomposition of a space X and that for 0=t", "Language": "en", "Citations": "0"},
{"Title": "Building an intelligent tutoring system for chess endgames", "Authors": ["Guid M.", "Mozina M.", "Bohak C.", "Sadikov A.", "Bratko I."], "Keywords": ["Artificial Intelligence", "Chess", "Chess Endgames", "Education", "Intelligent Tutoring Systems", "Problem Solving"], "Date": "2013", "Abstract": "We present the development of an intelligent tutoring system for chess endgames, and explain in detail the system's architecture that is comprised of five essential components. The rule-based domain model contains a conceptualized domain theory, which serves as a bridge between the basic declarative domain theory and procedural knowledge for concrete problem solving. The search engine is used to generate new chess problems and to validate students' solutions on the fly. The tutoring model represents pedagogical knowledge and follows the standard model-tracing approach. The student model contains the knowledge about the user in the form of a skill meter, aiming to show the level of a student's understanding of particular skills. Finally, the user interface is where the interaction between a student and the tutor takes place.", "Language": "en", "Citations": "0"},
{"Title": "Genome-wide localization study of yeast pex11 identifies peroxisome-mitochondria interactions through the ERMES complex", "Authors": ["Mattiazzi Usaj M.", "Brloznik M.", "Kaferle P.", "Zitnik M.", "Wolinski H.", "Leitner F.", "Kohlwein S.D.", "Zupan B.", "Petrovic U."], "Keywords": ["computational image analysis", "high-content microscopy", "lipid metabolism", "organelle juxtaposition", "peroxisomal disorders"], "Date": "2015", "Abstract": "Pex11 is a peroxin that regulates the number of peroxisomes in eukaryotic cells. Recently, it was found that a mutation in one of the three mammalian paralogs, PEX11\u03b2, results in a neurological disorder. The molecular function of Pex11, however, is not known. Saccharomyces cerevisiae Pex11 has been shown to recruit to peroxisomes the mitochondrial fission machinery, thus enabling proliferation of peroxisomes. This process is essential for efficient fatty acid \u03b2-oxidation. In this study, we used high-content microscopy on a genome-wide scale to determine the subcellular localization pattern of yeast Pex11 in all non-essential gene deletion mutants, as well as in temperature-sensitive essential gene mutants. Pex11 localization and morphology of peroxisomes was profoundly affected by mutations in 104 different genes that were functionally classified. A group of genes encompassing MDM10, MDM12 and MDM34 that encode the mitochondrial and cytosolic components of the ERMES complex was analyzed in greater detail. Deletion of these genes caused a specifically altered Pex11 localization pattern, whereas deletion of MMM1, the gene encoding the fourth, endoplasmic-reticulum-associated component of the complex, did not result in an altered Pex11 localization or peroxisome morphology phenotype. Moreover, we found that Pex11 and Mdm34 physically interact and that Pex11 plays a role in establishing the contact sites between peroxisomes and mitochondria through the ERMES complex. Based on these results, we propose that the mitochondrial/cytosolic components of the ERMES complex establish a direct interaction between mitochondria and peroxisomes through Pex11.", "Language": "en", "Citations": "53"},
{"Title": "Ear biometric database in the wild", "Authors": ["Emersic Z.", "Peer P."], "Keywords": [], "Date": "2015", "Abstract": "Ear biometrics is gaining on popularity in recent years. One of the major problems in the domain is that there are no widely used, ear databases in the wild available. This makes comparison of existing ear recognition methods demanding and progress in the domain slower. Images that were taken under supervised conditions and are then used to train classifiers in ear recognition methods can in effect cause these classifiers classifiers to fail under application in the wild. In this paper we propose a new database which consists of ear images in the wild of known persons taken from the Internet. This ensures different indoor and outdoor lightning conditions, different viewing angles, occlusions, and a variety of image sizes and quality. In experiments we demonstrate that our database is more challenging than others.", "Language": "en", "Citations": "3"},
{"Title": "Learning representations for text-level discourse parsing", "Authors": ["Weiss G."], "Keywords": [], "Date": "2015", "Abstract": "In the proposed doctoral work we will design an end-to-end approach for the challenging NLP task of text-level discourse parsing. Instead of depending on mostly hand-engineered sparse features and independent components for each subtask, we propose a unified approach completely based on deep learning architectures. To train more expressive representations that capture communicative functions and semantic roles of discourse units and relations between them, we will jointly learn all discourse parsing subtasks at different layers of our architecture and share their intermediate representations. By combining unsupervised training of word embeddings with our layer-wise multi-task learning of higher representations we hope to reach or even surpass performance of current state-of-the-art methods on annotated English corpora.", "Language": "en", "Citations": "0"},
{"Title": "Reality considerations when designing a TDMA-FDMA based link-layer for real-time WSN", "Authors": ["Riliskis L.", "Berdajs J.", "Osipov E.", "Brodnik A."], "Keywords": ["bootstrapping", "Dependable protocol", "medium access control", "time synchronization", "wireless sensor network", "wsn"], "Date": "2012", "Abstract": "In this article we elaborate on reality considerations when designing and implementing application tailored TDMA-FDMA medium access protocol with guaranteed end-to-end delay. We highlight importance of considering underlaying hardware and software components when designing communication protocols for resource constrained platforms. We also show that by combining medium access protocol, bootstrapping, and time synchronization mechanisms within the link-layer, we can limit on average clock drift in the network to 0.5 \u03bcs, as well as achieve 81% energy efficiency while keeping collision probability at its minimum of 1%. Finally, we conclude with challenges and lessons learned in real-world deployment of TDMA/FDMA based link-layer with guaranteed end-to-end delay in WSN. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "Robust visual tracking using an adaptive coupled-layer visual model", "Authors": ["Cehovin L.", "Kristan M.", "Leonardis A."], "Keywords": ["Image processing and computer vision", "tracking"], "Date": "2012", "Abstract": "This paper addresses the problem of tracking objects which undergo rapid and significant appearance changes. We propose a novel coupled-layer visual model that combines the target's global and local appearance by interlacing two layers. The local layer in this model is a set of local patches that geometrically constrain the changes in the target's appearance. This layer probabilistically adapts to the target's geometric deformation, while its structure is updated by removing and adding the local patches. The addition of these patches is constrained by the global layer that probabilistically models the target's global visual properties, such as color, shape, and apparent local motion. The global visual properties are updated during tracking using the stable patches from the local layer. By this coupled constraint paradigm between the adaptation of the global and the local layer, we achieve a more robust tracking through significant appearance changes. We experimentally compare our tracker to 11 state-of-the-art trackers. The experimental results on challenging sequences confirm that our tracker outperforms the related trackers in many cases by having a smaller failure rate as well as better accuracy. Furthermore, the parameter analysis shows that our tracker is stable over a range of parameter values. \u00a9 1979-2012 IEEE.", "Language": "en", "Citations": "149"},
{"Title": "Supporting diagnostics of coronary artery disease with neural networks", "Authors": ["Kukar M.", "Groselj C."], "Keywords": ["coronary artery disease", "explanation", "medical diagnostics", "multi-layered perceptron", "radial basis function network"], "Date": "2011", "Abstract": "Coronary artery disease is one of its most important causes of early mortality in western world. Therefore, clinicians seek to improve diagnostic procedures in order to reach reliable early diagnoses. In the clinical setting, coronary artery disease diagnostics is often performed in a sequential manner, where the four diagnostic steps typically consist of evaluation of (1) signs and symptoms of the disease and electrocardiogram (ECG) at rest, (2) sequential ECG testing during the controlled exercise, (3) myocardial perfusion scintigraphy, and (4) finally coronary angiography, that is considered as the \"gold standard\" reference method. Our study focuses on improving diagnostic and probabilistic interpretation of scintigraphic images obtained from the penultimate step. We use automatic image parameterization on multiple resolutions, based on spatial association rules. Extracted image parameters are combined into more informative composite parameters by means of principle component analysis, and finally used to build automatic classifiers with neural networks and naive Bayes learning methods. Experiments show that our approach significantly increases diagnostic accuracy, specificity and sensitivity with respect to clinical results. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "Data-bound variables for WS-BPEL executable processes", "Authors": ["Krizevnik M.", "Juric M.B."], "Keywords": ["Data synchronization", "Service composition", "Service oriented architecture", "WS-BPEL"], "Date": "2012", "Abstract": "Standard BPEL (Business Process Execution Language) variables, if used to store the data from a data store, cannot be automatically synchronized with the data source in case other applications change the data during the BPEL process execution, which is a common occurrence particularly for long-running BPEL processes. BPEL also does not provide a mechanism for active monitoring of changes of data that would support automated detection and handling of such changes. This paper proposes a new type of BPEL variables, called data-bound variables. Data-bound variables are automatically synchronized with the data source and thus eliminate the need to implement data synchronization manually. To provide support for data-bound variables, we propose specific extensions to BPEL and the use of appropriate Data Access Services (DAS) that act as data providers. We introduce new BPEL activities to load, create and delete remote data. We also introduce observed properties, observed property groups and a variable handler. Using this mechanism, the BPEL process is able to automatically adapt to changes to data, made inside or outside the process scope, by following the Event, Condition, Action (ECA) paradigm. As a proof-of-concept, we have developed a prototype implementation of our proposed BPEL extensions and tested it by implementing three pilot projects. We have confirmed that our proposed solution decreases BPEL process size and complexity, increases readability and reduces semantic gap between BPMN process model and BPEL. \u00a9 2012 Elsevier Ltd.", "Language": "en", "Citations": "3"},
{"Title": "0:1 virtual-real duel", "Authors": ["Cigon A.", "Vidmar K.", "Macek T.", "Dovgan E.", "Sinkovec M.", "Klopcic U.", "Batagelj B."], "Keywords": ["Dance", "Interactivity", "Motion tracking", "Multi-monitor system", "Video", "VJ (visual jockey)", "Wii remote"], "Date": "2009", "Abstract": "In this paper we will present a complex system of new technology uses in dance and video. It is a system that allows interaction between the recorded videos and dancing with the help of two computers, four monitors, webcam, projector and Wii remote. The interaction is happening through the temporal and spatial manipulation of video outputs and real-time processing of followed movements. For realisation of this project we used the existing systems, enabling the creation of diversified forms of VJ-ing (Visual Jockey methods), especially important is the VJ-ing on several monitors at once. Dancers entrance to the video space is allowed with the interaction through Wii remote, mobile multi-monitors and through the program, which tracks dancers motion through five markers.", "Language": "en", "Citations": "0"},
{"Title": "Bootstrapping bilingual lexicons from comparable corpora for closely related languages", "Authors": ["Ljubesic N.", "Fiser D."], "Keywords": ["bilingual lexicon extraction", "cognates", "comparable corpora"], "Date": "2011", "Abstract": "In this paper we present an approach to bootstrap a Croatian-Slovene bilingual lexicon from comparable news corpora from scratch, without relying on any external bilingual knowledge resource. Instead of using a dictionary to translate context vectors, we build a seed lexicon from identical words in both languages and extend it with context-based cognates and translation candidates of the most frequent words. By enlarging the seed dictionary for only 7% we were able to improve the baseline precision from 0.597 to 0.731 on the mean reciprocal rank for the ten top-ranking translation candidates with a 50.4% recall on the gold standard of 500 entries. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "3"},
{},
{"Title": "Application for viral hepatitis infection risk assessment-HEPY", "Authors": ["Ajanovic A.", "Ulcar A.", "Peterlin A.", "Pocivavsek K.", "Fele-Zorz G.", "Gradisek A.", "Gams M.", "Maticic M."], "Keywords": ["Health education", "Infection", "Preventive medicine", "Viral hepatitis", "Web application"], "Date": "2018", "Abstract": "We present a web application to inform users about different types of viral hepatitises. The core of the application is a questionnaire about past behavior and risk factors. Based on the answers, it produces a personalised overview of any risky actions that the user might have taken in the past. The site also contains general information about these diseases, which can help users identify them or seek proper precautions in order to avoid them.", "Language": "en", "Citations": "0"},
{"Title": "Trajectory estimation of a moving target from Ultra-wideband ranging measurements", "Authors": ["Zupanec Z.", "Ricciato F.", "Sajn L."], "Keywords": ["Blind node", "Indoor positioning system (IPS)", "Localization error", "Mobile node", "Non-linear least squares (NLS)", "Nonline-of-sight (NLOS)", "Ranging measurements", "Target node", "Ultrawideband (UWB)", "Velocity estimation"], "Date": "2016", "Abstract": "In this paper we propose a method for trajectory estimation of a moving node based on minimizing the residual sum defined as the difference between a reported and actual distance from the anchor nodes. We devise extensive and complex indoor experiments with exploratory data analysis and interpretation of the results. Our findings show a slight improvement over an existing point-based localization system in an indoor environment.", "Language": "en", "Citations": "0"},
{"Title": "LLLR parsing", "Authors": ["Slivnik B."], "Keywords": ["Left parse", "LL parsing", "LR languages"], "Date": "2013", "Abstract": "The idea of an LLLR parsing is presented. An LLLR(k) parser can be constructed for any LR(k) grammar but it produces the left parse of the input string in linear time (in respect to the length of the derivation) without backtracking. If used as a basis for a syntax-directed translation, it triggers semantic actions using the top-down strategy just like the canonical LL(k) parser. Hence, from a compiler writer's point of view, it acts as an LL(k) parser. The backbone of the LLLR(k) parser is the LL(k) parser which triggers the embedded left LR(k) parser whenever an LL(k) conflict appears during parsing. Once the embedded LR(k) parser resolves the conflict, it passes the control back to the backbone LL(k) parser together with the left parse of the part of the input string scanned by the embedded LR parser, and LL parsing continues. Hence, LLLR parsing is similar to LL(", "Language": "en", "Citations": "2"},
{"Title": "A case analysis of embryonic data mining success", "Authors": ["Bole U.", "Popovic A.", "Zabkar J.", "Papa G.", "Jaklic J."], "Keywords": ["Case analysis", "Critical success factors", "Data mining", "Predictive analytics"], "Date": "2015", "Abstract": "Within highly competitive business environments, data mining (DM) is viewed as a significant technology to enhance decision-making processes by transforming data into valuable and actionable information to gain competitive advantage. There appears, however, to be a dearth of empirical case studies which consider in detail the initial stages in DM management to enable apt foundation for its later successful implementation. Our research applied a multi-method strategy to determine the critical success factors of embryonic DM implementation. We propose and validate, through a series of cases, a conceptual framework to guide practitioners' adoption of DM. Our findings reveal additional issues for applied decision making in the context of DM success.", "Language": "en", "Citations": "2"},
{"Title": "Where physically is the optical center?", "Authors": ["Peer P.", "Solina F."], "Keywords": ["Camera", "Computer vision", "Optical center"], "Date": "2006", "Abstract": "A simple and fast method of determining the position of the optical center without any specialized equipment is presented. The position of the optical center is a depth determining parameter in a panoramic depth imaging system [Peer, P., Solina, F., 2002. Panoramic depth imaging: single standard camera approach. Internat. J. Comput. Vision 47 (1/2/3), 149-160; Peer, P., Solina, F., 2005. Multiperspective panoramic depth imaging. In: Computer Vision and Robotics. Nova Science Publishers]. The reconstructed distances correspond well to the actual measured distances on the scene. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "6"},
{"Title": "Optimization framework for learning a hierarchical shape vocabulary for object class detection", "Authors": ["Fidler S.", "Boben M.", "Leonardis A."], "Keywords": [], "Date": "2009", "Abstract": "This paper proposes a stochastic optimization framework for unsupervised learning of a hierarchical vocabulary of object shape intended for object class detection. We build on the approach by [6], which has two drawbacks: 1.) learning is performed strictly bottom-up; and 2.) the selection of vocabulary shapes is done solely on their frequency of appearance. This makes the method prone to overfitting of certain parts of object shape while losing the more discriminative shape information. The idea of this paper is to cast the vocabulary learning into an optimization framework that iteratively improves the hierarchy as a whole. Optimization is two-fold: one that learns and selects the vocabulary of shapes at each layer in a bottom-up phase and the other that extends/improves it by top-down feedback from the higher layers. The algorithm then loops between the two learning stages several times. We have evaluated the proposed learning approach for object class detection on 11 diverse object classes taken from the standard recognition data sets. Compared to the original approach [6], we obtain a 3 times more compact vocabulary, a 2.5 times faster inference, and a 10% higher detection performance at the expense of 5 times longer training time (25min vs 5min). The approach attains a competitive detection performance with respect to the current state-of-the-art at both, faster inference as well as shorter training times. \u00a9 2009. The copyright of this document resides with its authors.", "Language": "en", "Citations": "5"},
{"Title": "Use of RFID temperature monitoring to test and improve fish packing methods in styrofoam boxes", "Authors": ["Trebar M.", "Lotric M.", "Fonda I."], "Keywords": ["Cold chain", "Ice", "Packing method", "RFID data logger", "Sea bass", "Temperature monitoring"], "Date": "2015", "Abstract": "To fulfil the temperature requirements of the cold chain, the fresh fish are usually packed, stored and transported to fish markets with ice in open styrofoam boxes. Some companies offer a more flexible service and they deliver the fish directly to private consumers. In these cases the fish are packed with artificial ice - hydrated and frozen gel pads in specially designed completely closed styrofoam boxes. This study presents the results of the comparison of seven packing methods with the aim to potentially improve them. The temperature outside and inside of the closed box and temperatures in the abdominal cavity of gutted sea bass (Dicentrarchus labrax) were measured during the logistics process using Radio Frequency Identification (RFID) technology. The aim of the presented study is to define the optimal cooling materials and methods for different handling options. As an important result, a new efficient, time and energy saving method of packing the fish with the combination of dry non-hydrated gel pads and wet ice instead of the use of frozen gel pads alone is proposed. This method ensures recommended storage temperatures between 0 \u00b0C and 4 \u00b0C and stable conditions inside the box at room temperatures (or higher) for a longer period of time under the same time-ambient conditions after delivery to the consumer. Furthermore it was established that the part of the ice that melted inside the box, due to higher ambient temperatures, was absorbed by the dry gel pad and only a small quantity of water remained on the bottom of the box.", "Language": "en", "Citations": "11"},
{"Title": "On the nilpotent commutator of a nilpotent matrix", "Authors": ["Oblak P."], "Keywords": ["commuting matrices", "maximal partition", "nilpotent commutator", "nilpotent matrix", "nilpotent orbit"], "Date": "2012", "Abstract": "We study the structure of the nilpotent commutator N ", "Language": "en", "Citations": "4"},
{"Title": "FRI rehab 3D: 3D-reconstruction of the cuboid based on a single camera used in rehabilitation of the human hand FRI rehab 3D: 3-D rekonstrukcija kvadra s pomo\u010djo kamere pri rehabilitaciji \u010dlove\u0161ke roke", "Authors": ["Simoncic B.", "Peer P."], "Keywords": ["3d reconstruction of a cuboid", "Arm rehabilitation", "Computer vision", "Cuboid model", "Real-time execution", "Web camera"], "Date": "2009", "Abstract": "The paper presents a rehabilitation method using a personal computer and a single web camera for patients who suffer from an arm or wrist injury. Patient holds in his hand a simple object, cuboid, and moves it around. A camera records his movement while the software calculates in real-time the position of the object in a 3D-space on the basis of the color information and the cuboid model. The program places the object in a virtual 3D-space in which another similar object is already present. In Chapter 2 we present the existing rehabilitation solutions, together with their accuracies and prices. As seen, they can be very expensive and need specialized hardware. Therefore, the goal of our work was to develop a much simpler and less expensive solution demanding only a personal computer and a single web camera. In Chapter 3 we present the environment required to achieve good results (Figure 1). To segment out the object more easily we need a black background and black glove. The lighting is one of the most important factors. In Chapters 4 and 5 we discuss the methods of obtaining cuboid characteristics such as edges and corners from input image. The edges were obtained by separating the sides of the cuboid from the original image and then intersecting them. This leaves only a very narrow set of points which we use as a basis for linear regression to determine the edges (Figure 2). Determining the corners is divided into three sub-problems. When only one side is visible, a bounding rectangle is calculated touching the cuboid on four points. We improve their location with local optimization. When two sides are visible, we use the intersection method to detect the first two corners. To detect the other four ones, we calculate and analyze perpendiculars to the intersected edge (Figure 3). In the case of three visible sides, the intersection method is used. The analysis of the resulting edges gives four corners. Approximations of the remaining three corners are calculated on the basis of the first four and refined by using optimization (Figure 4). In the Chapter 5 we discuss how the object is projected on the image plane (Figure 5) and how the 3D-coordinates of the cubiod can be obtained on this basis (Equation 1). We describe how the cuboid model is used to construct and solve the system of equations thus giving 3D-coordinates for one side of the cuboid. The rest of the sides can be calculated with a vector product. In Chapter 7, a method of matching a virtual and real object is presented (Figure 6). In Chapter 8 we evaluate the software. Our evaluation is based on a quantitative and qualitative analysis performed on a set of examples. The quantitative analysis gives a graph that presents the correlation between the distance of the object from the origin and the size of the error (Figure 7). In Tables 1 and 2 we present errors calculated with methods like geometric mean, median, average and standard deviation. In Figure 8 distribution of the quantitative and qualitative errors can be found. The qualitative analysis was performed on a set of three videos where all the frames (2160 frames in the first video with only one visible side, 2677 frames in the second video with two visible sides, 3477 frames in the third video with three visible sides) were assigned ranking: good, acceptable or bad. The analysis shows that 4,2 % of the frames were displayed incorrectly, but only 1 % of those were assigned the ranking bad. Such analysis however depends on many factors and results are to be taken as an approximation. In the concluding chapter we compare our solution to those of others (Table 3) and present some ways of how to improve the accuracy and efficiency of our software. Finally we conclude that under the condition that the user himself is willing to make his own effort, the presented solution serves its purpose well enough despite some deficinecies. It is suitable mostly in cases where accuracy is not critical and smaller deviations of 3D-reconstruction do not thwart the process of the rehabilitation. Although the accuracy is quite good, under appropriate conditions, we do think that this kind of approach has a potential for significantly better results.", "Language": "en", "Citations": "0"},
{"Title": "A platform for supporting learning process of visually impaired children", "Authors": ["Kavcic A.", "Pesek M.", "Marolt M."], "Keywords": [], "Date": "2017", "Abstract": "Although ICT supported tools and e-learning material are widely available in schools to support teaching and learning, there is still a lack of specific tools and material designated for children with impairments. The costs of adapting and preparing such material is often economically not justifiable due to a small number of such children, and commonly, for the best leaning outcome the material has to be adapted for each individual child and their deficits and level of impairments. Our solution to this problem is a web platform for delivering customized exercises intended for visually impaired children. There are two sorts of exercises already prepared: a tutorial for learning and practicing Braille and ten-finger typing, and various exercises for practicing vision, memory, and motor skills. For each individual impaired learner, the teacher can select appropriate type of exercise and customize it by adjusting visual aspects of the exercise, setting the specific content (e.g., words for typing, or items to sort), or selecting the level of difficulty (e.g., set timing, complexity levels, or number of shown images). A set of such customized exercises is given to a learner for practicing and their progress is constantly monitored and saved for later inspection.", "Language": "en", "Citations": "0"},
{"Title": "On triangle-free distance-regular graphs with an eigenvalue multiplicity equal to the valency", "Authors": ["Coolsaet K.", "Jurisic A.", "Koolen J."], "Keywords": [], "Date": "2007", "Abstract": "Let \u0393 be a triangle-free distance-regular graph with diameter d \u2265 3, valency k \u2265 3 and intersection number a", "Language": "en", "Citations": "4"},
{"Title": "Development, debugging, and assessment of parkinsoncheck attributes through visualisation", "Authors": ["Groznik V.", "Mozina M.", "Zabkar J.", "Georgiev D.", "Bratko I.", "Sadikov A."], "Keywords": [], "Date": "2015", "Abstract": "Parkinson\u2019s disease is estimated to affect between four and six million people over the age of 50 worldwide, and that number is expected to double by the year 2030. The disease often presents itself with a Parkinsonian tremor (PT) and other movement disorders and these symptoms can be similar to those of another disease, the essential tremor (ET)\u2013the most prevalent movement disorder. Spirography is a method involving drawing, usually of spirals, which are then analysed for various distinguishing features to detect signs of various tremors. Spiral drawing as assessment of tremor is recommended by the Movement Disorder Society. This chapter describes the inner making of ParkinsonCheck, a mobile application for self-checking for signs of PT and ET. ParkinsonCheck uses digitalised spirography on smartphones and tablets to detect the signs. The chapter focuses primarily on spirography and the knowledge extraction, formulation, debugging, and assessment for ParkinsonCheck through the use of visualisation.", "Language": "en", "Citations": "0"},
{"Title": "Automatic Spiral Analysis for Objective Assessment of Motor Symptoms in Parkinson's Disease", "Authors": ["Memedi M.", "Sadikov A.", "Groznik V.", "Zabkar J.", "Mozina M.", "Bergquist F.", "Johansson A.", "Haubenberger D.", "Nyholm D."], "Keywords": ["bradykinesia", "digital spiral analysis", "dyskinesia", "machine learning", "motor fluctuations", "objective measures", "Parkinson\u2019s disease", "remote monitoring", "time series analysis", "visualization"], "Date": "2015", "Abstract": "A challenge for the clinical management of advanced Parkinson's disease (PD) patients is the emergence of fluctuations in motor performance, which represents a significant source of disability during activities of daily living of the patients. There is a lack of objective measurement of treatment effects for in-clinic and at-home use that can provide an overview of the treatment response. The objective of this paper was to develop a method for objective quantification of advanced PD motor symptoms related to off episodes and peak dose dyskinesia, using spiral data gathered by a touch screen telemetry device. More specifically, the aim was to objectively characterize motor symptoms (bradykinesia and dyskinesia), to help in automating the process of visual interpretation of movement anomalies in spirals as rated by movement disorder specialists. Digitized upper limb movement data of 65 advanced PD patients and 10 healthy (HE) subjects were recorded as they performed spiral drawing tasks on a touch screen device in their home environment settings. Several spatiotemporal features were extracted from the time series and used as inputs to machine learning methods. The methods were validated against ratings on animated spirals scored by four movement disorder specialists who visually assessed a set of kinematic features and the motor symptom. The ability of the method to discriminate between PD patients and HE subjects and the test-retest reliability of the computed scores were also evaluated. Computed scores correlated well with mean visual ratings of individual kinematic features. The best performing classifier (Multilayer Perceptron) classified the motor symptom (bradykinesia or dyskinesia) with an accuracy of 84% and area under the receiver operating characteristics curve of 0.86 in relation to visual classifications of the raters. In addition, the method provided high discriminating power when distinguishing between PD patients and HE subjects as well as had good test-retest reliability. This study demonstrated the potential of using digital spiral analysis for objective quantification of PD-specific and/or treatment-induced motor symptoms. ", "Language": "en", "Citations": "13"},
{"Title": "Loosely connected virtual development teams as an alternative to the traditional development teams \u0160ibko povezane virtualne razvojne skupine kot alternativa tradicionalnim razvojnim skupinam", "Authors": ["Vavpotic D.", "Zrnec A."], "Keywords": ["Approach", "Methodology", "Pilot project", "Software development", "Virtual teams"], "Date": "2009", "Abstract": "Companies dealing in software development often face human-resource related problems. They are partially due to the extremely fast technological development and the fact that different projects often require different skills and knowledges which quickly become obsolete, as even the best practioneers can't master all the newest approaches and technologies from their field of work old approaches and technologies remain in use. Another difficulty is variation in the number of the manpower needed for each project in a company; at the beginning of a new project, more manpower is needed than at the end and after the project is completed and their knowledge becomes obsolete. The usual solution to the problem is additional (or interim) employment and additional training of the existing manpower. As it is often very difficult to find a sufficient number of people with adequate knowledge and skills locally, extensive additional training should be provided. Employing new manpower only for the duration of one project completion is very expensive. The second solution is giving away the tasks to subcontractors, i.e. outsourcing, which can also be relatively expensive especially when highly sophisticated technology should be used. A promising alternative is formation of a virtual development team enabling choosing the members of a team from a large set of people who can collaborate using the Internet and already have appropriate technological knowledge and experience. However, here, too, we have to cope with specific difficulties related to communication and human factors. As people working in virtual teams need to have special social and communication characteristics, they should be adequately trained. Such training is expensive and there is also the risk that the project managed by traditional be successful. Besides, formation of the classically designed virtual teams is time consuming and their members don't change during the project. This means that the selection of the team members very much effects successfulness of the project. The paper presents an approach enabling companies to benefit from skilled experts accessible on-line and at the same time to lessen their problems typically arising in such development teams. The backbone of the proposed approach consists of four key components: system architecture, project management, development process and human-resource management. This backbone is managed centrally. Its main purpose is to synchronize work of the members of the virtual team. The approach has been tested on a pilot project. Although the results of the experiment were encouraging, we are aware that such development is only possible for systems that can be decomposed into relatively small and independent parts. In chapter 2 we present our approach for IS development in loosely connected virtual teams and in chapter 3 we describe how we tested the approach. In chapter 4 we give some final conclusions and proposals for further research.", "Language": "en", "Citations": "2"},
{"Title": "Design of information processing in cells using artificial gene repressors Projektowanie przetwarzania informacji w kom\u00f3rkach przy u\u017cyciu sztucznych represor\u00f3w", "Authors": ["Ster B.", "Gaber R.", "Avbelj M.", "Jerala R.", "Dobnikar A."], "Keywords": ["Artificial repressors", "Information processing", "Logical gates", "Synthetic biology"], "Date": "2012", "Abstract": "The progress of synthetic biology allows one to design artificial repressors that inhibit selected genes. Combination of repressors enables construction of NOR logical gates that could form the foundation for information processing within cells. The theoretical potentials and limitations of constructing NOR gates were analyzed. They could be experimentally realized in bacterial cells. The number of required artificial repressors was analysed and temporal simulations of an example function were performed.", "Language": "en", "Citations": "1"},
{"Title": "Toolbox for ear biometric recognition evaluation", "Authors": ["Emersic Z.", "Peer P."], "Keywords": [], "Date": "2015", "Abstract": "Ears are not subjected to facial expressions like faces are and do not require closer inspection like fingerprints do. However, there is a problem of occlusion, different lightning conditions and angles. These properties mean that the final outcome depends heavily on the selected database and classification procedures used in the evaluation process. Moreover, the results metrics are often difficult to compare, different sections of evaluation procedure mask the important steps, and frameworks that are usually build on-the-fly take time to develop. With our toolbox we propose the solution to those problems enabling faster development in the field of ear biometric recognition.", "Language": "en", "Citations": "3"},
{"Title": "Promiscuous RNA Binding Ensures Effective Encapsidation of APOBEC3 Proteins by HIV-1", "Authors": ["Apolonia L.", "Schulz R.", "Curk T.", "Rocha P.", "Swanson C.M.", "Schaller T.", "Ule J.", "Malim M.H."], "Keywords": [], "Date": "2015", "Abstract": "The apolipoprotein B mRNA-editing enzyme catalytic polypeptide-like 3 (APOBEC3) proteins are cell-encoded cytidine deaminases, some of which, such as APOBEC3G (A3G) and APOBEC3F (A3F), act as potent human immunodeficiency virus type-1 (HIV-1) restriction factors. These proteins require packaging into HIV-1 particles to exert their antiviral activities, but the molecular mechanism by which this occurs is incompletely understood. The nucleocapsid (NC) region of HIV-1 Gag is required for efficient incorporation of A3G and A3F, and the interaction between A3G and NC has previously been shown to be RNA-dependent. Here, we address this issue in detail by first determining which RNAs are able to bind to A3G and A3F in HV-1 infected cells, as well as in cell-free virions, using the unbiased individual-nucleotide resolution UV cross-linking and immunoprecipitation (iCLIP) method. We show that A3G and A3F bind many different types of RNA, including HIV-1 RNA, cellular mRNAs and small non-coding RNAs such as the Y or 7SL RNAs. Interestingly, A3G/F incorporation is unaffected when the levels of packaged HIV-1 genomic RNA (gRNA) and 7SL RNA are reduced, implying that these RNAs are not essential for efficient A3G/F packaging. Confirming earlier work, HIV-1 particles formed with Gag lacking the NC domain (Gag \u0394NC) fail to encapsidate A3G/F. Here, we exploit this system by demonstrating that the addition of an assortment of heterologous RNA-binding proteins and domains to Gag \u0394NC efficiently restored A3G/F packaging, indicating that A3G and A3F have the ability to engage multiple RNAs to ensure viral encapsidation. We propose that the rather indiscriminate RNA binding characteristics of A3G and A3F promote functionality by enabling recruitment into a wide range of retroviral particles whose packaged RNA genomes comprise divergent sequences.", "Language": "en", "Citations": "42"},
{"Title": "2D versus 3D colour space face detection", "Authors": ["Kovac J.", "Peer P.", "Solina F."], "Keywords": ["Art", "Color", "Computer vision", "Digital cameras", "Face detection", "Humans", "Lighting", "Peer to peer computing", "Pixel", "Skin"], "Date": "2003", "Abstract": "Computer vision is one out of many areas that want to understand the process of human functionality and copy that process with intention to complement human life with intelligent machines. For better human-computer interaction it is necessary for the machine to see people. This can be achieved by employing face detection algorithms, like the one used in the installation \"15 seconds of fame\" Franc Solina et al., (2002). Mentioned installation unites the areas of modern art and technology. Its algorithm is based on skin colour detection. One of the problems this and similar algorithms have to deal with is sensitivity to the illumination conditions under which the input image is captured. Hence illumination sensitivity influences face detection results. One of the aspects from which we can observe illumination influence is the choosing of the proper colour space. Since some colour spaces are designed to eliminate the influence of illumination when describing colour of an object, an idea of using such a colour space for skin-colour detection was taken under consideration and some of the methods were researched and tested.", "Language": "en", "Citations": "34"},
{"Title": "On approach for the implementation of data mining to business process optimisation in commercial companies", "Authors": ["Pivk A.", "Vasilecas O.", "Kalibatiene D.", "Rupnik R."], "Keywords": ["business process", "CRISP-DM", "data mining", "ontology", "SOA"], "Date": "2013", "Abstract": "Nowadays, organisations aim to automate their business processes to improve operational efficiency, reduce costs, improve the quality of customer service and reduce the probability of human error. Business process intelligence aims to apply data warehousing, data analysis and data mining techniques to process execution data, thus enabling the analysis, interpretation, and optimisation of business processes. Data mining approaches are especially effective in helping us to extract insights into customer behaviour, habits, potential needs and desires, credit associated risks, fraudulent transactions and etc. However, the integration of data mining into business processes still requires a lot of coordination and manual adjustment. This paper aims at reducing this effort by reusing successful data mining solutions. We propose an approach for implementation of data mining into a business process. The confirmation of the suggested approach is based on the results achieved in eight commercial companies, covering different industries, such as telecommunications, banking and retail. \u00a9 2013 Copyright Vilnius Gediminas Technical University (VGTU) Press Technika.", "Language": "en", "Citations": "4"},
{"Title": "Impact of changes in climate on air pollution in Slovenia between 2002 and 2017", "Authors": ["Faganeli Pucer J.", "Strumbelj E."], "Keywords": ["Adjustment", "Air pollutant levels", "Emissions", "Meteorology", "Statistical models", "Trend"], "Date": "2018", "Abstract": "Air pollutant levels depend on emissions but can also be affected by the meteorological situation. We examined air pollutant trends (PM", "Language": "en", "Citations": "3"},
{"Title": "Coloring weighted series-parallel graphs", "Authors": ["Fijavz G."], "Keywords": ["Circular coloring", "Graph coloring", "Weighted graphs"], "Date": "2006", "Abstract": "Let G be a series-parallel graph with integer edge weights. A p-coloring of G is a mapping of vertices of G into \u2124 ", "Language": "en", "Citations": "1"},
{"Title": "A framework for developing distributed location based applications", "Authors": ["Krevl A.", "Ciglaric M."], "Keywords": [], "Date": "2006", "Abstract": "Location based services and applications are buzzwords nowadays, yet they have been around for quite some time in a variety of applications. However these applications are scarce because of the high costs associated with the positioning equipment. This paper presents different options for determining location of mobile devices such as mobile phones and Pocket PCs. It describes positioning possibilities using WiFi networks, GSM networks, Bluetooth beacons and the GPS system. Furthermore, it proposes a framework for developing distributed location based applications. The paper specifies which components comprise the framework, data structures that are used for spatial data interchange and Web Services that are used for communication between components. It also describes a location aware application prototype built on top of the proposed framework. It concludes that building applications on top of the proposed framework is feasible and discusses benefits and drawbacks of this approach. \u00a9 2006 IEEE.", "Language": "en", "Citations": "7"},
{"Title": "Non-negative Spectral Measures and Representations of C*-Algebras", "Authors": ["Zalar A."], "Keywords": ["*-Representations", "C*-algebras", "operator-valued measures"], "Date": "2014", "Abstract": "Regular normalized W-valued spectral measures on a compact Hausdorff space X are in one-to-one correspondence with unital *-representations \u03c1: C(X, \u2102) \u2192 W, where W stands for a von Neumann algebra. In this paper we show that for every compact Hausdorff space X and every von Neumann algebras W ", "Language": "en", "Citations": "0"},
{"Title": "Robot George - Interactive continuous learning of visual concepts", "Authors": ["Zillich M.", "Zhou K.", "Skocaj D.", "Kristan M.", "Vrecko A.", "Mahnic M.", "Janicek M.", "Kruijff G.-J.M.", "Keller T.", "Hanheide M.", "Hawes N."], "Keywords": ["Algorithms", "Concepts", "Human Robot Interaction", "Learning", "Robotics"], "Date": "2014", "Abstract": "The video presents the robot George learning visual concepts in dialogue with a tutor. \u00a9 2013 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "Effective collection and selection of research articles for a systematic review", "Authors": ["Rozanc I."], "Keywords": [], "Date": "2018", "Abstract": "The first step in any research work is to gain a better understanding of selected research topic and its placement in the research field. To achieve this a Systematic Review (SR) is usually performed in one of two forms. A Systematic Literature Review (SLR) addresses a specific research question by providing insight into selected literature, while a Systematic Mapping Study (SMS) uses a principle of classification of a large number of collected articles to present a wider picture of the research topic. In both cases, the collection and selection of proper articles are crucial. Due to a lot of available articles from different sources and its diverse quality, a lot of tedious (often manual) work is required. This article addresses the issues of effective collection and selection of an appropriate set of articles. First, the importance of using suitable guidelines for conducting an SR is presented. Then, the practical considerations for the collection of articles are shown with an emphasis on effective search in Digital Libraries (DLs). Finally, the issues connected with an effective selection of an appropriate set of articles (from all collected) is described. In this part, we advise to collect as many articles as possible and perform sequential three-step selection to efficiently eliminate the obviously incorrect ones without a tedious manual screening of entire article contents.", "Language": "en", "Citations": "0"},
{"Title": "A methodology for sustainable monitoring of micro locations at remote, hard-to-access and unsafe places", "Authors": ["Trcek-Pecak T.", "Trcek D.", "Belic I."], "Keywords": ["Intelligent systems", "Measurement and modeling", "Multidisciplinary research", "Preservation of artworks", "Sensors", "Smart structures", "Sustainablemonitoring"], "Date": "2015", "Abstract": "Smart structures and intelligent systems play pivotal roles in numerous areas of applied sciences ranging from civil engineering to computer and communications systems engineering. Although such structures and systems have been intensively deployed in these areas, they have been, interestingly, very rarely deployed in the field of cultural heritage preservation.This paper presents one of thefirst such attempts. A new methodology is describedthat deploys smart structures andlinks them with artificial intelligence methods.These solutions are referred toas advanced hybrid engineering artefacts. By their use, important environmental factors can be monitoredin hard to access, remote or unsafe locationsby minimizing the need for human involvement. In addition toproviding safety the methodologyalso reduces costs and, most importantly, providesa new way to modelany particular micro-environment in a much more efficient way than this is possible with traditional ways. Last but not least, although themethodology has been developed for cultural heritage preservation, its application areas are much broader and it is expected that it will find its applicationin other domains like civil engineering and ecology.", "Language": "en", "Citations": "0"},
{"Title": "Immunohistochemistry for EGFR Mutation Detection in Non\u2013Small-Cell Lung Cancer", "Authors": ["Hitij N.T.", "Kern I.", "Sadikov A.", "Knez L.", "Stanic K.", "Zwitter M.", "Cufer T."], "Keywords": ["Cost-effectiveness", "Epidermal growth factor receptor mutation", "Mutation-specific antibodies", "NSCLC", "Survival"], "Date": "2017", "Abstract": "We evaluated the use of immunohistochemistry (IHC) for detection of epidermal growth factor receptor (EGFR) mutations in non&#x2013;small-cell lung cancer on a cohort of 79 EGFR-mutated Whites. IHC demonstrated high accuracy for detection of common EGFR mutations as well as for predicting response to EGFR tyrosine kinase inhibitors as compared with standard polymerase chain reaction&#x2013;based methodology. Cost-effective use of upfront IHC depends mainly on the population EGFR mutation positivity probability. Introduction The sensitivity and specificity of immunohistochemistry (IHC) was compared with the standard polymerase chain reaction (PCR)-based method for detecting common activating epidermal growth factor receptor (EGFR) mutations in non&#x2013;small-cell lung cancer (NSCLC). Additionally, we evaluated predictive value of IHC EGFR mutation&#x2013;positive status for EGFR tyrosine kinase inhibitor (TKI) treatment outcome and estimated cost-effectiveness for the upfront IHC testing. Methods The trial included 79 consecutive EGFR mutation&#x2013;positive and 29 EGFR mutation&#x2013;negative NSCLC cases diagnosed with reflex PCR-based testing. Two mutation-specific antibodies against the most common exon 19 deletion, namely E746-A750del (clone SP111) and L858R mutation (clone SP125) were tested by using automated immunostainer. Sixty of 79 EGFR mutation&#x2013;positive cases were treated with EGFR TKIs for advanced disease and included in treatment outcome analysis. A decision tree was used for the cost-effectiveness analysis. Results The overall sensitivity and specificity of the IHC-based method compared with the PCR-based method were 84.8% (95% confidence interval [CI] 74.6&#x2013;91.6) and 100% (95% CI 85.4&#x2013;100), respectively. The median progression-free survival (PFS) and overall survival (OS) of patients with IHC-positive EGFR mutation status were highly comparable to the total cohort (PFS: 14.3 vs. 14.0 months; OS: 34.4 vs. 34.4 months). The PCR and IHC cost ratio needs to be approximately 8-to-1 and 4-to-1 in White and Asian populations, respectively, to economically justify upfront use of IHC. Conclusion The trial confirmed an excellent specificity with fairly good sensitivity of IHC with mutation-specific antibodies for common EGFR mutations and the accuracy of IHC testing for predicting response to EGFR TKIs. The use of upfront IHC depends mainly on the population EGFR mutation positivity probability.", "Language": "en", "Citations": "3"},
{"Title": "Computational trust management in economics phenomena research", "Authors": ["Trcek D."], "Keywords": ["Agent technologies", "Computational trust management", "Economics research", "Simulations", "Trust"], "Date": "2018", "Abstract": "Economics phenomena are notably governed by dynamic, non-linear, bottom-up processes emerging from agents\u2019 interactions. Therefore traditional top-down approaches provide a rather limited insight into these phenomena. Further, research in economics has been mostly focused on addressing tangible factors, while human agents in economic settings often do not adhere to rational reasoning, and trust is one such kind of reasoning. Thanks to recent technological advancements new approaches are enabled, and this paper proposes a novel and anticipatory research methodology for studying economics phenomena that enables inclusion of trust. The methodology, called auxiliary composite simulations, builds upon recent advancements in computational trust management. By doing so it enables bottom-up simulations of trust driven economic phenomena. The paper provides also epistemic evaluation of the methodology and ends up with an example application of the proposed apparatus.", "Language": "en", "Citations": "0"},
{"Title": "Activation of computer science teachers in Slovenia", "Authors": ["Brodnik A.", "Lokar M.", "Mori N."], "Keywords": ["Community of practice", "General secondary school", "Master teachers", "Programming", "Teaching resources"], "Date": "2017", "Abstract": "The paper describes an approach of improving Slovenian Computer Science Education in general secondary school by forming an active and sustainable Computer Science Community of Practice (CS CoP). In project NAPOJ three systems teachers use in teaching programming are combined: CS e-textbook, LMS Moodle and TOMO, automatic assessment system for learning programming. Group of master teachers were selected, who prepared the initial set of in-class resources and material at a half a week workshop. This was followed by the regional workshops for other CS teachers throughout the Slovenia and run by master teachers. Development of CoP was observed and analyzed through various data gathering tools, such as questionnaires, discussions and observations, and preliminary results are highlighted.", "Language": "en", "Citations": "0"},
{"Title": "Object serialization analysis and comparison in Java and .NET", "Authors": ["Hericko M.", "Juric M.B.", "Rozman I.", "Beloglavec S.", "Zivkovic A."], "Keywords": [".NET", "Binary", "Java", "Serialization", "XML"], "Date": "2003", "Abstract": "This article compares binary and XML object serialization on Java and Microsoft .NET platforms from the performance and size perspective. It uses three different types of objects and different number of objects to make a comparison which reflects real-world circumstances. The article has the following contributions: (1) it compares binary and XML serialization between Java and .NET to compare the efficiency of both platforms; (2) it compares binary and XML serialization within the platforms to compare the differences between the two serialization types; (3) it studies the reasons for performance differences and provides possible performance optimizations.", "Language": "en", "Citations": "23"},
{"Title": "Data Generators for Learning Systems Based on RBF Networks", "Authors": ["Robnik-Sikonja M."], "Keywords": ["Artificial data", "data generator", "data mining", "data similarity", "radial basis function (RBF) networks", "semiartificial data."], "Date": "2016", "Abstract": "There are plenty of problems where the data available is scarce and expensive. We propose a generator of semiartificial data with similar properties to the original data, which enables the development and testing of different data mining algorithms and the optimization of their parameters. The generated data allow large-scale experimentation and simulations without danger of overfitting. The proposed generator is based on radial basis function networks, which learn sets of Gaussian kernels. These Gaussian kernels can be used in a generative mode to generate new data from the same distributions. To assess the quality of the generated data, we evaluated the statistical properties of the generated data, structural similarity, and predictive similarity using supervised and unsupervised learning techniques. To determine usability of the proposed generator we conducted a large scale evaluation using 51 data sets. The results show a considerable similarity between the original and generated data and indicate that the method can be useful in several development and simulation scenarios. We analyze possible improvements in the classification performance by adding different amounts of the generated data to the training set, performance on high-dimensional data sets, and conditions when the proposed approach is successful.", "Language": "en", "Citations": "16"},
{"Title": "K6-minors in projective planar graphs", "Authors": ["Fijavz G.", "Mohar B."], "Keywords": [], "Date": "2003", "Abstract": "It is shown that every 5-connected graph embedded in the projective plane with face-width at least 3 contains the complete graph on 6 vertices as a minor.", "Language": "en", "Citations": "12"},
{"Title": "Non-sequential multi-view detection, localization and identification of people using multi-modal feature maps", "Authors": ["Mandeljc R.", "Kovacic S.", "Kristan M.", "Pers J."], "Keywords": [], "Date": "2013", "Abstract": "We present a novel multi-modal fusion framework for non-sequential person detection, localization and identification from multiple views. Our goal is independent processing of randomly-accessed sections of video, either individual frames or small batches thereof. This way, we aim to limit the error propagation that makes the existing approaches unsuitable for fully-autonomous tracking of multiple people in long video sequences. Our framework uses one or more trained classifiers to fuse multiple weak feature maps. We perform experimental validation on a challenging dataset, demonstrating how the framework can, depending on the provided feature maps, be used either only to improve generic person detection, or enable simultaneous detection and recognition of individuals. Finally, we show that tracking-by-identification using the output of the proposed framework outperforms the state-of-the-art identification-by-tracking approach in terms of preserved track identities. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "A Krull-Schmidt theorem for infinite products of modules", "Authors": ["Franetic D."], "Keywords": ["Direct-product decomposition", "Krull-schmidt-remak-azumayazumaya theorem", "Semiperfect ring", "Slender module"], "Date": "2014", "Abstract": "We prove a unique decomposition theorem for direct products of finitely generated modules over certain classes of rings, which is analogous to the classical Krull-Schmidt-Remak-Azumaya theorem for direct-sum decompositions of modules. \u00a9 2014 Elsevier Inc.", "Language": "en", "Citations": "2"},
{"Title": "Segmentation-based deep-learning approach for surface-defect detection", "Authors": ["Tabernik D.", "Sela S.", "Skvarc J.", "Skocaj D."], "Keywords": ["Computer vision", "Deep learning", "Industry 4.0", "Quality control", "Segmentation networks", "Surface-defect detection", "Visual inspection"], "Date": "2019", "Abstract": "Automated surface-anomaly detection using machine learning has become an interesting and promising area of research, with a very high and direct impact on the application domain of visual inspection. Deep-learning methods have become the most suitable approaches for this task. They allow the inspection system to learn to detect the surface anomaly by simply showing it a number of exemplar images. This paper presents a segmentation-based deep-learning architecture that is designed for the detection and segmentation of surface anomalies and is demonstrated on a specific domain of surface-crack detection. The design of the architecture enables the model to be trained using a small number of samples, which is an important requirement for practical applications. The proposed model is compared with the related deep-learning methods, including the state-of-the-art commercial software, showing that the proposed approach outperforms the related methods on the specific domain of surface-crack detection. The large number of experiments also shed light on the required precision of the annotation, the number of required training samples and on the required computational cost. Experiments are performed on a newly created dataset based on a real-world quality control case and demonstrates that the proposed approach is able to learn on a small number of defected surfaces, using only approximately 25\u201330 defective training samples, instead of hundreds or thousands, which is usually the case in deep-learning applications. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. The dataset is also made publicly available to encourage the development and evaluation of new methods for surface-defect detection.", "Language": "en", "Citations": "0"},
{"Title": "Content networks: Distributed routing decisions in presence of repeated queries", "Authors": ["Ciglaric M."], "Keywords": ["Content network", "content-based routing", "distributed search", "flooding", "peer-to-peer networking"], "Date": "2004", "Abstract": "Content networks are overlay networks, enabling access to distributed contents on centralized servers or individual computers. Since flooding-based routing scheme features poor scalability, we present a modification, which reduces the total network traffic while retaining the original efficiency. In choosy routing, as we call it, each node, while passing an answer, remembers which neighbor it came from. Subsequently repeated queries about the same content are forwarded only to that neighbor. This way, the network learns effective routes. The simulations on several topology types have shown the expected behavior, with up to three-fold reduction in the overall query traffic. \u00a9 2004 World Scientific Publishing Company.", "Language": "en", "Citations": "3"},
{"Title": "Using machine learning to understand operator\u2019s skill", "Authors": ["Bratko I.", "Suc D."], "Keywords": [], "Date": "2002", "Abstract": "Controlling complex dynamic systems requires skills that operators often cannot completely describe, but can demonstrate. This paper describes research into the understanding of such tacit control skills. Understanding tacit skills has practical motivation in respect of communicating skill to other operators, operator training, and also mechanising and optimising human skill. This paper is concerned with approaches whereby, using techniques of machine learning, controllers that emulate the human operators are generated from examples of control traces. This process is also called \u201dbehavioural cloning\u201d. The paper gives a review of ML-based approaches to behavioural cloning, representative experiments, and an assessment of the results. Some recent work is presented with particular emphasis on understanding human tacit skill, and generating explanation of how it works. This includes the extraction of the operator\u2019s subconscious sub-goals and the use of qualitative control strategies. We argue for qualitative problem representations and decomposition of the machine learning problem involved.", "Language": "en", "Citations": "2"},
{"Title": "Clustering gene expression data with temporal abstractions", "Authors": ["Sacchi L.", "Bellazzi R.", "Larizza C.", "Magni P.", "Curk T.", "Petrovic U.", "Zupan B."], "Keywords": ["Bioinfonnatics", "cluster analysis"], "Date": "2004", "Abstract": "This paper describes a new technique for clustering short time series comingfrom gene expression data. The technique is based on the labelling of the time series through temporal trend abstractions and a consequent clustering of the series on the basis of their labels. Clustering is performed at three different levels of aggregation of the original time series, so that the results are organized and visualized as a three-levels hierarchical tree. Results on simulated and on yeast data are shown. The technique appears robust and efficient and the results obtained are easy to be interpreted. \u00a9 2004 IMIA. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "MAC based lightweight protocols for strong authentication and key exchange", "Authors": ["Trcek D."], "Keywords": [], "Date": "2005", "Abstract": "Protocols that provide authentication and key distribution are mainly based on symmetric and asymmetric ciphers. In recent years, some approaches have been introduced that are based on strong one-way hash functions, and further enhancements to these approaches are given in this paper. The lightweight protocols that are presented in this paper are suitable for implementation with simple logic. They enable early recognition of attacks and make distributed session key generation possible. They are intended for use in environments with limited processing capabilities, where relatively short messages are being exchanged, e.g., agents environments. message authentication codes, one-way hash functions, lightweight protocols, authentication and key exchange, MBAKE schemes.", "Language": "en", "Citations": "3"},
{"Title": "New media art projects, panoramic images and live video as interface between real and virtual worlds", "Authors": ["Solina F."], "Keywords": ["CAD", "Internet video server", "Panoramic images", "Slovenia", "Virtual gallery"], "Date": "2014", "Abstract": "This paper starts with the concept of a virtual gallery where art works, mainly pictures, are presented on the world wide web. One of the first such galleries, the Slovenian Virtual Gallery existing from 1995 is being considered. Today, such galleries are conceptually not much different, however, they typically contain much more material of higher resolution in 2D and 3D. Several open source tools are available for development of such virtual galleries. To gain some notion of immediacy, efforts were made to mix such virtual worlds with live video from actual spaces and locations. A novel user interface was developed using panoramic images of the location where the internet camera is located. The live video image is shown overlaid on the static panoramic image so that when a tele-operator manipulates the camera he gets a much better understanding of the remote location. A new media art installation was produced. First, successfully the concept of the virtual gallery and the live video panoramic interface and later interactive art installations was taken up where video or images were used as primary means of interaction and feedback. Five such art installations are presented; the sixth project shows how this concept can be applied to adaptive digital signage. \u00a9 2014, DESIDOC.", "Language": "en", "Citations": "1"},
{"Title": "Load-balanced generation of a restricted set of combinations", "Authors": ["Slivnik B.", "Rozanc I."], "Keywords": ["Combinatorial problems", "Generating combinations", "Load balancing"], "Date": "2011", "Abstract": "A simple yet efficient load-balanced algorithm for generating a restricted set of combinations of k items taken from a set A of n items is described. The set can be restricted by a number of constraints each specifying that exactly (or at most, or at least) k\u2032 elements must be taken from a set A\u2032, for different k\u2032 < k and A\u2032 \u2286 A. The UNRANK procedure is not needed and we demonstrate that arbitrary precision arithmetic (operating on integers) can be avoided in most practical applications. The algorithm is thus suitable for computing environments with limited resources (like GPUs). The empirical evaluation of the algorithm is included.", "Language": "en", "Citations": "0"},
{"Title": "Notes on memorizing in fuzzy decision process", "Authors": ["Mraz M.", "Oseli D.", "Zimic N.", "Iztok L.B."], "Keywords": ["Application software", "Databases", "Fuzzy control", "Fuzzy sets", "Information science"], "Date": "2002", "Abstract": "In this paper we present basic notes on memorizing in a fuzzy decision process. As fuzzy approaches become widely used in different applications we could expect that research and technology will lead us towards more efficient fuzzy processing. It will be directly enabled by fuzzy specialized computers on which the fuzzy decision process will take place. Considering the main computer's characteristics, which are the possibilities of processing, memorizing and transferring data, the lack of knowledge or technology is mainly in the field of memorizing in the fuzzy decision process. Questions arise such as: what should be memorized in the fuzzy decision process, and how does one memorize something in a fuzzy decision process? In our paper we attempt to answer these questions.", "Language": "en", "Citations": "0"},
{"Title": "Software systems through complex networks science: Review, analysis and applications", "Authors": ["Subelj L.", "Bajec M."], "Keywords": ["Network analysis", "Software engineering", "Software networks", "Software systems"], "Date": "2012", "Abstract": "Complex software systems are among most sophisticated human-made systems, yet only little is known about the actual structure of 'good' software. We here study different software systems developed in Java from the perspective of network science. The study reveals that network theory can provide a prominent set of techniques for the exploratory analysis of large complex software system. We further identify several applications in software engineering, and propose different network-based quality indicators that address software design, eficiency, reusability, vulnerability, controllability and other. We also highlight various interesting findings, e.g., software systems are highly vulnerable to processes like bug propagation, however, they are not easily controllable. Categories and Subject Descriptors D.2.8 [Software Engineering]: Metrics|complexity mea- sures, performance measures, software science General Terms Theory, algorithms, experimentation.", "Language": "en", "Citations": "25"},
{"Title": "Natural and machine learning, intelligence and consciousness", "Authors": ["Kononenko I."], "Keywords": [], "Date": "2009", "Abstract": "If you understand others you are intelligent. If you understand yourself you are enlightened. - Lao Tse In the first part of this chapter, which is more \u201cscientific\u201d, we discuss learning and its relation to natural and artificial intelligence, and we relate learning and intelligence to (phenomenal) consciousness. Practising, imitating the teacher, and repeated trial and error is called learning. The process of transformation due to learning is called knowledge acquisition. Learning by a living system is called natural learning; if the learner is a machine - a computer - it is called machine learning. Learning is a precondition for intelligent behavior. Intelligence can be defined as the ability to adapt to the environment and to solve problems. Artificial intelligence research deals with the development of systems that act more or less intelligently and are able to solve relatively hard problems. With respect to the complexity of the learning process we differentiate several learning types and learners use different search strategies which are also used in machine learning. Important aspects for understanding the abilities of artificial intelligence are the impact of learning on intelligence, the speed of problem solving, the principal limitations of algorithms (stemming from the theory of computability), and the imitation of intelligent behavior. We expect an intelligent system to be (at least to some extent) intelligent in all the areas which are characteristic for human problem solving. However, most speculations about artificial intelligence do not take into account yet another level: consciousness. Consciousness seems to be fundamentally related to the following notions: life, intelligence, and free will. The world (the real-ity - the analogyin mathematics are real numbers, which are in great majority irrational) is most probably non-describable by any of the symbolic formalisms which we are able to use with our rational mind (that can tackle only the rational part of reality - the analogy in mathematics are rational numbers), and the same limitation holds for computers. In the second part, which is less scientific and expresses the author\u2019s own viewpoint, we discuss the relation between objectivity (in the sense of measurability and describability), and (phenomenal) subjectivity (in the sense of the 1st person, direct experience) which seem to be the relation between intelligent and conscious behavior and is in turn the relation between objective science and subjective spirituality (in the sense of spiritual, phenomenal experience). Phenomenal consciousness is highly subjective (in the sense of the 1st person experience), while science - by definition - is struggling to be objective. Scientists mostly use the rational mind (intellect) in order to indirectly and objectively study matter and to derive knowledge. Mystics on the other hand mostly use the intuitive mind (inner sense, heart) in order to directly and subjectively experience consciousness and to attain wisdom. Both extremes, objective science and experiental (phenomenal) spirituality search for the truth. They are complementary to each other and we need both.", "Language": "en", "Citations": "0"},
{},
{"Title": "Learning compositional hierarchies of a sensorimotor system", "Authors": ["Zabkar J.", "Leonardis A."], "Keywords": ["compositional hierarchy", "computational modeling", "sensorimotor representation"], "Date": "2013", "Abstract": "We address the problem of learning static spatial representation of a robot motor system and the environment to solve a general forward/inverse kinematics problem. The latter proves complex for high degree-of-freedom systems. The proposed architecture relates to a recent research in cognitive science, which provides a solid evidence that perception and action share common neural architectures. We propose to model both a motor system and an environment with compositional hierarchies and develop an algorithm for learning them together with a mapping between the two. We show that such a representation enables efficient learning and inference of robot states. We present our experiments in a simulated environment and with a humanoid robot Nao. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "Danzer's configuration revisited: Dedicated to the memory of Ludwig Danzer (1927-2011)", "Authors": ["Boben M.", "Gevay G.", "Pisanski T."], "Keywords": ["Cayley-Salmon configuration", "Coxeter (283)-configuration", "Danzer configuration", "Danzer graph", "Kronecker cover,V-construction,Hexagrammum Mysticum", "Odd graph", "point-circle configuration", "Steiner-Pl\u00fccker configuration"], "Date": "2015", "Abstract": "We revisit the configuration DCD(4) of Danzer, a great inspiration for our work. This configuration of type (35", "Language": "en", "Citations": "4"},
{"Title": "Compression of convolutional neural networks: A short survey", "Authors": ["Pilipovic R.", "Bulic P.", "Risojevic V."], "Keywords": ["compact network architectures", "Compression", "Convolutional neural networks", "precision", "pruning"], "Date": "2018", "Abstract": "Nowadays, convolutional neural networks (CNN) are considered as the state-of-the-art algorithms for various tasks, especially for image classification and recognition. Because of that, more and more attention is aimed towards implementation of CNNs on embedded systems. Main obstacles for implementing CNNs on embedded systems are their large model size and large number of operations needed for inference. In order to surpass these obstacles, algorithms for CNN compression tend to lower model size and number of operations needed for inference. In this paper we review the state-of-the-art in CNN compression. To this end we divided all approaches for CNN compression into three groups: precision reduction, network pruning and design of compact network architectures. After presenting the main approaches in each group we conclude that the future CNN compression algorithms should be co-designed with hardware which will process deep learning algorithms.", "Language": "en", "Citations": "0"},
{"Title": "Gender differences in Parkinson's disease: A clinical perspective", "Authors": ["Georgiev D.", "Hamberg K.", "Hariz M.", "Forsgren L.", "Hariz G.-M."], "Keywords": ["activities of daily living", "gender differences", "motor symptoms", "non-motor symptoms", "Parkinson's disease", "quality of life"], "Date": "2017", "Abstract": "Available data indicate that there are gender differences in many features of Parkinson's disease (PD). Precise identification of the gender differences is important to tailor treatment, predict outcomes, and meet other individual and social needs in women and men with PD. The aim of this study was to review the available clinical data on gender differences in PD. Original articles and meta-analyses published between 1990 and 2016 systematically exploring gender differences in PD were reviewed. There is slight male preponderance in incidence and prevalence of PD. PD starts earlier in men. Women tend to be more prone to develop tremor-dominant PD but are less rigid than men. Motor improvement after deep brain stimulation is equal in both sexes, but women tend to show better improvement in activities of daily living. Furthermore, women with PD show better results on tests for general cognitive abilities, outperform men in verbal cognitive tasks, show more pain symptoms, and score higher on depression scales. It seems, however, that the differences in cognition, mood, and pain perception are not disease specific as similar gender differences can be found in healthy subjects and in other neurological conditions. Despite PD being the most frequently studied movement disorder, studies investigating gender differences in PD are still scarce with most of the studies being cross-sectional. Good-quality, prospective, longitudinal studies analyzing gender differences in PD and comparing them to matched healthy controls are needed in order to properly address the issues of gender differences in PD.", "Language": "en", "Citations": "14"},
{"Title": "Improving vision-based obstacle detection on USV using inertial sensor", "Authors": ["Bovcon B.", "Mandeljc R.", "Pers J.", "Kristan M."], "Keywords": ["inertial measurement unit", "Markov random fields", "obstacle detection", "Unmanned surface vehicles"], "Date": "2017", "Abstract": "We present a new semantic segmentation algorithm for obstacle detection in unmanned surface vehicles. The novelty lies in the graphical model that incorporates boat tilt measurements from the on-board inertial measurement unit (IMU). The IMU readings are used to estimate the location of horizon line in the image, and automatically adjusts the priors in the probabilistic semantic segmentation algorithm. We derive the necessary horizon projection equations, an efficient optimization algorithm for the proposed graphical model, and a practical IMU-camera-USV calibration. A new challenging dataset, which is the largest multi-sensor dataset of its kind, is constructed. Results show that the proposed algorithm significantly outperforms state of the art, with 32% improvement in water-edge detection accuracy, an over 15 % reduction of false positive rate, an over 70 % reduction of false negative rate, and an over 55 % increase of true positive rate, while running in real-time on a single core in Matlab.", "Language": "en", "Citations": "2"},
{"Title": "Feasibility of biometric authentication using wearable ECG body sensor based on higher-order statistics", "Authors": ["Sprager S.", "Trobec R.", "Juric M.B."], "Keywords": [], "Date": "2017", "Abstract": "Besides its principal purpose in the field of biomedical applications, ECG can also serve as a biometric trait due to its unique identity properties, including user-specific deviations in ECG morphology and heart rate variability. In this paper, we exploit the possibility to use long-term ECG data acquired by unobtrusive chest-worn ECG body sensor during daily living for accurate user authentication and identification. Therefore, we propose a novel framework for wearable ECG-based user recognition. The core of the framework is based on the approach that employs higher-order statistics on cyclostationary data, already efficiently applied for inertial-sensor-based gait recognition. Experimental data was collected by four subjects during their regular daily activities with more than 6 hours of ECG data per subject and then applied to the proposed framework. Preliminary results (equal error rate from 6% to 13%, depending on the experimental parameters) indicate that such authentication is feasible and reveal clear guidelines towards future work.", "Language": "en", "Citations": "5"},
{"Title": "Collaborative view-aligned annotations in web-based 3D medical data visualization", "Authors": ["Lavric P.", "Bohak C.", "Marolt M."], "Keywords": [], "Date": "2017", "Abstract": "the paper presents our web-based 3D medical data visualization framework with emphasis on user collaboration. The framework supports visualization of volumetric data and 3D meshes in web browsers. The paper focuses on integration of user-shareable 3D view-aligned hand drawn or written annotations into the visualization framework. Annotations are created on separate transparent canvases which are aligned with selected views. View parameters are part of annotations and can be shared with other users over the network. Our implementation allows for real-time sharing of annotations during creation. Annotations from the same or different users can be overlaid within the same view. Annotations were implemented through adaptation of the framework's rendering pipeline, which allows for combining multiple visualization layers into a unified final render. View aligned annotations were added in addition to text annotations pinned to 3D locations on the displayed model. In the framework, users can list through all annotations, whereby upon selection of a 3D view-aligned annotation the camera is positioned according to the stored parameters and the annotation is displayed.", "Language": "en", "Citations": "7"},
{"Title": "Learning random numbers: A matlab anomaly", "Authors": ["Savicky P.", "Robnik-Sikonja M."], "Keywords": [], "Date": "2008", "Abstract": "We describe how dependencies between random numbers generated with some popular pseudo-random number generators can be detected using general purpose machine-learning techniques. This is a novel approach, since usually pseudo-random number generators are evaluated using tests specifically designed for this purpose. Such specific tests are more sensitive. Hence, detecting the dependence using machine-learning methods implies that the dependence is indeed very strong. The most important example of a generator, where dependencies may easily be found using our approach, is MATLAB's function rand if the method state is used. This method was the default in MATLAB versions between 5 (1995) and 7.3 (2006b), i.e., for more than 10 years. In order to evaluate the strength of the dependence in it, we used the same machine-learning tools to detect dependencies in some other random number generators, which are known to be bad or insufficient for large simulations: the infamous RANDU, ANSIC, the oldest generator in C library, minimal standard generator, suggested by Park and Miller (1988), and the rand function in Microsoft C compiler.", "Language": "en", "Citations": "5"},
{"Title": "Information technology deployment in a transition economy: Results from Slovenia", "Authors": ["Hovelja T."], "Keywords": ["Enterprises", "Information technology deployment", "Institutions", "Organisational transformation", "Slovenia"], "Date": "2009", "Abstract": "Information technology (IT) is increasingly establishing itself as one of the major topics of study in the OECD area. The resulting OECD studies found that IT has an enormous productive potential; however before an economy can gain most of IT's benefits, several challenges need to be successfully addressed. The key challenges these studies identified are adequate organisational transformations of the enterprises and adequate reorganisation of key national institutions. How these two challenges are tackled by the economies that are going through the transition from a socialist towards a coordinated/liberal market economy is, unfortunately, not equally well documented. To improve this situation in this paper I present new findings from one transition economy concerning the issues that the developed OECD countries already highlighted as critical for the successful deployment of IT, and issues that seem specific to the transition environments. The presented findings are based on the study I conducted into 94 enterprises, representing the population of the 914 biggest added value generating enterprises in Slovenia. This article thus tries to allow Slovenia and other economies in a similar situation to draw broad and important conclusions with managerial and political implications on how to deploy all available IT potential.", "Language": "en", "Citations": "3"},
{"Title": "Toward in vivo digital synchronous sequential circuits", "Authors": ["Moskon M.", "Ciglic M.", "Zimic N.", "Mraz M."], "Keywords": ["Biological modelling", "Circuits", "Master-slaves flip-flop", "Ordinary differential equations", "Protein-based computing", "Sequential logic", "Synchronization", "Synthetic biology"], "Date": "2009", "Abstract": "We present the model of a first digital synchronous sequential circuit, i.e. 1-bit synchronous counter, which is about to be realized within the living cell. Mathematical model was constructed upon gene expression based logic with ordinary differential equations (ODEs), particularly Hill equations. The behaviour of the counter was simulated in Matlab/ Simulink environment. With the promising simulation results presented in the paper physical realization of the circuit described within the living cell can be initiated. Introduction of synchronization with a special signal, i.e. clock signal, brings many benefits to the field. Realization of such circuit would therefore present an important step toward the construction of complex biological information processing capable systems.", "Language": "en", "Citations": "5"},
{"Title": "Computational framework for modeling multiple noncooperative transcription factor binding and its application to the analysis of nuclear factor kappa B oscillatory response", "Authors": ["Bizjak M.", "Zimic N.", "Mraz M.", "Moskon M."], "Keywords": ["Computational analysis", "Gene regulatory networks", "Noncooperative transcription factor binding", "Quantitative modeling", "Transcription factor NF-\u03baB"], "Date": "2016", "Abstract": "Recent studies have shown that regulation of many important genes is achieved with multiple transcription factor (TF) binding sites with low or no cooperativity. Additionally, noncooperative binding sites are gaining more and more importance in the field of synthetic biology. Here, we introduce a computational framework that can be applied to dynamical modeling and analysis of gene regulatory networks with multiple noncooperative TF binding sites. We propose two computational methods to be used within the framework, that is, average promoter state approximation and expression profiles based modeling. We demonstrate the application of the proposed framework on the analysis of nuclear factor kappa B (NF-\u03baB) oscillatory response. We show that different promoter expression hypotheses in a combination with the number of TF binding sites drastically affect the dynamics of the observed system and should not be ignored in the process of quantitative dynamical modeling, as is usually the case in existent state-of-the-art computational analyses.", "Language": "en", "Citations": "0"},
{"Title": "The CHEMDNER corpus of chemicals and drugs and its annotation principles", "Authors": ["Krallinger M.", "Rabal O.", "Leitner F.", "Vazquez M.", "Salgado D.", "Lu Z.", "Leaman R.", "Lu Y.", "Ji D.", "Lowe D.M.", "Sayle R.A.", "Batista-Navarro R.T.", "Rak R.", "Huber T.", "Rocktaschel T.", "Matos S.", "Campos D.", "Tang B.", "Xu H.", "Munkhdalai T.", "Ryu K.H.", "Ramanan S.V.", "Nathan S.", "Zitnik S.", "Bajec M.", "Weber L.", "Irmer M.", "Akhondi S.A.", "Kors J.A.", "Xu S.", "An X.", "Sikdar U.K.", "Ekbal A.", "Yoshioka M.", "Dieb T.M.", "Choi M.", "Verspoor K.", "Khabsa M.", "Giles C.L..", "Liu H.", "Ravikumar K.E.", "Lamurias A.", "Couto F.M.", "Dai H.-J.", "Tsai R.T.-H.", "Ata C.", "Can T.", "Usie A.", "Alves R.", "Segura-Bedmar I.", "Martinez P.", "Oyarzabal J.", "Valencia A."], "Keywords": [], "Date": "2015", "Abstract": "The automatic extraction of chemical information from text requires the recognition of chemical entity mentions as one of its key steps. When developing supervised named entity recognition (NER) systems, the availability of a large, manually annotated text corpus is desirable. Furthermore, large corpora permit the robust evaluation and comparison of different approaches that detect chemicals in documents. We present the CHEMDNER corpus, a collection of 10,000 PubMed abstracts that contain a total of 84,355 chemical entity mentions labeled manually by expert chemistry literature curators, following annotation guidelines specifically defined for this task. The abstracts of the CHEMDNER corpus were selected to be representative for all major chemical disciplines. Each of the chemical entity mentions was manually labeled according to its structure-associated chemical entity mention (SACEM) class: abbreviation, family, formula, identifier, multiple, systematic and trivial. The difficulty and consistency of tagging chemicals in text was measured using an agreement study between annotators, obtaining a percentage agreement of 91. For a subset of the CHEMDNER corpus (the test set of 3,000 abstracts) we provide not only the Gold Standard manual annotations, but also mentions automatically detected by the 26 teams that participated in the BioCreative IV CHEMDNER chemical mention recognition task. In addition, we release the CHEMDNER silver standard corpus of automatically extracted mentions from 17,000 randomly selected PubMed abstracts. A version of the CHEMDNER corpus in the BioC format has been generated as well. We propose a standard for required minimum information about entity annotations for the construction of domain specific corpora on chemical and drug entities. The CHEMDNER corpus and annotation guidelines are available at: http://www.biocreative.org/resources/biocreative-iv/chemdner-corpus/.", "Language": "en", "Citations": "48"},
{"Title": "Towards RFID traceability systems of farmed fish supply chain", "Authors": ["Trebar M.", "Grah A.", "Melcon A.A.", "Parreno A."], "Keywords": [], "Date": "2011", "Abstract": "In the project \"RFID from Farm to Fork\" an implementation of RFID technologies will be used along the food supply chain: from farm to the consumer. The paper is intended to highlight two examples of how to define farmed fish traceability system suitable for small and medium sized enterprises (SMEs). The first one presents a change from a manual collection of data to an electronic RFID implementation in a small company that performs a complete supply chain from fish farm to retail and private customers. In the second one, a part of already automated process of packing fish using barcode labeling will be upgraded by RFID technology, and traceability will be extended back to breeding and on-growing fish farms, which currently use manual collection of data. The proposed design and selection of fixed and mobile RFID readers in different steps of pilot implementation are defined as modules which could be used as a general approach to apply an automated business process. \u00a9 2011 University of Split.", "Language": "en", "Citations": "10"},
{"Title": "Identifying data dependencies with a sufficiently large distance between memory references in a multimedia vectorizing compiler", "Authors": ["Bulic P.", "Dobravec T."], "Keywords": [], "Date": "2008", "Abstract": "In this paper we consider the problem of testing a single dependence equation for a constrained integer solution in parallelization for microprocessors with a multimedia extension. For the short SIMD parallelism extraction it is essential that, if dependency exists, then the distance between memory references is greater than or equal to the number of data processed in the SIMD register. This implies that some loops that could not be vectorized on traditional vector processors can still be parallelized for the short SIMD execution. There are a number of data dependence tests that have been proposed in the literature. In all of these tests the parallelization may be prohibited when actually there is no parallelism restriction relating to the short SIMD execution model. In the paper we present a simple and efficient data dependence testing method. The presented method is a special application of dependence testing and takes the dependence distance into consideration. The presented method is suitable for use in a dependence analyzer that is organized as a series of tests, progressively increasing in accuracy, as a one of the first used simple and inexpensive tests.", "Language": "en", "Citations": "0"},
{"Title": "Students' perceptions of scrum practices", "Authors": ["Mahnic V.", "Rozanc I."], "Keywords": [], "Date": "2012", "Abstract": "In order to prepare students for increasing use of agile methods in industry, teaching these methods is becoming an important part of the Software Engineering curricula. At the University of Ljubljana Scrum has been systematically taught since 2009 in the framework of the software engineering capstone course. The paper describes the course content and analyzes results of the survey that was performed among students with the aim of identifying those practices that students perceive most important for the success of Scrum-based software projects. Students' opinions on 12 typical practices representing possible success factors are described and compared to opinions of professional developers in order to find out similarities and differences in their perceptions. Both groups of respondents identified team-work and communication among team members, as well as good communication with the Product Owner, most important. Students also stressed the importance of strict adherence to the notion of \"done\", while professional developers ranked third Sprint Planning Meetings and maintenance of Sprint Backlog. Accuracy of user stories and velocity estimation was rated least important by both groups of respondents. \u00a9 2012 MIPRO.", "Language": "en", "Citations": "13"},
{"Title": "Computational models reveal genotype-phenotype associations in Saccharomyces cerevisiae", "Authors": ["Franco-Duarte R.", "Mendes I.", "Umek L.", "Drumonde-Neves J.", "Zupan B.", "Schuller D."], "Keywords": ["Data mining", "Microsatellite", "Nearest-neighbour classifier", "Phenotypic characterization", "Saccharomyces cerevisiae"], "Date": "2014", "Abstract": "Genome sequencing is essential to understand individual variation and to study the mechanisms that explain relations between genotype and phenotype. The accumulated knowledge from large-scale genome sequencing projects of Saccharomyces cerevisiae isolates is being used to study the mechanisms that explain such relations. Our objective was to undertake genetic characterization of 172 S. cerevisiae strains from different geographical origins and technological groups, using 11 polymorphic microsatellites, and computationally relate these data with the results of 30 phenotypic tests. Genetic characterization revealed 280 alleles, with the microsatellite ScAAT1 contributing most to intrastrain variability, together with alleles 20, 9 and 16 from the microsatellites ScAAT4, ScAAT5 and ScAAT6. These microsatellite allelic profiles are characteristic for both the phenotype and origin of yeast strains. We confirm the strength of these associations by construction and cross-validation of computational models that can predict the technological application and origin of a strain from the microsatellite allelic profile. Associations between microsatellites and specific phenotypes were scored using information gain ratios, and significant findings were confirmed by permutation tests and estimation of false discovery rates. The phenotypes associated with higher number of alleles were the capacity to resist to sulphur dioxide (tested by the capacity to grow in the presence of potassium bisulphite) and the presence of galactosidase activity. Our study demonstrates the utility of computational modelling to estimate a strain technological group and phenotype from microsatellite allelic combinations as tools for preliminary yeast strain selection. \u00a9 2014 John Wiley & Sons, Ltd.", "Language": "en", "Citations": "8"},
{"Title": "Development of a framework for dynamic creation of web-interfaces to support data acquisition in clinical settings", "Authors": ["Smrdel A."], "Keywords": ["Dynamic creation of web-interface", "Relational database", "Web-interface framework"], "Date": "2017", "Abstract": "We present a new framework for dynamic creation of web-interfaces to acquire data and to manage the acquired data. The requirements for the framework are such that it is suitable for acquiring the person-related data and requires minimal programming skills to set it up and use. The developed framework is connected to a relational database management system and reads the structure of the records from the database. According to the structure of the records, the framework generates a web-interface consisting of several web-pages. The structure of the records is also used to manage the data in the database. Positioning of the web-page elements is achieved by using cascading style sheets. These features enable changes to the existing structure of the records which are automatically reflected in the generated web-interface without the need to change the underlying code. All the above features, in combination with the person-related data acquisition design, make this framework unique, and enable changes to the structure of the records and use of the framework for different purposes without the need for altering the programming code. We also present a case-study of using the framework.", "Language": "en", "Citations": "0"},
{"Title": "Research about measurability of information quality", "Authors": ["Fidler M.", "Lavbic D."], "Keywords": ["Accuracy", "Big data", "Completeness", "Consistency", "Information quality", "Information quality dimensions", "Inter rater reliability", "Objectivity"], "Date": "2015", "Abstract": "This article will discuss ongoing research about Information Quality (IQ). Raters evaluating various IQ dimensions (accuracy, completeness, objectivity\u2026) of same object showed low agreement level, therefore making IQ not measurable. Increase of IQ measurability to sufficient level would present an opportunity for guidelines to replace information of low with high quality. Speculations why IQ dimensions are not measurable have been made but at the same time mechanisms that improve agreement level have been proposed by researchers for validation. Moreover context in which information is being evaluated has not been yet addressed by existing research. This article will describe and explain a study that aims to create a robust model that will validate and measure effect of three different IQ aspects. Although this article is still work in progress, current results regarding research construction and preliminary testing will be presented as well as future steps.", "Language": "en", "Citations": "0"},
{"Title": "Extending traditional learning by enforcing collaboration and self-assessment", "Authors": ["Vicic J.", "Kavsek B.", "Kljun M.", "Brodnik A."], "Keywords": ["Blended learning", "Collaboration", "E-learning", "Information Communication Technology (ICT)", "Learning Management System (LMS)", "Self-assessment", "Social constructivism", "Traditional learning"], "Date": "2007", "Abstract": "The paper presents a way to overcome the shortcomings of traditional learning by enforcing collaboration between students and introducing self-assessment as part of the process of final grade formation. Treating collaboration and self-assessment as two elements of a modern learning process that are very closely bounded together, the authors argue that these elements should by no means replace the traditional (ex-cathedra) way of learning but rather extend it. A specifically designed computer science course is presented as an illustration of how the introduction of self-assessment combined with teacher evaluation can encourage collaboration between students. The benefits and drawbacks of this method are discussed.", "Language": "en", "Citations": "2"},
{"Title": "The language of moving pictures in computer-based visualizations of a literary-history database Jezik gibljivih slik v racunalni\u0161kih vizualizacijah literarnozgodovinske podatkovne zbirke", "Authors": ["Bovcon N."], "Keywords": ["\"Women writers\" database", "Diagrammatic knowledge", "Digital humanities", "Information visualization", "Literary history", "Literature and new media", "User interface"], "Date": "2014", "Abstract": "A digital humanities project has to find an adequate way for presenting the contents of the database it researches. To resolve this task an interdisciplinary team is formed in which a researcher of humanities, a graphic designer and a computer engineer collaborate. The user interface that structures the ordering of the database and guides the queries, as well as its final stage, visualization of the retrieved results, are based on the principles of graphic design, montage of a moving image and the principles of new media. In the background of information visualization and information design is the ability for diagrammatic thinking. The first part of the paper explicates how coding of meaning is based on technologies and different communication media, such as film, video and new media objects. This awareness is a necessary condition for understanding of the complex functioning of information visualization on computers. The usage of quantitative approaches in humanities research is problematized, as it is the basis for the majority of visualization methods, while humanities operate with qualitative, complex, not easily reduced and quantified entities. The second part of the paper presents an experiment in computer-based visualizations of a literary-history database WomenWriters, where the theoretical concepts were tested in practice. The screen-images of selected visualizations show the user interfaces: how meaning is coded in graphical signs organized on the surface of the computer screen and how moving images are used in cases of interaction and animation. The results of the experiment are interpreted and evaluated in the conclusion by considering the relation between the contents of the concrete database and the outcomes of its visualizations.", "Language": "en", "Citations": "1"},
{"Title": "Automatic pluviograph strip chart reading Ra\u010dunalni\u0161ko branje padavinskih grafov", "Authors": ["Derganc G.", "Peer P."], "Keywords": ["Computer vision", "Digitalisation", "Meteorology", "Pluviograph", "Rainfall"], "Date": "2010", "Abstract": "An algorithm aimed at automatic detection and digitalization of the rainfall signal recorded by the float based rain gauges on paper strip charts (Fig. 1) is presented. The algorithm consists of several steps that gradually lead to the desired goal. The rainfall signal is extracted from the digital image of the strip chart. By using the moving average method (Fig. 3) and curve edge following method (Fig. 2) the rainfall curve is detected and uniquely determined. In each image column there is one single point representing the rainfall curve plotline. From the curve plotline a high-resolution rainfall time series is obtained. Besides image analysis techniques in the design of the algorithm, the mechanical features of the recording instrument were taken into consideration. The availability of high resolution rainfall time series is required in many applications, including rainfall classification, analysis of extreme rainfall events, calibration of raifall-runoff models, weather prediction models and many research projects. The algorithm was tested on 58 pluviograph strip chart images. A comparison between the data obtained with the proposed algorithm and the official data from the Environmental Agency of the Republic of Slovenia shows that the algorithm usually accurately detects the rainfall curve and consequently an accurate rainfall time series is obtained (Tab. 2). Since it is not always 100 % reliable, it should be used as a component of a system that would enable inspection of the detected curve and when required, it should also enable interactive changing of the parts needing correction.", "Language": "en", "Citations": "0"},
{"Title": "The CPLD-based Takagi-Sugeno type fuzzy controller Mehki krmilnik Takagi-Sugeno realiziran v programabilnem logi\u010dnem polju CPLD", "Authors": ["Oseli D.", "Zimic N."], "Keywords": ["CPLD", "Fuzzy controller", "Fuzzy logic", "Programmable logic", "Takagi-Sugeno", "VHDL"], "Date": "2003", "Abstract": "An overview is given of the use of the Takagi-Sugeno fuzzy controllers with trapezoidal membership functions for the programmable logic implementation. This combination allows for a quick development of an optimal fuzzy controller in the MATLAB environment using the fuzzy toolbox ANFIS tool. Two examples of such an approach are given: a one input/one output and a two input/output fuzzy controller. These two controllers are coded in VHDL, synthesized using the Cypress WARP tool and tested using the Cypress CPLD development kit.", "Language": "en", "Citations": "0"},
{"Title": "The ternary quantum-dot cellular automata memorizing cell", "Authors": ["Pecar P.", "Janez M.", "Zimic N.", "Mraz M.", "Bajec I.L."], "Keywords": [], "Date": "2009", "Abstract": "Quantum-dot Cellular Automata (QCA) were demonstrated to be a possible candidate for the implementation of a future multi-valued processing platform. Recent papers show that the introduction of adiabatic switching and the elegant application of the adiabatic pipelining concept in the QCA logic design can be used to efficiently solve the issues of the elementary ternary QCA logic primitives. The architectures of the resulting ternary QCAs become similar to their binary counterparts and thus the design rules for large circuit design remain similar to those developed for the binary QCA domain. In spite of this the design of the binary QCA SR memorizing cell cannot be directly transferred to the ternary domain, mostly because the control logic cannot properly handle the third value. We here propose a ternary QCA memorizing cell that efficiently exploits the pipelining mechanism at a wire level. It is centered on the circulating memory model (i.e. the memory in motion concept), which proved to be an efficient concept in memorizing cell design in the binary QCA domain. The proposed memorizing cell is capable of serving as one trit (ternary digit) of memory and represents a step forward to the ternary register, one of the basic building blocks of a ternary processor. \u00a9 2009 IEEE.", "Language": "en", "Citations": "6"},
{"Title": "Measuring the success of the strategic information systems planning in enterprises in Slovenia Mjerenje uspjeha strate\u0161kog planiranja informacijskih sustava u poduze\u0107ima u Sloveniji", "Authors": ["Hovelja T.", "Rozanec A.", "Rupnik R."], "Keywords": [], "Date": "2010", "Abstract": "As consultants for the largest enterprises in Slovenia, we found that even though the literature lists plenty of strategic information systems planning (SISP) methods with clear theoretical merits, the enterprises find these methods too abstract and/or too cumbersome to use in practice. To address this issue, we developed a new approach for the measurement of SISP success that attempts to combine key predictors of SISP success from the fields of strategic information systems planning and strategic business planning in a way that would be as practical as possible for everyday use in enterprises. We hope that our method will thus enable enterprises to validly, reliably and with greater ease measure and control the outcome of the SISP process by clearly defining the SISP success predictors that need to be monitored and by identifying the stakeholders responsible for their management.", "Language": "en", "Citations": "16"},
{"Title": "Modeling binding and cross-modal learning in Markov logic networks", "Authors": ["Vrecko A.", "Leonardis A.", "Skocaj D."], "Keywords": ["Binding", "Cognitive systems", "Cross-modal learning", "Graphical models", "Markov logic networks"], "Date": "2012", "Abstract": "Binding - the ability to combine two or more modal representations of the same entity into a single shared representation - is vital for every cognitive system operating in a complex environment. In order to successfully adapt to changes in a dynamic environment the binding mechanism has to be supplemented with cross-modal learning. In this paper we define the problems of high-level binding and cross-modal learning. By these definitions we model a binding mechanism in a Markov logic network and define its role in a cognitive architecture. We evaluate a prototype binding system off-line, using three different inference methods. \u00a9 2012 Elsevier B.V.", "Language": "en", "Citations": "1"},
{"Title": "Context driven focus of attention for object detection", "Authors": ["Perko R.", "Leonardis A."], "Keywords": [], "Date": "2007", "Abstract": "Context plays an important role in general scene perception. In particular, it can provide cues about an object's location within an image. In computer vision, object detectors typically ignore this information. We tackle this problem by presenting a concept of how to extract and learn contextual information from examples. This context is then used to calculate a focus of attention, that represents a prior for object detection. State-of-the-art local appearance-based object detection methods are then applied on selected parts of the image only. We demonstrate the performance of this approach on the task of pedestrian detection in urban scenes using a demanding image database. Results show that context awareness provides complementary information over pure local appearance-based processing. In addition, it cuts down the search complexity and increases the robustness of object detection. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "9"},
{"Title": "Rolling Shutter Correction in Manhattan World", "Authors": ["Purkait P.", "Zach C.", "Leonardis A."], "Keywords": [], "Date": "2017", "Abstract": "A vast majority of consumer cameras operate the rolling shutter mechanism, which often produces distorted images due to inter-row delay while capturing an image. Recent methods for monocular rolling shutter compensation utilize blur kernel, straightness of line segments, as well as angle and length preservation. However, they do not incorporate scene geometry explicitly for rolling shutter correction, therefore, information about the 3D scene geometry is often distorted by the correction process. In this paper we propose a novel method which leverages geometric properties of the scene - in particular vanishing directions - to estimate the camera motion during rolling shutter exposure from a single distorted image. The proposed method jointly estimates the orthogonal vanishing directions and the rolling shutter camera motion. We performed extensive experiments on synthetic and real datasets which demonstrate the benefits of our approach both in terms of qualitative and quantitative results (in terms of a geometric structure fitting) as well as with respect to computation time.", "Language": "en", "Citations": "2"},
{"Title": "An adaptive individual-based model of a prey group facing predator attacks Adaptivni ra\u010dunalni\u0161ki model skupinskega vedenja plena ob napadu plenilca", "Authors": ["Grbec D.", "Demsar J.", "Bajec I.L."], "Keywords": ["Adaptive model", "Behaviour types", "Collective animal behaviour", "Genetic algorithms", "Predator attack"], "Date": "2016", "Abstract": "Computational models have been extensively used to investigate various properties of collective behaviour, such as: transfer of information across the group, benefits of grouping (defence against predation, foraging), group decision\u0161making process, and group behaviour types. Based on empirical studies and existing models of collective behaviour there are four distinct types of behaviour: swarming, milling, dynamic parallel movement, and highly parallel movement. Swarming is most often associated with insects. Milling, where individuals perpetually rotate around an empty core, can at special occasions be exhibited by fish schools. Dynamic and highly parallel movement is most often associated with bird flocks and fish schools. In the existing models, these types of behaviour are achieved by tuning certain parameters of the model. In this paper we present an adaptive individual-based model of a prey group facing predator attacks; the prey group adapts its behaviour by changing specific parameters based on the predators\u0161 distance. Using a genetic algorithm we investigate a) which type of behaviour is the optimal defence against various predation tactics, and b) if the prey group will resort to transitions between various types of behaviour as a form of advanced defence tactic.", "Language": "en", "Citations": "0"},
{"Title": "Solving the vaguely defined assignment problems", "Authors": ["Moskon M."], "Keywords": ["Assignment problems", "Fuzzy Hungarian algorithm", "Fuzzy logic", "Hungarian algorithm", "Optimal resource assignment"], "Date": "2011", "Abstract": "Assignment problems are defined with two sets of inputs, i.e. set of resources and set of demands. Assignment of each resource to each demand has its own cost. Exactly one resource has to be assigned to each of the demands in such way, that maximal cost of the assignment is minimal when comparing to other assignments. Hungarian algorithm (also known as Kuhn-Munkres algorithm) is able to find an optimal solution of assignment problems in polynomial time, but is only able to solve assignment problems with precisely defined demands and resources. This presents a major problem in many real-life scenarios while the nature of these problems is such that inputs are commonly defined only vaguely (i.e. fuzzily). In order to solve them, their precise formalization is needed. Formalization of their properties is normally far from being a straightforward procedure and can present large costs in the meaning of time and money. Fuzzy logic on the other hand successfully copes with the processing of imprecise data. The article presents an extension of the Hungarian algorithm with the introduction of fuzzy logic methods - fuzzy Hungarian algorithm. Vaguely defined resources and demands can be easily described with fuzzy values which present an input to fuzzy Hungarian algorithm. The extended version of the algorithm is therefore able to cope with vaguely defined assignment problems, can be used more efficiently (i.e. with no further formalization of vaguely defined terms) and in a wider scope of assignment problems than the basic approach. Basic version of the Hungarian algorithm which was firstly presented by Harold Kuhn is presented in this article. Its extension with fuzzy logic methods is described and its usage on an example of vaguely defined assignment problem is demonstrated. Its benefits were also justified by the comparison of the results between the basic version of Hungarian algorithm and the fuzzy version of Hungarian algorithm on the same problem.", "Language": "en", "Citations": "0"},
{"Title": "Validation of slovenian version of jefferson scale of empathy for students", "Authors": ["Ster M.P.", "Ster B.", "Petek D.", "Gorup E.C."], "Keywords": ["Empathy", "Jefferson scale of empathy", "Slovenia", "Students", "Validation"], "Date": "2014", "Abstract": "Objective: Empathy is the most frequently mentioned humanistic dimension of patient care and is considered to be an important quality in physicians. The importance of fostering the development of empathy in undergraduate students is continuously emphasised in international recommendations for medical education. Our aim was to validate and adapt the Slovenian version of the Jefferson Scale of Empathy- Students version (JSE-S) on a sample of first-year medical students. Methods: First-year students of the Medical faculty in Ljubljana participated in the research. JSE-S version, a selfadministered 20-item questionnaire, was used for collecting the data. Descriptive statistics at the item level and at the scale level, factor analysis, internal consistency and test-retest reliability (two weeks after the first administration) of the JSE-S were performed. Results: 234 out of 298 (response rate 78.5%) students completed JSE-S. The mean score for the items on the 7-point Likert scale ranged from 3.27 (SD 1.72) to 6.50 (SD 0.82). The mean score for the scale (possible range from 20 to 140) was 107.6 (from 71 to 131, SD 12.6). Using factor analysis, we identified six factors, describing 57.2% of total variability. The Cronbach alpha as a measure of internal consistency was 0.79. The instrument has good temporal stability (test-retest reliability ICC = 0.703). Conclusion: Findings support the construct validity and reliability of JSE-S for measuring empathy in medical students in Slovenia. Future research is required to evaluate factors contributing to empathy.", "Language": "en", "Citations": "5"},
{"Title": "The thermal infrared visual object tracking VOT-TIR2016 challenge results", "Authors": ["Felsberg M.", "Kristan M.", "Matas J.", "Leonardis A.", "Pflugfelder R.", "Hager G.", "Berg A.", "Eldesokey A.", "Ahlberg J.", "Cehovin L.", "Vojir T.", "Lukezic A.", "Fernandez G.", "Petrosino A.", "Martin A.G.", "Montero A.S.", "Varfolomieiev A.", "Erdem A.", "Han B.", "Chang C.-M.", "Du D.", "Erdem E.", "Khan F.S.", "Porikli F.", "Zhao F.", "Bunyak F.", "Battistone F.", "Zhu G.", "Seetharaman G.", "Li H.", "Qi H.", "Bischof H.", "Possegger H.", "Nam H.", "Valmadre J.", "Zhu J.", "Feng J.", "Lang J.", "Martinez J.M.", "Palaniappan K.", "Lebeda K.", "Gao K.", "Mikolajczyk K.", "Wen L.", "Bertinetto L.", "Poostchi M.", "Maresca M.", "Danelljan M.", "Arens M.", "Tang M.", "Baek M.", "Fan N.", "Shakarji N.A.", "Miksik O.", "Akin O.", "Torr P.H.S.", "Huang Q.", "Martin-Nieto R.", "Pelapur R.", "Bowden R.", "Laganiere R.", "Krah S.B.", "Li S.", "Yao S.", "Hadfield S.", "Lyu S.", "Becker S.", "Golodetz S.", "Hu T.", "Mauthner T.", "Santopietro V.", "Li W.", "Hubner W.", "Li X.", "Li Y.", "Xu Z.", "He Z."], "Keywords": ["Object tracking", "Performance evaluation", "Thermal IR", "VOT"], "Date": "2016", "Abstract": "The Thermal Infrared Visual Object Tracking challenge 2016, VOT-TIR2016, aims at comparing short-term single-object visual trackers that work on thermal infrared (TIR) sequences and do not apply pre-learned models of object appearance. VOT-TIR2016 is the second benchmark on short-term tracking in TIR sequences. Results of 24 trackers are presented. For each participating tracker, a short description is provided in the appendix. The VOT-TIR2016 challenge is similar to the 2015 challenge, the main difference is the introduction of new, more difficult sequences into the dataset. Furthermore, VOT-TIR2016 evaluation adopted the improvements regarding overlap calculation in VOT2016. Compared to VOT-TIR2015, a significant general improvement of results has been observed, which partly compensate for the more difficult sequences. The dataset, the evaluation kit, as well as the results are publicly available at the challenge website.", "Language": "en", "Citations": "18"},
{"Title": "TweetCaT: A tool for building Twitter corpora of smaller languages", "Authors": ["Ljubesic N.", "Fiser D.", "Erjavec T."], "Keywords": ["Croatian", "Less-resourced languages", "Open source", "Serbian", "Slovene", "Twitter corpora"], "Date": "2014", "Abstract": "This paper presents TweetCaT, an open-source Python tool for building Twitter corpora that was designed for smaller languages. Using the Twitter search API and a set of seed terms, the tool identifies users tweeting in the language of interest together with their friends and followers. By running the tool for 235 days we tested it on the task of collecting two monitor corpora, one for Croatian and Serbian and the other for Slovene, thus also creating new and valuable resources for these languages. A post-processing step on the collected corpus is also described, which filters out users that tweet predominantly in a foreign language thus further cleans the collected corpora. Finally, an experiment on discriminating between Croatian and Serbian Twitter users is reported.", "Language": "en", "Citations": "13"},
{"Title": "Reactive motion planning with qualitative constraints", "Authors": ["Soberl D.", "Bratko I."], "Keywords": [], "Date": "2017", "Abstract": "Qualitative modeling tends to be closer to human type of reasoning than traditional numerical modeling and proved to be very useful in certain branches of cognitive robotics. However, due to the lack of precise numerical relations, planning with qualitative models has been achieved to a limited extent. Typically, it is bound to predicting possible future behaviors of the system, and demands additional exploration of numerical relations, before constructed plans can be executed. In this paper we show how qualitative models can be interpreted in terms of reactive planning, to produce executable actions without the need for additional numerical learning. We demonstrate our method on two classical motion planning problems \u2013 pursuing and obstacle avoidance, and a complex problem of pushing objects.", "Language": "en", "Citations": "1"},
{"Title": "A novel 2DGE protein-segmentation algorithm", "Authors": ["Peer P.", "Corzo L.G."], "Keywords": ["Image analysis", "Segmentation", "Two-dimensional gel-electrophoresis"], "Date": "2007", "Abstract": "Two-dimensional gel-electrophoresis (2DGE) images show the expression levels of several hundreds of proteins where each protein is represented as a blob-shaped spot of grey level values. The spot detection, i.e. the segmentation process, has to be efficient as it is the first step in the gel processing. Such extraction of information is a very complex task. In this paper we propose a novel spot detector that is basically a morphology-based method with the use of a seeded region growing as a central paradigm and which relies on the spot-correlation information. The method is tested on our synthetic as well as on real gels with human samples from SWISS-2DPAGE (two-dimensional Polyacrylamide gel electrophoresis) database. A comparison of results is done with a method called Pixel Value Collection (PVC). Since our algorithm efficiently uses local spot information, segments the spot by collecting pixel values, and its affinity with PVC, we named it Local Pixel Value Collection (LPVC). The results show that LPVC achieves similar segmentation results as PVC, but is much faster than PVC.", "Language": "en", "Citations": "0"},
{"Title": "Modelling and Analysing the Information Processing Capabilities of Simple Biological Systems", "Authors": ["Moskon M.", "Mraz M."], "Keywords": ["chemical master equation", "metrics", "models for synthetic biology", "stochastic modelling", "unconventional computing"], "Date": "2012", "Abstract": "Biological systems that present basic logic primitives for information processing have already been realized. Models for simulating their dynamics have also been implemented. However there is a lack of metrics that would objectively evaluate the information processing capabilities of these primitives and possibilities of their interconnectivity. With the introduction of such processing and performance descriptive quantities complex biological systems capable of information processing could be built more straightforwardly. That would bring us closer to the realization of a biological computer. \u00a9 2012 Copyright Vilnius Gediminas Technical University.", "Language": "en", "Citations": "2"},
{"Title": "Machine learning for survival analysis: A case study on recurrence of prostate cancer", "Authors": ["Zupan B.", "Demsar J.", "Kattan M.W.", "Beck J.R.", "Bratko I."], "Keywords": ["Censored data", "Data weighting", "Machine learning", "Outcome prediction after radical prostatectomy", "Prognostic models in medicine", "Prostate cancer recurrence", "Survival analysis"], "Date": "2000", "Abstract": "Machine learning techniques have recently received considerable attention, especially when used for the construction of prediction models from data. Despite their potential advantages over standard statistical methods, like their ability to model non-linear relationships and construct symbolic and interpretable models, their applications to survival analysis are at best rare, primarily because of the difficulty to appropriately handle censored data. In this paper we propose a schema that enables the use of classification methods - including machine learning classifiers - for survival analysis. To appropriately consider the follow-up time and censoring, we propose a technique that, for the patients for which the event did not occur and have short follow-up times, estimates their probability of event and assigns them a distribution of outcome accordingly. Since most machine learning techniques do not deal with outcome distributions, the schema is implemented using weighted examples. To show the utility of the proposed technique, we investigate a particular problem of building prognostic models for prostate cancer recurrence, where the sole prediction of the probability of event (and not its probability dependency on time) is of interest. A case study on preoperative and postoperative prostate cancer recurrence prediction shows that by incorporating this weighting technique the machine learning tools stand beside modern statistical methods and may, by inducing symbolic recurrence models, provide further insight to relationships within the modeled data. (C) 2000 Elsevier Science B.V.", "Language": "en", "Citations": "55"},
{"Title": "Fuzzy student model in InterMediActor platform", "Authors": ["Kavcic A.", "Pedraza-Jimenez R.", "Molina-Bulla H.", "Albacete F.J.V.", "Cid-Sueiro J.", "Navia-Vazquez A."], "Keywords": ["Educational systems", "Fuzzy inference", "Personalized navigation", "User model"], "Date": "2004", "Abstract": "The paper deals with personalization of navigation in the educational content, introduced in a competence-based instructional design system InterMediActor. The system constructs an individualized navigation graph for each student and thus suggests the learning objectives the student is most prepared to attain. The navigation tools rely on the graph of dependencies between competences, and the student model. We use fuzzy set theory for dealing with uncertainty in the assessment of students: the marks of assessment tests are transformed into linguistic terms, which are assigned to linguistic variables. Fuzzy IF-THEN rules are applied to obtain the appropriate categories of competences in the navigation graph.", "Language": "en", "Citations": "5"},
{"Title": "Structural descriptions in human-assisted robot visual learning", "Authors": ["Kruijff G.-J.M.", "Kelleher J.D.", "Berginc G.", "Leonardis A."], "Keywords": ["Cognitive vision and learning", "Natural language dialogue"], "Date": "2006", "Abstract": "The paper presents an approach to using structural descriptions, obtained through a human-robot tutoring dialogue, as labels for the visual object models a robot learns, The paper shows how structural descriptions enable relating models for different aspects of one and the same object, and how being able to relate descriptions for visual models and discourse referents enables incremental updating of model descriptions through dialogue (either robot- or human-initiated). The approach has been implemented in an integrated architecture for human-assisted robot visual learning.", "Language": "en", "Citations": "5"},
{"Title": "Geographical mapping of visitor flow in tourism: A user-generated content approach", "Authors": ["Cvelbar L.K.", "Mayr M.", "Vavpotic D."], "Keywords": ["big data", "destination management", "economic planning in tourism", "user-generated content", "visitor flows"], "Date": "2018", "Abstract": "The available technology enables us to access a large amount of data shared by tourists on tourism web platforms. Such data include the exact geographical location visited, the time of a visit, and the identifier of a visitor. This article aims to identify the visitor flows in the North East Adriatic region. Visitor flows are groups of repetitive movements of visitors through the geographical space within a certain travel. We identified 31 groups of strategic visitor flows between 188 destinations in the region. The proposed methodological approach is unique and had not been used in this context before. By connecting new approaches in destination management and economic planning, we aim to improve the theoretical and practical knowledge in this field.", "Language": "en", "Citations": "0"},
{"Title": "Electrocardiogram ST-segment morphology delineation method using orthogonal transformations", "Authors": ["Amon M.", "Jager F."], "Keywords": [], "Date": "2016", "Abstract": "Differentiation between ischaemic and non-ischaemic transient ST segment events of long term ambulatory electrocardiograms is a persisting weakness in present ischaemia detection systems. Traditional ST segment level measuring is not a sufficiently precise technique due to the single point of measurement and severe noise which is often present. We developed a robust noise resistant orthogonal-transformation based delineation method, which allows tracing the shape of transient ST segment morphology changes from the entire ST segment in terms of diagnostic and morphologic feature-vector time series, and also allows further analysis. For these purposes, we developed a new Legendre Polynomials based Transformation (LPT) of ST segment. Its basis functions have similar shapes to typical transient changes of ST segment morphology categories during myocardial ischaemia (level, slope and scooping), thus providing direct insight into the types of time domain morphology changes through the LPT feature-vector space. We also generated new Karhunen and Lo \u00e8ve Transformation (KLT) ST segment basis functions using a robust covariance matrix constructed from the ST segment pattern vectors derived from the Long Term ST Database (LTST DB). As for the delineation of significant transient ischaemic and non-ischaemic ST segment episodes, we present a study on the representation of transient ST segment morphology categories, and an evaluation study on the classification power of the KLT- and LPT-based feature vectors to classify between ischaemic and non-ischaemic ST segment episodes of the LTST DB. Classification accuracy using the KLT and LPT feature vectors was 90% and 82%, respectively, when using the k-Nearest Neighbors (k = 3) classifier and 10-fold cross-validation. New sets of feature-vector time series for both transformations were derived for the records of the LTST DB which is freely available on the PhysioNet website and were contributed to the LTST DB. The KLT and LPT present new possibilities for human-expert diagnostics, and for automated ischaemia detection.", "Language": "en", "Citations": "2"},
{"Title": "Orange: Data mining fruitful and fun - A historical perspective", "Authors": ["Demsar J.", "Zupan B."], "Keywords": ["Applications", "Data mining", "Machine learning"], "Date": "2013", "Abstract": "Orange (http://orange.biolab.si) is a general-purpose machine learning and data mining tool. Its multilayer architecture is suitable for different kinds of users, from data mining beginners to programmers who prefer a scripting interface. In this paper we outline the history of Orange's development and present its achievements, current status, and future challenges.", "Language": "en", "Citations": "9"},
{"Title": "Different effects of dopaminergic medication on perceptual decision-making in Parkinson's disease as a function of task difficulty and speed-accuracy instructions", "Authors": ["Huang Y.-T.", "Georgiev D.", "Foltynie T.", "Limousin P.", "Speekenbrink M.", "Jahanshahi M."], "Keywords": ["Dopaminergic medication", "Effort-based decision-making", "Impulsivity", "Parkinson's disease", "Response threshold", "Speed-accuracy trade-off"], "Date": "2015", "Abstract": "When choosing between two options, sufficient accumulation of information is required to favor one of the options over the other, before a decision is finally reached. To establish the effect of dopaminergic medication on the rate of accumulation of information, decision thresholds and speed-accuracy trade-offs, we tested 14 patients with Parkinson's disease (PD) on and off dopaminergic medication and 14 age-matched healthy controls on two versions of the moving-dots task. One version manipulated the level of task difficulty and hence effort required for decision-making and the other the urgency, requiring decision-making under speed vs. accuracy instructions. The drift diffusion model was fitted to the behavioral data.As expected, the reaction time data revealed an effect of task difficulty, such that the easier the perceptual decision-making task was, the faster the participants responded. PD patients not only made significantly more errors compared to healthy controls, but interestingly they also made significantly more errors ON than OFF medication. The drift diffusion model indicated that PD patients had lower drift rates when tested ON compared to OFF medication, indicating that dopamine levels influenced the quality of information derived from sensory information.On the speed-accuracy task, dopaminergic medication did not directly influence reaction times or error rates. PD patients OFF medication had slower RTs and made more errors with speed than accuracy instructions compared to the controls, whereas such differences were not observed ON medication. PD patients had lower drift rates and higher response thresholds than the healthy controls both with speed and accuracy instructions and ON and OFF medication. For the patients, only non-decision time was higher OFF than ON medication and higher with accuracy than speed instructions.The present results demonstrate that when task difficulty is manipulated, dopaminergic medication impairs perceptual decision-making and renders it more errorful in PD relative to when patients are tested OFF medication. In contrast, for the speed/accuracy task, being ON medication improved performance by eliminating the significantly higher errors and slower RTs observed for patients OFF medication compared to the HC group. There was no evidence of dopaminergic medication inducing impulsive decisions when patients were acting under speed pressure. For the speed-accuracy instructions, the sole effect of dopaminergic medication was on non-decision time, which suggests that medication primarily affected processes tightly coupled with the motor symptoms of PD. Interestingly, the current results suggest opposite effects of dopaminergic medication on the levels of difficulty and speed-accuracy versions of the moving dots task, possibly reflecting the differential effect of dopamine on modulating drift rate (levels of difficulty task) and non-decision time (speed-accuracy task) in the process of perceptual decision making.", "Language": "en", "Citations": "14"},
{"Title": "Weighted archetypal analysis of the multi-element graph for query-focused multi-document summarization", "Authors": ["Canhasi E.", "Kononenko I."], "Keywords": ["Matrix factorization", "Multi-element graph", "Query-focused document summarization", "Weighted archetypal analysis"], "Date": "2013", "Abstract": "Most existing research on applying the matrix factorization approaches to query-focused multi-document summarization (Q-MDS) explores either soft/hard clustering or low rank approximation methods. We employ a different kind of matrix factorization method, namely weighted archetypal analysis (wAA) to Q-MDS. In query-focused summarization, given a graph representation of a set of sentences weighted by similarity to the given query, positively and/or negatively salient sentences are values on the weighted data set boundary. We choose to use wAA to compute these extreme values, archetypes, and hence to estimate the importance of sentences in target documents set. We investigate the impact of using the multi-element graph model for query focused summarization via wAA. We conducted experiments on the data of document understanding conference (DUC) 2005 and 2006. Experimental results evidence the improvement of the proposed approach over other closely related methods and many of state-of-the-art systems. \u00a9 2012 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "39"},
{"Title": "Segmentation and Reconstruction of 3D Models from a Point Cloud with Deep Neural Networks", "Authors": ["Slabanja J.", "Meden B.", "Peer P.", "Jaklic A.", "Solina F."], "Keywords": ["3D reconstruction", "computer vision", "deep neural networks", "Keras", "point cloud", "segmentation", "superquadrics", "Tensor-Flow"], "Date": "2018", "Abstract": "The need to model visual information with compact representations has existed since the early days of computer vision. We implemented in the past a segmentation and model recovery method for range images which is unfortunately too slow for current size of 3D point clouds and type of applications. Recently, neural networks have become the popular choice for quick and effective processing of visual data. In this article we demonstrate that with a convolutional neural network we could achieve comparable results, that is to determine and model all objects in a given 3D point cloud scene. We started off with a simple architecture that could predict the parameters of a single object in a scene. Then we expanded it with an architecture similar to Faster R-CNN, that could predict the parameters for any number of objects in a scene. The results of the initial neural network were satisfactory. The second network, that performed also segmentation, still gave decent results comparable to the original method, but compared to the initial one, performed somewhat worse. Results, however, are encouraging but further experiments are needed to build CNNs that will be able to replace the state-of-the-art method.", "Language": "en", "Citations": "0"},
{"Title": "A bilingual spoken dialog system for Slovenian and Croatian weather forecasts", "Authors": ["Martincic-Ipsic S.", "Zibert J.", "Ipsic I.", "Mihelic F."], "Keywords": ["Language identification", "Speech analysis tools", "Speech database", "Speech recognition"], "Date": "2003", "Abstract": "In the paper we present a design strategy, current activities and some results of a joint project in development of a spoken dialog system for Slovenian and Croatian weather forecasts. We give a brief description of system design, procedures we have performed in order to obtain domain specific speech databases and monolingual and bilingual speech recognition experiments. Recognition results for Croatian and Slovenian speech arc presented, as well as bilingual speech recognition results when using common acoustic models. We propose two different approaches to the language identification problem and show recognition results for the two acoustically similar languages like Slovenian and Croatian. We describe the software tools we have used for speech database design as well as tools for acoustic and language modelling.", "Language": "en", "Citations": "1"},
{"Title": "Simplicity matters: user evaluation of the Slovene reference corpus", "Authors": ["Arhar Holdt S.", "Dobrovoljc K.", "Logar N."], "Keywords": ["Corpus concordancer", "Gigafida", "Reference corpus", "Usability assessment", "User evaluation", "User satisfaction"], "Date": "2019", "Abstract": "The latest reference corpus of written Slovene, the Gigafida corpus, was created as part of the \u2018Communication in Slovene\u2019 project. In the same project, a web concordancer was designed for the broadest possible use, and tailored to the needs and abilities of user groups such as translators, writers, proofreaders and teachers. Two years after the corpus was published within the new tool, its features were assessed by the users. With an average rate of 4.36 on a scale between 1 and 5 (1 = I strongly disagree, 5 = I strongly agree), the results indicate that most survey participants agreed or strongly agreed with positive statements about the new implementations (e.g. \u201cThe corpus results are displayed in a clear manner\u201d). This is a considerable improvement in user experience from the previous reference corpus of Slovene, i.e. the FidaPLUS corpus within the ASP32 concordancer (rated with 3.67). In the user feedback, the simplicity of search options and the interface clarity are highlighted as the main advantages, while for the future development, advanced visualizations of corpus data and improved search of word-phrases are suggested. The evaluation also highlighted some relevant user habits, such as not taking the time to learn systematically about the tool before they start using it. The findings will be implemented in future editions of the Gigafida corpus, but are relevant to any project that aims at facilitating a wider use of reference corpora and corpus-based resources.", "Language": "en", "Citations": "0"},
{"Title": "Modified Dunn's cluster validity index based on graph theory Zmodyfikowany indeks oceny klastr\u00f3w dunna oparty na teorii graf\u00f3w", "Authors": ["Ilc N."], "Keywords": ["Cluster analysis", "Cluster validation", "Dunn's index", "Gabriel graph"], "Date": "2012", "Abstract": "Clustering methods serve as common tools for efficient data analysis in many fields of science. The essential, yet often neglected, step in the cluster analysis is validation of the clustering results. This paper presents a novel cluster validity index, which is the modification of the well-known Dunn's index. Our proposal is based on its generalization considering the shortest paths between data points in the Gabriel graph. The experiments show that the proposed index can be successfully applied in the validation of the partitions, even when they contain complex-shaped clusters.", "Language": "en", "Citations": "8"},
{"Title": "Robust and efficient vision system for group of cooperating mobile robots with application to soccer robots", "Authors": ["Klancar G.", "Kristan M.", "Kovacic S.", "Orqueda O."], "Keywords": ["Camera calibration", "Classification", "Computer vision", "Non-uniform ilumination correction", "Segmentation"], "Date": "2004", "Abstract": "In this paper a global vision scheme for estimation of positions and orientations of mobile robots is presented. It is applied to robot soccer application which is a fast dynamic game and therefore needs an efficient and robust vision system implemented. General applicability of the vision system can be found in other robot applications such as mobile transport robots in production, warehouses, attendant robots, fast vision tracking of targets of interest and entertainment robotics. Basic operation of the vision system is divided into two steps. In the first, the incoming image is scanned and pixels are classified into a finite number of classes. At the same time, a segmentation algorithm is used to find corresponding regions belonging to one of the classes. In the second step, all the regions are examined. Selection of the ones that are a part of the observed object is made by means of simple logic procedures. The novelty is focused on optimization of the processing time needed to finish the estimation of possible object positions. Better results of the vision system are achieved by implementing camera calibration and shading correction algorithm. The former corrects camera lens distortion, while the latter increases robustness to irregular illumination conditions. \u00a9 2004 ISA - The Instrumentation, Systems, and Automation Society.", "Language": "en", "Citations": "15"},
{"Title": "3D volume localization using miniatures", "Authors": ["Sajn L.", "Radojevic M.", "Dobravec T."], "Keywords": [], "Date": "2014", "Abstract": "The prediction of the position of a given volume sample in a full body atlas, also known as a volume localization, is a part of an initial stage of image retrieval in most of the dedicated CAD systems. In this paper we present two methods for volume localization, namely histogram matching and classifier regression. Since the histogram matching method ignores the spatial orientation, it is used when the orientation of the volume cubes are not the same. On the other hand the classifier regression is much faster and can be used as a quick estimation and as a tool to reduce the scope of the initial problem. Both presented methods were tested on a dataset with 3962 volumes of a human body atlas. The accuracy and the speed of execution was compared and is presented in this paper. \u00a9 2014 MIPRO.", "Language": "en", "Citations": "0"},
{"Title": "Factors affecting diminishing returns for searching deeper", "Authors": ["Guid M.", "Bratko I."], "Keywords": [], "Date": "2007", "Abstract": "The phenomenon of diminishing returns for additional search effort has been observed by several researchers. We study experimentally additional factors which influence the behaviour of diminishing returns that manifest themselves in go-deep experiments. The results obtained on a large set of more than 40,000 positions from chess grandmaster games using the programs CRAFTY, RYBKA, and SHREDDER show that diminishing returns depend on (a) the values of the positions, (b) the quality of the evaluation function of the program used, and to some extent also on (c) the phase of the game, and the amount of material on the board.", "Language": "en", "Citations": "5"},
{"Title": "Analysis of the limitations of multiple client handling in a java server environment", "Authors": ["Beloglavec S.", "Hericko M.", "Juric M.B.", "Rozman I."], "Keywords": ["Event-Driven Server", "Java Networking", "Threaded Server"], "Date": "2005", "Abstract": "A server infrastructure in web servers, message servers and other parallel systems use a variation of two software architectures for providing concurrency: threaded or event-driven. This paper analyzes the performance limitations of concurrent applications implemented in Java. Both architectures have been evaluated and compared with various design patterns, which combine the best practices from both architectures. For each architecture the suitability for handling a large volume of client requests, the efficient management of a server load, the influence of client request structures, and the physical size of a client request, have been studied. The discussed Java APIs are core technologies for high-level APIs, used in developing web and distributed applications. The research also includes performance comparison on various platforms and discusses performance variation on various versions of a Java runtime. The paper contributes to the understanding of Java-based server architecture capabilities. Core server software architectures and required Java libraries are compared, the reasons for the limitations are identified and guidelines for choosing proper combinations are given.", "Language": "en", "Citations": "2"},
{"Title": "Solving the mesh-partitioning problem with an ant-colony algorithm", "Authors": ["Korosec P.", "Silc J.", "Robic B."], "Keywords": ["Algorithms", "Ant-colony optimisation", "Finite-element method", "Mesh partitioning"], "Date": "2004", "Abstract": "Many real-world engineering problems can be expressed in terms of partial differential equations and solved by using the finite-element method, which is usually parallelised, i.e. the mesh is divided among several processors. To achieve high parallel efficiency it is important that the mesh is partitioned in such a way that workloads are well balanced and interprocessor communication is minimised. In this paper we present an enhancement of a technique that uses a nature-inspired metaheuristic approach to achieve higher-quality partitions. The so-called multilevel ant-colony algorithm, which is a relatively new metaheuristic search technique for solving optimisation problems, was applied and studied, and the possible parallelisation of this algorithm is discussed. The multilevel ant-colony algorithm performed very well and is superior to classical k-METIS and Chaco algorithms; it is even comparable with the combined evolutionary/multilevel scheme used in the JOSTLE evolutionary algorithm and returned solutions that are better than the currently available solutions in the Graph Partitioning Archive. \u00a9 2004 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "50"},
{"Title": "My watch says I'm busy: Inferring cognitive load with low-cost wearables", "Authors": ["Gjoreski M.", "Lustrek M.", "Pejovic V."], "Keywords": ["Cognitive load inference", "Mobile sensing", "Wearable sensing"], "Date": "2018", "Abstract": "To prevent undesirable effects of attention grabbing at times when a user is occupied with a difficult task, ubiquitous computing devices should be aware of the user's cognitive load. However, inferring cognitive load is extremely challenging, especially when performed without obtrusive, expensive, and purpose-built equipment. In this study we examine the potential for inferring one's cognitive load using merely cheap wearable sensing devices. We subject 25 volunteers to varying cognitive load using six different Primary tasks. In parallel, we collect physiological data with a cheap device, extract features, and then construct machine learning models for cognitive load prediction. As metrics for the load we use one subjective measure, the NASA Task Load Index (NASA-TLX), and two objective measures: task difficulty and reaction time. The leave-one-subject-out evaluation shows a significant influence of the task type and the chosen cognitive load metric on the prediction accuracy.", "Language": "en", "Citations": "0"},
{"Title": "Comparison of CORBA and RMI distributed object models with an emphasis on performance Primerjava porazdeljenih objektnih modelov CORBA in RMI s poudarkom na zmogljivostih", "Authors": ["Juric M.B.", "Rozman I.", "Hericko M."], "Keywords": [], "Date": "2000", "Abstract": "A series of experiments and tests were run to give an insight into the performance levels that can be expected from Java applications when used in conjunction with CORBA on one and RMI on the other hand. With the several testing scenarios, the performances for methods that returned different data types are measured. The measurements on a single computer are completed on two connected computers and under a heavy client load with up to eight simultaneous clients which invoked methods without delays.", "Language": "en", "Citations": "0"},
{"Title": "An integrative architecture for a sensor-supported trust management system", "Authors": ["Trcek D."], "Keywords": ["Human agents", "Modeling and simulation", "Multidisciplinary research", "Sensors", "Trust management"], "Date": "2012", "Abstract": "Trust plays a key role not only in e-worlds and emerging pervasive computing environments, but also already for millennia in human societies. Trust management solutions that have being around now for some fifteen years were primarily developed for the above mentioned cyber environments and they are typically focused on artificial agents, sensors, etc. However, this paper presents extensions of a new methodology together with architecture for trust management support that is focused on humans and human-like agents. With this methodology and architecture sensors play a crucial role. The architecture presents an already deployable tool for multi and interdisciplinary research in various areas where humans are involved. It provides new ways to obtain an insight into dynamics and evolution of such structures, not only in pervasive computing environments, but also in other important areas like management and decision making support. \u00a9 2012 by the authors; licensee MDPI, Basel, Switzerland.", "Language": "en", "Citations": "6"},
{"Title": "Model for integrated monitoring of BPEL business processes", "Authors": ["Srdic G.", "Juric M.B."], "Keywords": ["bpelx4bam", "business activity monitoring", "business process execution language", "extensions", "monitoring data and metrics"], "Date": "2013", "Abstract": "Business process execution language (BPEL) does not provide a native support for defining data and metrics, required to perform business activity monitoring (BAM). Existing industry solutions are vendor-specific while platform-independent scientific approaches separate tightly coupled monitoring definitions from the core BPEL processes into external descriptors, thus increasing the complexity of the development, packaging and deployment processes. Furthermore, existing monitoring solutions reuse platform-specific audit trail events. This requires monitor model developers to be familiar with the details of the process implementation and the custom BPEL engine meta-model. To overcome these issues, this paper presents bpelx4bam - a new approach to defining monitoring data and metrics within BPEL processes, based on BPEL extensions. The bpelx4bam promotes these definitions into the first class citizens of the BPEL specification in order to unify and fully integrate BAM into the BPM lifecycle. It enables a cross-latform migration of BAM-enabled business processes, removes the need for separate models and specifications, eases development, packaging and deployment processes, aligns the separation of concerns with actual process roles and introduces BAM-specific and optimized events. Therefore, it presents a solid ground for a future specification or a standard in this area. \u00a9 2013 World Scientific Publishing Company.", "Language": "en", "Citations": "2"},
{"Title": "Analysis of Content-Aware Image Compression with VGG16", "Authors": ["Selimovic A.", "Meden B.", "Peer P.", "Hladnik A."], "Keywords": [], "Date": "2018", "Abstract": "Content-aware compression based on the use of saliency maps aims to improve the interpretability of an image by encoding the more relevant image regions with a higher quality than the rest of the image. This paper revisits two convolutional neural network (CNN) models based on VGG16, multi-structure region of interest (MS-ROI) and class activation map (CAM), which enable the localization of salient image regions. While the MS-ROI model allows for the localization of multiple salient image regions, the CAM model, on the other hand, tends to localize only the most relevant class. We use the contextual information provided by the obtained saliency maps to guide the compression. By encoding more important image regions at a higher bitrate and less important ones at a lower bitrate, different qualities of compression for the regions of interest and the background are obtained, while also achieving smooth transitions from salient to non-salient regions. The performance of both models is evaluated on images from the MIT Saliency Benchmark dataset and the General-100 dataset, and the results of the compression are compared to the standard JPEG compression at different quality factors. Experimental results show that for the files of approximately same size, the compression methods based on the two CNN models outperform the standard JPEG compression. When comparing the compression based on the MS-ROI model to the compression based on the CAM model, the former is characterized by a higher PSNR and a better visual quality of the obtained images.", "Language": "en", "Citations": "0"},
{"Title": "Threshold-coloring and unit-cube contact representation of planar graphs", "Authors": ["Alam M.J.", "Chaplick S.", "Fijavz G.", "Kaufmann M.", "Kobourov S.G.", "Pupyrev S.", "Toeniskoetter J."], "Keywords": ["Graph coloring", "Planar graphs", "Threshold-coloring", "Unit-cube contact representation"], "Date": "2017", "Abstract": "In this paper we study threshold-coloring of graphs, where the vertex colors represented by integers are used to describe any spanning subgraph of the given graph as follows. A pair of vertices with a small difference in their colors implies that the edge between them is present, while a pair of vertices with a big color difference implies that the edge is absent. Not all planar graphs are threshold-colorable, but several subclasses, such as trees, some planar grids, and planar graphs with no short cycles can always be threshold-colored. Using these results we obtain unit-cube contact representation of several subclasses of planar graphs. Variants of the threshold-coloring problem are related to well-known graph coloring and other graph-theoretic problems. Using these relations we show the NP-completeness for two of these variants, and describe a polynomial-time algorithm for another.", "Language": "en", "Citations": "0"},
{"Title": "Fighting knowledge acquisition bottleneck with argument based machine learning", "Authors": ["Mozina M.", "Guid M.", "Krivec J.", "Sadikov A.", "Bratko I."], "Keywords": [], "Date": "2008", "Abstract": "Knowledge elicitation is known to be a difficult task and thus a major bottleneck in building a knowledge base. Machine learning has long ago been proposed as a way to alleviate this problem. Machine learning usually helps the domain expert to uncover some of the more tacit concepts. However, the learned concepts are often hard to understand and hard to extend. A common view is that a combination of a domain expert and machine learning would yield the best results. Recently, argument based machine learning (ABML) has been introduced as a combination of argumentation and machine learning. Through argumentation, ABML enables the expert to articulate his knowledge easily and in a very natural way. ABML was shown to significantly improve the comprehensibility and accuracy of the learned concepts. This makes ABML a most natural tool for constructing a knowledge base. The present paper shows how this is accomplished through a case study of building a knowledge base of an expert system used in a chess tutoring application.", "Language": "en", "Citations": "13"},
{"Title": "Two-layer synchronized ternary quantum-dot cellular automata wire crossings", "Authors": ["Bajec I.L.", "Pecar P."], "Keywords": ["Multi-layer design", "Quantum-dot cellular automata", "Ternary processing", "Wire crossing"], "Date": "2012", "Abstract": "Quantum-dot cellular automata are an interesting nanoscale computing paradigm. The introduction of the ternary quantum-dot cell enabled ternary computing, and with the recent development of a ternary functionally complete set of elementary logic primitives and the ternary memorizing cell design of complex processing structures is becoming feasible. The specific nature of the ternary quantum-dot cell makes wire crossings one of the most problematic areas of ternary quantum-dot cellular automata circuit design. We hereby present a two-layer wire crossing that uses a specific clocking scheme, which ensures the crossed wires have the same effective delay. \u00a9 2012 Lebar Bajec and Pecar.", "Language": "en", "Citations": "7"},
{"Title": "Modeling gene regulatory networks using Petri Nets", "Authors": ["Bordon J.", "Moskon M.", "Mraz M."], "Keywords": [], "Date": "2012", "Abstract": "Petri nets (PNs) are a qualitative approach to modeling the gene regulatory networks (GRNs) capable of characterising the dynamical properties of complex systems while also describing their structure with PN graphs. In this paper we show how to construct basic building blocks for GRNs using PNs. We present solutions to some of the known weaknesses of using conventional PNs for descrbing GRNs by using the Coloured Petri nets (CPNs), an extension of the ordinary PNs. We show that by using CPNs we can create building blocks with the dynamical properties of GRNs described more accurately. We also consider solving the problem of the undefined kinetic data by using the fuzzy logic methods.", "Language": "en", "Citations": "0"},
{"Title": "Face deidentification with generative deep neural networks", "Authors": ["Meden B.", "Malli R.C.", "Fabijan S.", "Ekenel H.K.", "Struc V.", "Peer P."], "Keywords": [], "Date": "2017", "Abstract": "Face deidentification is an active topic amongst privacy and security researchers. Early deidentification methods relying on image blurring or pixelisation have been replaced in recent years with techniques based on formal anonymity models that provide privacy guaranties and retain certain characteristics of the data even after deidentification. The latter aspect is important, as it allows the deidentified data to be used in applications for which identity information is irrelevant. In this work, the authors present a novel face deidentification pipeline, which ensures anonymity by synthesising artificial surrogate faces using generative neural networks (GNNs). The generated faces are used to deidentify subjects in images or videos, while preserving non-identity-related aspects of the data and consequently enabling data utilisation. Since generative networks are highly adaptive and can utilise diverse parameters (pertaining to the appearance of the generated output in terms of facial expressions, gender, race etc.), they represent a natural choice for the problem of face deidentification. To demonstrate the feasibility of the authors' approach, they perform experiments using automated recognition tools and human annotators. Their results show that the recognition performance on deidentified images is close to chance, suggesting that the deidentification process based on GNNs is effective.", "Language": "en", "Citations": "8"},
{"Title": "Estimating reliability for assessing and correcting individual streaming predictions", "Authors": ["Rodrigues P.P.E.", "Bosnic Z.", "Gama J.", "Kononenko I."], "Keywords": [], "Date": "2012", "Abstract": "Several predictive systems are nowadays vital for operations and decision support. The quality of these systems is most of the time defined by their average accuracy which has low or no information at all about the estimated error of each individual prediction. In these cases, users should be allowed to associate a measure of reliability to each prediction. However, with the advent of data streams, batch state-of-the-art reliability estimates need to be redefined. In this chapter we adapt and evaluate five empirical measures for online reliability estimation of individual predictions: similarity-based (k-NN) error, local sensitivity (bias and variance) and online bagging predictions (bias and variance). Evaluation is performed with a neural network base model on two different problems, with results showing that online bagging and k-NN estimates are consistently correlated with the error of the base model. Furthermore, we propose an approach for correcting individual predictions based on the CNK reliability estimate. Evaluation is done on a real-world problem (prediction of the electricity load for a selected European geographical region), using two different regression models: neural network and the k nearest neighbors algorithm. Comparison is performed with corrections based on the Kalman filter. The results show that our method performs better than the Kalman filter, significantly improving the original predictions to more accurate values.", "Language": "en", "Citations": "3"},
{"Title": "Independent-valued minimax: Pathological or beneficial?", "Authors": ["Lustrek M.", "Bratko I.", "Gams M."], "Keywords": ["Minimax pathology", "Minimax principle", "Position independence", "Real values"], "Date": "2012", "Abstract": "Minimax search, which is used by most game-playing programs, is considered pathological when deeper searches produce worse evaluations than shallower ones. This phenomenon was first observed in theoretical analyses under seemingly reasonable conditions. It was most commonly explained by the lack of dependence between nearby positions in the analyses: if nearby positions have similar values, as is typically the case in real games, the pathology no longer occurs. In this paper, we show that the pathology can be eliminated even without position-value dependence, by assigning enough different values to the positions and modeling the heuristic error as normally distributed noise that is independent of the depth in the game tree. This leads to the conclusion that minimax is less prone to the pathology than was previously thought and indicates the importance of the number of different position values. \u00a9 2011 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "0"},
{"Title": "SR proteins are NXF1 adaptors that link alternative RNA processing to mRNA export", "Authors": ["Muller-McNicoll M.", "Botti V.", "de Jesus Domingues A.M.", "Brandl H.", "Schwich O.D.", "Steiner M.C.", "Curk T.", "Poser I.", "Zarnack K.", "Neugebauer K.M."], "Keywords": ["Alternative 3\u2032 end processing", "iCLIP", "mRNA export", "NXF1", "SR protein", "SRSF3", "SRSF7"], "Date": "2016", "Abstract": "Nuclear export factor 1 (NXF1) exports mRNA to the cytoplasm after recruitment to mRNA by specific adaptor proteins. How and why cells use numerous different export adaptors is poorly understood. Here we critically evaluate members of the SR protein family (SRSF1\u20137) for their potential to act as NXF1 adaptors that couple pre-mRNA processing to mRNA export. Consistent with this proposal, >1000 endogenous mRNAs required individual SR proteins for nuclear export in vivo. To address the mechanism, transcriptome-wide RNA-binding profiles of NXF1 and SRSF1\u20137 were determined in parallel by individual-nucleotide-resolution UV cross-linking and immunoprecipitation (iCLIP). Quantitative comparisons of RNA-binding sites showed that NXF1 and SR proteins bind mRNA targets at adjacent sites, indicative of cobinding. SRSF3 emerged as the most potent NXF1 adaptor, conferring sequence specificity to RNA binding by NXF1 in last exons. Interestingly, SRSF3 and SRSF7 were shown to bind different sites in last exons and regulate 3\u2032 untranslated region length in an opposing manner. Both SRSF3 and SRSF7 promoted NXF1 recruitment to mRNA. Thus, SRSF3 and SRSF7 couple alternative splicing and polyadenylation to NXF1-mediated mRNA export, thereby controlling the cytoplasmic abundance of transcripts with alternative 3\u2032 ends.", "Language": "en", "Citations": "72"},
{"Title": "Analysis of object serialization in Java and .NET \u010casovna in prostorska analiza serializacije objektov v Javi in .NET", "Authors": ["Hericko M.", "Juric M.B.", "Rozman I."], "Keywords": [".NET", "Binary", "Java", "Serialization", "Time ana space analysis", "XML"], "Date": "2003", "Abstract": "Object serialization is the process of writing the state of an object to a stream. Deserialization is the process of rebuilding the stream back into an object. Serialization has become an important concept in modern software platforms, such as Java and .NET. Both platforms provide extensive support for serialization, which eases the development. Serialization is the underlying concept of many other technologies. It is particularly important for remote method invocation and the distributed object models, where it allows marshaling objects by value and sending them across process and computer boundaries. Serialization can represent the serialized state of an object using binary or XML format. We would expect that binary serialization is more effective in terms of memory space and time, as it needs to do much less transformations than XML serialization. XML serialization on the other hand is more portable than binary, it is human readable and it is used in software architectures based on web services. Therefore, in this paper we compare binary and XML serialization in terms of space and time on lava and .NET platforms. For the purposes of this paper we measured the time required to serialize the TestObject and Contract objects. We also measured the memory space used for serialization. The time required for serialization assesses the efficiency of built-in serialization algorithms. The memory space assesses the efficiency of the in-memory representation in case of binary serialization and the XML vocabulary used for XML serialization. Binary and XML serialization differ in the way they represent the serialized state of the objects. Therefore, the selection will depend on our requirements and on what we will do with the serialized objects. The key criteria include the need for data sharing between different systems, accessibility of the data, execution speed and limitations of the object model. Binary serialization is a representative of deep serialization. It includes all object attributes, including private and protected members. Binary serialization is also capable of handling object graphs with cyclic references. XML serialization on the other hand serializes only public class members and is not capable of handling object graphs with cyclic references. XML serialization is a representative of shallow serialization. Our measurements showed that both Java and .NET provide adequate support for serialization. Java performs better in binary serialization; though it requires more memory for serialized objects than .NET. For XML serialization .NET performs much better than Java. The memory footprint is however comparable. For simpler object .NET is more effective, whilst for complex Java is better. The major reason for the poor Java performance in XML serialization is the Castor framework, which uses SAX for deserialization. Java is namely particularly slow in XML deserialization. The other differences relate to the size of metadata included with serialization and the efficiency of the XML vocabulary for serialization, where the major differences are in the way data types are specified. XML and binary serialization are very useful concepts. They are well supported. XML serialization requires more attention in the Java platform.", "Language": "en", "Citations": "0"},
{"Title": "OCT4 and the acquisition of oocyte developmental competence during folliculogenesis", "Authors": ["Zuccotti M.", "Merico V.", "Belli M.", "Mulas F.", "Sacchi L.", "Zupan B.", "Redi C.A.", "Prigione A.", "Adjaye J.", "Bellazzi R.", "Garagna S."], "Keywords": ["Developmental competence", "OCT4", "Oocyte", "Pluripotency", "Preimplantation development"], "Date": "2012", "Abstract": "The role that the transcription factor OCT4 plays during oocyte growth is yet unknown. In this review, we summarise the data on its potential role in the acquisition of oocyte developmental competence in the mouse. These studies describe the presence in MII oocytes and 2-cell embryos of an OCT4 transcriptional network that might be part of the molecular signature of maternal origin on which the inner cell mass and the embryonic stem cell-associated pluripotency is assembled and shaped. The Oct4-gene regulatory network thus provides a connection between eggs, early preimplantation embryos and embryonic stem cells. \u00a9 2013 UBC Press.", "Language": "en", "Citations": "14"},
{"Title": "Evaluating ERP as a composition of different functionalities from key stakeholder perspectives", "Authors": ["Habjan N.", "Hovelja T.", "Vavpotic D."], "Keywords": [], "Date": "2016", "Abstract": "The field of evaluation of information systems (IS) success has a long and established history. However, models in the field usually evaluate success on the level of the whole IS while neglecting the differences in impacts of specific IS functionalities on IS success. Moreover, evaluation models often focus only on technological or only on social perspective neglecting that IS should be considered a socio-technical system and that all key stakeholders' perspectives should be evaluated. Thus, three main perspectives of IS evaluation have to be recognized, namely user/social, technological and managerial perspectives. In this paper we present a novel model for evaluation of ERP success considering all key stakeholder perspectives. The proposed model provides the evaluation of individual functionalities from these perspectives in contrast to established models that consider system as a whole. The model has been tested in a case study in a company from the nautical industry. The results confirmed the usefulness of the evaluation model, especially its ability to provide specific new information to management on the influence of the individual functionalities on ERP success.", "Language": "en", "Citations": "0"},
{"Title": "Influence of search depth on position evaluation", "Authors": ["Guid M.", "Bratko I."], "Keywords": [], "Date": "2017", "Abstract": "By using a well-known chess program and a large data set of chess positions from real games we demonstrate empirically that with increasing search depth backed-up evaluations of won positions tend to increase, while backed-up evaluations of lost positions tend to decrease. We show three implications of this phenomenon in practice and in the theory of computer game playing. First, we show that heuristic evaluations obtained by searching to different search depths are not directly comparable. Second, we show that fewer decision changes with deeper search are a direct consequence of this property of heuristic evaluation functions. Third, we demonstrate that knowing this property may be used to develop a method for detecting fortresses in chess, which is an unsolved task in computer chess.", "Language": "en", "Citations": "0"},
{"Title": "Robust detection of heart beats in multimodal records using slope- and peak-sensitive band-pass filters", "Authors": ["Pangerc U.", "Jager F."], "Keywords": ["bedside monitors", "detecting pacemaker pattern", "multimodal records", "robust heart beat detection", "slope- and peaksensitive band-pass filters"], "Date": "2015", "Abstract": "In this work, we present the development, architecture and evaluation of a new and robust heart beat detector in multimodal records. The detector uses electrocardiogram (ECG) signals, and/or pulsatile (P) signals, such as: blood pressure, artery blood pressure and pulmonary artery pressure, if present. The base approach behind the architecture of the detector is collecting signal energy (differentiating and low-pass filtering, squaring, integrating). To calculate the detection and noise functions, simple and fast slope- and peak-sensitive band-pass digital filters were designed. By using morphological smoothing, the detection functions were further improved and noise intervals were estimated. The detector looks for possible pacemaker heart rate patterns and repairs the ECG signals and detection functions. Heart beats are detected in each of the ECG and P signals in two steps: a repetitive learning phase and a follow-up detecting phase. The detected heart beat positions from the ECG signals are merged into a single stream of detected ECG heart beat positions. The merged ECG heart beat positions and detected heart beat positions from the P signals are verified for their regularity regarding the expected heart rate. The detected heart beat positions of a P signal with the best match to the merged ECG heart beat positions are selected for mapping into the noise and no-signal intervals of the record. The overall evaluation scores in terms of average sensitivity and positive predictive values obtained on databases that are freely available on the Physionet website were as follows: the MIT-BIH Arrhythmia database (99.91%), the MGH/MF Waveform database (95.14%), the augmented training set of the follow-up phase of the PhysioNet/Computing in Cardiology Challenge 2014 (97.67%), and the Challenge test set (93.64%).", "Language": "en", "Citations": "12"},
{"Title": "Developing object wrappers for legacy integration in distributed object architecture Razvoj objektnih ovojev za integracijo obstojecih sistemov v porazdeljeni objektni arhitekturi", "Authors": ["Juric M.B.", "Rozman I.", "Beloglavec S.", "Hericko M.", "Welzer T.D.", "Gyorkos J."], "Keywords": [], "Date": "2000", "Abstract": "Distributed object technology extends object technology with the power of client/server architecture and integrates legacy systems. Legacy systems are important for current business operations and represent assets that business can not afford to write off. Integration with wrapping makes legacy systems look like distributed objects. General understanding of legacy systems gives a solid basis for making a decision about the distributed object architecture and presents a praxis proven wrapping method. Integration is also useful for decomposing and decoupling large legacy systems into separate components. Each component can have its own wrapper and so each component can be reused and upgraded independently of others.", "Language": "en", "Citations": "0"},
{"Title": "Prediction Quality Assessment", "Authors": ["Kukar M."], "Keywords": ["Confidence Estimation", "Kullback-Leibler Divergence", "Quality Assessment", "Reliability Estimation", "Transduction", "Typicalness"], "Date": "2014", "Abstract": "In the last decade machine learning and data mining were established as highly valuable data analysis tools. Their pervasive use means that they are used in several risk-sensitive domains, where failed predictions may cause substantial financial, economic, health, reputational, or other damage. For such use, most data mining approaches are less than ideal, since more often than not, they cannot produce reliable and unbiased assessments of their predictions' quality. In recent years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, transductive reliability estimation, and conformal prediction. In the chapter we describe a general approach to estimating quality of data mining predictions, based upon transductive reliability estimation and conformal prediction frameworks. The resulting conformal predictors produce confidence values in the statistical sense (e.g., a confidence level of 95% means that in 95% the predicted class is also a true class), as well as provide a general principle that is independent of the particular underlying data mining method. \u00a9 2014 Elsevier Inc. All rights reserved..", "Language": "en", "Citations": "0"},
{"Title": "Learning statistically relevant edge structure improves low-level visual descriptors", "Authors": ["Tabernik D.", "Kristan M.", "Boben M.", "Leonardis A."], "Keywords": [], "Date": "2012", "Abstract": "Over the recent years, low-level visual descriptors, among which the most popular is the histogram of oriented gradients (HOG), have shown excellent performance in object detection and categorization. We form a hypothesis that the low-level image descriptors can be improved by learning the statistically relevant edge structures from natural images. We validate this hypothesis by introducing a new descriptor called the histogram of compositions (HoC). HoC exploits a learnt vocabulary of parts from a state-of-the-art hierarchical compositional model. Furthermore, we show that HoC is a complementary HoC descriptor to HOG. We experimentally compare our descriptor to the popular HOG descriptor on the task of object categorization. We have observed approximately 4% improved categorization performance of HoC over HOG at lower dimensionality of the descriptor. Furthermore, in comparison to HOG, we show a categorization improvement of approximately 10% when combining HOG with the proposed HoC. \u00a9 2012 ICPR Org Committee.", "Language": "en", "Citations": "4"},
{"Title": "A balanced\u00a0scorecard-based model for evaluating e-learning and conventional pedagogical activities in nursing", "Authors": ["Hovelja T.", "Vavpotic D.", "Zvanut B."], "Keywords": ["cost effectiveness", "curriculum", "e-learning", "evaluation"], "Date": "2015", "Abstract": "The evaluation of e-learning and conventional pedagogical activities in nursing programmes has focused either on a single pedagogical activity or the entire curriculum, and only on students\u2019 or teachers\u2019 perspective. The goal of this study was to design and test a novel approach for evaluation of e-learning and conventional pedagogical activities that considers students\u2019, teachers\u2019 and managers\u2019 perspectives. A case study of the proposed approach was performed at a publicly funded nursing faculty with Slovenian and Italian students from 2009 to 2012. The case study was combined with focus group discussions, interviews, direct observation and survey. The proposed approach allows management to compare the value of different pedagogical activities through the students\u2019, teachers\u2019 and managers\u2019 perspectives. The approach proved useful in the evaluation of pedagogical activities and provided valid arguments for long-term pedagogical process improvement.", "Language": "en", "Citations": "0"},
{"Title": "Recognizing objects by their appearance using eigenimages", "Authors": ["Bischof H.", "Leonardis A."], "Keywords": [], "Date": "2000", "Abstract": "The appearance-based approaches to vision problems have recently received a renewed attention in the vision community due to their ability to deal with combined effects of shape, reflectance properties, pose in the scene, and illumination conditions. Besides, appearancebased representations can be acquired through an automatic learning phase which is not the case with traditional shape representations. The approach has led to a variety of successful applications, e. g., visual positioning and tracking of robot manipulators, visual inspection, and human face recognition. In this paper we will review the basic methods for appearance-based object recognition. We will also identify the major limitations of the standard approach and present algorithms how these limitations can be alleviated leading to an object recognition system which is applicable in real world situations.", "Language": "en", "Citations": "1"},
{"Title": "Empirical evaluation of feature selection methods in classification", "Authors": ["Cehovin L.", "Bosnic Z."], "Keywords": ["Feature selection", "Gini index", "Random forest feature selector", "ReliefF", "Sequential backward selection", "Sequential forward selection"], "Date": "2010", "Abstract": "In the paper, we present an empirical evaluation of five feature selection methods: ReliefF, random forest feature selector, sequential forward selection, sequential backward selection, and Gini index. Among the evaluated methods, the random forest feature selector has not yet been widely compared to the other methods. In our evaluation, we test how the implemented feature selection can affect (i.e. improve) the accuracy of six different classifiers by performing feature selection. The results show that ReliefF and random forest enabled the classifiers to achieve the highest increase in classification accuracy on the average while reducing the number of unnecessary attributes. The achieved conclusions can advise the machine learning users which classifier and feature selection method to use to optimize the classification accuracy, which may be important especially in risk-sensitive applications of Machine Learning (e.g. medicine, business decisions, control applications) as well as in the aim to reduce costs of collecting, processing and storage of unnecessary data. \u00a9 2010 - IOS Press and the authors.", "Language": "en", "Citations": "19"},
{"Title": "Fusion of acoustic and prosodic features for speaker clustering", "Authors": ["Zibert J.", "Mihelic F."], "Keywords": [], "Date": "2009", "Abstract": "This work focus on a speaker clustering methods that are used in speaker diarization systems. The purpose of speaker clustering is to associate together segments that belong to the same speakers. It is usually applied in the last stage of the speaker-diarization process. We concentrate on developing of proper representations of speaker segments for clustering and explore different similarity measures for joining speaker segments together. We realize two different competitive systems. The first is a standard approach using a bottom-up agglomerative clustering principle with the Bayesian Information Criterion (BIC) as a merging criterion. In the next approach a fusion speaker clustering system is developed, where the speaker segments are modeled by acoustic and prosody representations. The idea here is to additionally model the speaker prosody characteristics and add it to basic acoustic information estimated from the speaker segments. We construct 10 basic prosody features derived from the energy of the audio signals, the estimated pitch contours, and the recognized voiced and unvoiced regions in speech. In this way we impose higher-level information in the representations of the speaker segments, which leads to improved clustering of the segments in the case of similar speaker acoustic characteristics or poor acoustic conditions. \u00a9 2009 Springer Berlin Heidelberg.", "Language": "en", "Citations": "6"},
{"Title": "Do PageRank-based author rankings outperform simple citation counts?", "Authors": ["Fiala D.", "Subelj L.", "Zitnik S.", "Bajec M."], "Keywords": ["Citations", "Importance", "PageRank", "Rankings", "Scholars"], "Date": "2015", "Abstract": "The basic indicators of a researcher's productivity and impact are still the number of publications and their citation counts. These metrics are clear, straightforward, and easy to obtain. When a ranking of scholars is needed, for instance in grant, award, or promotion procedures, their use is the fastest and cheapest way of prioritizing some scientists over others. However, due to their nature, there is a danger of oversimplifying scientific achievements. Therefore, many other indicators have been proposed including the usage of the PageRank algorithm known for the ranking of webpages and its modifications suited to citation networks. Nevertheless, this recursive method is computationally expensive and even if it has the advantage of favouring prestige over popularity, its application should be well justified, particularly when compared to the standard citation counts. In this study, we analyze three large datasets of computer science papers in the categories of artificial intelligence, software engineering, and theory and methods and apply 12 different ranking methods to the citation networks of authors. We compare the resulting rankings with self-compiled lists of outstanding researchers selected as frequent editorial board members of prestigious journals in the field and conclude that there is no evidence of PageRank-based methods outperforming simple citation counts.", "Language": "en", "Citations": "26"},
{"Title": "iCLIP predicts the dual splicing effects of TIA-RNA interactions", "Authors": ["Ule J.", "Wang Z.", "Kayikci M.", "Briese M.", "Zarnack K.", "Luscombe N.M.", "Rot G.", "Zupan B.", "Curk T."], "Keywords": [], "Date": "2010", "Abstract": "The regulation of alternative splicing involves interactions between RNA-binding proteins and pre-mRNA positions close to the splice sites. T-cell intracellular antigen 1 (TIA1) and TIA1-like 1 (TIAL1) locally enhance exon inclusion by recruiting U1 snRNP to 5\u2032 splice sites. However, effects of TIA proteins on splicing of distal exons have not yet been explored. We used UV-crosslinking and immunoprecipitation (iCLIP) to find that TIA1 and TIAL1 bind at the same positions on human RNAs. Binding downstream of 5\u2032 splice sites was used to predict the effects of TIA proteins in enhancing inclusion of proximal exons and silencing inclusion of distal exons. The predictions were validated in an unbiased manner using splice-junction microarrays, RT-PCR, and minigene constructs, which showed that TIA proteins maintain splicing fidelity and regulate alternative splicing by binding exclusively downstream of 5\u2032 splice sites. Surprisingly, TIA binding at 5\u2032 splice sites silenced distal cassette and variable-length exons without binding in proximity to the regulated alternative 39 splice sites. Using transcriptome-wide high-resolution mapping of TIA-RNA interactions we evaluated the distal splicing effects of TIA proteins. These data are consistent with a model where TIA proteins shorten the time available for definition of an alternative exon by enhancing recognition of the preceding 5\u2032 splice site. Thus, our findings indicate that changes in splicing kinetics could mediate the distal regulation of alternative splicing. \u00a9 2010 Wang et al.", "Language": "en", "Citations": "140"},
{"Title": "A proposal for an open source graphical environment for simulating X-ray optics", "Authors": ["Del Rio M.S.", "Rebuffi L.", "Demsar J.", "Canestrari N.", "Chubar O."], "Keywords": ["Graphical user interface", "Optics simulations", "Software", "Virtual experiment", "X-ray optics"], "Date": "2014", "Abstract": "A new graphic environment to drive X-ray optics simulation packages such as SHADOW and SRW is proposed. The aim is to simulate a virtual experiment, including the description of the electron beam and simulate the emitted radiation, the optics, the scattering by the sample and radiation detection. Python is chosen as common interaction language. The ingredients of the new application, a glossary of variables for optical component, the selection of visualization tools, and the integration of all these components in a high level workflow environment built on Orange are presented.", "Language": "en", "Citations": "11"},
{"Title": "Developing software for school administration and management: Incorporating flexibility", "Authors": ["Bajec M.", "Krisper M.", "Rupnik R."], "Keywords": ["Business rules technologies", "Software flexibility"], "Date": "2001", "Abstract": "Flexibility is one of the most important characteristics of software systems for computerised school management and administration, in particular if the software has to be used in several institutions. Given that school institutions are diverse in many aspects and have their own specific needs, several problems are faced when developing unified software. In this paper we describe an approach to information system planning and development, which we believe can help to gain the required software flexibility. The main purpose of the paper is not to examine technical issues on information technologies but to emphasise the possibilities that have to be considered when developing software in support of management and administration in schools.", "Language": "en", "Citations": "0"},
{"Title": "Predicting exploitations of information systems vulnerabilities through attackers\u2019 characteristics", "Authors": ["Dobrovoljc A.", "Trcek D.", "Likar B."], "Keywords": ["CVSS", "Prioritization policy", "Security management", "Threat agent", "Vulnerability"], "Date": "2017", "Abstract": "The main goal of proactive security is to prevent attacks before they happen. In modern information systems it largely depends on the vulnerability management process, where prioritization is one of the key steps. A widely used prioritization policy based only upon a common vulnerability scoring system (CVSS) score is frequently criticised for bad effectiveness. The main reason is that the CVSS score alone is not a good predictor of vulnerability exploitation in the wild. Therefore, the aim of the research in this field is to determine in what way we can improve our prediction abilities. Clearly, software vulnerabilities are commodities used by attackers. Hence, it makes sense considering their characteristics in vulnerability prioritization. In contrast, one should be able to measure and compare the effectiveness of various policies. Therefore, an important goal of this paper was to develop an evaluation model, which would allow such comparisons. For this purpose, we developed an agent-based simulation model which measures the exposure of information system to exploitable vulnerabilities. Besides, some policies which take into account human threats were defined and then compared with the most popular existing methods. Experimental results imply that the proposed policy, which is based on CVSS vectors and attacker characteristics, achieves the highest effectiveness among existing methods.", "Language": "en", "Citations": "2"},
{"Title": "Hierarchical compositional architecture for activity detection and recognition Hierarhicna kompozicionalna arhitektura za detekcijo in razpoznavanje aktivnosti", "Authors": ["Pers J.", "Kristan M.", "Mandeljc R.", "Kovacic S.", "Leonardis A."], "Keywords": [], "Date": "2013", "Abstract": "Many activity detection and recognition approaches focus on designing or learning high-quality, low-level spatiotemporal descriptors. These low-level descriptors may become overly complex, encoding motion, shape and texture in uncontrolled and unpredictable ways. While such complexity helps increasing recognition rates, it slows down the process of inference and obscures the reasoning behind the learned descriptors. We present an alternative approach, which is based on primitive features that encode pure motion. These are coupled with a hierarchical scheme to learn motion patterns (compositions) from a single short video. During the inference process, these learned patterns are extracted from the analyzed videos and used with \u03c7", "Language": "en", "Citations": "0"},
{"Title": "Obstacle Detection for USVs by Joint Stereo-View Semantic Segmentation", "Authors": ["Bovcon B.", "Kristan M."], "Keywords": [], "Date": "2018", "Abstract": "We propose a stereo-based obstacle detection approach for unmanned surface vehicles. Obstacle detection is cast as a scene semantic segmentation problem in which pixels are assigned a probability of belonging to water or non-water regions. We extend a single-view model to a stereo system by adding a constraint which prefers consistent class labels assignment to pixels in the left and right camera images corresponding to the same parts of a 3D scene. Our approach jointly fits a semantic model to both images, leading to an improved class-label posterior map from which obstacles and water edge are extracted. In overall F-measure, our approach outperforms the current state-of-the-art monocular approach by 0.495, a monocular CNN by 0.798 and their stereo extensions by 0.059 and 0.515, respectively on the task of obstacle detection while running real-time on a single CPU.", "Language": "en", "Citations": "0"},
{"Title": "Moment problems for operator polynomials", "Authors": ["Cimpric J.", "Zalar A."], "Keywords": ["Moment problems", "Operator polynomials", "Operator-valued measures", "Real algebraic geometry"], "Date": "2013", "Abstract": "Haviland's theorem states that given a closed subset K in Rn, each functional L:R[x-]\u2192R positive on Pos(K){colon equals}{p\u2208R[x-]{pipe}p{pipe}K\u22650} admits an integral representation by a positive Borel measure. Schm\u00fcdgen proved that in the case of compact semialgebraic set K it suffices to check positivity of L on a preordering T, having K as the non-negativity set. Further he showed that the compactness of K is equivalent to the archimedianity of T. The aim of this paper is to extend these results from functionals on the usual real polynomials to operators mapping from the real matrix or operator polynomials into R,Mn(R) or B(K). \u00a9 2012 Elsevier Ltd.", "Language": "en", "Citations": "8"},
{"Title": "The Total Graphs of Finite Commutative Semirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Finite commutative semiring", "total graph", "zero-divisor"], "Date": "2017", "Abstract": "Anderson and Badawi (J Algebra 320(7):2706\u20132719, 2008) characterized all commutative rings having total graphs without any 3-cycles. In this paper we expand those results to the semiring setting and obtain the characterization of finite commutative semirings having total graphs without any 3-cycles.", "Language": "en", "Citations": "1"},
{"Title": "Conceptualizing procedural knowledge targeted at students of different skill levels", "Authors": ["Mozina M.", "Guid M.", "Sadikov A.", "Groznik V.", "Krivec J.", "Bratko I."], "Keywords": [], "Date": "2010", "Abstract": "Conceptualizing procedural knowledge is one of the most challenging tasks of building systems for intelligent tutoring. We present a novel algorithm that enables teachers to accomplish this task (semi)automatically. Furthermore, it is desired to adapt the level of conceptualization to the skill level of particular students. We argue that our algorithm facilitates such adaptation in a straight-forward fashion. We demonstrate this feature of the algorithm with a case study.", "Language": "en", "Citations": "2"},
{"Title": "Bias and pathology in minimax search", "Authors": ["Sadikov A.", "Bratko I.", "Kononenko I."], "Keywords": ["Bias", "Evaluation-function quality", "KRK chess endgame", "Minimax pathology", "Minimax principle"], "Date": "2005", "Abstract": "This article presents the results of experiments designed to gain insight into the effect of the minimax algorithm on the error of a heuristic evaluation function. Two types of effect of minimax are considered: (a) evaluation accuracy (Are the minimax backed-up values more accurate than the heuristic values themselves?), and (b) decision accuracy (Are moves played by deeper minimax search better than those by shallower search?). The experiments were performed in the King-Rook-King (KRK) chess endgame and in randomly generated game trees. The results show that, counter-intuitively, evaluation accuracy may decline with search depth, whereas at the same time decision accuracy improves with depth. In the article, this is explained by the fact that minimax in combination with a noisy evaluation function introduces a bias into the backed-up evaluations, which masks the evaluation effectiveness of minimax, but this bias still permits decision accuracy to improve with depth. This observed behaviour of minimax in the KRK endgame is discussed in the light of previous studies of pathology in minimax. It is shown that explaining the behaviour of minimax in an actual chess endgame in terms of previously known results requires special care. \u00a9 2005 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "12"},
{"Title": "Orange: Data mining toolbox in python", "Authors": ["Demsar J.", "Curk T.", "Erjavec A.", "Gorup C.", "Hocevar T.", "Milutinovic M.", "Mozina M.", "Polajnar M.", "Toplak M.", "Staric A.", "Stajdohar M.", "Umek L.", "Zagar L.", "Zbontar J.", "Zitnik M.", "Zupan B."], "Keywords": ["Data mining", "Machine learning", "Python", "Scripting", "Toolbox"], "Date": "2013", "Abstract": "Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationallyintensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining. \u00a9 2013 Janez Dem\u0161ar, Toma\u017e Curk, Ale\u0161 Erjavec, \u010crt Gorup, Toma\u017e Ho\u010devar, Mitar Milutinovi\u010d, Martin Mo\u017eina, Matija Polajnar, Marko Toplak, An\u017ee Stari\u010d, Miha Stajdohar, Lan Umek, Lan \u017dagar, Jure \u017dbontar, Marinka \u017ditnik and Bla\u017e Zupan.", "Language": "en", "Citations": "361"},
{"Title": "IT Governance Mechanisms and Contingency Factors: Towards an Adaptive IT Governance Model", "Authors": ["Levstek A.", "Hovelja T.", "Pucihar A."], "Keywords": ["IT Governance", "ITG contingency factors", "ITG framework", "ITG mechanisms"], "Date": "2018", "Abstract": "Background and Purpose: In this paper, we aim to propose a guideline for further research towards development of an adaptive strategic IT governance (ITG) model for small and medium-sized enterprises (SMEs). The use of IT has the potential to be the major driver for success, as well it provides an opportunity to achieve competitive advantage and support digital transformation. In order to achieve IT benefits, enterprises need an effective and successful ITG model, which follows and adapts to business needs. Available ITG models are too generic and do not differentiate for enterprises of different industry, size, maturity etc. Methodology: In order to review existing ITG mechanisms, their definitions and identify contingency factors, we performed an extensive literature review (LR). For the initial set of databases, we used the list of journals, which are indexed in the Journal Citation Reports. We also used Web of Science to identify articles with the highest number of citations. Results: This paper provides the most important definitions of ITG and proposes its comprehensive definition. Next to this, we introduce ITG mechanisms, which are crucial for the effective implementation and use of ITG. Lastly, we identify contingency factors that influence ITG implementation and its use. Conclusion: Despite extensive research in ITG area, considerable work is still needed to improve understanding of ITG, its definition and mechanisms. Multiple efforts to develop methods for governing IT failed to achieve any significant adoption rate of ITG mechanisms. To enable ITG to become an integral part of Corporate Governance, further research needs to focus on the development of an adaptive strategic ITG model. In this paper, we propose a next step for more practical method for ITG implementation and its use.", "Language": "en", "Citations": "0"},
{"Title": "The prognostic value of whole blood SOX2, NANOG and OCT4 mRNA expression in advanced small-cell lung cancer", "Authors": ["Sodja E.", "Rijavec M.", "Koren A.", "Sadikov A.", "Korosec P.", "Cufer T."], "Keywords": ["cancer stem cell markers", "mRNA expression", "NANOG", "OCT4", "prognosis", "small-cell lung cancer", "SOX2"], "Date": "2016", "Abstract": "The data on expression and clinical impact of cancer stem cell markers SOX2, NANOG and OCT4 in lung cancer is still lacking. The aim of our study was to compare SOX2, NANOG and OCT4 mRNA expression levels in whole blood between advanced small-cell lung cancer (SCLC) patients and healthy controls, and to correlate mRNA expression with progression-free survival (PFS) after first-line chemotherapy and overall survival (OS) in advanced SCLC patients. 50 advanced SCLC patients treated with standard chemotherapy and followed at University Clinic Golnik, Slovenia, between 2009 and 2013 were prospectively included. SOX2, NANOG and OCT4 mRNA expression levels were determined using TaqMan qPCR in whole blood collected prior to chemotherapy. Whole blood of 34 matched healthy individuals with no cancerous disease was also tested. SOX2 mRNA expression was significantly higher in whole blood of SCLC patients compared to healthy controls (p = 0.006). Significant correlation between SOX2 mRNA expression levels and the number of distant metastatic sites was established (p = 0.027). In survival analysis, patients with high SOX2 expression had shorter OS (p = 0.017) and PFS (p = 0.046). In multivariate Cox analysis, an independent value of high SOX2 expression for shorter OS (p = 0.002), but not PFS was confirmed. No significant differences were observed for NANOG or OCT4 expression levels when comparing SCLC patients and healthy controls neither when analysing survival outcomes in SCLC patients. SOX2 mRNA expression in whole blood might be a promising non-invasive marker for molecular screening of SCLC and important prognostic marker in advanced chemotherapy-treated SCLC patients, altogether indicating important role of cancer stem-like cell (CSC) regulators in cancer spread. Further evaluation of SOX2 as a possible screening/prognostic marker and a therapeutic target of SCLC is warranted.", "Language": "en", "Citations": "13"},
{"Title": "Using stakeholder-driven process performance measurement for monitoring the performance of a Scrum-based software development process", "Authors": ["Mahnic V.", "Vrana I."], "Keywords": ["Scrum", "Software metrics", "Software process improvement"], "Date": "2007", "Abstract": "We describe a metrics plan for monitoring and improving the performance of the software development process based on the Scrum agile method. After a short introduction to Scrum concepts a detailed description of the proposed metrics is provided. The metrics are defined using the principles of stakeholder-driven process performance measurement that requires a balanced approach considering viewpoints of different stakeholders. The goals of each stakeholder are defined first followed by the choice of appropriate performance indicators. The evaluation of each indicator is based on metric values collected during process execution. The metrics plan enables a stepwise introduction of metrics which can be incorporated into the Scrum method seamlessly without affecting the agility of the development process.", "Language": "en", "Citations": "12"},
{"Title": "Estimation of individual prediction reliability using the local sensitivity analysis", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": [], "Date": "2008", "Abstract": "For a given prediction model, some predictions may be reliable while others may be unreliable. The average accuracy of the system cannot provide the reliability estimate for a single particular prediction. The measure of individual prediction reliability can be important information in risk-sensitive applications of machine learning (e.g. medicine, engineering, business). We define empirical measures for estimation of prediction accuracy in regression. Presented measures are based on sensitivity analysis of regression models. They estimate reliability for each individual regression prediction in contrast to the average prediction reliability of the given regression model. We study the empirical sensitivity properties of five regression models (linear regression, locally weighted regression, regression trees, neural networks, and support vector machines) and the relation between reliability measures and distribution of learning examples with prediction errors for all five regression models. We show that the suggested methodology is appropriate only for the three studied models: regression trees, neural networks, and support vector machines, and test the proposed estimates with these three models. The results of our experiments on 48 data sets indicate significant correlations of the proposed measures with the prediction error. \u00a9 2007 Springer Science+Business Media, LLC.", "Language": "en", "Citations": "34"},
{"Title": "Deformable Parts Correlation Filters for Robust Visual Tracking", "Authors": ["Lukezic A.", "Cehovin Zajc L.", "Kristan M."], "Keywords": ["Computer vision", "correlation filters", "short-term tracking", "spring systems", "visual object tracking"], "Date": "2018", "Abstract": "Deformable parts models show a great potential in tracking by principally addressing nonrigid object deformations and self occlusions, but according to recent benchmarks, they often lag behind the holistic approaches. The reason is that potentially large number of degrees of freedom have to be estimated for object localization and simplifications of the constellation topology are often assumed to make the inference tractable. We present a new formulation of the constellation model with correlation filters that treats the geometric and visual constraints within a single convex cost function and derive a highly efficient optimization for maximum a posteriori inference of a fully connected constellation. We propose a tracker that models the object at two levels of detail. The coarse level corresponds a root correlation filter and a novel color model for approximate object localization, while the mid-level representation is composed of the new deformable constellation of correlation filters that refine the object location. The resulting tracker is rigorously analyzed on a highly challenging OTB, VOT2014, and VOT2015 benchmarks, exhibits a state-of-the-art performance and runs in real-time.", "Language": "en", "Citations": "20"},
{"Title": "Computer aided method for colour calibration and analysis of digital rock photographs", "Authors": ["Potocnik M.", "Klemenc B.", "Solina F.", "Herlec U."], "Keywords": ["Colour analysis", "Colour calibration", "Digital photography", "Digital rock photograph colour analysis", "Munsell colour system"], "Date": "2015", "Abstract": "The methods used in geology to determine colour and colour coverage are expensive, time consuming, and/ or subjective. Estimates of colour coverage can only be approximate since they are based on rough comparisonbased measuring etalons and subjective estimation, which is dependent upon the skill and experience of the person performing the estimation. We present a method which accelerates, simplifies, and objectifies these tasks using a computer application. It automatically calibrates the colours of a digital photo, and enables the user to read colour values and coverage, even after returning from field work. Colour identification is based on the Munsell colour system. For the purposes of colour calibration we use the X-Rite ColorChecker Passport colour chart placed onto the photographed scene. Our computer application detects the ColorChecker colour chart, and finds a colour space transformation to calibrate the colour in the photo. The user can then use the application to read colours within selected points or regions of the photo. The results of the computerised colour calibration were compared to the reference values of the ColorChecker chart. The values slightly deviate from the exact values, but the deviation is around the limit of human capability for visual comparison. We have devised an experiment, which compares the precision of the computerised colour analysis and manual colour analysis performed on a variety of rock samples with the help of geology students using Munsell Rock-color Chart. The analysis showed that the precision of manual comparative identification on multicoloured samples is somewhat problematic, since the choice of representative colours and observation points for a certain part of a sample are subjective. The computer based method has the edge in verifiability and repeatability of the analysis since the application the original photo to be saved with colour calibration, and tagging of colouranalysed points and regions.", "Language": "en", "Citations": "0"},
{"Title": "AT4 family and 2-homogeneous graphs", "Authors": ["Jurisic A."], "Keywords": ["1-homogeneous", "2-homogeneous", "Antipodal", "Distance-regular graphs", "Krein parameters", "Smith graphs", "Tight graphs"], "Date": "2003", "Abstract": "Let \u0393 denote an antipodal distance-regular graph of diameter four, with eigenvalues k= \u03b8 ", "Language": "en", "Citations": "10"},
{"Title": "Modeling functional requirements for configurable content- and context-aware dynamic service selection in business process models", "Authors": ["Frece A.", "Juric M.B."], "Keywords": ["BPMN", "Business process model", "Content/context awareness", "Dynamic service selection", "Functional requirement"], "Date": "2012", "Abstract": "In this article, we propose a meta-model for formal specification of functional requirements for configurable content- and context-aware dynamic service selection in business process models with the objective to enable greater flexibility of the modeled processes. The dynamic service selection can cope with highly dynamic business environments that today's business processes must handle. Modeling functional requirements for dynamic service selection in business process models is not well covered in literature. Some partial solutions exist but none of them allows modeling a complete set of functional requirements for the selection similar to the one we are addressing in this article. Our meta-model enables formal specification of service selection relevant data extracted from service request message, custom configuration data (e.g., thresholds), process and task definition/instance metadata, and service selection rules. The meta-model is configurable and content- and context-aware. Processes leveraging our meta-model can adapt to changing requirements without redesign of the process flow. Proposed meta-model allows users to additionally configure the models at run time (e.g., raising a threshold). Modeling can be divided into roles with different required competences. We implement our meta-model in BPMN 2.0 (Business Process Model and Notation) through specific extensions to the BPMN semantic and diagram elements. By measuring complexity of real-world sample process models we show that using our solution modelers can efficiently model business processes that need to address frequent changing demands. Compared to available alternatives, models using our solution have on average ~13% fewer activities, ~16% fewer control-flow elements and ~22% fewer control paths. By reading ~10% smaller models (by volume) model readers get more flexible process models that capture all functional requirements for the dynamic selection. \u00a9 2012 Elsevier Ltd.", "Language": "en", "Citations": "14"},
{"Title": "Towards practical PPM spam filtering: Experiments for the TREC 2006 spam track", "Authors": ["Bratko A.", "Filipic B.", "Zupan B."], "Keywords": [], "Date": "2006", "Abstract": "This paper summarizes our participation in the TREC 2006 spam track. We submitted a single filter for the evaluation, based on the Prediction by Partial Matching compression scheme, a method that performed well in the previous TREC evaluation. A major focus of our effort was to improve efficiency of the method, particularly in terms of memory consumption, in order to establish whether compressionbased filters are in fact a viable solution for practical applications. Our system exhibited fair performance, despite the fact that the filtering techniques remained virtually unchanged from the previous evaluation. We did not investigate methods for tackling delayed user feedback. A very simple strategy of training on most recent examples was used for the active learning task, and found to work surprisingly well given its simplicity.", "Language": "en", "Citations": "0"},
{"Title": "miR669a and miR669q prevent skeletal muscle differentiation in postnatal cardiac progenitors", "Authors": ["Crippa S.", "Cassano M.", "Messina G.", "Galli D.", "Galvez B.G.", "Curk T.", "Altomare C.", "Ronzoni F.", "Toelen J.", "Gijsbers R.", "Debyser Z.", "Janssens S.", "Zupan B.", "Zaza A.", "Cossu G.", "Sampaolesi M."], "Keywords": [], "Date": "2011", "Abstract": "Postnatal heart stem and progenitor cells are a potential therapeutic tool for cardiomyopathies, but little is known about the mechanisms that control cardiac differentiation. Recent work has highlighted an important role for microribonucleic acids (miRNAs) as regulators of cardiac and skeletal myogenesis. In this paper, we isolated cardiac progenitors from neonatal \u03b2-sarcoglycan (Sgcb)-null mouse hearts affected by dilated cardiomyopathy. Unexpectedly, Sgcb-null cardiac progenitors spontaneously differentiated into skeletal muscle fibers both in vitro and when transplanted into regenerating muscles or infarcted hearts. Differentiation potential correlated with the absence of expression of a novel miRNA, miR669q, and with down-regulation of miR669a. Other miRNAs are known to promote myogenesis, but only miR669a and miR669q act upstream of myogenic regulatory factors to prevent myogenesis by directly targeting the MyoD 3' untranslated region. This finding reveals an added level of complexity in the mechanism of the fate choice of mesoderm progenitors and suggests that using endogenous cardiac stem cells therapeutically will require specially tailored procedures for certain genetic diseases. \u00a9 2011 Crippa et al.", "Language": "en", "Citations": "39"},
{"Title": "Segmentation of CT images Segmentacija tomografskih slik", "Authors": ["Rupar A.", "Leonardis A."], "Keywords": ["Computer vision", "CT images", "Medical image analysis", "Segmentation"], "Date": "2005", "Abstract": "In this paper we present a procedure for two- and three-dimensional segmention of bone from CT images. An algorithm obtaining information on local orientation of structures in image data is described. It is based on image analysis in the frequency domain. This information is captured using quadrature filters. We discuss a procedure for designing multidimensional digital filters to be applied on data which is sampled with different frequencies in different dimensions. We implemented three-dimensional filtering with the overlap-add algorithm, which makes use of the fast Fourier transformation algorithm (FFT). A region growing algorithm is employed for segmentation. It is based on the use of two criteria; the grey level value and information on edges obtained from information on local orientation of structures by edge thinning. In this way the algorithm exploits both global and local properties of image data. We present experimental results of the segmentation algorithm on two- and three-dimensional data and evaluate its performance.", "Language": "en", "Citations": "0"},
{"Title": "Barriers and challenges to global clinical cancer research", "Authors": ["Seruga B.", "Sadikov A.", "Cazap E.L.", "Delgado L.B.", "Digumarti R.", "Leighl N.B.", "Meshref M.M.", "Minami H.", "Robinson E.", "Yamaguchi N.H.", "Pyle D.", "Cufer T."], "Keywords": ["Barrier", "Cancer research", "Global"], "Date": "2014", "Abstract": "Background. There are concerns about growing barriers to cancer research. We explored the characteristics of and barriers to global clinical cancer research. Methods. The American Society of Clinical Oncology International Affairs Committee invited 300 selected oncologists with research experience from 25 countries to complete a Webbased survey. Fisher's exact test was used to compare answers between participants from high-income countries (HICs) and low- and middle-income countries (LMICs). Barriers to clinical cancer research were ranked from 1 (most important) to 8 (least important). Mann-Whitney's nonparametric test was used to compare the ranks describing the importance of investigated obstacles. Results. Eighty oncologists responded, 41 from HICs and 39 from LMICs. Most responders were medical oncologists (62%) atacademic hospitals (90%). Researchers from HICs weremore involved with academic and industry-driven research than were researchers from LMICs. Significantly higher proportions of those who considered their ability to conduct academic research and industry-driven research over the past 5 years more difficult were from HICs (73% vs. 27% and 70% vs. 30%, respectively). Concerning academic clinical cancer research, a lack of funding was ranked the most important (score: 3.16) barrier, without significant differences observed between HICs and LMICs. Lackof time orcompeting prioritiesandprocedures from competent authorities were the second most important barriers to conducting academic clinical research in HICs and LMICs, respectively. Conclusion. Lack of funding, lack of time and competing priorities, and procedures from competent authorities might be the main global barriers to academic clinical cancer research. \u00a9 AlphaMed Press 2014.", "Language": "en", "Citations": "10"},
{"Title": "D-test: An extension to Banerjee test for a fast dependence analysis in a multimedia vectorizing compiler", "Authors": ["Bulic P.", "Gustin V."], "Keywords": [], "Date": "2004", "Abstract": "There are a number of data dependence tests that have been proposed in the literature. In each test there is a different trade-off between accuracy and efficiency. The most widely used approximate data dependence tests are the Banerjee inequality and the GCD test; whereas the Omega test is a well-known exact data dependence test. In this paper we consider parallelization for microprocessors with a multimedia extension (the short SIMD execution model). For the short SIMD parallelism extraction it is essential that, if dependency exists, then the dependence distance is greater than or equal to the number of data processed in the SIMD register. This implies that some loops that could not be vectorized on traditional vector processors can still be parallelized for the short SIMD execution. In all of these tests the parallelization would be prohibited when actually there is no parallelism restriction relating to the short SIMD execution model. In this paper we present a new, fast and accurate data dependence test (D-test) for array references with linear subscripts, which is used in a vectorizing compiler for microprocessors with a multimedia extension. Our method extends Banerjee test in such a way that the dependence analysis will be correct in many cases where dependence exists with the dependence distance that is greater than or equal to the number of data processed in the SIMD register. These special cases in which Banerjee test fails to prove the independece can than be attacked with D-test.", "Language": "en", "Citations": "1"},
{"Title": "Identifying typical approaches and errors in Prolog programming with argument-based machine learning", "Authors": ["Mozina M.", "Lazar T.", "Bratko I."], "Keywords": ["Abstract syntax tree", "Argument-based machine learning", "Programming tutors", "Rule learning", "Syntactic patterns"], "Date": "2018", "Abstract": "Students learn programming much faster when they receive feedback. However, in programming courses with high student-teacher ratios, it is practically impossible to provide feedback to all homeworks submitted by students. In this paper, we propose a data-driven tool for semi-automatic identification of typical approaches and errors in student solutions. Having a list of frequent errors, a teacher can prepare common feedback to all students that explains the difficult concepts. We present the problem as supervised rule learning, where each rule corresponds to a specific approach or error. We use correct and incorrect submitted programs as the learning examples, where patterns in abstract syntax trees are used as attributes. As the space of all possible patterns is immense, we needed the help of experts to select relevant patterns. To elicit knowledge from the experts, we used the argument-based machine learning (ABML) method, in which an expert and ABML interactively exchange arguments until the model is good enough. We provide a step-by-step demonstration of the ABML process, present examples of ABML questions and corresponding expert's answers, and interpret some of the induced rules. The evaluation on 42 Prolog exercises further shows the usefulness of the knowledge elicitation process, as the models constructed using ABML achieve significantly better accuracy than the models learned from human-defined patterns or from automatically extracted patterns.", "Language": "en", "Citations": "2"},
{"Title": "Converting metamodels to graph grammars: doing without advanced graph grammar features", "Authors": ["Furst L.", "Mernik M.", "Mahnic V."], "Keywords": ["Graph grammar", "Metamodel", "Parsability", "Parsing", "Semantic analysis", "UML"], "Date": "2015", "Abstract": "In this paper, we present a method to convert a metamodel in the form of a UML class diagram into a context-sensitive graph grammar whose language comprises precisely the set of model graphs (UML object diagrams) that conform to the input metamodel. Compared to other approaches that deal with the same problem, we use a graph grammar formalism that does not employ any advanced graph grammar features, such as application conditions, precedence rules, and production schemes. Specifically, we use Rekers and Sch\u00fcrr\u2019s Layered Graph Grammars, which may be regarded as a pure generalization of standard context-sensitive string grammars. We show that elementary grammatical features, i.e., grammar labels and context-sensitive graph rewrite rules, suffice to represent metamodels with arbitrary multiplicities and inheritance. Inspired by attribute string grammars, we also propose a graph-grammar-based approach to the semantic analysis of model graphs.", "Language": "en", "Citations": "10"},
{"Title": "An empirical analysis of business process execution language usage", "Authors": ["Hertis M.", "Juric M.B."], "Keywords": ["complexity measure", "empirical study", "process complexity", "process comprehension", "process patterns", "service composition", "WS-BPEL Analysis"], "Date": "2014", "Abstract": "The current state of executable business process languages allows for and demands optimization of design practices and specifications. In this paper, we present the first empirical study that analyses Web Services Business Process Execution Language (WS-BPEL or BPEL) usage and characteristics of real world executable business processes. We have analysed 1,145 BPEL processes by measuring activity usage and process complexity. In addition, we investigated the occurrence of activity usage patterns. The results revealed that the usage frequency of BPEL activities varies and that some activities have a strong co-occurrence. BPEL activities often appear in activity patterns that are repeated in multiple processes. Furthermore, the current process complexity metrics have proved to be inadequate for measuring BPEL process complexity. The empirical results provide fundamental knowledge on how BPEL specification and process design practices can be improved. We propose BPEL design guidelines and BPEL language improvements for the design of more understandable and less complex processes. The results are of interest to business process language designers, business process tool developers, business process designers and developers, and software engineering researchers, and contribute to the general understanding of BPEL and service-oriented architecture. \u00a9 2014 IEEE.", "Language": "en", "Citations": "10"},
{"Title": "Solving all-pairs shortest path by single-source computations: Theory and practice", "Authors": ["Brodnik A.", "Grgurovic M."], "Keywords": ["All pairs shortest path", "Single source shortest path"], "Date": "2017", "Abstract": "Given an arbitrary directed graph G=(V,E) with non-negative edge lengths, we present an algorithm that computes all pairs shortest paths in time O(m", "Language": "en", "Citations": "0"},
{"Title": "New approaches: Uncovering semantic structures in ethnological materials Novi pristopi. Odkrivanje semanti\u010dnih struktur v etnolo\u0161kih vsebinah", "Authors": ["Strle G.", "Marolt M."], "Keywords": ["Computational methods", "Folk songs", "Folklore", "LDA (Latent Dirichlet Allocation)", "LSA (Latent Semantic Analysis)", "Natural language processing", "Semantic analysis"], "Date": "2014", "Abstract": "The article addresses computational approaches to semantic analysis and extraction of meaningful structures from the contents. These methods are becoming more relevant and often necessary, especially in the analysis of large digital collections where manual classification of materials is not possible. Moreover, computational semantic analysis offers insight into the broader conceptual structure of the contents; selection of desired parameters; and discovery of a general semantic structure (e.g. thematic patterns) and context. The materials analyzed for this purpose contain a corpus of Slovene folk songs.", "Language": "en", "Citations": "1"},
{"Title": "Persuasive technologies in m-learning for training professionals: how to keep learners engaged with adaptive triggering", "Authors": ["Kljun M.", "Krulec R.", "Copic Pucihar K.", "Solina F."], "Keywords": ["adaptive triggering", "Companies", "Electronic learning", "Globalization", "m-learning", "Market research", "mobile learning", "persuasive technologies", "professional training", "Task analysis", "Training", "triggering"], "Date": "2018", "Abstract": "Global corporations are characterised by a large number of employees and geographically dispersed offices. Moreover, the competitiveness in the global market requires them to invest in their human resources to be able to remain a step ahead of competition. Implementing large scale classical education in such environments is challenging and costly. Mobile e-learning (m-learning) allows users to tailor their professional training and education to their needs and time constraints. However, in self-paced education it is very hard to keep user retention and engagement. To achieve the latter we have designed and developed an m-learning platform for corporate environments based on the triggering persuasive technology principle that try to incite users in regularly using the platform. We have evaluated the application in-the-wild in corporate environments of differently sized companies with 300 users. Users were subjected to three different conditions: no triggering, simple regular triggering and adaptive triggering. The results show that the use of adaptive triggering in m-learning increases user engagement as well as course completion rates more than simple regular triggering and no triggering.", "Language": "en", "Citations": "0"},
{"Title": "Recommending VideoLectures with linear regression", "Authors": ["Mozina M.", "Sadikov A.", "Bratko I."], "Keywords": [], "Date": "2011", "Abstract": "This paper describes our approach to the task 1 of the ECML PKDD 2011 VideoLectures.Net Recommender System Challenge. The task was to select a set of lectures to be recommended to a visitor of the VideoLecture.Net homepage after already seeing another lecture. Our proposed approach is a hybrid recommender system combining content and collaborative approaches. The core of the system is a linear regression model for predicting the rank of a lecture, whereas by rank we mean the lecture's position in the list of all lectures ordered by the interest of the visitor. Due to the complexity of the problem, the model could not be learned by a classical approach - instead, we had to employ the stochastic gradient descent optimization. The present paper furthermore, through evaluation, identifies and describes some interesting properties of the domain and of the algorithm that were crucial to achieve a higher prediction accuracy. The final accuracy of the model was enough to take the third place in the competition.", "Language": "en", "Citations": "2"},
{"Title": "Computer aided melodic analysis using suffix tree", "Authors": ["Jekovec M.", "Demsar J.", "Brodnik A."], "Keywords": [], "Date": "2012", "Abstract": "Quality melodic analysis of the score is one of the most significant, but also awkward and time consuming musi-cological tasks. In this paper we propose a computer aided melodic analysis based on suffix tree. Our data structure represents hierarchically combined melodic patterns for a given score. The potential interestingness of melodic patterns to the musicologist is then estimated from their diversity, length and frequency. We tested the proposed method on 48 fugues from J. S. Bach's Well-Tempered Clavier opus. All our approaches were integrated into the free musicological application Harmonia, which allows musicologists to explore the most common theme, its sub-melodies, common motifs and other melody-based score features.", "Language": "en", "Citations": "0"},
{"Title": "A two-stage dynamic model for visual tracking", "Authors": ["Kristan M.", "Kovacic S.", "Leonardis A.", "Pers J."], "Keywords": ["Blob tracking", "dynamic models", "particle filters", "probabilistic tracking", "two-stage models"], "Date": "2010", "Abstract": "We propose a new dynamic model which can be used within blob trackers to track the target's center of gravity. A strong point of the model is that it is designed to track a variety of motions which are usually encountered in applications such as pedestrian tracking, hand tracking, and sports. We call the dynamic model a two-stage dynamic model due to its particular structure, which is a composition of two models: a liberal model and a conservative model. The liberal model allows larger perturbations in the target's dynamics and is able to account for motions in between the random-walk dynamics and the nearly constant-velocity dynamics. On the other hand, the conservative model assumes smaller perturbations and is used to further constrain the liberal model to the target's current dynamics. We implement the two-stage dynamic model in a two-stage probabilistic tracker based on the particle filter and apply it to two separate examples of blob tracking: 1) tracking entire persons and 2) tracking of a person's hands. Experiments show that, in comparison to the widely used models, the proposed two-stage dynamic model allows tracking with smaller number of particles in the particle filter (e.g., 25 particles), while achieving smaller errors in the state estimation and a smaller failure rate. The results suggest that the improved performance comes from the model's ability to actively adapt to the target's motion during tracking. \u00a9 2010 IEEE.", "Language": "en", "Citations": "49"},
{"Title": "Open-Source Tools for Data Mining", "Authors": ["Zupan B.", "Demsar J."], "Keywords": [], "Date": "2008", "Abstract": "With a growing volume of biomedical databases and repositories, the need to develop a set of tools to address their analysis and support knowledge discovery is becoming acute. The data mining community has developed a substantial set of techniques for computational treatment of these data. In this article, we discuss the evolution of open-source toolboxes that data mining researchers and enthusiasts have developed over the span of a few decades and review several currently available open-source data mining suites. The approaches we review are diverse in data mining methods and user interfaces and also demonstrate that the field and its tools are ready to be fully exploited in biomedical research. \u00a9 2008 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "16"},
{"Title": "Understanding the importance of proper incentives for critical infrastructures management \u2013 How system dynamics can help", "Authors": ["Trcek D.", "Gonzalez J.J."], "Keywords": ["Critical infrastructures", "Management", "Modeling and simulation", "Policies"], "Date": "2017", "Abstract": "Computer and information systems are now at the core of numerous critical infrastructures. However, their security management is by far not a trivial issue. Further, these systems, by their very nature, belong to the domain of complex systems, where system dynamics (SD) is an established method, which aims at modelling such systems, their analysis and understanding. Further, on this basis it enables simulation of various policies to properly manage complex systems. More precisely, through understanding of the basic elements of the whole mosaic and their interplay, proper incentives can be tested. And this is important, because proper incentives can lead to the desired patterns of behavior of such systems, which may often be counter-intuitive. Therefore this paper presents a novel approach by using SD for managing critical infrastructures (more precisely the internet) when it comes to security related incentives. Based on already developed archetypes it provides a template model that bridges these conceptual models with concrete models that are suited to particular environments, and enable quantitative simulations.", "Language": "en", "Citations": "0"},
{"Title": "Dynamic fuzzy paths and cycles in multi-level directed graphs", "Authors": ["Petelin B.", "Kononenko I.", "Malacic V.", "Kukar M."], "Keywords": ["Dynamic fuzzy cycles", "Dynamic fuzzy paths", "Multi-level directed graphs", "Oceanography"], "Date": "2014", "Abstract": "In this paper we propose improved algorithms for the discovery of significant paths and cycles that dynamically evolve through time in a series of multi-level directed graphs. First, we search for the most probable paths and combine them into clusters based on similar edges. We combine paths into dynamic fuzzy paths. We also detect cycles in different paths and combine them into dynamic fuzzy cycles. We obtain dynamic fuzzy structures using the hierarchical clustering of individual structures. For paths, the clustering distance depends on common edges, while for cycles we calculate the distance on the basis of common vertices. We apply the developed algorithms to a time series of multi-level directed graphs obtained from the results from the numerical model Mediterranean Ocean Forecasting System during the period 1999-2011. We compare the results with known structures from the oceanographic literature. With our approach we find a high similarity between the resulting dynamic fuzzy paths and cycles and structures found by oceanographic experts. When comparing the cycles, the expert sees our results as a convex hull of the average of individual cycles. On the other hand, the method reveals undiscovered paths and gyres, which can be verified through observation.", "Language": "en", "Citations": "3"},
{"Title": "Producing the left parse during bottom-up parsing", "Authors": ["Slivnik B.", "Vilfan B."], "Keywords": ["Compilers", "Formal languages", "Parsing"], "Date": "2005", "Abstract": "Schmeiser and Barnard described a method for producing the left parse at the end of the bottom-up parsing process. We improve their method in the sense that the left parse is actually produced during the bottom-up parsing process (i.e., with considerably less delay). \u00a9 2005 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "11"},
{"Title": "Towards Accessories-Aware Ear Recognition", "Authors": ["Emersic Z.", "Playa N.O.", "Struc V.", "Peer P."], "Keywords": ["Accessories removal", "Biometrics", "Ear accessories", "Ear biometrics", "Ear recognition"], "Date": "2018", "Abstract": "Automatic ear recognition is gaining popularity within the research community due to numerous desirable properties, such as high recognition performance, the possibility of capturing ear images at a distance and in a covert manner, etc. Despite this popularity and the corresponding research effort that is being directed towards ear recognition technology, open problems still remain. One of the most important issues stopping ear recognition systems from being widely available are ear occlusions and accessories. Ear accessories not only mask biometric features and by this reduce the overall recognition performance, but also introduce new non-biometric features that can be exploited for spoofing purposes. Ignoring ear accessories during recognition can, therefore, present a security threat to ear recognition and also adversely affect performance. Despite the importance of this topic there has been, to the best of our knowledge, no ear recognition studies that would address these problems. In this work we try to close this gap and study the impact of ear accessories on the recognition performance of several state-of-the-art ear recognition techniques. We consider ear accessories as a tool for spoofing attacks and show that CNN-based recognition approaches are more susceptible to spoofing attacks than traditional descriptor-based approaches. Furthermore, we demonstrate that using inpainting techniques or average coloring can mitigate the problems caused by ear accessories and slightly outperforms (standard) black color to mask ear accessories.", "Language": "en", "Citations": "0"},
{"Title": "Classification of ischaemic episodes with ST/HR diagrams", "Authors": ["Faganeli Pucer J.", "Demsar J.", "Kukar M."], "Keywords": ["Coronary artery disease", "ECG analysis", "Ischaemic episode", "ST/HR diagram"], "Date": "2012", "Abstract": "Coronary artery disease is the developed world's premier cause of mortality and the most probable cause of myocardial ischaemia. More advanced diagnostic tests aside, in electrocardiogram (ECG) analysis it manifests itself as a ST segment deviation, targeted by both exercise ECG and ambulatory ECG. In ambulatory ECG, besides ischaemic ST segment deviation episodes there are also non-ischaemic heart rate related episodes which aggravate real ischaemia detection. We present methods to transform the features developed for the heart rate adjustment of ST segment depression in exercise ECG for use in ambulatory ECG. We use annotations provided by the Long-Term ST Database to plot the ST/HR diagrams and then estimate the overall and maximal slopes of the diagrams in the exercise and recovery phase for each ST segment deviation episode. We also estimate the angle at the extrema of the ST/HR diagrams. Statistical analysis shows that ischaemic ST segment deviation episodes have significantly steeper overall and maximal slopes than heart rate related episodes, which indicates the explored features' utility for distinguishing between the two types of episodes. This makes the proposed features very useful in automated ECG analysis. \u00a9 2012 European Federation for Medical Informatics and IOS Press. All rights reserved.", "Language": "en", "Citations": "1"},
{"Title": "Decentralized computation of homology in wireless sensor networks using spanning trees", "Authors": ["Soberl D.", "Kosta N.M.", "Skraba P."], "Keywords": ["Computational homology", "Coverage problem", "Rips complex", "Simplicial homology", "Wireless sensor networks"], "Date": "2017", "Abstract": "When deploying a wireless sensor network over an area of interest, the information on signal coverage is critical. It has been shown that even when geometric position and orientation of individual nodes is not known, useful information on coverage can still be deduced based on connectivity data. In recent years, homological criteria have been introduced to verify complete signal coverage, given only the network communication graph. However, their algorithmic implementation has been limited due to high computational complexity of centralized algorithms, and high demand for communication in decentralized solutions, where a network employs the processing power of its nodes to check the coverage autonomously. To mitigate these problems, known approaches impose certain limitations on network topologies. In this paper, we propose a novel distributed algorithm which uses spanning trees to verify homology-based network coverage criteria, and works for arbitrary network topologies. We demonstrate that its communication demands are suitable even for low-bandwidth wireless sensor networks.", "Language": "en", "Citations": "0"},
{"Title": "Traffic characterization and Internet usage in rural Africa", "Authors": ["Johnson D.L.", "Pejovic V.", "Belding E.M.", "Van Stam G."], "Keywords": ["internet usage", "interviews", "rural networks"], "Date": "2011", "Abstract": "While Internet connectivity has reached a significant part of the world's population, those living in rural areas of the developing world are still largely disconnected. Recent efforts have provided Internet connectivity to a growing number of remote locations, yet Internet traffic demands cause many of these networks to fail to deliver basic quality of service needed for simple applications. For an in-depth investigation of the problem, we gather and analyze network traces from a rural wireless network in Macha, Zambia. We supplement our analysis with on-site interviews from Macha, Zambia and Dwesa, South Africa, another rural community that hosts a local wireless network. The results reveal that Internet traffic in rural Africa differs significantly from the developed world. We observe dominance of web-based traffic, as opposed to peer-to-peer traffic common in urban areas. Application-wise, online social networks are the most popular, while the majority of bandwidth is consumed by large operating system updates. Our analysis also uncovers numerous network anomalies, such as significant malware traffic. Finally, we find a strong feedback loop between network performance and user behavior. Based on our findings, we conclude with a discussion of new directions in network design that take into account both technical and social factors. \u00a9 2011 ACM.", "Language": "en", "Citations": "31"},
{"Title": "Rapid Ontology Development model based on business rules management approach for the use in business applications", "Authors": ["Lavbic D.", "Krisper M."], "Keywords": [], "Date": "2008", "Abstract": "Ontologies as means for knowledge manipulation in IT have gained its popularity in recent years. The scenarios of successful implementation can mainly be found in the World Wide Web domain and within academia while there are only a few in business environment. This paper introduces Rapid Ontologfy Development model and accompanying intelliOnto support tool to facilitate ontology construction for inclusion in business applications. Emphasis is given to simplification of the development process of functional components by bridging the gap between formal syntax of captured knowledge and acquisition of knowledge in semi formal way. Verification of the model will be presented with running examples from the domain of financial portfolio management and organisation of a rent-a-car etc.", "Language": "en", "Citations": "0"},
{"Title": "NIMFA: A python library for nonnegative matrix factorization", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": ["Initialization methods", "Nonnegative matrix factorization", "Python", "Quality measures", "Scripting"], "Date": "2012", "Abstract": "NIMFA is an open-source Python library that provides a unified interface to nonnegative matrix factorization algorithms. It includes implementations of state-of-the-art factorization methods, initialization approaches, and quality scoring. It supports both dense and sparse matrix representation. NIMFA's component-based implementation and hierarchical design should help the users to employ already implemented techniques or design and code new strategies for matrix factorization tasks. \u00a9 2012 Marinka \u017ditnik and Bla\u017e Zupan.", "Language": "en", "Citations": "18"},
{"Title": "High levels of uPA and PAI-1 predict a good response to anthracyclines", "Authors": ["Borstnar S.", "Sadikov A.", "Mozina B.", "Cufer T."], "Keywords": ["Anthracycline-based chemotherapy", "Balancing patients' and tumor characteristics", "Breast cancer", "PAI-1", "Predictive value", "uPA"], "Date": "2010", "Abstract": "Urokinase-type plasminogen activator (uPA) and its main inhibitor (PAI-1) were shown with level 1 evidence to be prognostic factors for primary breast cancer. Our preliminary retrospective study on a cohort of 1,220 consecutive patients hinted that uPA and PAI-1 could also serve as predictive factors for systemic therapy, namely that patients with high levels of the two markers benefit much more from anthracycline-based chemotherapy than patients with low levels of the two markers. The latter could equally well be treated with less toxic CMF-based chemotherapy (cyclophosphamide, methotrexate, and fluorouracil). The retrospective study, however, suffered from severely uneven patient and tumor characteristics as the patients were treated per institutional guidelines valid at the time and were not randomized between the anthracycline and CMF arms. In the present paper, we attempted to remedy this shortcoming and recheck our previous observations on more balanced data. To this end we employed a custom-made computer algorithm that selected 180 patients out of a total of 1,220 patients such that we obtained very well balanced anthracycline and CMF arms according to patient and tumor characteristics. Moreover, the low and high uPA/PAI-1 subgroups within both arms were also completely balanced. The algorithm in a way created a similar setting to that of a randomized study at the expense of greatly reducing the number of patients included into the study. In this setting, we observed the 3-year disease-free survival (DFS) in all four subgroups (according to treatment and levels of markers: both uPA and PAI-1 low versus one or both high). We report that the 3-year DFS in the CMF arm differed significantly: 87.1% for patients with low levels of markers versus 77.0% for patients with high levels of markers (P = 0.044, HR = 2.81, 95% CI = 0.98-8.04). On the other hand, the 3-year DFS in the anthracycline arm did not differ much between the two marker level subgroups: 85.2% for patients with low levels of markers versus 81.8% for patients with high levels of markers. Our observation points out that worse prognosis correlated to high uPA and PAI-1 levels can be reversed by treatment efficacy achieved through anthracycline-based chemotherapy. Based on this observation, we hypothesize that uPA/PAI-1 combination could be predictive for response to systemic therapy. \u00a9 Springer Science+Business Media, LLC. 2009.", "Language": "en", "Citations": "21"},
{"Title": "Trust management for pervasive computing environments", "Authors": ["Trcek D."], "Keywords": ["Internet security", "Modeling and simulation", "Multi-agent systems", "Qualitative algebra", "Reasoning and judgment", "Trust management"], "Date": "2010", "Abstract": "Trust is essential for further and wider acceptance of contemporary e-services. It was first addressed almost thirty years ago in Trusted Computer System Evaluation Criteria standard by the US DoD. But this and other proposed approaches of that period were actually solving security. Roughly some ten years ago, methodologies followed that addressed trust phenomenon at its core, and they were based on Bayesian statistics and its derivatives, while some approaches were based on game theory. However, trust is a manifestation of judgment and reasoning processes. It has to be dealt with in accordance with this fact and adequately supported in cyber environment. On the basis of the results in the field of psychology and our own findings, a methodology called qualitative algebra has been developed, which deals with so far overlooked elements of trust phenomenon. It complements existing methodologies and provides a basis for a practical technical solution that supports management of trust in contemporary computing environments. Such solution is also presented at the end of this paper.", "Language": "en", "Citations": "1"},
{"Title": "Similarity-based cross-layered hierarchical representation for object categorization", "Authors": ["Fidler S.", "Boben M.", "Leonardis A."], "Keywords": [], "Date": "2008", "Abstract": "This paper proposes a new concept in hierarchical representations that exploits features of different granularity and specificity coming from all layers of the hierarchy. The concept is realized within a cross-layered compositional representation learned from the visual data. We show how similarity connections among discrete labels within and across hierarchical layers can be established in order to produce a set of layer-independent shape-terminals, i.e. shapinals. We thus break the traditional notion of hierarchies and show how the category-specific layers can make use of all the necessary features stemming from all hierarchical layers. This, on the one hand, brings higher generalization into the representation, yet on the other hand, it also encodes the notion of scales directly into the hierarchy, thus enabling a multi-scale representation of object categories. By focusing on shape information only, the approach is tested on the Caltech 101 dataset demonstrating good performance in comparison with other state-of-the-art methods. \u00a92008 IEEE.", "Language": "en", "Citations": "45"},
{"Title": "Analyzing educational process through a chain of data marts", "Authors": ["Mahnic V."], "Keywords": ["Data mart", "Data warehouse", "Star schema data model"], "Date": "2003", "Abstract": "We describe the development strategy, architecture, and logical design of a data warehouse that can be built gradually, exploiting the benefits of the bottom-up, data mart approach. Connections between individual data marts are planned in advance with the aim of building a sequence of data marts that makes it possible to analyze the educational process as a value chain. Queries can be made across different subject areas (viz. enrollment applications, enrollment, examination, and degree records) in order to obtain a snapshot or a slice of the entire value chain that shows how far a subset of students has moved from the enrollment application to their final degree.", "Language": "en", "Citations": "0"},
{"Title": "A computer vision based system for a rehabilitation of a human hand", "Authors": ["Peer P.", "Jaklic A.", "Sajn L."], "Keywords": ["3D reconstruction", "Arm rehabilitation", "Computer vision", "Cuboid model", "Injury", "Real time execution", "Stroke", "Web camera"], "Date": "2013", "Abstract": "Paper presents a rehabilitation systemfor patientswho suffer fromarmor wrist injury or similar. The idea of the rehabilitation using computer and additional hardware is not new, but our solution differs significantly. We tried to make it easily accessible and thus started with a limitation that only a personal computer and one standardweb camera is required. Patient holds a simple object, cuboid, and moves it around. Camera records hismovement while the software in real-time calculates position of the object in 3D space on the basis of color information and cuboid model. Object is then placed in the virtual 3D space, where another similar object is already present. The patient's task is to move the real object in the position, which matches the position of the virtual object.Doing so the patient trains specific movements that speed up the recovery. Evaluation of the system shows that presented solution is suitable in cases where accuracy is not very critical and smaller 3D reconstruction deviations do not thwart the process of rehabilitation.", "Language": "en", "Citations": "3"},
{"Title": "Distance-regular graphs with light tails", "Authors": ["JuriSic A.", "Terwilliger P.", "Zitnik A."], "Keywords": [], "Date": "2010", "Abstract": "Let \u0393 be a distance-regular graph with valency k\u22653 and diameter d\u22652. It is well known that the Schur product Eo{cyrillic}F of any two minimal idempotents of \u0393 is a linear combination of minimal idempotents of \u0393. Situations where there is a small number of minimal idempotents in the above linear combination can be very interesting, since they usually imply strong structural properties, see for example Q-polynomial graphs, tight graphs in the sense of Juri\u0160i\u0107, Koolen and Terwilliger, and 1- or 2-homogeneous graphs in the sense of Nomura. In the case when E=F, the rank one minimal idempotent E0 is always present in this linear combination and can be the only one only if E=E0 or E=Ed and \u0393 is bipartite. We study the case when E o E \u2208 span{E", "Language": "en", "Citations": "4"},
{"Title": "Subthalamic deep brain stimulation sweet spots and hyperdirect cortical connectivity in Parkinson's disease", "Authors": ["Akram H.", "Georgiev D.", "Mahlknecht P.", "Hyam J.", "Foltynie T.", "Limousin P.", "Jahanshahi M.", "Hariz M.", "Zrinzo L.", "Ashburner J.", "Behrens T.", "Sotiropoulos S.N.", "Jbabdi S.", "De Vita E."], "Keywords": ["Connectivity", "Diffusion weighted imaging (DWI)", "Hyperdirect pathway", "Parkinson's disease (PD)", "Subthalamic nucleus (STN)", "Volume of tissue activated (VTA)"], "Date": "2017", "Abstract": "                             Objectives Firstly, to identify subthalamic region stimulation clusters that predict maximum improvement in rigidity, bradykinesia and tremor, or emergence of side-effects; and secondly, to map-out the cortical fingerprint, mediated by the hyperdirect pathways which predict maximum efficacy. Methods High angular resolution diffusion imaging in twenty patients with advanced Parkinson's disease was acquired prior to bilateral subthalamic nucleus deep brain stimulation. All contacts were screened one-year from surgery for efficacy and side-effects at different amplitudes. Voxel-based statistical analysis of volumes of tissue activated models was used to identify significant treatment clusters. Probabilistic tractography was employed to identify cortical connectivity patterns associated with treatment efficacy. Results All patients responded well to treatment (46% mean improvement off medication UPDRS-III [p < 0.0001]) without significant adverse events. Cluster corresponding to maximum improvement in tremor was in the posterior, superior and lateral portion of the nucleus. Clusters corresponding to improvement in bradykinesia and rigidity were nearer the superior border in a further medial and posterior location. The rigidity cluster extended beyond the superior border to the area of the zona incerta and Forel-H                             ", "Language": "en", "Citations": "22"},
{"Title": "Formal apparatus for measurement of lightweight protocols", "Authors": ["Trcek D.", "Kovac D."], "Keywords": ["Lightweight protocols", "Security", "Security services metrics", "Standardization", "Ubiquitous computing"], "Date": "2009", "Abstract": "Lightweight protocols are an important topic in the area of computer communications. With the proliferation of security services not only ordinary communication protocols, but also cryptographic protocols, i.e. security services, have become a subject of research into possible appropriate lightweight solutions. At first glance it may seem surprising, but the evidence suggests that there is a permanent need for lightweight protocols. And this need is ever increasing, due to the gap between desktop (and other ordinary computing devices) and mobile wireless devices that have inherently limited resources. However, the notion of lightweight protocol has not been formally addressed in the literature, which is the purpose of this paper. A formal model that can be used to evaluate lightweight properties of protocols is presented and the appropriate metrics are introduced. Despite the fact that the model and the metrics target weak processing devices, they can be deployed for ordinary computing environments and may present a methodology for evaluation of lightweight cryptographic protocols in standardization processes. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "5"},
{"Title": "Team ULjubljana\u2019s solution to the JRS 2012 data mining competition", "Authors": ["Zbontar J.", "Zitnik M.", "Zidar M.", "Majcen G.", "Potocnik M.", "Zupan B."], "Keywords": ["Multi-label classification", "Sparse datasets", "Stacking", "Topical classification"], "Date": "2012", "Abstract": "The task of the JRS 2012 data mining competition was to infer a prediction model capable of associating biomedical journal articles with a subset of topics. Our approach consisted of training a set of base learners, stacking their results, and thresholding the predictions on each label separately. Our method obtained an F-score of 0.53579, which was enough to claim first prize in the competition.", "Language": "en", "Citations": "3"},
{"Title": "Illumination independent color-based face detection", "Authors": ["Kovac J.", "Peer P.", "Solina F."], "Keywords": ["Art", "Computer vision", "Face detection", "Humans", "Information science", "Life testing", "Lighting", "Machine intelligence", "Peer to peer computing", "Performance evaluation"], "Date": "2003", "Abstract": "Computer vision is one out of many areas that want to understand the process of human functionality and copy that process with intention to complement human life with intelligent machines. For better human-computer interaction it is necessary for the machine to see people. This can be achieved by employing face detection algorithms, like the one used in the installation \"15 Seconds of Fame\" (F. Solina et al., 2002). Mentioned installation unites the areas of modern art and technology. Its algorithm is based on skin-color detection. One of the problems this and similar algorithms have to deal with is sensitivity to the illumination conditions under which the input image is captured. Hence illumination sensitivity influences face detection results. This problem is being more or less successfully solved by the use of color compensation and color constancy methods. In this work some of these methods are described, realized and tested. Their basic intention is to eliminate the influence of non-standard illumination from images. Tests that were performed showed that methods apply positive influence on face detection results.", "Language": "en", "Citations": "13"},
{"Title": "A Note on a Matrix Version of the Farkas Lemma", "Authors": ["Zalar A."], "Keywords": ["Farkas Lemma", "Matrix polynomials", "Positivstellensatz"], "Date": "2012", "Abstract": "A linear polyomial non-negative on the non-negativity domain of finitely many linear polynomials can be expressed as their non-negative linear combination. Recently, under several additional assumptions, Helton, Klep, and McCullough extended this result to matrix polynomials. The aim of this article is to study which of these additional assumptions are really necessary. \u00a9 2012 Copyright Taylor and Francis Group, LLC.", "Language": "en", "Citations": "3"},
{"Title": "Contractible Subgraphs, Thomassen's Conjecture and the Dominating Cycle Conjecture for Snarks", "Authors": ["Broersma H.", "Fijavz G.", "Kaiser T.", "Kuzel R.", "Ryjacek Z.", "Vrana P."], "Keywords": ["contractible graph", "cubic graph", "dominating cycle", "hamiltonian graph", "line graph", "snark"], "Date": "2007", "Abstract": "We show that the conjectures by Matthews and Sumner (every 4-connected claw-free graph is hamiltonian), by Thomassen (every 4-connected line graph is hamiltonian) and by Fleischner (every cyclically 4-edge-connected cubic graph has either a 3-edge-coloring or a dominating cycle), which are known to be equivalent, are equivalent with the statement that every snark (i.e. a cyclically 4-edge-connected cubic graph of girth at least five that is not 3-edge-colorable) has a dominating cycle. We use a refinement of the contractibility technique which was introduced by Ryj\u00e1\u010dek and Schelp in 2003 as a common generalization and strengthening of the reduction techniques by Catlin and Veldman and of the closure concept introduced by Ryj\u00e1\u010dek in 1997. \u00a9 2007 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "0"},
{"Title": "Long-term ST database: a research resource for algorithm development and physiologic studies of transient myocardial ischemia", "Authors": ["Jager F.", "Taddei A.", "Emdin M.", "Antolic G.", "Dorn R.", "Moody G.B.", "Glavic B.", "Smrdel A.", "Varanini M.", "Zabukovec M.", "Bordigiago S.", "Marchesi C.", "Mark R.G."], "Keywords": [], "Date": "2000", "Abstract": "We present the Long Term ST Database, a collection of eighty 24-hour two and three lead ECG records from ambulatory subjects with transient ST segment abnormalities. The database provides a comprehensive standard research resource for quantitatively assessing the performance of automated detectors of transient ischemia, and for supporting basic research into the mechanisms and dynamics of transient ischemia. Records of the database contain annotated significant transient ischemic ST episodes, non-ischemic ST episodes caused by heart-rate related changes, non-ischemic ST events due to axis shifts or QRS conduction changes, and individual QRS and rhythm annotations, all made by human experts.", "Language": "en", "Citations": "13"},
{"Title": "Online bookmakers' odds as forecasts: The case of European soccer leagues", "Authors": ["Strumbelj E.", "Sikonja M.R."], "Keywords": ["Betting", "Brier score", "Soccer", "Sports forecasting", "Statistical tests"], "Date": "2010", "Abstract": "In this paper we examine the effectiveness of using bookmaker odds as forecasts by analyzing 10,699 matches from six major European soccer leagues and the corresponding odds from 10 different online bookmakers. We show that the odds from some bookmakers are better forecasts than those of others, and provide empirical evidence that (a) the effectiveness of using bookmaker odds as forecasts has increased over time, and (b) bookmakers offer more effective forecasts for some soccer leagues for than others. \u00a9 2009 International Institute of Forecasters.", "Language": "en", "Citations": "10"},
{"Title": "Average Step Length Estimation Models' Evaluation Using Inertial Sensors: A Review", "Authors": ["Vezocnik M.", "Juric M.B."], "Keywords": ["Inertial sensors", "review", "step length estimation", "step length estimation models"], "Date": "2019", "Abstract": "Inertial sensors of smartphones and other Internet-of-Things devices present a very promising tool to monitor users' activity including their step length. In this review paper, we deal with an in-depth analysis and comparison of 13 representative step length estimation models using smartphone inertial sensors: step-frequency-based, acceleration-based, angle-based, and multiparameter. Hereby, we have studied the influence of different walking speeds and four typical sensor positions on the models' performance. Results indicate that smartphone position affected the performance of most acceleration-based models derived from a gait model. Their performance deteriorated if smartphone was carried in hand or pocket. Walking speed affected the performance of models that include step frequency when tuned with personalized sets of constants. Most of them performed better for fast and normal walking speeds. During this research, we also established an open-source dataset that contains over 22 km of gait measurements obtained from a group of 15 healthy adults.", "Language": "en", "Citations": "0"},
{"Title": "Construction and application of hierarchical socioeconomic decision models", "Authors": ["Krisper M.", "Zupan B."], "Keywords": ["Comparative analysis", "Data visualization", "Decision support", "Hierarchical decision models", "Socioeconomic development", "Socioeconomic models", "What-if analysis"], "Date": "2002", "Abstract": "The article presents a utility of multi-attributed hierarchical modelling approach to represent, analyze and study socioeconomic processes. The models are based on criteria tree for which the expert specifies the utility functions. The specific advantages of the approach are structuring the problem domain, a relative ease to build the models and the existence of underlying tools for comparative and what-if type of data analysis. We use these tools to construct two socioeconomic models, one for assessment of country's knowledge infrastructure and the other one for assessment of quality of political and economic system. We demonstrate the utility of these two models through experimental application in the analysis of real-world data from Word Competitiveness Yearbook.", "Language": "en", "Citations": "0"},
{"Title": "Computational design of synchronous sequential structures in biological systems", "Authors": ["Magdevska L.", "Pusnik Z.", "Mraz M.", "Zimic N.", "Moskon M."], "Keywords": ["Computational design", "D flip-flop", "Johnson counter", "Modelling and simulation", "Transcriptional logic"], "Date": "2017", "Abstract": "Numerous applications of synthetic biology require the implementation of scalable and robust biological circuits with information processing capabilities. Basic logic structures, such as logic gates, have already been implemented in prokaryotic as well as in eukaryotic cells. Biological memory structures have also been implemented either in vitro or in vivo. However, these implementations are still in their infancy compared to their electronic equivalents. Their response is mainly asynchronous. We may learn from electronic computer systems that robust and scalable computing devices can be implemented only with edge-triggered synchronous sequential structures. Implementation of such structures, however, has yet to be performed in the synthetic biological systems even on the conceptual level. Herein we describe the computational design and analysis of edge-triggered D flip-flop in master\u2013slave configuration based on transcriptional logic. We assess the robustness of the proposed structure with its global sensitivity as well as parameter sweep analysis. Furthermore, we describe the design of a robust Johnson counter, which can count up to 2n cellular events using a sequence of n flip-flops. Changing the state of the counter is edge-triggered either with synchronization, i.e. clock signal, or with a pulse, which corresponds to the occurrence of observed event within the cellular environment. To the best of our knowledge this represents the design of the first biological synchronous sequential structure on such level of complexity.", "Language": "en", "Citations": "0"},
{"Title": "Analyzing attribute dependencies", "Authors": ["Jakulin A.", "Bratko I."], "Keywords": [], "Date": "2003", "Abstract": "Many effective and efficient learning algorithms assume independence of attributes. They often perform well even in domains where this assumption is not really true. However, they may fail badly when the degree of attribute dependencies becomes critical. In this paper, we examine methods for detecting deviations from independence. These dependencies give rise to \"interactions\" between attributes which affect the performance of learning algorithms. We first formally define the degree of interaction between attributes through the deviation of the best possible \"voting\" classifier from the true relation between the class and the attributes in a domain. Then we propose a practical heuristic for detecting attribute interactions, called interaction gain. We experimentally investigate the suitability of interaction gain for handling attribute interactions in machine learning. We also propose visualization methods for graphical exploration of interactions in a domain.", "Language": "en", "Citations": "110"},
{"Title": "ILP turns 20: Biography and future challenges", "Authors": ["Muggleton S.", "De Raedt L.", "Poole D.", "Bratko I.", "Flach P.", "Inoue K.", "Srinivasan A."], "Keywords": ["(Statistical) relational learning", "Inductive Logic Programming", "Structured data in Machine Learning"], "Date": "2012", "Abstract": "Inductive Logic Programming (ILP) is an area of Machine Learning which has now reached its twentieth year. Using the analogy of a human biography this paper recalls the development of the subject from its infancy through childhood and teenage years. We show how in each phase ILP has been characterised by an attempt to extend theory and implementations in tandem with the development of novel and challenging real-world applications. Lastly, by projection we suggest directions for research which will help the subject coming of age. \u00a9 2011 The Author(s).", "Language": "en", "Citations": "62"},
{"Title": "Computational modelling of genome-scale metabolic networks and its application to CHO cell cultures", "Authors": ["Rejc Z.", "Magdevska L.", "Trselic T.", "Osolin T.", "Vodopivec R.", "Mraz J.", "Pavliha E.", "Zimic N.", "Cvitanovic T.", "Rozman D.", "Moskon M.", "Mraz M."], "Keywords": ["Chinese hamster ovary cells", "Flux balance analysis", "Genome-scale metabolic models", "Metabolic networks", "Modelling and analysis"], "Date": "2017", "Abstract": "Genome-scale metabolic models (GEMs) have become increasingly important in recent years. Currently, GEMs are the most accurate in silico representation of the genotype-phenotype link. They allow us to study complex networks from the systems perspective. Their application may drastically reduce the amount of experimental and clinical work, improve diagnostic tools and increase our understanding of complex biological phenomena. GEMs have also demonstrated high potential for the optimisation of bio-based production of recombinant proteins. Herein, we review the basic concepts, methods, resources and software tools used for the reconstruction and application of GEMs. We overview the evolution of the modelling efforts devoted to the metabolism of Chinese Hamster Ovary (CHO) cells. We present a case study on CHO cell metabolism under different amino acid depletions. This leads us to the identification of the most influential as well as essential amino acids in selected CHO cell lines.", "Language": "en", "Citations": "7"},
{"Title": "Weighted hierarchical archetypal analysis for multi-document summarization", "Authors": ["Canhasi E.", "Kononenko I."], "Keywords": ["Comparative summarization", "General", "Multi-document summarization framework", "Query-focused", "Update", "Weighted hierarchical archetypal analysis"], "Date": "2016", "Abstract": "Multi-document summarization (MDS) is becoming a crucial task in natural language processing. MDS targets to condense the most important information from a set of documents to produce a brief summary. Most existing extractive multi-document summarization methods employ different sentence selection approaches to obtain the summary as a subset of sentences from the given document set. The ability of the weighted hierarchical archetypal analysis to select \"the best of the best\" summary sentences motivates us to use this method in our solution to multi-document summarization tasks. In this paper, we propose a new framework for various multi-document summarization tasks based on weighted hierarchical archetypal analysis. The paper demonstrates how four variant summarization tasks, including general, query-focused, update, and comparative summarization, can be modeled as different versions acquired from the proposed framework. Experiments on summarization data sets (DUC04-07, TAC08) are conducted to demonstrate the efficiency and effectiveness of our framework for all four kinds of the multi-document summarization tasks.", "Language": "en", "Citations": "9"},
{"Title": "Deep Multi-class Eye Segmentation for Ocular Biometrics", "Authors": ["Rot P.", "Emersic Z.", "Struc V.", "Peer P."], "Keywords": ["Convolutional neural networks (CNN)", "Deep learning", "Iris", "Ocular biometrics", "Pupil", "Sclera", "Segmentation"], "Date": "2018", "Abstract": "Segmentation techniques for ocular biometrics typically focus on finding a single eye region in the input image at the time. Only limited work has been done on multi-class eye segmentation despite a number of obvious advantages. In this paper we address this gap and present a deep multi-class eye segmentation model build around the SegNet architecture. We train the model on a small dataset (of 120 samples) of eye images and observe it to generalize well to unseen images and to ensure highly accurate segmentation results. We evaluate the model on the Multi-Angle Sclera Database (MASD) dataset and describe comprehensive experiments focusing on: i) segmentation performance, ii) error analysis, iii) the sensitivity of the model to changes in view direction, and iv) comparisons with competing single-class techniques. Our results show that the proposed model is viable solution for multi-class eye segmentation suitable for recognition (multi-biometric) pipelines based on ocular characteristics.", "Language": "en", "Citations": "2"},
{"Title": "Speech features extraction using cone-shaped kernel distribution", "Authors": ["Zibert J.", "Mihelic F.", "Pavesic N."], "Keywords": [], "Date": "2002", "Abstract": "The paper reviews two basic time-frequency distributions, spectrogram and cone-shaped kernel distribution. We study, analyze and compare properties and performance of these quadratic representations on speech signals. Cone-shaped kernel distribution was successfully applied to speech features extraction due to several useful properties in time-frequency analysis of speech signals.", "Language": "en", "Citations": "2"},
{"Title": "From hype to reality: Data science enabling personalized medicine", "Authors": ["Frohlich H.", "Balling R.", "Beerenwinkel N.", "Kohlbacher O.", "Kumar S.", "Lengauer T.", "Maathuis M.H.", "Moreau Y.", "Murphy S.A.", "Przytycka T.M.", "Rebhan M.", "Rost H.", "Schuppert A.", "Schwab M.", "Spang R.", "Stekhoven D.", "Sun J.", "Weber A.", "Ziemek D.", "Zupan B."], "Keywords": ["Artificial intelligence", "Big data", "Biomarkers", "Machine learning", "P4 medicine", "Personalized medicine", "Precision medicine", "Stratified medicine"], "Date": "2018", "Abstract": "Background: Personalized, precision, P4, or stratified medicine is understood as a medical approach in which patients are stratified based on their disease subtype, risk, prognosis, or treatment response using specialized diagnostic tests. The key idea is to base medical decisions on individual patient characteristics, including molecular and behavioral biomarkers, rather than on population averages. Personalized medicine is deeply connected to and dependent on data science, specifically machine learning (often named Artificial Intelligence in the mainstream media). While during recent years there has been a lot of enthusiasm about the potential of 'big data' and machine learning-based solutions, there exist only few examples that impact current clinical practice. The lack of impact on clinical practice can largely be attributed to insufficient performance of predictive models, difficulties to interpret complex model predictions, and lack of validation via prospective clinical trials that demonstrate a clear benefit compared to the standard of care. In this paper, we review the potential of state-of-the-art data science approaches for personalized medicine, discuss open challenges, and highlight directions that may help to overcome them in the future. Conclusions: There is a need for an interdisciplinary effort, including data scientists, physicians, patient advocates, regulatory agencies, and health insurance organizations. Partially unrealistic expectations and concerns about data science-based solutions need to be better managed. In parallel, computational methods must advance more to provide direct benefit to clinical practice.", "Language": "en", "Citations": "12"},
{"Title": "Version management of service interfaces in SOA", "Authors": ["Sasa A.", "Juric M.B."], "Keywords": ["SOA", "Version management", "WSDL"], "Date": "2010", "Abstract": "Version management is an important aspect of SOA development, which has not been adequately addressed so far. In this article, we address version management of services and process in SOA. We propose extensions to WSDL. We address service-level and operation-level versioning, service endpoint mapping, and version sequencing. The proposed extensions represent a complete solution for service and process level versioning at development, deployment, and run-time. \u00a9 2010 IEEE.", "Language": "en", "Citations": "1"},
{"Title": "Automatic golf ball trajectory reconstruction and visualization", "Authors": ["Zupancic T.", "Jaklic A."], "Keywords": ["3D", "Golf", "Stereo", "Tracking", "Trajectory", "Video"], "Date": "2009", "Abstract": "The article presents the steps required to reconstruct a 3D trajectory of a golf ball flight, bounces and roll in short game. Two video cameras were used to capture the last parts of the trajectories including the bounces and roll. Each video sequence is processed and the ball is detected and tracked until is stops. Detected positions from both video sequences are then matched and 3D trajectory is obtained and presented as an X3D model. \u00a9 Springer-Verlag Berlin Heidelberg 2009.", "Language": "en", "Citations": "3"},
{"Title": "Sound pressure based automated cooking", "Authors": ["Jazbec A.", "Bajec I.L.", "Mraz M."], "Keywords": ["Automated cooking process", "Fuzzy control", "Sound based control"], "Date": "2006", "Abstract": "This paper presents an analysis of the automated cooking process based on sound pressure as an alternative indicator of the state inside the pan. In this scope the comparison between cooking process led by a skilled cook and automated cooking process is presented. The automation was achieved using a simple fuzzy controller, a fuzzy controller with changing membership functions and with a fuzzy temporal controller. The main goals of the automation are the minimization of the time spent behind the kitchen range and less power consumption.", "Language": "en", "Citations": "0"},
{"Title": "Educational hypermedia: An evaluation study", "Authors": ["Kavcic A.", "Privosnik M.", "Marolt M.", "Divjak S."], "Keywords": ["Adaptation technologies", "Adaptive hypermedia", "Educational systems", "Evaluation"], "Date": "2002", "Abstract": "The role of computers in education has changed significantly in the last years. Educational systems have exceeded passive learning systems and now actively participate in the learning process. Web-based educational systems, for example, supported by incorporated intelligent tutoring techniques, are able to adapt information and its presentation to each individual user, and dynamically support navigation through the hypermedia material. This paper deals with the evaluation of educational hypermedia. An adaptive hypermedia system was developed and implemented for this purposes. The system is based on a concept domain model and also considers the elements of knowledge uncertainty in the process of user modelling. Both models are used for system adaptation, which builds on adaptive link insertion in addition to traditional adaptive navigation support technologies, like annotation and direct guidance. The system has been evaluated in a real environment and the results of the experiments are discussed here. In the paper, adaptive hypermedia systems in general are described first. Then our system is described, which was used in the evaluation study. In the end, the experiments are described and the results analysed in details.", "Language": "en", "Citations": "3"},
{"Title": "The COST278 pan-European broadcast news database", "Authors": ["Vandecatseye A.", "Martens J.-P.", "Neto J.", "Meinedo H.", "Garcia-Mateo C.", "Dieguez J.", "Mihelic F.", "Zibert J.", "Nouza J.", "David P.", "Pleva M.", "Cizmar A.", "Papageorgiou H.", "Alexandris C."], "Keywords": [], "Date": "2004", "Abstract": "This paper describes a pan-European multilingual audio and video database of broadcast news shows. The database was constructed by seven institutions that are collaborating in the European COST278 action on Spoken Language Interaction in Telecommunications. At present, the database comprises broadcast news shows in seven languages, namely Dutch, Portuguese, Galician, Czech, Slovenian, Slovakian and Greek, but the policy is to attract new partners that bring in new data which are constructed and transcribed according to the rules and procedures outlined in this paper. The data comes with evaluation software that should facilitate a comparison of experiments.", "Language": "en", "Citations": "42"},
{"Title": "Decision Support System to support decision processes with Data Mining", "Authors": ["Rupnik R.", "Kukar M."], "Keywords": ["Data mining", "Decision support", "Decision support system", "Integration", "Knowledge discovery"], "Date": "2007", "Abstract": "Traditional techniques of data analysis do not enable the solution of all kind of problems and for that reason they have become insufficient. This caused a new interdisciplinary field of data mining to arise, encompassing both classical statistical, and modern machine learning techniques to support the data analysis and knowledge discovery from data. Data mining methods are powerful in dealing with large quantities of data, but on the other hand they are difficult to master by business users to facilitate decision support. In this paper we introduce our approach to integration of decision support system with data mining. We discuss the role of data mining to facilitate decision support, the use of data mining methods in decision support systems, discuss applied approaches and introduce a data mining decision support system called DMDSS - Data Mining Decision Support System. We also present some obtained results and plans for future development.", "Language": "en", "Citations": "12"},
{"Title": "Decisions at hand: A decision support system on handhelds", "Authors": ["Zupan B.", "Porenta A.", "Vidmar G.", "Aoki N.", "Bratko I.", "Beck J.R."], "Keywords": ["Decision support", "Handhelds", "Logistic regression"], "Date": "2001", "Abstract": "One of the applications of clinical information systems is decision support. Although the advantages of utilizing such aids have never been theoretically disputed, they have been rarely used in practice. The factor that probably often limits the utility of clinical decision support systems is the need for computing power at the very site of decision making - at the place where the patient is interviewed, in discussion rooms, etc. The paper reports on a possible solution to this problem. A decision-support shell LogReg is presented, which runs on a handheld computer. A general schema for handheld-based decision support is also proposed, where decision models are developed on personal computers/workstations, encoded in XML and then transferred to handhelds, where the models are used within a decision support shell. A use case where LogReg has been applied to clinical outcome prediction in crush injury is presented. \u00a9 2001 IMIA. All right reserved.", "Language": "en", "Citations": "8"},
{"Title": "Analysis of design patterns Analiza vzorcev nacrtovanja", "Authors": ["Domajnko T.", "Rozman I.", "Hericko M.", "Vajde-Horvat R.", "Juric M.B.", "Gustin V."], "Keywords": [], "Date": "2000", "Abstract": "Reuse of a higher level of abstraction in the field of software is discussed. The field of software design focuses on the publication of design pattern catalogs. A set of criteria classifying design patterns into four main categories is defined. Language dependent design patterns present language constructs that are presented in the form of design patterns. Related design patterns present patterns as application of ground design patterns. Object oriented idioms present basic knowledge a skilled software engineer should have. A set of criterion for the design pattern specification precision is also defined.", "Language": "en", "Citations": "0"},
{"Title": "Comparison of two automatic cell-counting solutions for fluorescent microscopic images", "Authors": ["Lojk J.", "Cibej U.", "Karlas D.", "Sajn L.", "Pavlin M."], "Keywords": ["Automatic cell counting", "Cell viability", "Cellcounter", "Evolutionary algorithm", "Fluorescence microscopy", "Learn123", "Transfection"], "Date": "2015", "Abstract": "Cell counting in microscopic images is one of the fundamental analysis tools in life sciences, but is usually tedious, time consuming and prone to human error. Several programs for automatic cell counting have been developed so far, but most of them demand additional training or data input from the user. Most of them do not allow the users to online monitor the counting results, either. Therefore, we designed two straightforward, simple-to-use cell-counting programs that also allow users to correct the detection results. In this paper, we present the Cellcounter and Learn123 programs for automatic and semiautomatic counting of objects in fluorescent microscopic images (cells or cell nuclei) with a user-friendly interface. Although Cellcounter is based on predefined and fine-tuned set of filters optimized on sets of chosen experiments, Learn123 uses an evolutionary algorithm to determine the adapt filter parameters based on a learning set of images. Cellcounter also includes an extension for analysis of overlaying images. The efficiency of both programs was assessed on images of cells stained with different fluorescent dyes by comparing automatically obtained results with results that were manually annotated by an expert. With both programs, the correlation between automatic and manual counting was very high (R", "Language": "en", "Citations": "5"},
{"Title": "Explanation and reliability of individual predictions", "Authors": ["Kononenko I.", "Strumbelj E.", "Bosnic Z.", "Pevec D.", "Kukar M.", "Robnik-Sikonja M."], "Keywords": ["Explanation", "Individual predictions", "Prediction intervals", "Reliability"], "Date": "2013", "Abstract": "Classification and regression models, either automatically generated from data by machine learning algorithms, or manually encoded with the help of domain experts, are daily used to predict the labels of new instances. Each such individual prediction, in order to be accepted/trusted by users, should be accompanied by an explanation of the prediction as well as by an estimate of its reliability. We have recently developed a general methodology for explaining individual predictions as well as for estimating their reliability. Both, explanation and reliability estimation are general techniques, independent of the underlying model and provide on-line (effective and efficient) support to the users of prediction models.", "Language": "en", "Citations": "5"},
{"Title": "Algorithms for sum of powers of integers Algoritmi za vsoto potenc naravnih stevil", "Authors": ["Mihelic J."], "Keywords": [], "Date": "2013", "Abstract": "Sequences are of key importance in mathematics, theoretical computer science, analysis of algorithms, computability theory, computational complexity and many other exact sciences. In this paper we focus on the sequences of k-th powers of the first n natural numbers. For any k and n we derive several formulas to calculate the partial sum of the sequence, i.e., the sum of the first n terms. Based on these formulas, we develop five algorithms which for any n and K compute all partial sums for any k between 0 and K. The first two algorithms are based on the definition of the partial sum and the remaining three on the recurrence equation which calculates the sum from the lower order sums. For all the algorithms we give implementations and analyze asymptotic time complexity.", "Language": "en", "Citations": "0"},
{"Title": "Triangle- and pentagon-free distance-regular graphs with an eigenvalue multiplicity equal to the valency", "Authors": ["Jurisic A.", "Koolen J.", "Miklavic S."], "Keywords": ["2-homogeneous graphs", "Almost bipartity graphs", "Distance-regular graphs", "Eigen value multiplicity", "Triangle and pentagon free"], "Date": "2005", "Abstract": "We classify triangle- and pentagon-free distance-regular graphs with diameter d \u2265 2, valency k, and an eigenvalue multiplicity k. In particular, we prove that such a graph is isomorphic to a cycle, a k-cube, a complete bipartite graph minus a matching, a Hadamard graph, a distance-regular graph with intersection array {k, k - 1, k - c, c, 1; 1, c, k - c, k - 1, k}, where k = \u03b3(\u03b3", "Language": "en", "Citations": "6"},
{"Title": "Extending Web services with event-driven semantics", "Authors": ["Juric M.B."], "Keywords": [], "Date": "2011", "Abstract": "SOAP Web services have so far supported operation invocation semantics only. This paper proposes extensions for web services to support event-driven semantics. Our solution enables services to act as event producers and event consumers. We have identified and designed the required extensions to WSDL. We propose a flexible XML representation of the event payload data. We introduce event sinks, sources, and triggers. \u00a9 2011 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "Addressing polysemy in bilingual lexicon extraction from comparable corpora", "Authors": ["Fiser D.", "Ljubesic N.", "Kubelka O."], "Keywords": ["Bilingual lexicon extraction", "Comparable corpora", "Polysemy"], "Date": "2012", "Abstract": "This paper presents an approach to extract translation equivalents from comparable corpora for polysemous nouns. As opposed to the standard approaches that build a single context vector for all occurrences of a given headword, we first disambiguate the headword with third-party sense taggers and then build a separate context vector for each sense of the headword. Since state-of-the-art word sense disambiguation tools are still far from perfect, we also tried to improve the results by combining the sense assignments provided by two different sense taggers. Evaluation of the results shows that we outperform the baseline (0.473) in all the settings we experimented with, even when using only one sense tagger, and that the best-performing results are indeed obtained by taking into account the intersection of both sense taggers (0.720).", "Language": "en", "Citations": "2"},
{"Title": "Classical mechanics approach applied to analysis of genetic oscillators", "Authors": ["Vasylchenkova A.", "Mraz M.", "Zimic N.", "Moskon M."], "Keywords": ["dynamical systems", "genetic oscillators", "ordinary differential equations", "Oscillatory dynamics"], "Date": "2017", "Abstract": "Biological oscillators present a fundamental part of several regulatory mechanisms that control the response of various biological systems. Several analytical approaches for their analysis have been reported recently. They are, however, limited to only specific oscillator topologies and/or to giving only qualitative answers, i.e., is the dynamics of an oscillator given the parameter space oscillatory or not. Here, we present a general analytical approach that can be applied to the analysis of biological oscillators. It relies on the projection of biological systems to classical mechanics systems. The approach is able to provide us with relatively accurate results in the meaning of type of behavior system reflects (i.e., oscillatory or not) and periods of potential oscillations without the necessity to conduct expensive numerical simulations. We demonstrate and verify the proposed approach on three different implementations of amplified negative feedback oscillator.", "Language": "en", "Citations": "1"},
{"Title": "Robust localization using an omnidirectional appearance-based subspace model of environment", "Authors": ["Jogan M.", "Leonardis A."], "Keywords": ["Appearance-based model", "Panoramic eigenspace", "Robustners", "Self-localization"], "Date": "2003", "Abstract": "Appearance-based visual learning and recognition techniques that are based on models derived from a training set of 2D images are being widely used in computer vision applications. In robotics, they have received most attention in visual servoing and navigation. In this paper we discuss a framework for visual self-localization of mobile robots using a parametric model built from panoramic snapshots of the environment. In particular, we propose solutions to the problems related to robustness against occlusions and invariance to the rotation of the sensor. Our principal contribution is an \"eigenspace of spinning-images\", i.e., a model of the environment which successfully exploits some of the specific properties of panoramic images in order to efficiently calculate the optimal subspace in terms of principal components analysis (PCA) of a set of training snapshots without actually decomposing the covariance matrix. By integrating a robust recover-and-select algorithm for the computation of image parameters we achieve reliable localization even in the case when the input images are partly occluded or noisy. In this way, the robot is capable of localizing itself in realistic environments. \u00a9 2003 Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "46"},
{"Title": "Dictionary of the Slovenian sign language on the WWW", "Authors": ["Cempre L.", "Besir A.", "Solina F."], "Keywords": ["multimedia dictionary", "sign language", "video player", "video sprite", "web application"], "Date": "2013", "Abstract": "The article describes technical and user-interface issues of transferring the contents and functionality of the CD-ROM version of the Slovenian sing language dictionary to the web. The dictionary of Slovenian sign language consist of video clips showing the demonstration of signs that deaf people use for communication, text description of the words corresponding to the signs and pictures illustrating the same word/sign. A new technical solution - a video sprite - for concatenating subsections of video clips necessary for their smooth display on most available platforms was developed. The contents of the dictionary which were re-edited are combined also with other resources available on the web. Added were also new exercises for learning the sign language. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "Holistic approach to fraud management in health insurance", "Authors": ["Furlan S.", "Bajec M."], "Keywords": ["Activities", "Characteristics", "Fraud management system", "Health care", "Insurance"], "Date": "2008", "Abstract": "Fraud presents an immense problem for health insurance companies and the only way to fight fraud, is by using specialized fraud management systems. Current research community has focused great efforts on different fraud detection techniques, while neglecting other also important activities of fraud management. We propose a holistic approach that focuses on all 6 activities of fraud management, namely, (1) deterrence, (2) prevention, (3) detection, (4) investigation, (5) sanction and redress, and (6) monitoring. The main contribution of the paper are 15 key characteristics of a fraud management system, which enable effective and efficient support to all fraud management activities. We base our research on literature review, interviews with experts from different fields, and a case study. The case study provides additional confirmation to expert opinions, as it puts our holistic framework into practice.", "Language": "en", "Citations": "9"},
{"Title": "Local pixel value collection algorithm for spot segmentation in two-dimensional gel electrophoresis research", "Authors": ["Peer P.", "Corzo L.G."], "Keywords": [], "Date": "2007", "Abstract": "Two-dimensional gel-electrophoresis (2-DE) images show the expression levels of several hundreds of proteins where each protein is represented as a blob-shaped spot of grey level values. The spot detection, that is, the segmentation process has to be efficient asit is the first step in the gel processing. Such extraction of information is a very complex task. In this paper, we propose a novel spot detector that is basically a morphology-based method with the use of a seeded region growing as a central paradigm and which relies on the spot correlation information. The method is tested on our synthetic as well as on real gels with human samples from SWISS-2DPAGE (two-dimensional polyacrylamide gel electrophoresis) database. A comparison of results is done with a method called pixel value collection (PVC). Since our algorithm efficiently uses local spot information, segments the spot by collecting pixel values and its affinity with PVC, we named it local pixel value collection (LPVC). The results show that LPVC achieves similar segmentation results as PVC, but is much faster than PVC.", "Language": "en", "Citations": "13"},
{"Title": "From Scrum to Kanban: Introducing Lean Principles to a Software Engineering Capstone Course", "Authors": ["Mahnic V."], "Keywords": ["Capstone course", "Kanban", "Lean software development", "Scrumban", "Software engineering education"], "Date": "2015", "Abstract": "In this paper, a capstone course in software engineering is described that exposes students to lean principles advocated by Kanban. While retaining the main characteristics of its predecessor course, which concentrated on teaching agile software development using Scrum, the new course also introduces the most important Kanban concepts, i.e., visualization of the workflow and limitation of the work in progress. Kanban concepts are introduced in two ways: in combination with Scrum (as Scrumban) or as a \"pure\" Kanban (omitting some of the Scrum activities considered waste). Students are required to work in teams responsible for the implementation of a set of user stories defined by a project domain expert playing the role of the Product Owner. During the course, they must maintain a Kanban board and measure lead time. The paper discusses the use of different Kanban boards and work in progress limits, and analyzes the students' progress in reducing lead time.A summary of the lessons learned and recommendations is given reflecting the issues to be considered when teaching similar courses.Asurvey among students has shown that they liked both approaches and were overwhelmingly positive about the course.", "Language": "en", "Citations": "10"},
{"Title": "Multiresolution image parametrization for improving texture classification", "Authors": ["Sajn L.", "Kononenko I."], "Keywords": [], "Date": "2008", "Abstract": "In the paper an innovative alternative to automatic image parametrization on multiple resolutions, based on texture description with specialized association rules, and image evaluation with machine learning methods is presented. The algorithm ArTex for parameterizing textures with association rules belonging to structural parametrization algorithms was developed. In order to improve the classification accuracy a multiresolution approach is used. The algorithm ARes for finding more informative resolutions based on the SIFT algorithm is described. The presented algorithms are evaluated on several public domains and the results are compared to other well-known parametrization algorithms belonging to statistical and spectral parametrization algorithms. Significant improvement of classification results was observed when combining parametrization attributes at several image resolutions for most parametrization algorithms. Our results show that multiresolution image parametrization should be considered when improvement of classification accuracy in textural domains is required. These resolutions have to be selected carefully and may depend on the domain itself.", "Language": "en", "Citations": "8"},
{"Title": "Ubiquitousness of link-density and link-pattern communities in real-world networks", "Authors": ["Subelj L.", "Bajec M."], "Keywords": ["Interdisciplinary Physics"], "Date": "2012", "Abstract": "Community structure appears to be an intrinsic property of many complex real-world networks. However, recent work shows that real-world networks reveal even more sophisticated modules than classical cohesive (link-density) communities. In particular, networks can also be naturally partitioned according to similar patterns of connectedness among the nodes, revealing link-pattern communities. We here propose a propagation based algorithm that can extract both link-density and link-pattern communities, without any prior knowledge of the true structure. The algorithm was first validated on different classes of synthetic benchmark networks with community structure, and also on random networks. We have further applied the algorithm to different social, information, technological and biological networks, where it indeed reveals meaningful (composites of) link-density and link-pattern communities. The results thus seem to imply that, similarly as link-density counterparts, link-pattern communities appear ubiquitous in nature and design. \u00a9 2012 EDP Sciences, SIF, Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "19"},
{"Title": "Contractions of 6-connected toroidal graphs", "Authors": ["Fijavz G."], "Keywords": ["Minor-minimal 6-connected graph", "Toroidal graph"], "Date": "2007", "Abstract": "Let T", "Language": "fr", "Citations": "3"},
{"Title": "A robust PCA algorithm for building representations from panoramic images", "Authors": ["Skocaj D.", "Bischof H.", "Leonardis A."], "Keywords": [], "Date": "2002", "Abstract": "Appearance-based modeling of objects and scenes using PCA has been successfully applied in many recognition tasks. Robust methods which have made the recognition stage less susceptible to outliers, occlusions, and varying illumination have further enlarged the domain of applicability. However, much less research has been done in achieving robustness in the learning stage. In this paper, we propose a novel robust PCA method for obtaining a consistent subspace representation in the presence of outlying pixels in the training images. The method is based on the EM algorithm for estimation of principal subspaces in the presence of missing data. By treating the outlying points as missing pixels, we arrive at a robust PCA representation. We demonstrate experimentally that the proposed method is efficient. In addition, we apply the method to a set of panoramic images to build a representation that enables surveillance and view-based mobile robot localization.", "Language": "en", "Citations": "18"},
{"Title": "Comparing commercial IP reputation databases to open-source IP reputation algorithms", "Authors": ["Porenta J.", "Ciglaric M."], "Keywords": ["Anti-spam filtering", "Comparison", "Email", "IP reputation"], "Date": "2013", "Abstract": "In this paper we present a method to compare results from commercial IP reputation databases to results from open-source IP reputation algorithms. We have made an empirical comparison of six popular commercial IP reputation databases and three open-source IP reputation algorithms. After we processed and classified our email corpus, we compared the open-source IP reputation algorithms' results with commercial IP reputation databases by using the Spearman rank correlation coefficient. The results show lower correlation when the frequency of emails from a single IP is rising. Open-source algorithms performed sufficiently for IP addresses with more than five and less than 50 emails from a single IP, while the correlation dropped with a higher number of emails from a single IP. We also compared commercial IP reputation databases and found mixed correlations between them. \u00a9 2013 CRL Publishing Ltd.", "Language": "en", "Citations": "0"},
{"Title": "Mesh partitioning: A multilevel ant-colony-optimization algorithm", "Authors": ["Korosec P.", "Silc J.", "Robic B."], "Keywords": [], "Date": "2003", "Abstract": "Mesh partitioning is an important problem that has extensive applications in many areas. Multilevel algorithms are a successful class of optimization techniques which address the mesh partitioning problem. In this paper we present an enhancement of the technique that uses a nature inspired metaheuristic to achieve higher quality partitions. We apply and study a multilevel ant-colony (MACO) optimization, which is a relatively new metaheuristic search technique for solving optimization problems. The MACO algorithm performed very well and is superior to the classical k-METIS and Chaco algorithms. Furthermore, it is even comparable to the combined evolutionary/multilevel scheme used in the JOSTLE evolutionary algorithm. Our MACO algorithm returned also some solutions that are better than currently available solutions in the graph partitioning archive.", "Language": "en", "Citations": "4"},
{},
{"Title": "On the connectivity of Cartesian product of graphs", "Authors": ["Govorcin J.", "Skrekovski R."], "Keywords": ["Cartesian product", "Connectivity"], "Date": "2014", "Abstract": "We give a new alternative proof of Liouville's formula which states that for any graphs G and H on at least two vertices, \u03ba(GDH) = min {\u03ba(G)|H|, |G|\u03ba(H), \u03b4(G) + \u03b4(H)}, where \u03ba and \u03b4 denote the connectivity number and minimum degree of a given graph, respectively. The main idea of our proof is based on construction of a vertex-fan which connects a vertex from V(G\u25a1H) to a subgraph of G\u25a1H. We also discuss the edge version of this problem as well as formula for products with more than two factors. Copyright \u00a9 2014 DMFA Slovenije.", "Language": "en", "Citations": "3"},
{"Title": "An approach for creating project-specific software development methodologies", "Authors": ["Bajec M.", "Vavpotic D.", "Krisper M."], "Keywords": [], "Date": "2005", "Abstract": "Both practitioners and researchers agree that if software development methodologies were more adjustable to project-specific situations, this would increase their use in practice. Empirical investigations show that otherwise methodologies exist just on paper while in practice developers avoid them or do not follow them rigorously. In this paper we present an approach that deals with this problem. The approach that we call Process configuration tells how to create a project-specific methodology from an existing one, taking into account the project circumstances. Compared to other approaches that deal with the creation of project-specific methodologies, our approach tends to be easier to implement in practice as it introduces several simplifications. The proposed approach is practice-driven, i.e. it has been developed in cooperation with software development companies.", "Language": "en", "Citations": "1"},
{"Title": "Design and deployment of eHealth interventions using behavior change techniques, BPMN2 and OpenEHR", "Authors": ["Bestek M.", "Curtis K.", "Brodnik A."], "Keywords": ["behavior change interventions", "BPMN2", "eCare", "interoperability", "interventions", "OpenEHR"], "Date": "2015", "Abstract": "Healthcare Systems are transforming from focusing on acute care to focusing on managing chronic conditions. In this process they are becoming highly distributed and specialized. Innovative approaches are needed to fully support the design and deployment of new eHealth interventions. Design should be based on theory and evidence, and deployment should be supported by a sustainable ICT platform, that enables interoperability and reusability by focusing on open standards, open data, open source technology and knowledge modeling. We tested one such method that focuses on using behavior change techniques for the design phase, and tested OpenEHR and BPMN2 as the basis for the ICT platform to support the deployment phase.", "Language": "en", "Citations": "0"},
{"Title": "VizRank: Finding informative data projections in functional genomics by machine learning", "Authors": ["Leban G.", "Bratko I.", "Petrovic U.", "Curk T.", "Zupan B."], "Keywords": [], "Date": "2005", "Abstract": "Summary: VizRank is a tool that finds interesting two-dimensional projections of class-labeled data. When applied to multi-dimensional functional genomics datasets, VizRank can systematically find relevant biological patterns. \u00a9 Oxford University Press 2004; all rights reserved.", "Language": "en", "Citations": "30"},
{"Title": "Distributed environment for efficient virtual machine image management in federated Cloud architectures", "Authors": ["Kimovski D.", "Marosi A.", "Gec S.", "Saurabh N.", "Kertesz A.", "Kecskemeti G.", "Stankovski V.", "Prodan R."], "Keywords": ["Cloud federation", "distributed VMI repositories", "virtual machine images", "VMI redistribution", "VMI size optimization"], "Date": "2017", "Abstract": "The use of virtual machines (VMs) in Cloud computing provides various benefits in the overall software engineering lifecycle. These include efficient elasticity mechanisms resulting in higher resource utilization and lower operational costs. The VMs as software artifacts are created using provider-specific templates, called virtual machine images (VMI), and are stored in proprietary or public repositories for further use. However, some technology-specific choices can limit the interoperability among various Cloud providers and bundle the VMIs with nonessential or redundant software packages, leading to increased storage size, prolonged VMI delivery, stagnant VMI instantiation, and ultimately vendor lock-in. To address these challenges, we present a set of novel functionalities and design approaches for efficient operation of distributed VMI repositories, specifically tailored for enabling (1) simplified creation of lightweight and size optimized VMIs tuned for specific application requirements; (2) multi-objective VMI repository optimization; and (3) efficient reasoning mechanism to help optimizing complex VMI operations. The evaluation results confirm that the presented approaches can enable VMI size reduction by up to 55%, while trimming the image creation time by 66%. Furthermore, the repository optimization algorithms can reduce the VMI delivery time by up to 51% and cut down the storage expenses by 3%. Moreover, by implementing replication strategies, the optimization algorithms can increase the system reliability by 74%.", "Language": "en", "Citations": "3"},
{"Title": "On determining probability forecasts from betting odds", "Authors": ["Strumbelj E."], "Keywords": ["Betfair", "Betting exchange", "Calibration", "Fixed-odds", "Probability forecasting", "Shin's model", "Sports forecasting"], "Date": "2014", "Abstract": "We show that the probabilities determined from betting odds using Shin's model are more accurate forecasts than those determined using basic normalization or regression models. We also provide empirical evidence that some bookmakers are significantly different sources of probabilities in terms of forecasting accuracy, and that betting exchange odds are not always the best source, especially in smaller markets. The advantage of using Shin probabilities and the differences between bookmakers decrease with an increasing market size. \u00a9 2014 International Institute of Forecasters.", "Language": "en", "Citations": "22"},
{"Title": "Empirical evaluation of heuristic scheduling algorithms used in parallel systems design", "Authors": ["Papa G.", "Silc J.", "Robic B."], "Keywords": ["Automated synthesis", "Constructive algorithms", "Scheduling", "Transformational algorithms"], "Date": "2001", "Abstract": "The paper describes some heuristic scheduling algorithms that are used in the automated design of parallel systems. The results of empirical evaluation of time-constrained, resource-constrained, and time/resource-constrained scheduling algorithm are given. The paper focuses on differences between constructive and transformational algorithms.", "Language": "en", "Citations": "1"},
{"Title": "Automatic classification of transient ischaemic and transient non-ischaemic heart-rate related ST segment deviation episodes in ambulatory ECG records", "Authors": ["Faganeli J.", "Jager F."], "Keywords": ["Automatic classification", "Evaluation", "Predicting real-world performance", "Transient ischaemic ST segment episodes", "Transient non-ischaemic heart-rate related ST segment episodes"], "Date": "2010", "Abstract": "In ambulatory ECG records, besides transient ischaemic ST segment deviation episodes, there are also transient non-ischaemic heart-rate related ST segment deviation episodes present, which appear only due to a change in heart rate and thus complicate automatic detection of true ischaemic episodes. The goal of this work was to automatically classify these two types of episodes. The tested features to classify the ST segment deviation episodes were changes of heart rate, changes of the Mahalanobis distance of the first five Karhunen- Lo\u00e8ve transform (KLT) coefficients of the QRS complex, changes of time-domain morphologic parameters of the ST segment and changes of the Legendre orthonormal polynomial coefficients of the ST segment. We chose Legendre basis functions because they best fit typical shapes of the ST segment morphology, thus allowing direct insight into the ST segment morphology changes through the feature space. The classification was performed with the help of decision trees. We tested the classification method using all records of the Long-Term ST Database on all ischaemic and all non-ischaemic heart-rate related deviation episodes according to annotation protocol B. In order to predict the real-world performance of the classification we used second-order aggregate statistics, gross and average statistics, and the bootstrap method. We obtained the best performance when we combined the heart-rate features, the Mahalanobis distance and the Legendre orthonormal polynomial coefficient features, with average sensitivity of 98.1% and average specificity of 85.2%. \u00a9 2010 Institute of Physics and Engineering in Medicine.", "Language": "en", "Citations": "3"},
{"Title": "Kinect web kiosk framework", "Authors": ["Bohak C.", "Marolt M."], "Keywords": ["HCI", "interaction framework", "interactive kiosk presentation", "interactivity", "Kinect"], "Date": "2013", "Abstract": "In this paper we present a web kiosk framework based on Kinect sensor. The main idea is to use the framework for creation of simple interactive presentations for informing, advertising and presenting knowledge to the public. The use of such a framework simplifies adaptation of existing web materials for presentation with the kiosk. We can also make use of touchless interaction for browsing through the interactive content, to animate the user and encourage her to spend more time browsing the presented content. We present the structure of the framework and a simple case study on using the framework as an interactive presentation platform and as an education resource. The developed framework has been used for presenting information on educational programs at Faculty of Computer and Information Science, University of Ljubljana. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "ABC Transporters in Dictyostelium discoideum Development", "Authors": ["Miranda E.R.", "Zhuchenko O.", "Toplak M.", "Santhanam B.", "Zupan B.", "Kuspa A.", "Shaulsky G."], "Keywords": [], "Date": "2013", "Abstract": "ATP-binding cassette (ABC) transporters can translocate a broad spectrum of molecules across the cell membrane including physiological cargo and toxins. ABC transporters are known for the role they play in resistance towards anticancer agents in chemotherapy of cancer patients. There are 68 ABC transporters annotated in the genome of the social amoeba Dictyostelium discoideum. We have characterized more than half of these ABC transporters through a systematic study of mutations in their genes. We have analyzed morphological and transcriptional phenotypes for these mutants during growth and development and found that most of the mutants exhibited rather subtle phenotypes. A few of the genes may share physiological functions, as reflected in their transcriptional phenotypes. Since most of the abc-transporter mutants showed subtle morphological phenotypes, we utilized these transcriptional phenotypes to identify genes that are important for development by looking for transcripts whose abundance was unperturbed in most of the mutants. We found a set of 668 genes that includes many validated D. discoideum developmental genes. We have also found that abcG6 and abcG18 may have potential roles in intercellular signaling during terminal differentiation of spores and stalks. \u00a9 2013 Miranda et al.", "Language": "en", "Citations": "11"},
{"Title": "SSBC 2018: Sclera segmentation benchmarking competition", "Authors": ["Das A.", "Pal U.", "Ferrer M.A.", "Blumenstein M.", "Stepec D.", "Rot P.", "Emersic Z.", "Peer P.", "Struc V."], "Keywords": ["Sclera", "Segmentation"], "Date": "2018", "Abstract": "This paper summarises the results of the Sclera Segmentation Benchmarking Competition (SSBC 2018). It was organised in the context of the 11th IAPR International Conference on Biometrics (ICB 2018). The aim of this competition was to record the developments on sclera segmentation in the cross-sensor environment (sclera trait captured using multiple acquiring sensors). Additionally, the competition also aimed to gain the attention of researchers on this subject of research. For the purpose of benchmarking, we have developed two datasets of sclera images captured using different sensors. The first dataset was collected using a DSLR camera and the second one was collected using a mobile phone camera. The first dataset is the Multi-Angle Sclera Dataset (MASD version 1), which was used in the context of the previous versions of sclera segmentation competitions. The images in the second dataset were captured using.a mobile phone rear camera of 8-megapixel. As baseline manual segmentation mask of the sclera images from both the datasets were developed. Precision and recall-based statistical measures were employed to evaluate the effectiveness of the submitted segmentation technique and to rank them. Six algorithms were submitted towards the segmentation task. This paper analyses the results produced by these algorithms/system and defines a way forward for this subject of research. Both the datasets along with some of the accompanying ground truth/baseline mask will be freely available for research purposes upon request to authors by email.", "Language": "en", "Citations": "1"},
{"Title": "View-based object representations using RBF networks", "Authors": ["Bischof H.", "Leonardis A."], "Keywords": ["Minimum description length", "Neural networks", "Object representation", "Radial basis functions", "View interpolation"], "Date": "2001", "Abstract": "Radial basis function (RBF) networks have been proposed as suitable representations for 3-D objects, in particular, since they can learn view-based representations from a small set of training views. One of the basic questions that arise in the context of RBF networks concerns their complexity, i.e. the number of basis functions that are necessary for a reliable representation, which should balance the accuracy and the robustness. In this paper, we propose a systematic approach for building object representations in terms of RBF networks. We studied and designed two procedures: the off-line procedure, where the network is constructed after having a complete set of training views of an object, and the on-line procedure, where the network is incrementally built as new views of an object arrive. We tested the procedures both on synthetic and real data. \u00a9 2001 Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "Software process improvement based on the method engineering principles", "Authors": ["Bajec M.", "Vavpotic D.", "Furlan S.", "Krisper M."], "Keywords": [], "Date": "2007", "Abstract": "While it used to be a common belief that the use of rigorous methods in software development is beneficial if not compulsory to assure success of software development projects, the investigations in practice reveal developers often avoid to follow prescribed methods and that there is a wide gap between the organizations' official methods and the work actually performed by their developers in IT projects. According to the literature, there are many reasons contributing to this rather undesirable situation. The two of them are rigidity of methods and their social inappropriateness. In the MasterProc project we have addressed these issues by developing a framework and tool-support for the reengineering of software development methods. Using the framework an organisation can reengineer its existing ways of working into a method that is organisation-specific and auto-adjustable to specifics of its projects. The evaluation that was performed in five partner companies is motivating, as it shows the framework can be very useful in improving software development practice. This paper describes the framework philosophy and its main components. \u00a9 2007 International Federation for Information Processing.", "Language": "en", "Citations": "10"},
{"Title": "Space complexity optimization for nano electronic devices based on evolutionary computation", "Authors": ["Jazbec A.", "Zimic N.", "Pecar P.", "Bajec I.L.", "Mraz M."], "Keywords": ["Circuits design", "Evolutionary computation", "Optimization", "Quantum-dot cellular automata"], "Date": "2008", "Abstract": "Recent studies show Quantum-dot Cellular Automata (QCA) as one of the promising alternatives to CMOS technology. Optimization plays an important role in circuit design despite the used technology. One possibility is the minimization of the number of basic building blocks usually resulting in less energy consumption and fewer delays in processing. The principles of evolutionary computation have already been successfully used for logic optimization of majority gate-based nano electronic circuits. The realm was to reduce the number of the basic building blocks (majority gates) required for computing Boolean functions. Our study is focused on the space complexity optimization of simple and also more complex QCA devices by means of the minimization of the number of employed QCA cells. \u00a9 2008 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "The zero-divisor graphs of rings and semirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["graph", "Ring", "semiring", "zero-divisor"], "Date": "2012", "Abstract": "In this paper we study zero-divisor graphs of rings and semirings. We show that all zero-divisor graphs of (possibly noncommutative) semirings are connected and have diameter less than or equal to 3. We characterize all acyclic zero-divisor graphs of semirings and prove that in the case zero-divisor graphs are cyclic, their girths are less than or equal to 4. We find all possible cyclic zero-divisor graphs over commutative semirings having at most one 3-cycle, and characterize all complete k-partite and regular zero-divisor graphs. Moreover, we characterize all additively cancellative commutative semirings and all commutative rings such that their zero-divisor graph has exactly one 3-cycle. \u00a9 2012 World Scientific Publishing Company.", "Language": "en", "Citations": "11"},
{"Title": "Exploiting locality of interest in online social networks", "Authors": ["Wittie M.P.", "Pejovic V.", "Deek L.", "Almeroth K.C.", "Zhao B.Y."], "Keywords": ["D.4.3 [Information Systems Applications]: Communications Applications", "Design", "Economics", "H.3.4 [Information Storage and Retrieval]: Systems and Software", "Human Factors", "Performance"], "Date": "2010", "Abstract": "Online Social Networks (OSN) are fun, popular, and socially significant. An integral part of their success is the immense size of their global user base. To provide a consistent service to all users, Facebook, the world's largest OSN, is heavily dependent on centralized U.S. data centers, which renders service outside of the U.S. sluggish and wasteful of Internet bandwidth. In this paper, we investigate the detailed causes of these two problems and identify mitigation opportunities. Because details of Facebook's service remain proprietary, we treat the OSN as a black box and reverse engineer its operation from publicly available traces. We find that contrary to current wisdom, OSN state is amenable to partitioning and that its fine grained distribution and processing can significantly improve performance without loss in service consistency. Through simulations of reconstructed Facebook traffic over measured Internet paths, we show that user requests can be processed 79% faster and use 91% less bandwidth. We conclude that the partitioning of OSN state is an attractive scaling strategy for Facebook and other OSN services. \u00a9 2010 ACM.", "Language": "en", "Citations": "72"},
{"Title": "Distance learning Ucenje na daljavo", "Authors": ["Zrnec A.", "Solina F."], "Keywords": [], "Date": "2000", "Abstract": "Distance learning replaces a physical gap between the teacher and the students. The synchronous and asynchronous types of learning are compared. The synchronous type of learning uses video conferencing systems or computer video conference where all participants must partake in the learning process at the same time. The asynchronous type of distance learning does not demand an interaction between students and teachers in real time and is therefore more flexible than synchronous type of learning. The problems about learning materials, learning objects, standards and problems dealing with creating good educational software are also discussed.", "Language": "en", "Citations": "0"},
{"Title": "Products of commuting nilpotent operators", "Authors": ["Bukovsek D.K.", "Kosir T.", "Novak N.", "Oblak P."], "Keywords": ["Commuting matrices and operators", "Factorization", "Nilpotent matrices and operators", "Products"], "Date": "2007", "Abstract": "Matrices that are products of two (or more) commuting square-zero matrices and matrices that are products of two commuting nilpotent matrices are characterized. Also given are characterizations of operators on an infinite dimensional Hilbert space that are products of two (or more) commuting square-zero operators, as well as operators on an infinite-dimensional vector space that are products of two commuting nilpotent operators.", "Language": "en", "Citations": "3"},
{"Title": "Automatic extractive multi-document summarization based on archetypal analysis", "Authors": ["Canhasi E.", "Kononenko I."], "Keywords": [], "Date": "2016", "Abstract": "The applications of matrix factorization are an important tool for text summarization. In last years, several variations of the non-negative matrix factorization (NMF) methods have found their usage in multi-document summarization (MDS). For matrix factorization to work efficiently in MDS, it is essential to show the ability of selecting the most typical data points from the given data space. In the chapter, we first describe the archetypal analysis (AA) and its weighted version and then we present the AA-based document summarization method for the two most known summarization tasks, namely the general and the query-focused MDS. Archetypal analysis, also known as the convex NMF, in contrast to other NMF methods selects distinct (archetypal) sentences and therefore leads to variability and diversity in content of the generated summaries.We conducted experiments on the data of document understanding conference. Experimental results evidence the improvement of the proposed approach over other closely related methods including ones using the NMF.", "Language": "en", "Citations": "2"},
{"Title": "Information regarding Slovenian textile, clothing and leather production companies Informatizacija slovenskih tekstilnih, obla\u010dilnih in usnjarsko predelovalnih podjetij", "Authors": ["Elesini U.S.", "Zakrajsek S.", "Cerar E.", "Marolt M.", "Godec P.", "Urbas R."], "Keywords": ["Business information systems", "History review", "Production of clothes", "Production of leather and related products", "Production of textiles", "TOUP"], "Date": "2015", "Abstract": "Competing in the market means constant development throughout all areas, also in information about business processes, the development of which has significantly increased over last sixty years. Results of a research dealt with how the Slovenian textile, clothing and leather production (TOUP) industries have followed this development are presented in the article. The research was further directed towards a new age state. Based on the data collected from the literature, eight hypotheses were set up, which were examined through interviews and questionnaires. 111 (25.5 percent) of companies responded to the study. The results were analysed separately for large, medium, small-sized and micro companies, as the preliminary research showed that their views (and actual states) regarding business information systems are quite different, so any generalisation of the results wouldn\u2019t provide realistic treatment of the set hypotheses. Among the gathered data appropriate correlation was searched for using the Pearson \u03c72-test. All large and medium-sized TOUP companies are equipped with information systems and 80 percent of small-sized and 26.3 percent of micro companies. More than half of the companies (64.4 percent) prefer the information systems of domestic suppliers. Only 20 percent of large-sized companies and a smaller percentage of micro companies have developed their own business information systems. Medium-sized companies use purchased/licensed systems. Less than half of the large and medium-sized companies use two or more interconnected information systems at the same time. Business information systems support economic and commercial functions in 60.4 percent of companies, while in the other companies the production, controlling, CRM, investing etc. functions are also present. Business information systems in cloud are present in less than 15 percent of Slovenian TOUP companies. The business information systems in large and medium-sized companies are eight years old on average. During last year (2014), 40 percent of companies upgraded their business information systems. Investments into systems are small with the exceptions of some large-sized companies, where investments are reasonably bigger because of the systems\u2019 complexities.", "Language": "en", "Citations": "0"},
{"Title": "Position of modern peer-to-peer systems in the distributed systems architecture", "Authors": ["Ciglaric M.", "Vidmar T."], "Keywords": ["Architecture", "Content sharing", "Distributed systems", "Grid computing", "Peer-to-peer computing"], "Date": "2002", "Abstract": "Recently, the hottest development in the area of distributed computing is the appearance of peer-to-peer systems, employing a direct communication and free resources on desktop computers. In the paper, we describe peer-to-peer systems, compare the definitions of distributed systems with those of peer-to-peer systems and stress the characteristic aspects.", "Language": "en", "Citations": "2"},
{},
{"Title": "WSDL and UDDI extensions for version support in web services", "Authors": ["Juric M.B.", "Sasa A.", "Brumen B.", "Rozman I."], "Keywords": ["SOA", "UDDI", "Versioning", "Web services", "WSDL"], "Date": "2009", "Abstract": "Versioning is an important aspect of web service development, which has not been adequately addressed so far. In this article, we propose extensions to WSDL and UDDI to support versioning of web service interfaces at development-time and run-time. We address service-level and operation-level versioning, service endpoint mapping, and version sequencing. We also propose annotation extensions for developing versioned web services in Java. We have tested the proposed solution for versioning in two real-world environments and identified considerable improvements in service development and maintenance efficiency, improved service reuse, and simplified governance. \u00a9 2009 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "56"},
{"Title": "Solving selected probability contents with e-materials in 4th grade Re\u0161evanje problemov iz verjetnosti ob uporabi e-gradiva v \u010detrtem razredu", "Authors": ["Pristovnik T.", "Peer P.", "Cadez T.H."], "Keywords": ["e-materials", "Grade 4", "Mathematics", "Probability", "Teaching and learning with the help of ict"], "Date": "2011", "Abstract": "In Slovenia, the probability contents are not formally included in the mathematics curriculum at the primary level (pupils aged 6-11 years), but due to the efforts of didacticians of mathematics and opinions of various researchers recognizing certain advantages of the mentioned contents, they are included in some textbook sets in the first as well as in the second educational period. The basic idea of the research, which was to establish the probability concepts that pupils are able to comprehend at the age of 9 or 10 (4th grade of primary school), led to a more in-depth planning of the approach to teaching the selected probability concepts. In order to develop such a teaching approach, suitable learning materials had to be compiled so that for the purpose of this research we produced our own e-materials titled Probability. A case study is presented, in which the application of e-materials in lessons proved to be suitable for reaching the probability goals.", "Language": "en", "Citations": "0"},
{"Title": "Head pose estimation under weak-perspective projection model", "Authors": ["Kuzmanic Skelin A.", "Solina F.", "Zanchi V."], "Keywords": [], "Date": "2008", "Abstract": "Head pose estimation from 2D images is ill-posed problem since image intensity data is not view invariant, and does not provide 3D configuration of head. In this paper, we propose a method to estimate head pose in uncalibrated lowresolution images. Our approach to estimate 3D orientation of the face uses combination of appearance-based detector based on Haar-like features to localize head and model-based three-point representation of the face. We show that under weak-perspective assumption, given three feature points in image, such as eyes and mouth, pose of corresponding three points in space can be estimated. Experimental results on monocular images show efficacy of the approach. \u00a9 2008 by Mipro.", "Language": "en", "Citations": "1"},
{"Title": "Multi-state processing with quantum-dot cellular automata Ve\u010dstanjsko procesiranje v strukturah kvantnih celi\u010dnih avtomatov", "Authors": ["Bajec I.L.", "Mraz M."], "Keywords": ["Extended quantum-dot cellular automata", "Multi-state processing", "Nano-computing", "Quantum-dot cellular automata"], "Date": "2006", "Abstract": "Quantum-dot Cellular Automata (QCA) are becoming one of the possible alternative processing platforms of the future. Indeed, the technological processes that enable the tunnelling of electrons between quantum dots, as well QCA structures that implement the functionally complete set of binary logic functions, have already been developed [4, 5, 12, 13]. What is more, the first tools that enable the design and simulation of complex QCA structures have also been presented [14]. In this paper we present a definition of a new quantum dot cell. The cell extends the classical one in the sense of a larger set of stable states, which enables multi-valued processing [3]. The newly introduced cell is employed to implement the ternary logic functions that were originally presented by J. \u0141ukasiewicz [1]. It turns out that in order to implement the desired ternary logic functions only minor modifications of the QCA structures, which implement the binary logic functions, are necessary. The suggested solution is interesting in the sense of a possible 'bottom-up' approach to the design of computer structures of the future. Indeed, until now the implementational part of the design of future computer structures has mostly been an analytical 'top-down' approach, based on material science and constant miniaturisation. As in the sense of miniaturisation the technological limits will soon be reached (and in the sense of safety and ethics maybe even breached [9]) we believe the proposed solution is a possible approach for switching from the analytical 'top-down' to the synthetic 'bottom-up' concept for the design of computer structures of the future.", "Language": "en", "Citations": "0"},
{"Title": "Pseudo 1-homogeneous distance-regular graphs", "Authors": ["Jurisic A.", "Terwilliger P."], "Keywords": ["1-homogeneous property", "Cosine sequence", "Distance-regular graphs", "Locally strongly regular", "Primitive idempotents", "Pseudo 1-homogeneous", "Pseudo primitive idempotent", "Tight distance-regular graph", "Tight edges"], "Date": "2008", "Abstract": "Let \u0393 be a distance-regular graph of diameter d \u2265 2 and a ", "Language": "en", "Citations": "2"},
{"Title": "Fast dependence analysis in a multimedia vectorizing compiler", "Authors": ["Bulic P.", "Gustin V."], "Keywords": [], "Date": "2004", "Abstract": "There are a number of data dependence tests that have been proposed in the literature. In each test there is a different trade-off between accuracy and efficiency. The most widely used approximate data dependence tests are the Banerjee inequality and the GCD test; whereas the Omega test is a well-known exact data dependence test. In this paper we consider parallelization for microprocessors with a multimedia extension (the short SIMD execution model). For the short SIMD parallelism extraction it is essential that, if dependency exists, then the dependence distance is greater than or equal to the number of data processed in the SIMD register. This implies that some loops that could not be vectorized on traditional vector processors can still be parallelized for the short SIMD execution. In all of these tests the parallelization would be prohibited when actually there is no parallelism restriction relating to the short SIMD execution model. In this paper we present a new, fast data dependence test for array references with linear subscripts, which is used in a vectorizing compiler for microprocessors with a multimedia extension. Our test is suitable for use in a dependence analyser that is organized as a series of tests, progressively increasing in accuracy, as a replacement for the GCD or Banerjee tests.", "Language": "en", "Citations": "1"},
{"Title": "Ordinal evaluation: A new perspective on country images", "Authors": ["Robnik-Sikonja M.", "Brijs K.", "Vanhoof K."], "Keywords": [], "Date": "2009", "Abstract": "We present a novel use of ordinal evaluation (OrdEval) algorithm as a promising technique to study various marketing phenomena. OrdEval algorithm has originated in data mining and is a general tool to analyze data with ordinal attributes, including surveys. Its many favorable features, including context sensitivity, ability to exploit meaning of ordered features and ordered response, and robustness to noise and missing values in the data, offer marketing practitioners a perspective, not available with classical analytical toolbox. We present a case study applying OrdEval algorithm on country-of-origin (COO) information. We demonstrate some interesting advantages it has to offer and show how to extract and interpret new insights allowing marketing practitioners to further optimize the management of products abroad. Data for the empirical study was gathered by means of 1225 questionnaires. Results indicate that, contrary to the classical view on COO-effects, the processing of country-related cognitions, affects and conations is a non-linear and asymmetric phenomenon. The practical implications of this finding for marketers are discussed more in detail. \u00a9 2009 Springer Berlin Heidelberg.", "Language": "en", "Citations": "2"},
{"Title": "Dimensionality reduction for distributed vision systems using random projection", "Authors": ["Sulic V.", "Pers J.", "Kristan M.", "Kovacic S."], "Keywords": ["Dimensionality reduction", "Distributed vision systems", "Random projection"], "Date": "2010", "Abstract": "Dimensionality reduction is an important issue in the context of distributed vision systems. Processing of dimensionality reduced data requires far less network resources (e.g., storage space, network bandwidth) than processing of original data. In this paper we explore the performance of the random projection method for distributed smart cameras. In our tests, random projection is compared to principal component analysis in terms of recognition efficiency (i.e., object recognition). The results obtained on the COIL-20 image data set show good performance of the random projection in comparison to the principal component analysis, which requires distribution of a subspace and therefore consumes more resources of the network. This indicates that random projection method can elegantly solve the problem of subspace distribution in embedded and distributed vision systems. Moreover, even without explicit orthogonalization or normalization of random projection transformation subspace, the method achieves good object recognition efficiency. \u00a9 2010 IEEE.", "Language": "en", "Citations": "4"},
{"Title": "Speech/non-speech segmentation based on phoneme recognition features 90495", "Authors": ["Zibert J.", "Pavesic N.", "Mihelic F."], "Keywords": [], "Date": "2006", "Abstract": "This work assesses different approaches for speech and non-speech segmentation of audio data and proposes a new, high-level representation of audio signals based on phoneme recognition features suitable for speech/non-speech discrimination tasks. Unlike previous model-based approaches, where speech and non-speech classes were usually modeled by several models, we develop a representation where just one model per class is used in the segmentation process. For this purpose, four measures based on consonant-vowel pairs obtained from different phoneme speech recognizers are introduced and applied in two different segmentation-classification frameworks. The segmentation systems were evaluated on different broadcast news databases. The evaluation results indicate that the proposed phoneme recognition features are better than the standard mel-frequency cepstral co-efficients and posterior probability-based features (entropy and dynamism). The proposed features proved to be more robust and less sensitive to different training and unforeseen conditions. Additional experiments with fusion models based on cepstral and the proposed phoneme recognition features produced the highest scores overall, which indicates that the most suitable method for speech/non-speech segmentation is a combination of low-level acoustic features and high-level recognition features.", "Language": "en", "Citations": "13"},
{"Title": "Influence of domain and model properties on the reliability rstimates' performance", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": ["Accuracy", "Prediction error", "Regression", "Reliability", "Reliability estimate"], "Date": "2009", "Abstract": "In machine learning, the reliability estimates for individual predictions provide more information about individual prediction error than the average accuracy of predictive model (e.g. relative mean squared error). Such reliability estimates may represent decisive information in the risk-sensitive applications of machine learning (e.g. medicine, engineering, and business), where they enable the users to distinguish between more and less reliable predictions. In the authors' previous work they proposed eight reliability estimates for individual examples in regression and evaluated their performance. The results showed that the performance of each estimate strongly varies depending on the domain and regression model properties. In this paper they empirically analyze the dependence of reliability estimates' performance on the data set and model properties. They present the results which show that the reliability estimates perform better when used with more accurate regression models, in domains with greater number of examples and in domains with less noisy data. Copyright \u00a9 2009, IGI Global.", "Language": "en", "Citations": "0"},
{"Title": "Evolution of collective behaviour in an artificial world using linguistic fuzzy rule-based systems", "Authors": ["Demsar J.", "Bajec I.L."], "Keywords": [], "Date": "2017", "Abstract": "Collective behaviour is a fascinating and easily observable phenomenon, attractive to a wide range of researchers. In biology, computational models have been extensively used to investigate various properties of collective behaviour, such as: transfer of information across the group, benefits of grouping (defence against predation, foraging), group decision-making process, and group behaviour types. The question why,' however remains largely unanswered. Here the interest goes into which pressures led to the evolution of such behaviour, and evolutionary computational models have already been used to test various biological hypotheses. Most of these models use genetic algorithms to tune the parameters of previously presented non-evolutionary models, but very few attempt to evolve collective behaviour from scratch. Of these last, the successful attempts display clumping or swarming behaviour. Empirical evidence suggests that in fish schools there exist three classes of behaviour; swarming, milling and polarized. In this paper we present a novel, artificial lifelike evolutionary model, where individual agents are governed by linguistic fuzzy rule-based systems, which is capable of evolving all three classes of behaviour.", "Language": "en", "Citations": "2"},
{"Title": "Breast reconstruction following mastectomy for invasive breast cancer by free flaps from the abdomen is oncologically safe", "Authors": ["Snoj M.", "Arnez Z.M.", "Sadikov A.", "Suvorov N."], "Keywords": ["Breast cancer", "Reconstruction"], "Date": "2007", "Abstract": "Aims: To report the long-term results of oncological safety of breast reconstruction by autologous tissue following mastectomy for invasive breast cancer. Methods: One-hundred-fifty-six consecutive patients with invasive breast cancer treated with mastectomy and reconstruction by autologous tissue were reviewed throughout (from 1987 to 2003 with median follow up time of 66 months). Results: Median patient age was 45.9 years (range 26-68). The 157 observed tumors had mean diameter of 25 \u00b1 19 mm, 70 of them were poorly differentiated, and 137 were invasive ductal carcinoma. Multifocal disease was present in 44 patients. Breast reconstruction was carried out only by autologous tissue (free flaps were used in 95% and free TRAM flap transfer was the most common reconstructive procedure). There was only one local recurrence as first site of recurrence, thus yielding a local recurrence rate of 0.6%. Conclusions: Breast reconstruction by autologous tissue following mastectomy for invasive breast cancer is an oncologically safe procedure. \u00a9 2006 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "15"},
{"Title": "Discovery of abstract concepts by a robot", "Authors": ["Bratko I."], "Keywords": ["Autonomous discovery", "discovery of abstract concepts", "inductive logic programming", "robot learning"], "Date": "2010", "Abstract": "This paper reviews experiments with an approach to discovery through robot's experimentation in its environment. In addition to discovering laws that enable predictions, we are particularly interested in the mechanisms that enable the discovery of abstract concepts that are not explicitly observable in the measured data, such as the notions of a tool or stability. The approach is based on the use of Inductive Logic Programming. Examples of actually discovered abstract concepts in the experiments include the concepts of a movable object, an obstacle and a tool. \u00a9 2010 Springer-Verlag.", "Language": "en", "Citations": "2"},
{"Title": "Methotrexate reduces HbA1c concentration but does not produce chronic accumulation of ZMP in patients with rheumatoid or psoriatic arthritis", "Authors": ["Perdan-Pirkmajer K.", "Pirkmajer S.", "Thevis M.", "Thomas A.", "Praprotnik S.", "Hocevar A.", "Rotar Z.", "Gaspersic N.", "Sodin-Semrl S.", "Zibert J.", "Omersel J.", "Chibalin A.", "Tomsic M.", "Ambrozic A."], "Keywords": [], "Date": "2016", "Abstract": "Objectives: The mechanism by which methotrexate (MTX) improves glucose homeostasis in patients with rheumatoid (RA) and psoriatic arthritis (PsA) remains undetermined. Animal studies indicate a role for intracellular accumulation of 5-aminoimidazole-4-carboxamide-1-\u03b2-d-ribofuranosyl 5\u02b9-monophosphate (ZMP) but this has not been directly demonstrated in humans. We explored whether accumulation of ZMP is associated with improvements in glucose homeostasis during MTX therapy. Method: MTX-na\u00efve, non-diabetic RA (n\u00a0=\u00a016) and PsA (n\u00a0=\u00a010) patients received uninterrupted MTX treatment for 6\u00a0months. To evaluate whether ZMP accumulated during MTX therapy, we measured the concentration of ZMP in erythrocytes and the concentration of its dephosphorylated derivative 5-aminoimidazole-4-carboxamide-1-\u03b2-d-ribofuranoside (AICAR) in urine using liquid chromatography mass spectrometry (LC-MS/MS). To assess glucose homeostasis, we determined the concentration of glycated haemoglobin (HbA1c) and homeostasis model assessment of insulin resistance [HOMA-IR: fasting glucose (mmol/L) \u00d7 fasting insulin (\u03bcU/mL)/22.5]. Results: Erythrocyte ZMP and urinary AICAR concentrations did not increase during 6 months of MTX therapy. HbA1c concentration was reduced from 5.80 \u00b1 0.29% at baseline to 5.51 \u00b1 0.32% at 6 months (p\u00a0<\u00a00.001), while HOMA-IR remained unaltered. Reduction in HbA1c concentration was not associated with increased ZMP or AICAR concentrations. Conclusions: MTX therapy probably does not produce a chronic increase in erythrocyte ZMP or urinary AICAR concentrations. Collectively, our data do not support the hypothesis that MTX improves glucose homeostasis through chronic accumulation of ZMP.", "Language": "en", "Citations": "5"},
{"Title": "Rigidity and separation indices of graphs in surfaces", "Authors": ["Fijavz G.", "Mohar B."], "Keywords": ["5-Connected graph", "Polyhedral embedding", "Rigidity index", "Separation index"], "Date": "2010", "Abstract": "Let \u03a3 be a surface. We prove that rigidity indices of graphs which admit a polyhedral embedding in \u03a3 and 5-connected graphs admitting an embedding in \u03a3 are bounded by a constant depending on \u03a3. Moreover if the Euler characteristic of \u03a3 is negative, then the separation index of graphs admitting a polyhedral embedding in \u03a3 is also bounded. As a side result we show that distinguishing number of both \u03a3-polyhedral and 5-connected graphs which admit and embedding in \u03a3 is also bounded. \u00a9 2010 Springer.", "Language": "en", "Citations": "2"},
{"Title": "Teaching user stories within the scope of a software engineering capstone course: Analysis of students' opinions", "Authors": ["Mahnic V.", "Hovelja T."], "Keywords": ["Agile methods", "Scrum", "Software engineering", "User stories"], "Date": "2014", "Abstract": "Agile software development methods assume that user requirements are formulated as short user stories written on paper note cards. Students often seem to be suspicious about this approach, finding user stories not precise enough to describe the desired functionality. Therefore, practical experience is needed to overcome initial doubts and impart good understanding of the potential benefits and limitations. This paper describes how user stories are taught within the scope of the software engineering capstone course at the University of Ljubljana, Slovenia, and provides an in-depth analysis of students' opinions on the basis of several surveys that have been conducted since the 2009/10 academic year. The analysis indicates that students' opinions are mostly positive and significantly improve after they gain more experience. Students successfully grasp the main concepts and understand the advantages and limitations of user stories. However, better students are more confident about potential benefits and keener to use user stories in practice. Students' satisfaction can be largely attributed to proper instruction of the course, which stimulates learning through problem solving and requires close cooperation among students, the Product Owner, and the ScrumMaster. \u00a9 2014 TEMPUS Publications.", "Language": "en", "Citations": "11"},
{"Title": "Towards an Environment for Efficient and Transparent Virtual Machine Operations: The ENTICE Approach", "Authors": ["Kimovski D.", "Saurabh N.", "Gec S.", "Stefanic P.", "Kecskemeti G.", "Stankovski V.", "Prodan R.", "Fahringer T."], "Keywords": [], "Date": "2016", "Abstract": "Cloud computing is based on Virtual Machines (VM) or containers, which provide their own software execution environment that can be deployed by facilitating technologies on top of various physical hardware. The use of VMs or containers represents an efficient way to automatize the overall software engineering and operation life-cycle. Some of the benefits include elasticity and high scalability, which increases the utilization efficiency and decreases the operational costs. VMs or containers as software artifacts are created using provider-specific templates and are stored in proprietary or public repositories for further use. However, technology specific choices may reduce their portability, lead to a vendor lock-in, particularly when applications need to run in federated Clouds. In this paper we present the current state of development of the novel concept of a VM repository and operational environment for federated Clouds named ENTICE. The ENTICE environment has been designed to receive unmodified and functionally complete VM images from its users, and transparently tailor and optimise them for specific Cloud infrastructures with respect to their size, configuration, and geographical distribution, such that they are loaded, delivered, and executed faster and with improved QoS compared to their current behaviour. Furthermore, in this work a specific use case scenario for the ENTICE environment has been provided and the underlying novel technologies have been presented.", "Language": "en", "Citations": "1"},
{"Title": "Qualitative planning of object pushing by a robot", "Authors": ["Soberl D.", "Zabkar J.", "Bratko I."], "Keywords": [], "Date": "2015", "Abstract": "Pushing is often used by robots as a simple way to manipulate the environment and has in the past been well studied from kinematic and numerical perspective. The paper proposes a qualitative approach to pushing convex polygonal objects by a simple wheeled robot through a single point contact. We show that by using qualitative reasoning, pushing dynamics can be described in concise and intuitive manner, that is still sufficient to control the robot to successfully manipulate objects. Using the QUIN program on numerical data collected by our robot while experimentally pushing objects of various shapes, we induce a model of pushing. This model is then used by our planning algorithm to push objects of previously unused shapes to given goal configurations. The produced trajectories are compared to smooth geometric solutions. Results show the correctness of our qualitative model of pushing and efficiency of the planning algorithm.", "Language": "en", "Citations": "1"},
{"Title": "The Thermal Infrared Visual Object Tracking VOT-TIR2015 Challenge Results", "Authors": ["Felsberg M.", "Berg A.", "Hager G.", "Ahlberg J.", "Kristan M.", "Matas J.", "Leonardis A.", "Cehovin L.", "Fernandez G.", "Vojir T.", "Nebehay G.", "Pflugfelder R.", "Lukezic A.", "Garcia-Martin A.", "Saffari A.", "Li A.", "Montero A.S.", "Zhao B.", "Schmid C.", "Chen D.", "Du D.", "Khan F.S.", "Porikli F.", "Zhu G.", "Zhu G.", "Lu H.", "Kieritz H.", "Li H.", "Qi H.", "Jeong J.-C.", "Cho J.-I.", "Lee J.-Y.", "Zhu J.", "Li J.", "Feng J.", "Wang J.", "Kim J.-W.", "Lang J.", "Martinez J.M.", "Xue K.", "Alahari K.", "Ma L.", "Ke L.", "Wen L.", "Bertinetto L.", "Danelljan M.", "Arens M.", "Tang M.", "Chang M.-C.", "Miksik O.", "Torr P.H.S.", "Martin-Nieto R.", "Laganiere R.", "Hare S.", "Lyu S.", "Zhu S.-C.", "Becker S.", "Hicks S.L.", "Golodetz S.", "Choi S.", "Wu T.", "Hubner W.", "Zhao X.", "Hua Y.", "Li Y.", "Lu Y.", "Li Y.", "Yuan Z.", "Hong Z."], "Keywords": ["Australia", "Benchmark testing", "Cameras", "Object tracking", "Target tracking", "Visualization"], "Date": "2015", "Abstract": "The Thermal Infrared Visual Object Tracking challenge 2015, VOT-TIR2015, aims at comparing short-term single-object visual trackers that work on thermal infrared (TIR) sequences and do not apply pre-learned models of object appearance. VOT-TIR2015 is the first benchmark on short-term tracking in TIR sequences. Results of 24 trackers are presented. For each participating tracker, a short description is provided in the appendix. The VOT-TIR2015 challenge is based on the VOT2013 challenge, but introduces the following novelties: (i) the newly collected LTIR (Link - ping TIR) dataset is used, (ii) the VOT2013 attributes are adapted to TIR data, (iii) the evaluation is performed using insights gained during VOT2013 and VOT2014 and is similar to VOT2015.", "Language": "en", "Citations": "19"},
{"Title": "Algorithm for synchronization of electroporation pulse delivery with electrocardiogram", "Authors": ["Music B.", "Jarm T.", "Jager F.", "Miklavcic D."], "Keywords": [], "Date": "2003", "Abstract": "Electrochemotherapy is a local treatment of tumors in which high voltage electric pulses are used for permeabilization of cell membrane (electroporation) thus enabling increased entry of chemotherapeutic molecules into tumor cells. Application of electroporation pulses to tumors located close to the heart muscle could lead to fibrillation of the heart if the pulses are delivered during the vulnerable period of the heart or if the pulses coincide with heart arrhythmias due to lowered threshold for fibrillation. We present an algorithm based on the analysis of a single-lead electrocardiogram. The algorithm enables safe use of electrochemotherapy by allowing the electroporation pulses to be delivered only outside the vulnerable period and by preventing generation of the pulses in the presence of arrhythmias. The algorithm synchronizes delivery of electroporation pulses with normal heartbeats in real time. The performance of the algorithm was evaluated using 13 records of the European Society of Cardiology ST-T Database. Performance measures of the algorithm were calculated based on beat-by-beat comparison of the results of the algorithm with true heart beat annotations of signals of the database. In further studies we plan to implement the algorithm in instruments for clinical electroporation. Thus the usefulness of electrochemotherapy will be extended due to higher level of safety for patients.", "Language": "en", "Citations": "0"},
{"Title": "Two simple algorithms for document image preprocessing: Making a document scanning application more user-friendly", "Authors": ["Jaklic A.", "Vrabec B."], "Keywords": ["Detection", "Document scanning", "Hough transform", "Orientation", "Position"], "Date": "2005", "Abstract": "Automatic Document scanning is a useful part of an information system at personal identification checkpoints such as airports, border crossings, banks etc. Current applications usually require a great deal of carefulness of the scanner operators - the document has to be positioned horizontally and special care must be taken to detect corrupt scans that can occur. In this work we describe ideas for two independent algorithms for the document rotation correction and automatic detection of corrupt scans. One algorithm relies on the Hough transformation and the other on brightness gradient of the image. The output of each algorithm is a cropped image of the document in horizontal orientation, which can be used as input for further processing (such as OCR). Also the estimate of scan corruption is returned. Also shown are some testing results of the algorithm prototypes written in MATLAB environment.", "Language": "en", "Citations": "0"},
{"Title": "Polyketide synthase genes and the natural products potential of Dictyostelium discoideum", "Authors": ["Zucko J.", "Skunca N.", "Curk T.", "Zupan B.", "Long P.F.", "Cullum J.", "Kessin R.H.", "Hranueli D."], "Keywords": [], "Date": "2007", "Abstract": "Motivation: The genome of the social amoeba Dictyostelium discoideum contains an unusually large number of polyketide synthase (PKS) genes. An analysis of the genes is a first step towards understanding the biological roles of their products and exploiting novel products. Results: A total of 45 Type I iterative PKS genes were found, 5 of which are probably pseudogenes. Catalytic domains that are homologous with known PKS sequences as well as possible novel domains were identified. The genes often occurred in clusters of 2-5 genes, where members of the cluster had very similar sequences. The D.discoideum PKS genes formed a clade distinct from fungal and bacterial genes. All nine genes examined by RT-PCR were expressed, although at different developmental stages. The promoters of PKS genes were much more divergent than the structural genes, although we have identified motifs that are unique to some PKS gene promoters. \u00a9 The Author 2007. Published by Oxford University Press. All rights reserved.", "Language": "en", "Citations": "26"},
{"Title": "Using association rules mining for sweet potato (Ipomoea batatas L.) in Slovenia: A case study", "Authors": ["Kunstelj N.", "Znidarcic D.", "Ster B."], "Keywords": ["Association rule mining", "Ipomoea batatas", "Questionnaire", "Slovenia", "Sweet potato"], "Date": "2013", "Abstract": "The study presented the association rule mining in order to analyse the knowledge about sweet potato (Ipomoea batatas L.) in Slovenia. For this study, web survey was carried out between 1", "Language": "en", "Citations": "5"},
{"Title": "k-Same-Net: K-Anonymity with generative deep neural networks for face deidentification", "Authors": ["Meden B.", "Emersic Z.", "Struc V.", "Peer P."], "Keywords": ["Face deidentification", "Generative neural networks", "K-Same algorithm"], "Date": "2018", "Abstract": "Image and video data are today being shared between government entities and other relevant stakeholders on a regular basis and require careful handling of the personal information contained therein. A popular approach to ensure privacy protection in such data is the use of deidentification techniques, which aim at concealing the identity of individuals in the imagery while still preserving certain aspects of the data after deidentification. In this work, we propose a novel approach towards face deidentification, called k-Same-Net, which combines recent Generative Neural Networks (GNNs) with the well-known k-Anonymitymechanism and provides formal guarantees regarding privacy protection on a closed set of identities. Our GNN is able to generate synthetic surrogate face images for deidentification by seamlessly combining features of identities used to train the GNN model. Furthermore, it allows us to control the image-generation process with a small set of appearance-related parameters that can be used to alter specific aspects (e.g., facial expressions, age, gender) of the synthesized surrogate images. We demonstrate the feasibility of k-Same-Net in comprehensive experiments on the XM2VTS and CK+ datasets. We evaluate the efficacy of the proposed approach through reidentification experiments with recent recognition models and compare our results with competing deidentification techniques from the literature. We also present facial expression recognition experiments to demonstrate the utility-preservation capabilities of k-Same-Net. Our experimental results suggest that k-Same-Net is a viable option for facial deidentification that exhibits several desirable characteristics when compared to existing solutions in this area.", "Language": "en", "Citations": "4"},
{"Title": "Argument based machine learning applied to law", "Authors": ["Mozina M.", "Zabkar J.", "Bench-Capon T.", "Bratko I."], "Keywords": ["Argumentation", "CN2", "Legal information systems", "Legal knowledge discovery", "Machine learning", "Rule induction"], "Date": "2005", "Abstract": "In this paper we discuss the application of a new machine learning approach - Argument Based Machine Learning - to the legal domain. An experiment using a dataset which has also been used in previous experiments with other learning techniques is described, and comparison with previous experiments made. We also tested this method for its robustness to noise in learning data. Argumentation based machine learning is particularly suited to the legal domain as it makes use of the justifications of decisions which are available. Importantly, where a large number of decided cases are available, it provides a way of identifying which need to be considered. Using this technique, only decisions which will have an influence on the rules being learned are examined. \u00a9 Springer 2006.", "Language": "en", "Citations": "18"},
{"Title": "Final year medical students' understanding of family medicine.", "Authors": ["Petek Ster M.", "Svab I.", "Ster B."], "Keywords": [], "Date": "2014", "Abstract": "The European Academy of Teachers in General Practice / Family Medicine (EURACT) has developed an educational agenda, the key document for teaching family medicine in Europe. The aim of our study was to find out how final year medical students at the beginning of their family medicine clerkship understand the discipline of family medicine. The attitudes toward family medicine were paraphrased and developed into a 164-item questionnaire, which was administered to 335 final-year medical students at the beginning of their clerkship. Using combinatorial optimization with genetic algorithms we selected 30 items which yielded the highest Cronbach alpha reliability coefficient. Finally, we performed a factor analysis to find which dimensions of family medicine were recognised by the students and compared them with the domains defined in the EURACT definition. The 30-item questionnaire had a Cronbach alpha reliability coefficient of 0.919. The differences between male and female students were not very significant (p=0.061). With the factor analysis we recognised seven factors, belonging to three out of six domains of the EURACT educational agenda: primary care management, personcenteredness and comprehensive approach. Final-year medical students at the beginning of their family medicine clerkship understand some of the dimensions of family medicine rather well, but they are not aware of some important competences of family doctors. There is a necessity to teach students about specific problem solving skills and the importance of balance between the health needs of an individual patient and the community. Copyright \u00a9 2014 by Academy of Sciences and Arts of Bosnia and Herzegovina.", "Language": "en", "Citations": "6"},
{"Title": "Training Convolutional Neural Networks with Limited Training Data for Ear Recognition in the Wild", "Authors": ["Emersic Z.", "Stepec D.", "Struc V.", "Peer P."], "Keywords": [], "Date": "2017", "Abstract": "Identity recognition from ear images is an active field of research within the biometric community. The ability to capture ear images from a distance and in a covert manner makes ear recognition technology an appealing choice for surveillance and security applications as well as related application domains. In contrast to other biometric modalities, where large datasets captured in uncontrolled settings are readily available, datasets of ear images are still limited in size and mostly of laboratory-like quality. As a consequence, ear recognition technology has not benefited yet from advances in deep learning and convolutionalneural networks (CNNs) and is still lacking behind other modalities that experienced significant performance gains owing to deep recognition technology. In this paper we address this problem and aim at building a CNNbased ear recognition model. We explore different strategies towards model training with limited amounts of training data and show that by selecting an appropriate model architecture, using aggressive data augmentation and selective learning on existing (pre-trained) models, we are able to learn an effective CNN\u00b7based model using a little more than 1300training images. The result of our work is the first CNN\u00b7based approach to ear recognition that is also made publicly available to the research community. With our model we are able to improve on the rank one recognition rate of the previous state-of-the-art by more than 25% on a challenging dataset of ear images captured from the web (a.k.a, in the wild).", "Language": "en", "Citations": "16"},
{"Title": "Syntax-based analysis of programming concepts in python", "Authors": ["Mozina M.", "Lazar T."], "Keywords": ["Abstract syntax tree", "Educational data analysis", "Error diagnosis", "Learning programming", "Tree regular expressions"], "Date": "2018", "Abstract": "Writing programs is essential to learning programming. Most programming courses encourage students to practice with lab and homework assignments. By analyzing solutions to these exercises teachers can discover mistakes and concepts students are struggling with, and use that knowledge to improve the course. Students however tend to submit many different programs even for simple exercises, making such analysis difficult. We propose using tree regular expressions to encode common patterns in programs. Based on these patterns we induce rules describing common approaches and mistakes for a given assignment. In this paper we present a case study of rule-based analysis for an introductory Python exercise. We show that our rules are easy to interpret, and can be learned from a relatively small set of programs.", "Language": "en", "Citations": "0"},
{"Title": "Motor memory: Representation, learning and consolidation", "Authors": ["Zabkar J.", "Leonardis A."], "Keywords": ["Compositional hierarchy", "Deep learning", "Motor memory", "Motor memory consolidation"], "Date": "2016", "Abstract": "An efficient representation of motor system is vital to robot control and its ability to learn new skills. While the increasing sensor accuracy and the speed of signal processing failed to bridge the gap between the performance of artificial and human sensorimotor systems, the motor memory architecture seems to remain neglected. Despite the advances in robot skill learning, the latter remains limited to predefined tasks and pre-specified embodiment. We propose a new motor memory architecture that enables information sharing between different skills, on-line learning and off-line memory consolidation. We develop an algorithm for learning and consolidation of motor memory and study the space complexity of the representation in the experiments with humanoid robot Nao. Finally, we propose the integration of motor memory with sensor data into a common sensorimotor memory.", "Language": "en", "Citations": "0"},
{"Title": "An Efficient HOS-Based Gait Authentication of Accelerometer Data", "Authors": ["Sprager S.", "Juric M.B."], "Keywords": ["accelerometer", "Gait analysis", "gait authentication", "higher-order cumulants", "higher-order statistics", "inertial sensors"], "Date": "2015", "Abstract": "We propose a novel efficient and reliable gait authentication approach. It is based on the analysis of accelerometer signals using higher order statistics. Gait patterns are obtained by transformation of acceleration data in feature space represented with higher order cumulants. The proposed approach is able to operate on multichannel and multisensor data by combining feature-level and sensor-level fusion. Evaluation of the proposed approach was performed using the largest currently available data set OU-ISIR containing inertial data of 744 subjects. Authentication was performed by cross-comparison of gallery and probe gait patterns transformed in feature space. In addition, the proposed approach was evaluated using data set collected by McGill University, containing long-sequence acceleration signals of 20 subjects acquired by smartphone during casual walking. The results have shown an average equal error rate of 6% to 12%, depending on the selected experimental parameters and setup. When compared with the latest state of the art, evaluated performance reveal the proposed approach as one of the most efficient and reliable of the currently available accelerometer-based gait authentication approaches.", "Language": "en", "Citations": "15"},
{"Title": "Conceptual model for socio-technical evaluation of software development methodologies", "Authors": ["Vavpotic D.", "Bajec M.", "Krisper M."], "Keywords": [], "Date": "2005", "Abstract": "Today, many different software development methodologies (SDM) exist that should, as theory asserts, pave the way for more efficient software development and higher quality software. Interestingly, however, in practice not many software development organisations use SDM. Two important factors can be identified that at lest partially explain the phenomenon: methodologies are not attuned to the actual organisation and project needs, and methodologies do not fit the specific social and cultural characteristics of the development team and the organisation. The paper presents a conceptual model that forms basis for measuring SDM suitability considering both factors, and is based on two research areas, one socially oriented (diffusion and adoption of software process innovations) and the other technically oriented (situational method engineering). To show the practical value of the model we tested it in a software development company.", "Language": "en", "Citations": "0"},
{"Title": "dictyBase - A Dictyostelium bioinformatics resource update", "Authors": ["Fey P.", "Gaudet P.", "Curk T.", "Zupan B.", "Just E.M.", "Basu S.", "Merchant S.N.", "Bushmanova Y.A.", "Shaulsky G.", "Kibbe W.A.", "Chisholm R.I."], "Keywords": [], "Date": "2009", "Abstract": "dictyBase (http://dictybase.org) is the model organism database for Dictyostelium discoideum. It houses the complete genome sequence, ESTs and the entire body of literature relevant to Dictyostelium. This information is curated to provide accurate gene models and functional annotations, with the goal of fully annotating the genome. This dictyBase update describes the annotations and features implemented since 2006, including improved strain and phenotype representation, integration of predicted transcriptional regulatory elements, protein domain information, biochemical pathways, improved searching and a wiki tool that allows members of the research community to provide annotations. \u00a9 2008 The Author(s).", "Language": "en", "Citations": "53"},
{"Title": "The unconstrained ear recognition challenge", "Authors": ["Emersic Z.", "Stepec D.", "Struc V.", "Peer P.", "George A.", "Ahmad A.", "Omar E.", "Boult T.E.", "Safdaii R.", "Zhou Y.", "Zafeiriou S.", "Yaman D.", "Eyiokur F.I.", "Ekenel H.K."], "Keywords": [], "Date": "2017", "Abstract": "In this paper we present the results of the Unconstrained Ear Recognition Challenge (UERC), a group benchmarking effort centered around the problem of person recognition from ear images captured in uncontrolled conditions. The goal of the challenge was to assess the performance of existing ear recognition techniques on a challenging large-scale dataset and identify open problems that need to be addressed in the future. Five groups from three continents participated in the challenge and contributed six ear recognition techniques for the evaluation, while multiple baselines were made available for the challenge by the UERC organizers. A comprehensive analysis was conducted with all participating approaches addressing essential research questions pertaining to the sensitivity of the technology to head rotation, flipping, gallery size, large-scale recognition and others. The top performer of the UERC was found to ensure robust performance on a smaller part of the dataset (with 180 subjects) regardless of image characteristics, but still exhibited a significant performance drop when the entire dataset comprising 3,704 subjects was used for testing.", "Language": "en", "Citations": "4"},
{"Title": "Multithreaded processors", "Authors": ["Ungerer T.", "Robic B.", "Silc J."], "Keywords": [], "Date": "2002", "Abstract": "The instruction-level parallelism found in a conventional instruction stream is limited. Studies have shown the limits of processor utilization even for today's superscalar microprocessors. One solution is the additional utilization of more coarse-grained parallelism. The main approaches are the (single) chip multiprocessor and the multithreaded processor which optimize the throughput of multiprogramming workloads rather than single-thread performance. The chip multiprocessor integrates two or more complete processors on a single chip. Every unit of a processor is duplicated and used independently of its copies on the chip. In contrast, the multithreaded processor is able to pursue two or more threads of control in parallel within the processor pipeline. Unused instruction slots, which arise from pipelined execution of single-threaded programs by a contemporary microprocessor, are filled by instructions of other threads within a multithreaded processor. The execution units are multiplexed between the threads in the register sets. Underutilization of a superscalar processor due to missing instruction-level parallelism can be overcome by simultaneous multithreading, where a processor can issue multiple instructions from multiple threads each cycle. Simultaneous multithreaded processors combine the multithreading technique with a wide-issue superscalar processor such that the full issue bandwidth is utilized by potentially issuing instructions from different threads simultaneously. This survey paper explains and classifies the various multithreading techniques in research and in commercial microprocessors and compares multithreaded processors with chip multiprocessors.", "Language": "en", "Citations": "37"},
{"Title": "Trust management methodologies for the web", "Authors": ["Trcek D."], "Keywords": ["ergonomic methodologies", "qualitative assessment dynamics", "simulation", "trust", "trust management", "web technologies"], "Date": "2011", "Abstract": "Trust and its support with appropriate trust management methodologies and technologies is becoming one crucial element for wider acceptance of web services. In the computing society trust and related issues were addressed already in the nineties of the former century, but the approaches from that period were about security, more precisely security services and security mechanisms. These approaches were followed by more advanced ones, where the first branch was based on Bayesian statistics, the second branch was based on Dempster-Shafer theory of evidence and its successors, most notably subjective logic, and the third branch originated from game theory. It is, however, important to note that at the core of trust there are cognition, assessment processes, and they are governed by various factors. Consequently, trust management methodologies should take these factors, which may ne rational, irrational, contextual, etc., into account. This research contribution will therefore provide an extensive overview of existing methodologies in the computer sciences field, followed by their evaluation in terms of their advantages and disadvantages. Further, some latest experimental results will be given that identify and evaluate some of those most important factors mentioned above. Finally, we will present a new trust management methodology called Qualitative Assessment Dynamics, QAD (aka Qualitative Algebra) that complements existing methodologies mentioned above, and that is aligned with the results of the latest experimental findings. \u00a9 2011 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "5"},
{"Title": "Open and scalable IoT platform and its applications for real time access line monitoring and alarm correlation", "Authors": ["Kos A.", "Pristov D.", "Sedlar U.", "Sterle J.", "Volk M.", "Vidonja T.", "Bajec M.", "Bokal D.", "Bester J."], "Keywords": ["access line monitoring", "event correlation", "IoT platform", "real-time"], "Date": "2012", "Abstract": "We present an open intelligent communication platform that can be used to support various usage scenarios related to future internet of things. Its purpose is twofold: to support fusion of large amounts of data, irrespective of their source or structure and to provide users or devices with semantically analysed and enriched data according to their needs and context. Using the platform, users are able to access enriched data, receive warnings and notifications about events recognised by the system. We present applications for the real time access line monitoring and alarm correlation that respond to telecommunications operators' needs in the field of proactive line measurement in enabling the helpdesk and field technical teams to pinpoint the cause of service degradations. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "10"},
{"Title": "Learning by arguing in argument-based machine learning framework", "Authors": ["Guid M.", "Mozina M.", "Pavlic M.", "Tursic K."], "Keywords": ["Argument-based machine learning", "Automated feedback generation", "Financial statements", "Intelligent tutoring systems", "Learning by arguing"], "Date": "2019", "Abstract": "We propose an approach for the development of argument-based intelligent tutoring systems in which a domain that can be successfully addressed by supervised machine learning is taught in an interactive learning environment. The system is able to automatically select relevant examples and counter-examples to be explained by the students. The students learn by explaining specific examples, and the system provides automated feedback on students\u2019 arguments, including generating hints. The role of an argument-based intelligent tutoring system is then to train the students to find the most relevant arguments. The students learn about the high-level domain concepts and then use them to argue about automatically selected examples. We demonstrate our approach in an online application that allows students to learn through arguments with the goal of improving their understanding of financial statements.", "Language": "en", "Citations": "0"},
{"Title": "Version management of BPEL processes in SOA", "Authors": ["Juric M.B.", "Sasa A."], "Keywords": ["BPEL", "SOA", "Version management"], "Date": "2010", "Abstract": "Version management of WS-BPEL (Business Process Execution Language, BPEL) processes in SOA is not supported in a consistent way. Therefore, in this paper we propose specific extensions for BPEL to support versioning of processes and partner links. We introduce new activities and extends existing activities, including partner links, invoke, receive, import, and on-message activities. We propose version-related extensions to variables and introduces version handlers. The proposed extensions represent a complete solution for process-level and scope-level versioning at development, deployment, and run-time. \u00a9 2010 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Improving probabilistic interpretation of medical diagnoses with multi-resolution image parameterization: A case study", "Authors": ["Kukar M.", "Sajn L."], "Keywords": [], "Date": "2009", "Abstract": "Clinicians strive to improve established diagnostic procedures, especially those that allow them to reach reliable early diagnoses. Diagnostics is frequently performed in a stepwise manner which consists of several consecutive tests (steps). The ultimate step in this process is often the \"gold standard\" reference method. In stepwise testing, results of each diagnostic test can be interpreted in a probabilistic manner by using prior (pre-test) probability and test characteristics (sensitivity and specificity). By using Bayes' formula on these quantities, the posterior (post-test) probability is calculated. If the post-test probability is sufficiently high (or low) to confirm (or exclude) the presence of a disease, diagnostic process is stopped. Otherwise, it proceeds to the next step in sequence. Our case study focuses on improving probabilistic interpretation of scintigraphic images obtained from the penultimate step in coronary artery disease diagnostics. We use automatic image parameterization on multiple resolutions, based on texture description with specialized association rules. Extracted image parameters are combined into more informative composite parameters by means of principle component analysis, and finally used to build automatic classifiers with machine learning methods. Experiments show that the proposed approach significantly increases the number of reliable diagnoses as compared to clinical results in terms. \u00a9 2009 Springer Berlin Heidelberg.", "Language": "en", "Citations": "4"},
{"Title": "Diagnosing organizational risks in software projects: Stakeholder resistance", "Authors": ["Vrhovec S.L.R.", "Hovelja T.", "Vavpotic D.", "Krisper M."], "Keywords": ["Bank", "Risk management", "Software project", "Stakeholder resistance"], "Date": "2015", "Abstract": "Critical success and failure factors of software projects were extensively studied. However, software project risk management has rarely researched organizational risks even though most problems occur when the social aspects are not addressed. By employing the resistance to change theory, our paper develops an organizational risk diagnosing (ORD) framework in order to show how can organizational risks be better understood and managed. Organizational risk factors may have non-trivial underlying root causes. A failure to diagnose them may result in ineffective risk responses that address the symptoms. A case study of a loan application software project has been conducted in one of the biggest banks in South-Eastern Europe. An analysis of the risk management process in the studied case allows a better understanding of organizational risk management.", "Language": "en", "Citations": "20"},
{"Title": "An adaptive genetic algorithm for parameter estimation of biological oscillator models to achieve target quantitative system response", "Authors": ["Strazar M.", "Mraz M.", "Zimic N.", "Moskon M."], "Keywords": ["Adaptive genetic algorithm", "Biological oscillator", "Gene regulatory networks", "Parameter estimation"], "Date": "2014", "Abstract": "Mathematical modeling has become an integral part of synthesizing gene regulatory networks. One of the common problems is the determination of parameters, which are a part of the model description. In the present work, we propose a customized genetic algorithm as a method to determine the parameters such that the underlying oscillatory system exhibits the target behavior. We propose a problem specific, adaptive fitness function evaluation and a method to quantify the effect of a single parameter on the system response. The properties of the algorithm are highlighted and confirmed on two test cases of synthetic biological oscillators. \u00a9 2013 Springer Science+Business Media Dordrecht.", "Language": "en", "Citations": "1"},
{"Title": "Data exchange in computer networks Izmenjava podatkov v ra\u010dunalni\u0161kih omre\u017ejih", "Authors": ["Dobravec T.", "Robic B.", "Vilfan B."], "Keywords": ["Computer networks", "Parallel computing", "Routing"], "Date": "2001", "Abstract": "The communication between computing nodes of a parallel system is of essential importance because data exchange may become a bottleneck in the parallel computation. The performance of the communication depends on network topology, routing model, communication pattern, quality criterion, and routing algorithm. In this paper we focus on the communication patterns and routing algorithms. We describe three classes of routing problems. The first class contains routing problems where several nodes receive and send the same message. Such routing probles are gathering, scattering, and gossiping. The second class contains permutation routing problems, i.e. routing problems where the function that defines packet destinations is a permutation. The third class described consists of isotonic routing problems. Some of these are semicontraction, semiexpansion, packing, and unpacking. We determine lower bounds for the routing complexity as well as its dependence on several factors such as the network diameter, connectio n troughput, availability, and network bisection. In the last part of the paper we give several criteria that can be used to characterize routing algorithms. Finally, we describe several general strategies for solving the routing problems, such as mad-postman algorithm, hot-potato algorithm, and probabilistic algorithms.", "Language": "en", "Citations": "0"},
{"Title": "On different LL and LR parsers used in LLLR parsing", "Authors": ["Slivnik B."], "Keywords": ["Left parse", "LL parsing", "LR languages"], "Date": "2017", "Abstract": "As described in Slivnik (2016), LLLR parsing is a method that parses as much of its input string as possible using the backbone SLL(k) parser and uses small embedded canonical left LR(k) parsers to resolve LL conflicts. Once the LL conflict is resolved, the embedded parser produces the left parse of the substring it has just parsed and passes the control back to the backbone parser together with the information about how the backbone parser should realign its stack as a part of the input has been read by the embedded parser. The LLLR parser produces the left parse of the input string without any backtracking and, if used for a syntax-directed translation, it evaluates semantic actions using the same top-down strategy as the canonical LL(k) parser. In this paper, a more general approach towards LLLR parsing is presented as it is described how any kind of canonical LL(k) or LA(k)LL(k\u2032) parser can be used as the backbone parser and how different kinds of embedded canonical left LR(k) or left LA(k)LR(k\u2032) parsers can be used for LL conflict resolution.", "Language": "en", "Citations": "2"},
{"Title": "Ontology-based multi-agent system to support business users and management Daugiaagentine sistema grind\u017eiama ontologija verslo vartotojams ir vadybininkams paremti", "Authors": ["Lavbic D.", "Rupnik R.", "Vasilecas O."], "Keywords": ["Agent", "Business process management", "Business rules", "Data warehouse", "Decision support", "Information retrieval", "Multi-agent system", "Ontology"], "Date": "2010", "Abstract": "For some decision processes a significant added value is achieved when enterprises' internal Data Warehouse (DW) can be integrated and combined with external data gained from web sites of competitors and other relevant Web sources. In this paper we discuss the agent-based integration approach using ontologies (DSS-MAS). In this approach data from internal DW and external sources are scanned by coordinated group of agents, while semantically integrated and relevant data is reported to business users according to business rules. After data from internal DW, Web sources and business rules are acquired, agents using these data and rules can infer new knowledge and therefore facilitate decision making process. Knowledge represented in enterprises' ontologies is acquired from business users without extensive technical knowledge using user friendly user interface based on constraints and predefined templates. The approach presented in the paper was verified using the case study from the domain of mobile communications with the emphasis on supply and demand of mobile phones.", "Language": "en", "Citations": "15"},
{"Title": "Structural properties of recurrent neural networks", "Authors": ["Dobnikar A.", "Ster B."], "Keywords": ["Complex systems", "Dynamical systems", "Graph theory", "Recurrent neural networks"], "Date": "2009", "Abstract": "In this article we research the impact of the adaptive learning process of recurrent neural networks (RNN) on the structural properties of the derived graphs. A trained fully connected RNN can be converted to a graph by defining edges between pairs od nodes having significant weights. We measured structural properties of the derived graphs, such as characteristic path lengths, clustering coefficients and degree distributions. The results imply that a trained RNN has significantly larger clustering coefficient than a random network with a comparable connectivity. Besides, the degree distributions show existence of nodes with a large degree or hubs, typical for scale-free networks. We also show analytically and experimentally that this type of degree distribution has increased entropy. \u00a9 2009 Springer Science+Business Media, LLC.", "Language": "en", "Citations": "1"},
{"Title": "DEX methodology: Three decades of qualitative multi-attribute modeling", "Authors": ["Bohanec M.", "Znidarzic M.", "Rajkovic V.", "Bratko I.", "Zupan B."], "Keywords": ["Decision support", "Multi-criteria decision making", "Qualitative multi-attribute modeling"], "Date": "2013", "Abstract": "DEX is a qualitative multi-attribute decision modeling methodology that integrates multi-criteria decision modeling with rule-based expert systems. The method was conceived in 1979. Since, it has been continuously developed and implemented in a wide range of computer programs that have been applied in hundreds of practical decision-making studies. Here we present its main methodological concepts, contributions to the theory and practice of decision support, and outline a history of its development and evolution.", "Language": "en", "Citations": "50"},
{"Title": "Panoramic depth imaging with mosaicing Gradnja globinskih panoramskih slik s postopkom mozai\u010denja", "Authors": ["Peer P.", "Solina F."], "Keywords": ["Depth image", "Epipolar geometry", "Mosaicing", "Motion parallax effect", "Panoramic image", "Reconstruction", "Stereo vision"], "Date": "2001", "Abstract": "In this article we present a panoramic depth imaging system. The system is mosaic-based which means that we use a single rotating camera and assemble the captured images in a mosaic. Due to a setoff of the camera's optical center from the rotational center of the system (Fig. 1) we are able to capture the motion parallax effect which enables the stereo reconstruction. The camera is rotating on a circular path with the step defined by an angle \u03b8", "Language": "en", "Citations": "0"},
{"Title": "Deep hierarchies in the primate visual cortex: What can we learn for computer vision?", "Authors": ["Kruger N.", "Janssen P.", "Kalkan S.", "Lappe M.", "Leonardis A.", "Piater J.", "Rodriguez-Sanchez A.J.", "Wiskott L."], "Keywords": ["biological modeling", "Computer vision", "deep hierarchies"], "Date": "2013", "Abstract": "Computational modeling of the primate visual system yields insights of potential relevance to some of the challenges that computer vision is facing, such as object recognition and categorization, motion detection and activity recognition, or vision-based navigation and manipulation. This paper reviews some functional principles and structures that are generally thought to underlie the primate visual cortex, and attempts to extract biological principles that could further advance computer vision research. Organized for a computer vision audience, we present functional principles of the processing hierarchies present in the primate visual system considering recent discoveries in neurophysiology. The hierarchical processing in the primate visual system is characterized by a sequence of different levels of processing (on the order of 10) that constitute a deep hierarchy in contrast to the flat vision architectures predominantly used in today's mainstream computer vision. We hope that the functional description of the deep hierarchies realized in the primate visual system provides valuable insights for the design of computer vision algorithms, fostering increasingly productive interaction between biological and computer vision research. \u00a9 1979-2012 IEEE.", "Language": "en", "Citations": "158"},
{"Title": "hrWaC and slWac: Compiling web corpora for Croatian and Slovene", "Authors": ["Ljubesic N.", "Erjavec T."], "Keywords": ["Croatian", "Slovene", "topic modeling", "web corpus"], "Date": "2011", "Abstract": "Web corpora have become an attractive source of linguistic content, yet are for many languages still not available. This paper introduces two new annotated web corpora: the Croatian hrWaC and the Slovene slWaC. Both were built using a modified standard \"Web as Corpus\" pipeline having in mind the limited amount of available web data. The modifications are described in the paper, focusing on the content extraction from HTML pages, which combines high precision of extracted language content with a decent recall. The paper also investigates text-types of the acquired corpora using topic modeling, comparing the two corpora among themselves and with ukWaC. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "37"},
{"Title": "Reliability estimation of individual multi-target regression predictions", "Authors": ["Jakomin M.", "Bosnic Z."], "Keywords": ["Multi-target regression", "Prediction error", "Reliability estimate", "Supervised learning"], "Date": "2017", "Abstract": "To estimate the quality of the induced predictive model we generally use measures of averaged prediction accuracy, such as the relative mean squared error on test data. Such evaluation fails to provide local information about reliability of individual predictions, which can be important in risk-sensitive fields (medicine, finance, industry etc.). Related work presented several ways for computing individual predic- tion reliability estimates for single-target regression models, but has not considered their use with multi-target regression models that predict a vector of independent target variables. In this paper we adapt the existing single-target reliability estimates to multi-target models. In this way we try to design reliability estimates, which can estimate the prediction errors without knowing true prediction errors, for multi-target regression algorithms, as well. We approach this in two ways: by aggregating reliability estimates for individual target components, and by generalizing the existing reliability estimates to higher number of dimensions. The results revealed favorable performance of the reliability estimates that are based on bagging variance and local cross-validation approaches. The results are consistent with the related work in single-target reliability estimates and provide a support for multi-target decision making.", "Language": "en", "Citations": "0"},
{"Title": "Immediate, lag and time window effects of meteorological factors on ST-elevation myocardial infarction incidence", "Authors": ["Ravljen M.", "Hovelja T.", "Vavpotic D."], "Keywords": ["acute myocardial infarction", "ambient temperature", "humidity", "pressure", "season", "STEMI", "time lag", "Weather"], "Date": "2018", "Abstract": "The influence of several meteorological parameters on acute myocardial infarction (AMI) incidences with immediately and/or delayed effects has been widely reported. It remains unknown whether the individual AMI subtypes reveal similar patterns. To date, generally seasonal variation in ST elevation MI (STEMI) has been investigated. However, these approaches couldn\u2019t detect the effects of changes in multiple meteorological variables on STEMI incidence within a specific season. Therefore, the aim of our study is to explore immediate, delayed and cumulative effects of average daily temperature, atmospheric pressure and humidity on nation-wide STEMI hospital admissions. We linked daily hospitals\u2019 STEMI admission data with meteorological stations\u2019 data according to the patient\u2019s permanent residence. Subsequently, a multivariate analysis based on a main effect generalised linear model, assuming a log-link function with a Poisson distribution, was conducted. With the help of lags, we were able to analyse delayed effects, while the cumulative effects of specific meteorological variables were analysed utilising time windows. As a result, we confirmed immediate and delayed negative effect of low temperature and low relative humidity for all observed lags as well as cumulative average effects of low temperature and low relative humidity for all observed time windows. However, no delayed, single-day effect for atmospheric pressure was detected. Nevertheless, the cumulative average effect was confirmed in all time windows suggesting that prolonged low pressure influences the incidence of STEMI. A novelty of our approach is the comparative examination of immediate, delayed and cumulative effect of specific meteorological variables on the incidence of STEMI. This approach enables us to gain a new insight into the phenomenon studied.", "Language": "en", "Citations": "3"},
{"Title": "Online routing in convex subdivisions", "Authors": ["Bose P.", "Brodnik A.", "Carlsson S.", "Demaine E.D.", "Fleischer R.", "Lopez-Ortiz A.", "Morin P.", "Munro J.I."], "Keywords": ["Computational geometry", "Oblivious algorithms", "Online algorithms", "Routing"], "Date": "2002", "Abstract": "We consider online routing algorithms for finding paths between the vertices of plane graphs. We show (1) there exists a routing algorithm for arbitrary triangulations that has no memory and uses no randomization, (2) no equivalent result is possible for convex subdivisions, (3) there is no competitive online routing algorithm under the Euclidean distance metric in arbitrary triangulations, and (4) there is no competitive online routing algorithm under the link distance metric even when the input graph is restricted to be a Delaunay, greedy, or minimum-weight triangulation.", "Language": "en", "Citations": "25"},
{"Title": "TaskyApp: Inferring task engagement via smartphone sensing", "Authors": ["Urh G.", "Pejovic V."], "Keywords": ["Interruptibility", "Mobile sensing", "Multitasking", "Task engagement inference"], "Date": "2016", "Abstract": "The knowledge of a user's mental involvement, i.e.Task engagement opens up an array of possibilities for a seamles mobile computing device ? human interaction. Today's mos ubiquitous personal sensing devices, such as smartphones are equipped with an array of sensors that may be used t infer different aspects of human behavior. However, inferrin task engagement using smartphone sensors remain unexplored. In this paper we present our initial work on automate task engagement inference using only smartphon sensors. We design, develop and deploy a mobile sensin application TaskyApp, and collect 216 data points of senso readings and task engagement labels from eight users i an office setting. Using machine learning we demonstrat that with up to 67.6% accuracy, relying mostly on the movemen sensors, we can correctly infer a user's task engagement.", "Language": "en", "Citations": "2"},
{"Title": "Explanation of prediction models with explain prediction", "Authors": ["Robnik-Sikonja M."], "Keywords": ["Comprehensibility of models", "Explanation of models", "Machine learning", "Perturbation methods"], "Date": "2018", "Abstract": "State-of-the-art prediction models are getting increasingly complex and incomprehensible for humans. This is problematic for many application areas, especially those where knowledge discovery is just as important as predictive performance, for example medicine or business consulting. As machine learning and artificial intelligence are playing an increasingly large role in the society through data based decision making, this is problematic also from broader perspective and worries general public as well as legislators. As a possible solution, several explanation methods have been recently proposed, which can explain predictions of otherwise opaque models. These methods can be divided into two main approaches: gradient based approaches limited to neural networks, and more general perturbation based approaches, which can be used with arbitrary prediction models. We present an overview of perturbation based approaches, and focus on a recently introduced implementation of two successful methods developed in Slovenia, EXPLAIN and IME. We first describe their working principles and visualizations of explanations, followed by the implementation in ExplainPrediction package for R environment.", "Language": "en", "Citations": "0"},
{"Title": "Management of peer-to-peer systems", "Authors": ["Ciglaric M.", "Vidmar T."], "Keywords": ["Peer-to-peer system", "Peer-to-peer system management", "Policy-based management"], "Date": "2003", "Abstract": "In the near future, peer-to-peer architecture is likely to enter into several new application areas, including e-commerce networking. The paper presents selected management-related issues within the area of peer-to peer systems: management of network load, overhead traffic, message routing, security and anonymity. Environment factors affecting managerial decisions are introduced and suggestions for domain identification are given, followed by a comprehensive list of appropriate policy rules. After that, the paper describes possible actions to be taken in cases when the conditions representing unwanted system behaviour are met.", "Language": "en", "Citations": "0"},
{"Title": "Argument-based machine learning", "Authors": ["Bratko I.", "Zabkar J.", "Mozina M."], "Keywords": [], "Date": "2009", "Abstract": "The most common form of machine learning (ML) is learning from examples, also called inductive learning. Usually the problem of learning from examples is stated as: Given examples, find a theory that is consistent with the examples. We say that such a theory is induced from the examples. Roughly, we say that a theory is consistent with the examples if the examples can be derived from the theory. In the case of learning from imperfect, noisy data, we may not insist on perfect consistency between the examples and the theory. In such cases, a shorter and only \"approximately\" consistent theory may be more appropriate. \u00a9 2009 Springer-Verlag US.", "Language": "en", "Citations": "2"},
{"Title": "Investigation of developer's perceptions in xml schema development using textual and visual tool types", "Authors": ["Pusnik M.", "Pulko K.H.", "Hericko M.", "Juric M.B.", "Sumak B."], "Keywords": ["XML documents", "XML Schemas", "XML supporting tools"], "Date": "2014", "Abstract": "This paper analyses the influence of different tool types (visual or textual) on a developer's perception of efficiency during XML Schema development. We conducted a controlled experiment that focused on discovering which XML Schema development tool type enables better efficiency and also engenders a friendlier environment for developers while developing XML Schemas. The experiment was conducted with 240 participants and divided into two practical parts (visually developing an XML Schema and manually using a mark-up language intelligent textual editor). After the experiment, the participants' opinions were gathered via a web survey. In the survey, a technology acceptance model (TAM) was used as the basis for constructing the measurement items in order to answer the following questions: (1) which tool type is preferred and perceived as better, and (2) which variables influence the user's perceptions and decisions. In this study, we searched for an optimal way of building XML Schemas. The general tendency of most participants was towards a visual tool, suggesting that visual support is perceived as more useful, and can create better results with less effort. \u00a9 2014 World Scientific Publishing Company.", "Language": "en", "Citations": "1"},
{"Title": "SNPsyn: Detection and exploration of SNP-SNP interactions", "Authors": ["Curk T.", "Rot G.", "Zupan B."], "Keywords": [], "Date": "2011", "Abstract": "SNPsyn (http://snpsyn.biolab.si) is an interactive software tool for the discovery of synergistic pairs of single nucleotide polymorphisms (SNPs) from large genome-wide case-control association studies (GWAS) data on complex diseases. Synergy among SNPs is estimated using an information-theoretic approach called interaction analysis. SNPsyn is both a stand-alone C++Flash application and a web server. The computationally intensive part is implemented in C++ and can run in parallel on a dedicated cluster or grid. The graphical user interface is written in Adobe Flash Builder 4 and can run in most web browsers or as a stand-alone application. The SNPsyn web server hosts the Flash application, receives GWAS data submissions, invokes the interaction analysis and serves result files. The user can explore details on identified synergistic pairs of SNPs, perform gene set enrichment analysis and interact with the constructed SNP synergy network. \u00a9 2011 The Author(s).", "Language": "en", "Citations": "24"},
{"Title": "Family history based approach in risk prediction for Parkinson's disease: Additional contribution of familial associated disorders", "Authors": ["Vrecar I.", "Maver A.", "Pirtosek Z.", "Georgiev D.", "Klemenc Ketis Z.", "Peterlin B."], "Keywords": ["Family history", "Parkinson's disease", "Risk prediction"], "Date": "2015", "Abstract": "The aim of our study was to examine the contribution of family history of Parkinson's disease and its associated disorders in the assessment of predictive capacity of risk models for Parkinson's disease. In a population of 192 patients with Parkinson's disease and 1659 healthy individuals we investigated the impact of environmental factors and the effects of family history on Parkinson's disease risk. Pesticides exposure, positive family history of Parkinson's disease and a positive family history of dementia and melanoma were associated to an increased risk for Parkinson's disease, with results regarding family history of depression near to statistical significance. Smoking and caffeine intake were associated to a decreased risk for Parkinson's disease. Three risk prediction models were assessed using the area under the curve approach: first model was based on known environmental risk factors, in the second model we added family history of Parkinson's disease and in the third model we additionally included family history of dementia, melanoma and depression. We showed that inclusion of data on family history of associated disorders (AUC 0.76) improves predictive capacity of risk model for Parkinson's disease in comparison with the first (AUC 0.62) and the second model (AUC 0.71). We concluded that family history of associated disorders: dementia, depression and melanoma improves predictive capacity of risk models for Parkinson's disease.", "Language": "en", "Citations": "0"},
{"Title": "SymCHM-An unsupervised approach for pattern discovery in symbolic music with a compositional hierarchical model", "Authors": ["Pesek M.", "Leonardis A.", "Marolt M."], "Keywords": ["Compositional modelling", "Music information retrieval", "Pattern discovery", "Symbolic music representations"], "Date": "2017", "Abstract": "This paper presents a compositional hierarchical model for pattern discovery in symbolic music. The model can be regarded as a deep architecture with a transparent structure. It can learn a set of repeated patterns within individual works or larger corpora in an unsupervised manner, relying on statistics of pattern occurrences, and robustly infer the learned patterns in new, unknown works. A learned model contains representations of patterns on different layers, from the simple short structures on lower layers to the longer and more complex music structures on higher layers. A pattern selection procedure can be used to extract the most frequent patterns from the model. We evaluate the model on the publicly available JKU Patterns Datasetsand compare the results to other approaches.", "Language": "en", "Citations": "2"},
{"Title": "Constructing and analyzing graphs of the Slovene words Graditev in analiza grafov slovenskih besed", "Authors": ["Cibej U."], "Keywords": [], "Date": "2013", "Abstract": "Organizing various sets of objects into graphs has recently become a fundamental tool for gaining new insights into various fields. The most notable achievements have been done in social, biological, financial, and several other networks. In this paper we construct graphs of words in the Slovene language based on two metrics connecting two words when their distance in the respective metric is small. From these graphs we extract several standard features presenting a basis for further study and comparison among different languages.", "Language": "en", "Citations": "1"},
{"Title": "Transductive reliability estimation for medical diagnosis", "Authors": ["Kukar M."], "Keywords": ["Coronary artery disease", "Machine learning", "Medical diagnosis", "Reliability estimation", "Transduction"], "Date": "2003", "Abstract": "In the past decades, machine learning (ML) tools have been successfully used in several medical diagnostic problems. While they often significantly outperform expert physicians (in terms of diagnostic accuracy, sensitivity, and specificity), they are mostly not being used in practice. One reason for this is that it is difficult to obtain an unbiased estimation of diagnose's reliability. We discuss how reliability of diagnoses is assessed in medical decision-making and propose a general framework for reliability estimation in machine learning, based on transductive inference. We compare our approach with a usual (machine learning) probabilistic approach as well as with classical stepwise diagnostic process where reliability of diagnose is presented as its post-test probability. The proposed transductive approach is evaluated on several medical datasets from the University of California (UCI) repository as well as on a practical problem of clinical diagnosis of the coronary artery disease (CAD). In all cases, significant improvements over existing techniques are achieved. \u00a9 2003 Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "38"},
{"Title": "Informatization of learning and training in slovenian armed forces", "Authors": ["Kavcic A."], "Keywords": ["Blended learning", "E-learning", "LMS", "SCORM", "Standards"], "Date": "2009", "Abstract": "The paper presents the results of a national project e-VIZUS: Informatization of learning and training in Slovenian Armed Forces. The project's main goal was to strengthen the role of e-learning in Slovenian Armed Forces and prepare the guidelines for its successful integration in the training process. The project had two main focuses. The first was on technical solutions for e-learning environment (e-classroom), and selection of tools and standards for preparation of electronic learning material. The second concentrated on development of electronic courses following the prepared guidelines and proposed technical solutions. Besides, several e-courses were developed and successfully used in the military schools as part of their regular training process. \u00a9 2009 AACC.", "Language": "en", "Citations": "0"},
{"Title": "Part-based room categorization for household service robots", "Authors": ["Ursic P.", "Mandeljc R.", "Leonardis A.", "Kristan M."], "Keywords": [], "Date": "2016", "Abstract": "A service robot that operates in a previously-unseen home environment should be able to recognize the functionality of the rooms it visits, such as a living room, a bathroom, etc. We present a novel part-based model and an approach for room categorization using data obtained from a visual sensor. Images are represented with sets of unordered parts that are obtained by object-agnostic region proposals, and encoded using state-of-the-art image descriptor extractor - a convolutional neural network (CNN). An approach is proposed that learns category-specific discriminative parts for the part-based model. The proposed approach was compared to the state-of-the-art CNN trained specifically for place recognition. Experimental results show that the proposed approach outperforms the holistic CNN by being robust to image degradation, such as occlusions, modifications of image scaling, and aspect changes. In addition, we report non-negligible annotation errors and image duplicates in a popular dataset for place categorization and discuss annotation ambiguities.", "Language": "en", "Citations": "6"},
{"Title": "Affective experience of music emotional and color perception of folk and other musical genres", "Authors": ["Strle G.", "Pesek M.", "Marolt M."], "Keywords": ["Colors", "Emotions", "Music", "Music genre", "Music perception"], "Date": "2018", "Abstract": "The article presents an experimental study of the emotional and color perception of music, focusing on folk music in comparison with other musical genres. The analysis showed that some musical emotions are conveyed differently between different genres. Overall, Folk genre had the most balanced distribution of the emotions used in the ratings, along with Pop and Rock. On the other hand, Punk and Metal had over 40% of their ratings represented by only two emotions, liveliness and anger. This is further reflected in the emotion-color associations. The presented findings point to the genre specificity of musical emotions.", "Language": "en", "Citations": "1"},
{"Title": "Managing communications in critical infrastructures protection", "Authors": ["Denis T."], "Keywords": ["Critical infrastructures protection", "Decision support", "Modeling and simulation", "SCADA"], "Date": "2010", "Abstract": "After events of 9/11 critical infrastructure protection (CIP) gained increased attention. Although at the very beginning the problem seemed overwhelmingly demanding because these infrastructures are indeed very complex, it turned out that, step by step, a systematic approach could be taken, which would make CIP related problems manageable. However, existing reference approaches in this area (like the US national CIP initiatives) are based on assumptions about relationships between the most important critical infrastructures (i.e. water, electricity, internet), which are not reflecting the reality anymore due to technological advances. Therefore the paper provides the analysis of this new situation and gives a conceptual model that can be adapted to a particular nation-wide environment. The final goal is to develop a model of a sufficient generality, which can be incrementally refined to meet the needs of a particular environment in order to enable simulations for better decision making in CIP. \u00a9 2010 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Decision support modelling for efficient implementation of ict in schools", "Authors": ["Campelj B.", "Karnet I.", "Brodnik A.", "Jereb E.", "Rajkovic U."], "Keywords": ["Decision rules", "DEX", "ICT in education", "MCDM", "Qualitative model", "Self-evaluation"], "Date": "2017", "Abstract": "The implementation of information communication technology (ICT) in schools has not always an impact to all employees\u2019 plans and activities. This paper presents a new hierarchical evaluation model (tree of indicator) of the level of implementation of ICT in school. The model offers simplicity of the evaluation process and provides transparency, because the user assigns only simple indicators (106) and then the model automatically computes the values of complex indicators (65) based on qualitative relations among indicators at lower level of the tree. The model enables non-professional ICT-users to organise and harmonise existing information and knowledge, and encourage them to identify priority goals and realizable changes.", "Language": "en", "Citations": "1"},
{"Title": "Recommender system for learning SQL using hints", "Authors": ["Lavbic D.", "Matek T.", "Zrnec A."], "Keywords": ["improving classroom teaching", "Intelligent Tutoring Systems", "interactive learning environments", "programming and programming languages", "recommender system", "SQL learning"], "Date": "2017", "Abstract": "Today\u2019s software industry requires individuals who are proficient in as many programming languages as possible. Structured query language (SQL), as an adopted standard, is no exception, as it is the most widely used query language to retrieve and manipulate data. However, the process of learning SQL turns out to be challenging. The need for a computer-aided solution to help users learn SQL and improve their proficiency is vital. In this study, we present a new approach to help users conceptualize basic building blocks of the language faster and more efficiently. The adaptive design of the proposed approach aids users in learning SQL by supporting their own path to the solution and employing successful previous attempts, while not enforcing the ideal solution provided by the instructor. Furthermore, we perform an empirical evaluation with 93 participants and demonstrate that the employment of hints is successful, being especially beneficial for users with lower prior knowledge.", "Language": "en", "Citations": "3"},
{"Title": "Bilingual speech recognition of Slovenian and Croatian weather forecasts", "Authors": ["Zibert J.", "Martincic-Ipsic S.", "Ipsic I.", "Mihelic F."], "Keywords": ["Data acquisition", "Image databases", "Load forecasting", "Multimedia communication", "Natural languages", "Radio broadcasting", "Speech processing", "Speech recognition", "TV broadcasting", "Weather forecasting"], "Date": "2003", "Abstract": "In the paper we present some results of a joint project in speech data collection and speech recognition of Slovenian and Croatian weather forecasts. In the paper we describe the procedures we have performed in order to obtain domain specific speech databases from broadcast programmes. We further describe the speech recognition experiments for language identification and the speech recognition experiments of monolingual and bilingual speech.", "Language": "en", "Citations": "4"},
{"Title": "Panoramic volumes for robot localization", "Authors": ["Artac M.", "Jogan M.", "Leonardis A.", "Bakstein H."], "Keywords": ["Mobile robots", "Panoramic images", "Subspace representation", "Visual localization"], "Date": "2005", "Abstract": "We propose a method for visual robot localization using a panoramic image volume as the representation from which we can generate views from virtual viewpoints and match them to the current view. We use a geometric image-based rendering formalism in combination with a subspace representation of images, which allows us to synthesize views at arbitrary virtual viewpoints from a compact low-dimensional representation. \u00a9 2005 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "Loop near-rings and unique decompositions of H-spaces", "Authors": ["Franetic D.", "Pavesic P."], "Keywords": [], "Date": "2016", "Abstract": "For every H-space X, the set of homotopy classes [X, X] possesses a natural algebraic structure of a loop near-ring. Albeit one cannot say much about general loop near-rings, it turns out that those that arise from H-spaces are sufficiently close to rings to have a viable Krull\u2013Schmidt type decomposition theory, which is then reflected into decomposition results of H-spaces. In the paper, we develop the algebraic theory of local loop near-rings and derive an algebraic characterization of indecomposable and strongly indecomposable H-spaces. As a consequence, we obtain unique decomposition theorems for products of H-spaces. In particular, we are able to treat certain infinite products of H-spaces, thanks to a recent breakthrough in the Krull\u2013Schmidt theory for infinite products. Finally, we show that indecomposable finite p\u2013local H-spaces are automatically strongly indecomposable, which leads to an easy alternative proof of classical unique decomposition theorems of Wilkerson and Gray.", "Language": "en", "Citations": "1"},
{"Title": "Yeast as a cell factory: Current state and perspectives", "Authors": ["Kavscek M.", "Strazar M.", "Curk T.", "Natter K.", "Petrovic U."], "Keywords": ["Genome editing", "Orthogonality", "QTL", "Robustness development", "Substrate utilization"], "Date": "2015", "Abstract": "The yeast Saccharomyces cerevisiae is one of the oldest and most frequently used microorganisms in biotechnology with successful applications in the production of both bulk and fine chemicals. Yet, yeast researchers are faced with the challenge to further its transition from the old workhorse to a modern cell factory, fulfilling the requirements for next generation bioprocesses. Many of the principles and tools that are applied for this development originate from the field of synthetic biology and the engineered strains will indeed be synthetic organisms. We provide an overview of the most important aspects of this transition and highlight achievements in recent years as well as trends in which yeast currently lags behind. These aspects include: the enhancement of the substrate spectrum of yeast, with the focus on the efficient utilization of renewable feedstocks, the enhancement of the product spectrum through generation of independent circuits for the maintenance of redox balances and biosynthesis of common carbon building blocks, the requirement for accurate pathway control with improved genome editing and through orthogonal promoters, and improvement of the tolerance of yeast for specific stress conditions. The causative genetic elements for the required traits of the future yeast cell factories will be assembled into genetic modules for fast transfer between strains. These developments will benefit from progress in bio-computational methods, which allow for the integration of different kinds of data sets and algorithms, and from rapid advancement in genome editing, which will enable multiplexed targeted integration of whole heterologous pathways. The overall goal will be to provide a collection of modules and circuits that work independently and can be combined at will, depending on the individual conditions, and will result in an optimal synthetic host for a given production process.", "Language": "en", "Citations": "27"},
{"Title": "Approximate multiple kernel learning with least-angle regression", "Authors": ["Strazar M.", "Curk T."], "Keywords": ["Kernel approximation", "Kernel methods", "Least-angle regression", "Multiple kernel learning"], "Date": "2019", "Abstract": "Kernel methods provide a principled way for general data representations. Multiple kernel learning and kernel approximation are often treated as separate tasks, with considerable savings in time and memory expected if the two are performed simultaneously. Our proposed Mklaren algorithm selectively approximates multiple kernel matrices in regression. It uses Incomplete Cholesky Decomposition and Least-angle regression (LAR) to select basis functions, achieving linear complexity both in the number of data points and kernels. Since it approximates kernel matrices rather than functions, it allows to combine an arbitrary set of kernels. Compared to single kernel-based approximations, it selectively approximates different kernels in different regions of the input spaces. The LAR criterion provides a robust selection of inducing points in noisy settings, and an accurate modelling of regression functions in continuous and discrete input spaces. Among general kernel matrix decompositions, Mklaren achieves minimal approximation rank required for performance comparable to using the exact kernel matrix, at a cost lower than 1% of required operations. Finally, we demonstrate the scalability and interpretability in settings with millions of data points and thousands of kernels.", "Language": "en", "Citations": "0"},
{"Title": "An algorithm for synchronization of in vivo electroporation with ECG", "Authors": ["Mali B.", "Jarm T.", "Jager F.", "Miklavcic D."], "Keywords": [], "Date": "2005", "Abstract": "The combined treatment of tumours in which delivery of a chemotherapeutic agent is followed by high voltage electroporation pulses has been termed electrochemotherapy. The electrochemotherapy of tumours located relatively close to the heart muscle can lead to fibrillation of the heart, especially if electroporation pulses are delivered in the vulnerable period of the heart or in coincidence with heart arrhythmias. We built an electroporation pulse delivery algorithm that enables safer use of electrochemotherapy. The algorithm is designed to deliver pulses outside the vulnerable period and to prevent pulses from being generated in the presence of heart arrhythmias. We evaluated the algorithm's performance using records of the Long-Term ST Database, thus simulating real-world conditions. The results of the evaluation, a sensitivity of 91.751%, a positive predictivity of 100.000% and a delivery error rate of 8.268% for electroporation pulse delivery (medians), suggest that the algorithm is accurate and appropriate for application in electrochemotherapy of tumours regardless of tumour location. \u00a9 2005 Taylor & Francis.", "Language": "en", "Citations": "11"},
{"Title": "How computer vision can help in outdoor positioning", "Authors": ["Steinhoff U.", "Omercevic D.", "Perko R.", "Schiele B.", "Leonardis A."], "Keywords": ["Computer vision based positioning", "Local invariant features", "Sensor fusion for outdoor localization"], "Date": "2007", "Abstract": "Localization technologies have been an important focus in ubiquitous computing. This paper explores an underrepresented area, namely computer vision technology, for outdoor positioning. More specifically we explore two modes of positioning in a challenging real world scenario: single snapshot based positioning, improved by a novel highdimensional feature matching method, and continuous positioning enabled by combination of snapshot and incremental positioning. Quite interestingly, vision enables localization accuracies comparable to GPS. Furthermore the paper also analyzes and compares possibilities offered by the combination of different subsets of positioning technologies such as WiFi, GPS and dead reckoning in the same real world scenario as for vision based positioning. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "30"},
{"Title": "The Sylvester graph and Moore graphs", "Authors": ["Jurisic A.", "Vidali J."], "Keywords": [], "Date": "2018", "Abstract": "The combinatorial structure of a famous graph with large girth, namely the Sylvester graph, is studied. Simple techniques, such as two-way counting, partitions, circuit chasing and covers are used to identify smaller structures and to show that there are no other graphs that share a small number of regularity properties with it. As a consequence, we show the same for the Hoffman\u2013Singleton graph. We notice that some much larger graphs with large girth have similar properties, and could be studied using the same techniques. In particular, we show that just as the Hoffman\u2013Singleton graph contains the Sylvester graph, a Moore graph of valency 57, whose existence is a famous open problem, must contain a subgraph with a structure that is similar to the one we derived for the Sylvester graph.", "Language": "en", "Citations": "0"},
{"Title": "Induction of qualitative trees", "Authors": ["Suc D.", "Bratko I."], "Keywords": [], "Date": "2001", "Abstract": "We consider the problem of automatic construction of qualitative models by inductive learning from quantitative examples. We present an algorithm QUIN (QUalitative INduction) that learns qualitative trees from a set of examples described with numerical attributes. At difference with decision trees, the leaves of qualitative trees contain qualitative functional constraints as used in qualitative reasoning. A qualitative tree defines a partition of the attribute space into the areas with common qualitative behaviour of the chosen class variable. We describe a basic algorithm for induction of qualitative trees, improve it to the heuristic QUIN algorithm, and give experimental evaluation of the algorithms on a set of artificial domains. QUIN has already been used to induce qualitative control strategies in dynamic domains such as controlling a crane or riding a bicycle (described elsewhere) and can be applied to other domains as a general tool for qualitative system identification.", "Language": "en", "Citations": "16"},
{"Title": "A capstone course on agile software development using scrum", "Authors": ["Mahnic V."], "Keywords": ["Agile software development", "capstone course", "effort estimation", "Scrum", "software engineering education"], "Date": "2012", "Abstract": "In this paper, an undergraduate capstone course in software engineering is described that not only exposes students to agile software development, but also makes it possible to observe the behavior of developers using Scrum for the first time. The course requires students to work as Scrum Teams, responsible for the implementation of a set of user stories defined by a project domain expert playing the role of the Product Owner. During the course, data on project management activities are collected in order to analyze the amount of work completed, compliance with the release and iteration plans, productivity, ability in effort estimation, and the like. The paper discusses the achievement of teaching goals and provides empirical evaluation of students' progress in estimation and planning skills. A summary of lessons learned and recommendations is given, reflecting the issues to be considered when teaching courses in agile software development. Surveys of students have shown that they were overwhelmingly positive about the course, indicating that the course fully met or even exceeded their expectations. \u00a9 2011 IEEE.", "Language": "en", "Citations": "82"},
{"Title": "Outsourcing as an Economic Development Tool in Transition Economies: Scattered Global Software Development", "Authors": ["Vrhovec S.L.R.", "Trkman M.", "Kumer A.", "Krisper M.", "Vavpotic D."], "Keywords": ["global software development", "information and communication technology", "outsourcing", "transition economies"], "Date": "2015", "Abstract": "In transition economies, information and communication technology (ICT) is vital for successful companies and may compensate for an underdeveloped infrastructure and lack of resources. The development of complex ICT systems requires skilled ICT professionals who are often difficult to acquire. In this paper, we address this specific issue of transition economies and propose a novel global software development approach that aims to compensate for the lack of skilled ICT professionals by outsourcing independent development tasks globally to remote developers. The proposed approach was empirically tested in a pilot study at three different locations at University of Ljubljana in Slovenia. The test demonstrated the feasibility of the approach and indicated that task specification quality and developer skills are important success factors. The findings of the pilot study are primarily relevant for software development companies in transition economies even though the approach may also be applicable in other settings where lack of locally accessible skilled ICT professionals is present.", "Language": "en", "Citations": "5"},
{"Title": "Improved binding site assignment by high-resolution mapping of RNA-protein interactions using iCLIP", "Authors": ["Hauer C.", "Curk T.", "Anders S.", "Schwarzl T.", "Alleaume A.-M.", "Sieber J.", "Hollerer I.", "Bhuvanagiri M.", "Huber W.", "Hentze M.W.", "Kulozik A.E."], "Keywords": [], "Date": "2015", "Abstract": "Individual-nucleotide resolution crosslinking and immunoprecipitation (iCLIP) allows the determination of crosslinking sites of RNA-binding proteins (RBPs) on RNAs. iCLIP is based on ultraviolet light crosslinking of RBPs to RNA, reverse transcription and high-throughput sequencing of fragments terminating at the site of crosslinking. As a result, start sites of iCLIP fragments are expected to cluster with a narrow distribution, typically representing the site of direct interaction between the RBP and the RNA. Here we show that for several RBPs (eIF4A3, PTB, SRSF3, SRSF4 and hnRNP L), the start sites of iCLIP fragments show a fragment length-dependent broader distribution that can be shifted to positions upstream of the known RNA-binding site. We developed an analysis tool that identifies these shifts and can improve the positioning of RBP binding sites.", "Language": "en", "Citations": "15"},
{"Title": "Supporting diagnostics of coronary artery disease with multi-resolution image parameterization and data mining", "Authors": ["Kukar M.", "Saajn L."], "Keywords": ["Association rules", "Coronary artery disease", "Machine learning", "Medical diagnostics", "Multi-resolution image parameterization", "Principal component analysis"], "Date": "2009", "Abstract": "Coronary artery disease has been described as one of the curses of the western world, as it is one of the most important causes of mortality. Therefore, clinicians seek to improve diagnostic procedures, especially those that allow them to reach reliable early diagnoses. In the clinical setting, coronary artery disease diagnostics is typically performed in a stepwise manner. The four diagnostic levels consist of evaluation of (1) signs and symptoms of the disease and ECG (electrocardiogram) at rest, (2) sequential ECG testing during the controlled exercise, (3) myocardial perfusion scintigraphy, and finally (4) coronary angiography, that is considered as the gold standard- reference method. Our study focuses on improving diagnostic performance of the third diagnostic level. Myocardial scintigraphy is non invasive; it results in a series of medical images that are relatively inexpensively obtained. In clinical practice, these images are manually described (parameterized) by expert physicians. In the paper we present an innovative alternative to manual image evaluation - an automatic image parameterization in multiple resolutions, based on texture description with specialized association rules. Extracted image parameters are combined into more informative composite parameters by means of principle component analysis, and finally used to build automatic classifiers with machine learning methods. Our experiments with synthetic datasets show that association-rule-based multi-resolution image parameterization equals or surpasses other state-of-the-art methods for finding multiple informative resolutions. Experimental results in coronary artery disease diagnostics confirm these results as our approach significantly improves the clinical results in terms of quality of image parameters as well as diagnostic performance. \u00a9 Springer-Verlag Berlin Heidelberg 2009.", "Language": "en", "Citations": "0"},
{"Title": "Data Imputation in Epistatic MAPs by Network-Guided Matrix Completion", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": ["data integration", "epistatic miniarray profile", "gene network", "genetic interaction", "matrix completion", "missing value imputation"], "Date": "2015", "Abstract": "Epistatic miniarray profile (E-MAP) is a popular large-scale genetic interaction discovery platform. E-MAPs benefit from quantitative output, which makes it possible to detect subtle interactions with greater precision. However, due to the limits of biotechnology, E-MAP studies fail to measure genetic interactions for up to 40% of gene pairs in an assay. Missing measurements can be recovered by computational techniques for data imputation, in this way completing the interaction profiles and enabling downstream analysis algorithms that could otherwise be sensitive to missing data values. We introduce a new interaction data imputation method called network-guided matrix completion (NG-MC). The core part of NG-MC is low-rank probabilistic matrix completion that incorporates prior knowledge presented as a collection of gene networks. NG-MC assumes that interactions are transitive, such that latent gene interaction profiles inferred by NG-MC depend on the profiles of their direct neighbors in gene networks. As the NG-MC inference algorithm progresses, it propagates latent interaction profiles through each of the networks and updates gene network weights toward improved prediction. In a study with four different E-MAP data assays and considered protein-protein interaction and gene ontology similarity networks, NG-MC significantly surpassed existing alternative techniques. Inclusion of information from gene networks also allowed NG-MC to predict interactions for genes that were not included in original E-MAP assays, a task that could not be considered by current imputation approaches.", "Language": "en", "Citations": "5"},
{"Title": "Digital airbrush", "Authors": ["Batagelj B.", "Marovt J.", "Troha M.", "Mahnic D."], "Keywords": ["Airbrush", "Computer vision", "Digital airbrush", "Graffiti", "Interactive entertainment", "Virtual art"], "Date": "2009", "Abstract": "The basic idea is simple: graffiter paints with a specially modified can, which when pressed on the cap does not leave color traces, but emits infrared light. The computer application draws the appropriate graffiti on the canvas, which is positioned in front of the graffiter. Infrared camera provides detection of the light source for the application, which then correctly determines the color, size and density (the graffiter regulates these parameters through the speed buttons, which are located on the can) and in the end of this process, the current drawing track is projected on the canvas. The most important algorithms used in the implementation of the software solution are: an algorithm for locating the brightest area, the algorithm for the implementation of the drops and the algorithm for interpolation. Important component of simulation is also a Bluetooth connection to the Arduino BT platform, which is connected to three potentiometers. These are used by graffiter to manipulate painting parameters. The results of extensive tests have shown that the prototype has potential and that the final simulation perfectly follows the basic idea. The problem occurs only in the lack of quality of the equipment (infrared camera, computer power) for full implementation.", "Language": "en", "Citations": "4"},
{"Title": "Collective ontology-based information extraction using probabilistic graphical models", "Authors": ["Zitnik S."], "Keywords": ["Coreference resolution", "Information extraction", "Named entity recognition", "Ontology", "Relation extraction"], "Date": "2012", "Abstract": "Information Extraction (IE) is a process of extracting structured data from unstructured sources. It roughly consists of subtasks named entity recognition, relation extraction and coreference resolution. Researchers have primarily focused just on one subtask or their combination in a pipeline. In this paper we introduce an intelligent collective IE system combining all three subtasks by employing conditional random fields. The usage of same learning model enables us to easily communicate between iterations on the fly and to correct errors during iterative process execution. In addition to the architecture we introduce novel semantic and collective feature functions. The system's output is labelled according to an ontology and new instances are automatically created during runtime. The ontology as a schema encodes a set of constraints, defines optional manual rules or patterns and with instances provides semantic gazetteer lists. The proposed framework is being developed during ongoing PhD research. It's main contributions are intelligent iterative interconnection of the selected subtasks, extensive use of context-specific features and parameterless system that can be guided by an ontology. Some preliminary results combining just two subtasks already show promising results over traditional approaches.", "Language": "en", "Citations": "0"},
{"Title": "Learning betting tips from users' bet selections", "Authors": ["Strumbelj E.", "Sikonja M.R.", "Kononenko I."], "Keywords": ["Data mining", "Forecasting", "Machine learning", "Nearest neighbors", "Sports betting"], "Date": "2009", "Abstract": "In this paper we address the problem of using bet selections of a large number of mostly non-expert users to improve sports betting tips. A similarity based approach is used to describe individual users' strategies and we propose two different scoring functions to evaluate them. The information contained in users' bet selections improves on using only bookmaker odds. Even when only bookmaker odds are used, the approach gives results comparable to those of a regression-based forecasting model. \u00a9 2009 Springer Berlin Heidelberg.", "Language": "en", "Citations": "1"},
{"Title": "User interface for a better eye contact in videoconferencing", "Authors": ["Jaklic A.", "Solina F.", "Sajn L."], "Keywords": ["Eye contact", "Human-computer interface", "Videoconferencing"], "Date": "2017", "Abstract": "When people talk to each other, eye contact is very important for a trustful and efficient communication. Video-conferencing systems were invented to enable such communication over large distances, recently using mostly Internet and personal computers. Despite low cost of such solutions, a broader acceptance and use of these communication means has not happened yet. One of the most important reasons for this situation is that it is almost impossible to establish eye contact between distant parties on the most common hardware configurations of such videoconferencing systems, where the camera for face capture is usually mounted above the computer monitor, where the face of the correspondent is observed. Different hardware and software solutions to this problem of missing eye contact have been proposed over the years. In this article we propose a simple solution that can improve the subjective feeling of eye contact, which is based on how people perceive 3D scenes displayed on slanted surfaces, and offer some experiments in support of the hypothesis.", "Language": "en", "Citations": "2"},
{"Title": "An Extended ANSI C for Processors with a Multimedia Extension", "Authors": ["Bulic P.", "Gustin V."], "Keywords": ["ISA multimedia extensions", "SIMD processing", "Vector C"], "Date": "2003", "Abstract": "This paper presents the Multimedia C language, which is designed for the multimedia extensions included in all modern microprocessors. The paper discusses the language syntax, the implementation of its compiler and its use in developing multimedia applications. The goal was to provide programmers with the most natural way of using multimedia processing facilities in the C language. The MMC language has been used to develop some of the most frequently used multimedia kernels. The presented experiments on these scientific and multimedia applications have yielded good performance improvements.", "Language": "en", "Citations": "14"},
{"Title": "Implementation of a binary memory in simple biological circuits Realizacija dvoji\u0161kega pomnjenja v preprostih biolo\u0161kih sistemih", "Authors": ["Moskon M.", "Zimic N.", "Mraz M."], "Keywords": ["Johsnon counter", "Master-Slave D Flip-Flop", "Memory", "Modelling and simulations", "Synthetic biology"], "Date": "2016", "Abstract": "In the paper we give a structured overview of the state-of-theart in the synthetic biological memory structures. A reliable implementation of these structures and their integration with other combinatorial logic components into functional circuits presents an essential step in the implementation of a biological computer. Drawing an analogy from the digital electronic systems used in modern computers, we divide the biological memory structures in two groups. The Non-volatile structures that store the information directly into sequences composing the DNA strands and the Volatile structures that store the information in a form of concentrations of different chemical species, e.g., proteins, the expression of which is again defined by the corresponding DNA sequences, i.e. by their genes. We propose an implementation of a cellular program stored as a DNA sequence and describe its execution scheme. We introduce an implementation of a biological version of the Johnson's counter. The implementation of the counter is performed with a biological version of the Master-Slave D flip-flop, dynamics of which depends on the regulatory interactions among the selected proteins and their genes. We show how to use the counter to address and execute the cellular program.", "Language": "en", "Citations": "0"},
{"Title": "Bayesian Lasso and multinomial logistic regression on GPU", "Authors": ["Cesnovar R.", "Strumbelj E."], "Keywords": [], "Date": "2017", "Abstract": "We describe an efficient Bayesian parallel GPU implementation of two classic statistical models\u2014the Lasso and multinomial logistic regression. We focus on parallelizing the key components: matrix multiplication, matrix inversion, and sampling from the full conditionals. Our GPU implementations of Bayesian Lasso and multinomial logistic regression achieve 100-fold speedups on mid-level and high-end GPUs. Substantial speedups of 25 fold can also be achieved on older and lower end GPUs. Samplers are implemented in OpenCL and can be used on any type of GPU and other types of computational units, thereby being convenient and advantageous in practice compared to related work.", "Language": "en", "Citations": "1"},
{"Title": "ROC analysis of classifiers in machine learning: A survey", "Authors": ["Majnik M.", "Bosnic Z."], "Keywords": ["classification", "machine learning", "performance", "ROC", "ROC analysis"], "Date": "2013", "Abstract": "The use of ROC (Receiver Operating Characteristics) analysis as a tool for evaluating the performance of classification models in machine learning has been increasing in the last decade. Among the most notable advances in this area are the extension of two-class ROC analysis to the multi-class case as well as the employment of ROC analysis in cost-sensitive learning. Methods now exist which take instance-varying costs into account. The purpose of our paper is to present a survey of this field with the aim of gathering important achievements in one place. In the paper, we present application areas of the ROC analysis in machine learning, describe its problems and challenges and provide a summarized list of alternative approaches to ROC analysis. In addition to presented theory, we also provide a couple of examples intended to illustrate the described approaches. \u00a9 2013-IOS Press and the authors. All rights reserved.", "Language": "en", "Citations": "21"},
{"Title": "Robust object detection with interleaved categorization and segmentation", "Authors": ["Leibe B.", "Leonardis A.", "Schiele B."], "Keywords": ["Clustering", "Hough transform", "Hypothesis selection", "MDL", "Object categorization", "Object detection", "Segmentation"], "Date": "2008", "Abstract": "This paper presents a novel method for detecting and localizing objects of a visual category in cluttered real-world scenes. Our approach considers object categorization and figure-ground segmentation as two interleaved processes that closely collaborate towards a common goal. As shown in our work, the tight coupling between those two processes allows them to benefit from each other and improve the combined performance. The core part of our approach is a highly flexible learned representation for object shape that can combine the information observed on different training examples in a probabilistic extension of the Generalized Hough Transform. The resulting approach can detect categorical objects in novel images and automatically infer a probabilistic segmentation from the recognition result. This segmentation is then in turn used to again improve recognition by allowing the system to focus its efforts on object pixels and to discard misleading influences from the background. Moreover, the information from where in the image a hypothesis draws its support is employed in an MDL based hypothesis verification stage to resolve ambiguities between overlapping hypotheses and factor out the effects of partial occlusion. An extensive evaluation on several large data sets shows that the proposed system is applicable to a range of different object categories, including both rigid and articulated objects. In addition, its flexible representation allows it to achieve competitive object detection performance already from training sets that are between one and two orders of magnitude smaller than those used in comparable systems. \u00a9 2007 Springer Science+Business Media, LLC.", "Language": "en", "Citations": "721"},
{"Title": "Polymorphic Members of the lag Gene Family Mediate Kin Discrimination in Dictyostelium", "Authors": ["Benabentos R.", "Hirose S.", "Sucgang R.", "Curk T.", "Katoh M.", "Ostrowski E.A.", "Strassmann J.E.", "Queller D.C.", "Zupan B.", "Shaulsky G.", "Kuspa A."], "Keywords": ["CELLBIO", "EVO_ECOL", "MOLIMMUNO"], "Date": "2009", "Abstract": "Self and kin discrimination are observed in most kingdoms of life and are mediated by highly polymorphic plasma membrane proteins [1-7]. Sequence polymorphism, which is essential for effective recognition, is maintained by balancing selection [8-10]. Dictyostelium discoideum are social amoebas that propagate as unicellular organisms but aggregate upon starvation and form fruiting bodies with viable spores and dead stalk cells. Aggregative development exposes Dictyostelium to the perils of chimerism, including cheating, which raises questions about how the victims survive in nature and how social cooperation persists [11-13]. Dictyostelids can minimize the cost of chimerism by preferential cooperation with kin [14-16], but the mechanisms of kin discrimination are largely unknown. Dictyostelium lag genes encode transmembrane proteins with multiple immunoglobulin (Ig) repeats that participate in cell adhesion and signaling [17-22]. Here, we describe their role in kin discrimination. We show that lagB1 and lagC1 are highly polymorphic in natural populations and that their sequence dissimilarity correlates well with wild-strain segregation. Deleting lagB1 and lagC1 results in strain segregation in chimeras with wild-type cells, whereas elimination of the nearly invariant homolog lagD1 has no such consequences. These findings reveal an early evolutionary origin of kin discrimination and provide insight into the mechanism of social recognition and immunity. \u00a9 2009 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "96"},
{"Title": "Karhunen-Lo\u00e9ve expansion of a set of rotated templates", "Authors": ["Jogan M.", "Zagar E.", "Leonardis A."], "Keywords": ["Circulant matrices", "Eigenvectors", "Karhunen-Lo\u00e9ve expansion", "Symmetric matrices", "Toeplitz matrices"], "Date": "2003", "Abstract": "A novel method for calculating the eigenvectors of uniformly rotated images of a set of templates was proposed. It was shown that the images can be optimally approximated by a linear series of eigenvectors. The practical application of this method was demonstrated by constructing an appearance-based model of an environment.", "Language": "en", "Citations": "13"},
{"Title": "AgroDSS: A decision support system for agriculture and farming", "Authors": ["Rupnik R.", "Kukar M.", "Vracar P.", "Kosir D.", "Pevec D.", "Bosnic Z."], "Keywords": ["68T05", "68U35", "Agriculture", "Decision support system", "Farming"], "Date": "2019", "Abstract": "Decision support systems, data analysis and data mining have become significant tools for improving business in professional world. The emerging technologies are making the precision agriculture omnipresent and allow potential for enriching it with computer-assisted decision support systems for farm management. In this paper we describe a novel system AgroDSS that bridges the gap between agricultural systems and state-of-the-art decision support methodology. The described system is intended for integration into the existing farm management information systems and provides a cloud-based decision support toolbox, allowing farmers to upload their own data, utilize several data analysis methods and retrieve their outputs. The implemented tools include predictive modeling with explanation, accuracy evaluation, time series clustering and decomposition, and structural change detection. They can help users make predictions for simulated scenarios and better understand the dependencies (interactions) within their domain. We apply the AgroDSS system on a case study of pest population dynamics, illustrating the potential for its use.", "Language": "en", "Citations": "2"},
{"Title": "Differential expression of microRNAs and other small RNAs in muscle tissue of patients with ALS and healthy age-matched controls", "Authors": ["Kovanda A.", "Leonardis L.", "Zidar J.", "Koritnik B.", "Dolenc-Groselj L.", "Ristic Kovacic S.", "Curk T.", "Rogelj B."], "Keywords": [], "Date": "2018", "Abstract": "Amyotrophic lateral sclerosis is a late-onset disorder primarily affecting motor neurons and leading to progressive and lethal skeletal muscle atrophy. Small RNAs, including microRNAs (miRNAs), can serve as important regulators of gene expression and can act both globally and in a tissue-/cell-type-specific manner. In muscle, miRNAs called myomiRs govern important processes and are deregulated in various disorders. Several myomiRs have shown promise for therapeutic use in cellular and animal models of ALS; however, the exact miRNA species differentially expressed in muscle tissue of ALS patients remain unknown. Following small RNA-Seq, we compared the expression of small RNAs in muscle tissue of ALS patients and healthy age-matched controls. The identified snoRNAs, mtRNAs and other small RNAs provide possible molecular links between insulin signaling and ALS. Furthermore, the identified miRNAs are predicted to target proteins that are involved in both normal processes and various muscle disorders and indicate muscle tissue is undergoing active reinnervation/compensatory attempts thus providing targets for further research and therapy development in ALS.", "Language": "en", "Citations": "11"},
{"Title": "Towards automatic cross-lingual acoustic modelling applied to hmm-based speech synthesis for under-resourced languages Primjena automatskog medujezi\u010dnog akusti\u010dnog modeliranja na HMM sintezu govora za oskudne jezi \u010dne baze", "Authors": ["Justin T.", "Mihelic F.", "Zibert J."], "Keywords": ["Cross-language synthesis", "HMM-based speech synthesis", "Human language technologies", "UBM-MAP-GMM phoneme mapping", "Under-resourced languages", "Voice user interfaces"], "Date": "2016", "Abstract": "Nowadays Human Computer Interaction (HCI) can also be achieved with voice user interfaces (VUIs). To enable devices to communicate with humans by speech in the user\u2019s own language, low-cost language portability is often discussed and analysed. One of the most time-consuming parts for the language-adaptation process of VUIcapable applications is the target-language speech-data acquisition. Such data is further used in the development of VUIs subsystems, especially of speech-recognition and speech-production systems. The tempting idea to bypass a long-term process of data acquisition is considering the design and development of an automatic algorithms, which can extract the similar target-language acoustic from different language speech databases. This paper focus on the cross-lingual phoneme mapping between an under-resourced and a well-resourced language. It proposes a novel automatic phoneme-mapping technique that is adopted from the speaker-verification field. Such a phoneme mapping is further used in the development of the HMM-based speech-synthesis system for the under-resourced language. The synthesised utterances are evaluated with a subjective evaluation and compared by the expert knowledge cross-language method against to the baseline speech synthesis based just from the under-resourced data. The results reveals, that combining data from well-resourced and under-resourced language with the use of the proposed phoneme-mapping technique, can improve the quality of under-resourced language speech synthesis.", "Language": "en", "Citations": "0"},
{"Title": "A coarse-to-fine taxonomy of constellations for fast multi-class object detection", "Authors": ["Fidler S.", "Boben M.", "Leonardis A."], "Keywords": [], "Date": "2010", "Abstract": "In order for recognition systems to scale to a larger number of object categories building visual class taxonomies is important to achieve running times logarithmic in the number of classes [1,2]. In this paper we propose a novel approach for speeding up recognition times of multi-class part-based object representations. The main idea is to construct a taxonomy of constellation models cascaded from coarse-to-fine resolution and use it in recognition with an efficient search strategy. The taxonomy is built automatically in a way to minimize the number of expected computations during recognition by optimizing the cost-to-power ratio [3]. The structure and the depth of the taxonomy is not pre-determined but is inferred from the data. The approach is utilized on the hierarchy-of-parts model [4] achieving efficiency in both, the representation of the structure of objects as well as in the number of modeled object classes. We achieve speed-up even for a small number of object classes on the ETHZ and TUD dataset. On a larger scale, our approach achieves detection time that is logarithmic in the number of classes. \u00a9 2010 Springer-Verlag.", "Language": "en", "Citations": "7"},
{"Title": "Algorithms for subsetting attribute values with Relief", "Authors": ["Demsar J."], "Keywords": ["Attribute quality estimation", "Machine learning", "Relief"], "Date": "2010", "Abstract": "Relief is a measure of attribute quality which is often used for feature subset selection. Its use in induction of classification trees and rules, discretization, and other methods has however been hindered by its inability to suggest subsets of values of discrete attributes and thresholds for splitting continuous attributes into intervals. We present efficient algorithms for both tasks. \u00a9 The Author(s) 2009.", "Language": "en", "Citations": "9"},
{"Title": "Inertial sensor-based gait recognition: A review", "Authors": ["Sprager S.", "Juric M.B."], "Keywords": ["Biometry", "Gait analysis", "Gait authentication", "Gait identification", "Gait patterns", "Gait recognition", "Inertial data", "Inertial sensors", "Review"], "Date": "2015", "Abstract": "With the recent development of microelectromechanical systems (MEMS), inertial sensors have become widely used in the research of wearable gait analysis due to several factors, such as being easy-to-use and low-cost. Considering the fact that each individual has a unique way of walking, inertial sensors can be applied to the problem of gait recognition where assessed gait can be interpreted as a biometric trait. Thus, inertial sensor-based gait recognition has a great potential to play an important role in many security-related applications. Since inertial sensors are included in smart devices that are nowadays present at every step, inertial sensor-based gait recognition has become very attractive and emerging field of research that has provided many interesting discoveries recently. This paper provides a thorough and systematic review of current state-of-the-art in this field of research. Review procedure has revealed that the latest advanced inertial sensor-based gait recognition approaches are able to sufficiently recognise the users when relying on inertial data obtained during gait by single commercially available smart device in controlled circumstances, including fixed placement and small variations in gait. Furthermore, these approaches have also revealed considerable breakthrough by realistic use in uncontrolled circumstances, showing great potential for their further development and wide applicability.", "Language": "en", "Citations": "93"},
{"Title": "Consistent cycles in 1/2-arc-transitive graphs", "Authors": ["Boben M.", "Miklavic S.", "Potocnik P."], "Keywords": [], "Date": "2009", "Abstract": "A directed cycle C of a graph is called 1/k-consistent if there exists an automorphism of the graph which acts as a k-step rotation of C. These cycles have previously been considered by several authors in the context of arc-transitive graphs. In this paper we extend these results to the case of graphs which are vertex-transitive, edge-transitive but not arc-transitive.", "Language": "en", "Citations": "4"},
{"Title": "Nodewatcher: A substrate for growing your own community network", "Authors": ["Kos J.", "Milutinovic M.", "Cehovin L."], "Keywords": ["Community networks", "Management", "Mesh", "Monitoring", "Provisioning", "Wireless"], "Date": "2015", "Abstract": "Community networks differ from regular networks by their organic growth patterns - there is no central planning body that would decide how the network is built. Instead, the network grows in a bottom-up fashion as more people express interest in participating in the community and connect with their neighbors. People who participate in community networks are usually volunteers with limited free time. Due to these factors, making the management of community networks simpler and easier for all participants is the key component in boosting their growth. Specifics of individual networks often force communities to develop their own sets of tools and best practices which are hard to share and do not interoperate well with others. We propose a new general community network management platform nodewatcher that is built around the core principle of modularity and extensibility, making it suitable for reuse by different community networks. Devices are configured using a platform-independent configuration which nodewatcher can transform into deployable firmware images, eliminating any manual device configuration, reducing errors, and enabling participation of novice maintainers. An embedded monitoring system enables live overview and validation of the whole community network. We show how the system successfully operates in an actual community wireless network, wlan slovenija.", "Language": "en", "Citations": "3"},
{"Title": "On the use of the MMC language to utilize SIMD instruction set", "Authors": ["Bulic P.", "Gustin V."], "Keywords": [], "Date": "2007", "Abstract": "This paper presents the use of the Multimedia C (MMC) language to develop multimedia applications. The MMC language was designed to support operations with multimedia extensions included in all modern microprocessors. Although the idea to extend high programming languages to support vector operations is not novel, we show that integration of multimedia extensions into C is valuable. This is specially true for idiomatic expressions which are difficult for a compiler to identify. The MMC language has been used to develop some of the most frequently used multimedia kernels. The presented experiments on these scientific and multimedia applications have yielded good performance improvements. Although this paper discuses the use of MMC, the key features of the MMC language and implementation of its compiler are also presented. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "0"},
{"Title": "The added value of argumentation", "Authors": ["Modgil S.", "Toni F.", "Bex F.", "Bratko I.", "Chesnevar C.I.", "Dvorak W.", "Falappa M.A.", "Fan X.", "Gaggl S.A.", "Garcia A.J.", "Gonzalez M.P.", "Gordon T.F.", "Leite J.", "Mozina M.", "Reed C.", "Simari G.R.", "Szeider S.", "Torroni P.", "Woltran S."], "Keywords": [], "Date": "2013", "Abstract": "We discuss the value of argumentation in reaching agreements, based on its capability for dealing with conflicts and uncertainty. Logic-based models of argumentation have recently emerged as a key topic within Artificial Intelligence. Key reasons for the success of these models is that they are akin to human models of reasoning and debate, and their generalisation to frameworks for modelling dialogues. They therefore have the potential for bridging between human and machine reasoning in the presence of uncertainty and conflict. We provide an overview of a number of examples that bear witness to this potential, and that illustrate the added value of argumentation. These examples amount to methods and techniques for argumentation to aid machine reasoning (e.g. in the form of machine learning and belief functions) on the one hand and methods and techniques for argumentation to aid human reasoning (e.g. for various forms of decision making and deliberation and for the Web) on the other. We also identify a number of open challenges if this potential is to be realised, and in particular the need for benchmark libraries.", "Language": "en", "Citations": "53"},
{"Title": "Self-understanding and self-extension: A systems and representational approach", "Authors": ["Wyatt J.L.", "Aydemir A.", "Brenner M.", "Hanheide M.", "Hawes N.", "Jensfelt P.", "Kristan M.", "Kruijff G.-J.M.", "Lison P.", "Pronobis A.", "Sjoo K.", "Vrecko A.", "Zender H.", "Zillich M.", "Skocaj D."], "Keywords": ["Architectures", "representations", "robot learning", "robotics"], "Date": "2010", "Abstract": "There are many different approaches to building a system that can engage in autonomous mental development. In this paper, we present an approach based on what we term self-understanding, by which we mean the explicit representation of and reasoning about what a system does and does not know, and how that knowledge changes under action. We present an architecture and a set of representations used in two robot systems that exhibit a limited degree of autonomous mental development, which we term self-extension. The contributions include: representations of gaps and uncertainty for specific kinds of knowledge, and a goal management and planning system for setting and achieving learning goals. \u00a9 2009 IEEE.", "Language": "en", "Citations": "25"},
{"Title": "Solving the ternary quantum-dot cellular automata logic gate problem by means of adiabatic switching", "Authors": ["Pecar P.", "Mraz M.", "Zimic N.", "Janez M.", "Lebar Bajec I."], "Keywords": ["Adiabatic switching", "Hubbard model", "Inter-cellular hartree approximation", "Quantum-dot cellular automaton", "Ternary logic gates", "Ternary quantum-dot cellular automaton"], "Date": "2008", "Abstract": "Quantum-dot cellular automata (QCA) are one of the most promising alternative platforms of the future. Recent years have witnessed the development of basic logic structures as well as more complex processing structures, however most in the realm of binary logic. On the grounds that future platforms should not disregard the advantages of multi-valued logic, Lebar Bajec et al. were the first to show that quantum-dot cellular automata can be used for the implementation of ternary logic as well. In their study the ternary AND and OR logic functions proved to be the most troublesome primitive to implement. This research presents a revised solution that is based on adiabatic switching. \u00a9 2008 The Japan Society of Applied Physics.", "Language": "en", "Citations": "11"},
{"Title": "Deployment of trust management system in environment of e-commerce", "Authors": ["Zupancic E.", "Trcek D."], "Keywords": ["e-commerce", "multi-agent systems", "simulations", "trust management"], "Date": "2012", "Abstract": "Internet has become an important part of our everyday life, as it has created new opportunities for business and offered new ways of social gathering. A very critical topic concerning decision making by online transactions is trust issue. This paper presents a formal representation of a sample e-commerce environment with communicating agents, including their properties and assets. Further, we describe rules of an investment game that agents are playing. We propose two different scenarios. In the first one, agents' investment decisions are ad hoc. In the second one, there is trust management system deployed that affects the agents' decisions. Based on developed simulation tool, we investigate the proposed scenarios. We show that with introducing trust management system agents filter out bad agents and only good ones prosper. We also discuss other evident effects of trust management system on agents and their final investments values based on simulations results. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "Learning Qualitative Models", "Authors": ["Bratko I.", "Suc D."], "Keywords": [], "Date": "2003", "Abstract": "In general, modeling is a complex and creative task, and building qualitative models is no exception. One way of automating this task is by means of machine learning. Observed behaviors of a modeled system are used as examples for a learning algorithm that constructs a model that is consistent with the data. In this article, we review approaches to learning qualitative models, either from numeric data or qualitative observations. We describe the QUIN program that looks for qualitative patterns in numeric data and outputs the results of learning as \"qualitative trees.\" We illustrate this using applications associated with systems control, in particular, the identification and optimization of controllers and human operator's control skill. We also review approaches that learn models in terms of qualitative differential equations.", "Language": "en", "Citations": "40"},
{"Title": "Integration of load balancing in a corba environment", "Authors": ["Mocnik J.", "Trobec R.", "Robic B."], "Keywords": ["Algorithm", "CORBA", "Distributed computing", "Object-oriented programming"], "Date": "2003", "Abstract": "A framework for the distribution of program objects in heterogeneous computer systems using Common Object Request Broker Architecture (CORBA) is described. Any load balancing algorithm can be integrated transparently into the existing CORBA infrastructure. The implementation was tested with different load balancing strategies on an algorithm performing an exhaustive search over all possible paths, over all interconnection topologies of n nodes. The goal is to find the topology yielding the minimal sum of average and maximal distances among all pairs of nodes in minimal execution time.", "Language": "en", "Citations": "2"},
{"Title": "Virtual Skiing as an art installation", "Authors": ["Solina F.", "Batagelj B.", "Glamocanin S."], "Keywords": ["Art", "Computer vision", "Game", "Skiing", "Virtual reality"], "Date": "2008", "Abstract": "The Virtual Skiing game allows the user to immerse himself into the skiing sensation without using any obvious hardware interfaces. To achieve the movement down the virtual skiing slope the skier who stands on a pair of skis attached to the floor performs the same movements as on real skis, in particular this is the case on carving skis: tilting the body to the left initiates a left turn, tilting the body to the right initiates a right turn, by lowering the body, the speed is increased. The skier observes his progress down the virtual slope projected on the wall in front of him. The skier's movements are recorded using a video camera placed in front of him and processed on a PC in real time to drive the projected animation of the virtual slope.", "Language": "en", "Citations": "8"},
{"Title": "Multi-level association rules and directed graphs for spatial data analysis", "Authors": ["Petelin B.", "Kononenko I.", "Malacic V.", "Kukar M."], "Keywords": ["Lagrangian analysis", "Multi-level directed graphs", "Oceanography", "Spatial data mining", "Spatial-temporal association rules"], "Date": "2013", "Abstract": "We propose a methodology that upgrades the methods of the Lagrangian analysis of surface sea-water parcels. This methodology includes data mining with efficient visualization techniques, namely, spatial-temporal association rules and multi-level directed graphs with different levels of space and time granularity. In the resulting multi-level directed graphs we can intertwine knowledge from various disciplines related to oceanography (in our application) and perform the mining of such graphs. We eval- uate the proposed methodology on Lagrangian tracking of virtual particles in the velocity field of the numerical model called the Mediterranean Ocean Forecasting Model (MFS). We describe an efficient algo- rithm based on label propaga tion clustering, which finds cycles and paths in multi-level directed graphs and reveals how the number and size of the cycles depend on the seasons. In addition, we offer three interesting results of the visualization and mining of such graphs, that is, the 12 months periodicity of the exchange of water masses among sea areas, the separation of Mediterr anean Sea circulation in sum- mer and winter situations, obtained with the hierarchical clustering of multi-level directed graphs, and finally, with visualization with multi-level directed graphs we confirm the reversal of sea circulation in the Ionian Sea over the last decades. The aforementioned results received a very favorable evaluation from oceanograph ic experts. \u00a9 2013 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "11"},
{"Title": "SICSIM: A simulator of the educational SIC/XE computer for a system-software course", "Authors": ["Mihelic J.", "Dobravec T."], "Keywords": ["Computer architecture", "Educational software", "Simulator", "System software", "Virtual machine"], "Date": "2015", "Abstract": "A modern computer system provides its support via system software that consists of applications such as an assembler, a linker, a loader and virtual machines. It is of prime importance to give students that are learning system-software concepts a solid base of knowledge without any unnecessary details. To make the subject easy to understand we designed a simulator for a hypothetical computer that is already used in several courses on system software. In the paper, we describe the simulator's behavior as well as its design and implementation. Additionally, we present three case studies of using a simulator in teaching and describe our experience of its use in a course on system software. From the experience of using the simulator in a pedagogical process we conclude that it decreases the time invested by the students to comprehend the topic, and at the same time it enables in depth understanding.", "Language": "en", "Citations": "5"},
{"Title": "Automated Essay Evaluation Augmented with Semantic Coherence Measures", "Authors": ["Zupanc K.", "Bosnic Z."], "Keywords": ["Automated Scoring", "Essay Evaluation", "Natural Language Processing", "Semantic Attributes"], "Date": "2014", "Abstract": "Manual grading of students' essays is a time-consuming, labor-intensive and expensive activity for educational institutions. It is nevertheless necessary since essays are considered to be the most useful tool to assess learning outcomes. Automated essay evaluation represents a practical solution to this task, however, its main weakness is predominant focus on vocabulary and text syntax, and limited consideration of text semantics. In this work, we propose an extension to existing automated essay evaluation systems that incorporates additional semantic attributes. We design the novel attributes by transforming sequential parts of an essay into the semantic space and measuring changes between them to estimate coherence of the text. The resulting system (called SAGE - Semantic Automated Grader for Essays) achieves significantly higher grading accuracy compared with 8 other state-of-the-art automated essay evaluation systems.", "Language": "en", "Citations": "4"},
{"Title": "Dynamic multi-level appearance models and adaptive clustered decision trees for single target tracking", "Authors": ["Xiao J.", "Stolkin R.", "Leonardis A."], "Keywords": ["Adaptive clustered decision trees", "Multi-level appearance models", "Single target tracking"], "Date": "2017", "Abstract": "This paper presents a tracking algorithm for arbitrary objects in challenging video sequences. Targets are modelled at three different levels of granularity (pixel, parts and bounding box levels), which are cross-constrained to enable robust model relearning. The main contribution is an adaptive clustered decision tree method which dynamically selects the minimum combination of features necessary to sufficiently represent each target part at each frame, thereby providing robustness with computational efficiency. The adaptive clustered decision tree is used in two separate ways: firstly for parts level matching between successive frames; secondly to select the best candidate image regions for learning new parts of the target. We test the tracker using two different tracking benchmarks (VOT2013-2014 and CVPR2013 tracking challenges), based on two different test methodologies, and show it to be more robust than the state-of-the-art methods from both of those tracking challenges, while also offering competitive tracking precision. Additionally, we evaluate the contribution of each key component of the tracker to overall performance; test the sensitivity of the tracker under different initialization conditions; investigate the effect of using features in different orders within the decision trees; illustrate the flexibility of the method for handling arbitrary kinds of features, by showing how it easily extends to handle RGB-D data.", "Language": "en", "Citations": "4"},
{"Title": "Feature selection for object detection: The best group vs. the group of best", "Authors": ["Furst L.", "Leonardis A."], "Keywords": [], "Date": "2014", "Abstract": "The problem of visual object detection, the goal of which is to predict the locations and sizes of all objects of a given visual category (e.g., cars) in a given set of images, is often based on a possibly large set of local features, only a few of which might actually be useful for the given detection setup. Feature selection is concerned with finding a 'useful' subset of features. In this paper, we compare two approaches to feature selection in a visual object detection setup. One of them selects features based on their individual utility scores alone, regardless of possible interdependence with other features. The other approach employs the AdaBoost framework and hence implicitly deals with interdependence. Using two feature extraction methods and several image datasets, we experimentally confirm the significance of feature interdependence: features that perform well individually do not necessarily perform well as a group. \u00a9 2014 MIPRO.", "Language": "en", "Citations": "0"},
{"Title": "Providing better feedback for students solving programming tasks using project tomo", "Authors": ["Jerse G.", "Lokar M."], "Keywords": [], "Date": "2018", "Abstract": "Systems for automatic assessment of programming tasks have become a popular choice in programming courses as several advantages of using automatic assessment in teaching and learning process have been observed. One of the most important is the immediate feedback students get. However, the quality of the feedback is essential to achieve good learning results. At the University of Ljubljana we use our proprietary system called Project Tomo as a teaching tool. One of the most important aspects of our system is the possibility to return a detailed feedback and to analyze the student's solution since every submission is stored on the server. Until now we have collected more than 110, 000 attempts along with their history. We are currently in the process of analyzing them. Currently we are concentrating on how to use this data to further improve the quality of the feedback given to the student. Some of the observations and the preliminary results are presented in the paper.", "Language": "en", "Citations": "0"},
{"Title": "On semidefinite programming based heuristics for the graph coloring problem", "Authors": ["Dukanovic I.", "Govorcin J.", "Gvozdenovic N.", "Povh J."], "Keywords": ["Boundary point method", "Graph coloring problem", "Semidefinite programming"], "Date": "2011", "Abstract": "The Lov\u00e1sz theta number is a well-known lower bound on the chromatic number of a graph G, and \u03a8K(G) is its impressive strengthening. We apply semidefinite programming formulation of both functions to obtain suboptimal (matrix) solutions in a polynomial time. These matrices carry valuable information on how to color the graph. The resulting graph coloring heuristics utilizing these two functions are compared on medium sized graphs.", "Language": "en", "Citations": "0"},
{"Title": "GenePath: A system for inference of genetic networks and proposal of genetic experiments", "Authors": ["Zupan B.", "Bratko I.", "Demsar J.", "Juvan P.", "Curk T.", "Borstnik U.", "Beck J.R.", "Halter J.", "Kuspa A.", "Shaulsky G."], "Keywords": ["Abduction", "Background knowledge", "Bioinformatics", "Functional genomics", "Genetic networks", "Knowledge discovery"], "Date": "2003", "Abstract": "A genetic network is a formalism that is often used in biology to represent causalities and reason about biological phenomena related to genetic regulation. We present GenePath, a computer-based system that supports the inference of genetic networks from a set of genetic experiments. Implemented in Prolog, GenePath uses abductive inference to elucidate network constraints based on background knowledge and experimental results. Additionally, it can propose genetic experiments that may further refine the discovered network and establish relations between genes that could not be related based on the original experimental data. We illustrate GenePath's approach and utility on analysis of data on aggregation and sporulation of the soil amoeba Dictyostelium discoideum. \u00a9 2003 Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "18"},
{"Title": "Towards trust management standardization", "Authors": ["Trcek D."], "Keywords": ["E-business applications", "Standardization", "Trust management", "Virtual community", "XML"], "Date": "2004", "Abstract": "Recent research in the field of security has evolved into trust issues, which are now one of the interesting research topics. A majority of current approaches proposes techniques that support users' trust processes, while a minority of them addresses the essence of trust. The latter approaches form the basis for the work presented in this paper. Outer manifestations of trust phenomenon are formalized in order to facilitate the development in this field. The main goal is to provide means for computable trust that can be used in a standardized way for contemporary internet-based applications, independently of its cognitive principles. \u00a9 2004 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "11"},
{"Title": "Security policy formalization Formalizacija varnostnih politik", "Authors": ["Trampus M.", "Ciglaric M.", "Vidmar T."], "Keywords": ["Policy formalization", "Ponder", "Security policy"], "Date": "2005", "Abstract": "The paper discusses security policies, assisting access control and describing acceptable behaviour of any system. Since they reflect a certain view of the system security, there is a variety of modern access control models. Apart from their presentation, the paper discusses also, some formal languages as a tool for security policy specification. One of them - Ponder, developed within Policy Research Group of the Imperial College of Science and Technology of London, is presented more in detail. The paper ends by commenting on potentially conflicting security policy rules and suggests their ways of solving.", "Language": "en", "Citations": "0"},
{"Title": "Vector disambiguation for translation extraction from comparable corpora", "Authors": ["Apidianaki M.", "Ljubesic N.", "Fiser D."], "Keywords": ["Comparable corpora", "Sense clustering", "Word sense disambiguation"], "Date": "2013", "Abstract": "We present a new data-driven approach for enhancing the extraction of translation equivalents from comparable corpora which exploits bilingual lexico-semantic knowledge harvested from a parallel corpus. First, the bilingual lexicon obtained from word-aligning the parallel corpus replaces an external seed dictionary, making the approach knowledge-light and portable. Next, instead of using simple one-to-one mappings between the source and the target language, translation equivalents are clustered into sets of synonyms by a cross-lingual Word Sense Induction method. The obtained sense clusters enable us to expand the translation of vector features with several translation variants using a cross-lingual Word Sense Disambiguation method. Consequently, the vector features are disambiguated and translated with the translation variants included in the semantically most appropriate cluster, thus producing less noisy and richer vectors that allow for a more successful cross-lingual vector comparison than in previous methods.", "Language": "en", "Citations": "1"},
{"Title": "CS unplugged: Experiences and extensions", "Authors": ["Demsar I.", "Demsar J."], "Keywords": [], "Date": "2015", "Abstract": "CS Unplugged is a set of activities for teaching CS concepts without using computers. We translated it to Slovenian and used it in different contexts, from the classroom and afterschool activity to summer school to professional development courses. In the paper, we summarize our adaptations, extensions and experiences.", "Language": "en", "Citations": "1"},
{"Title": "Binding and cross-modal learning in Markov logic networks", "Authors": ["Vrecko A.", "Skocaj D.", "Leonardis A."], "Keywords": ["Binding", "Cognitive systems", "Cross-modal learning", "Graphical models", "Markov logic networks"], "Date": "2011", "Abstract": "Binding - the ability to combine two or more modal representations of the same entity into a single shared representation is vital for every cognitive system operating in a complex environment. In order to successfully adapt to changes in an dynamic environment the binding mechanism has to be supplemented with cross-modal learning. In this paper we define the problems of high-level binding and cross-modal learning. By these definitions we model a binding mechanism and a cross-modal learner in a Markov logic network and test the system on a synthetic object database. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "An adaptive BIC approach for robust audio stream segmentation", "Authors": ["Zibert J.", "Brodnik A.", "Mihelic F."], "Keywords": ["Audio segmentation", "Bayesian information criterion", "Speaker diarization"], "Date": "2009", "Abstract": "In this paper we focus on an audio segmentation. We present a novel method for robust estimation of decision-thresholds for accurate detection of acoustic change points in continuous audio streams. In standard segmentation procedures the decision-thresholds are usually set in advance and need to be tuned from development data. In the presented approach we tried to remove a need for using pre-determined decision-thresholds and propose a method for estimation of thresholds directly from the currently processed audio data. It employs change-detection methods from two well-established audio segmentation approaches based on the Bayesian Information Criterion. Following from that, we develop two audio segmentation procedures, which enable us to adaptively tune boundary-detection thresholds and to combine different audio representations in the segmentation process. The proposed segmentation procedures are tested on broadcast news audio data. Copyright \u00a9 2009 ISCA.", "Language": "en", "Citations": "4"},
{"Title": "The impact of the latest 3D technologies on the documentation of underwater heritage sites", "Authors": ["Eric M.", "Kovacic R.", "Berginc G.", "Pugelj M.", "Stopinsek Z.", "Solina F."], "Keywords": [], "Date": "2013", "Abstract": "Documenting underwater cultural heritage is a challenging undertaking. Underwater environment is not a man's natural habitat and special equipment and devices had to be invented so that he could enter and study this environment. Several decades of underwater research and many sacrifices were needed to fully understand the importance of underwater heritage and its protection. The means for accurate documentation underwater are very limited and demanding, due to required technical equipment it is also expensive. Emergence of modern 3D methods and accompanying software tools for processing of 3D data is therefore of utmost importance for documenting and protection of underwater cultural heritage. In comparison to manual and analog methods, 3D methods offer much better accuracy, they substantially shorten the necessary time spent underwater and in this way improve the safety at work as well as lower the entire cost of field work. For illustration of the above development we discuss archeological case studies from the North East Adriatic. \u00a9 2013 IEEE.", "Language": "en", "Citations": "15"},
{"Title": "Symptoms and medications change patterns for Parkinson's disease patients stratification", "Authors": ["Valmarska A.", "Miljkovic D.", "Konitsiotis S.", "Gatsios D.", "Lavrac N.", "Robnik-Sikonja M."], "Keywords": ["Analysis of disease progression", "Analysis of medications treatment", "Multitask learning", "Parkinson's disease", "Symptoms impact"], "Date": "2018", "Abstract": "Quality of life of patients with Parkinson's disease degrades significantly with disease progression. This paper presents a step towards personalized management of Parkinson's disease patients, based on discovering groups of similar patients. Similarity is based on patients\u2019 medical conditions and changes in the prescribed therapy when the medical conditions change. We present two novel approaches. The first algorithm discovers symptoms\u2019 impact on Parkinson's disease progression. Experiments on the Parkinson Progression Markers Initiative (PPMI) data reveal a subset of symptoms influencing disease progression which are already established in Parkinson's disease literature, as well as symptoms that are considered only recently as possible indicators of disease progression by clinicians. The second novelty is a methodology for detecting patterns of medications dosage changes based on the patient status. The methodology combines multitask learning using predictive clustering trees and short time series analysis to better understand when a change in medications is required. The experiments on PPMI data demonstrate that, using the proposed methodology, we can identify some clinically confirmed patients\u2019 symptoms suggesting medications change. In terms of predictive performance, our multitask predictive clustering tree approach is mostly comparable to the random forest multitask model, but has the advantage of model interpretability.", "Language": "en", "Citations": "4"},
{"Title": "Relating rubber melts' viscosity and molecular weight distribution by neural networks", "Authors": ["Lotric U.", "Susteric Z."], "Keywords": ["Modeling", "Neural networks", "Rubber morphology", "Viscosity"], "Date": "2001", "Abstract": "The relationship between flow-properties of rubber melts and their structural characteristics is usually given as a dependence of the melt's initial or zero-shear rate viscosity on a single average molecular weight to the power of 3.4. Such a description is for several reasons often inaccurate, if not wrong. Since both, the average molecular weight and the initial viscosity, depend on the entire molecular weight distribution of melts, an attempt is made to relate the initial viscosity directly to molecular weight distribution, rather than to a single average. As this is hardly feasible analytically, the computational technique of neural networks is used for the purpose. The technique has yielded good results, revealing also some deeper implications in property-structure relations, most significantly the dominance of rubber morphology over the molecular structure when viscosity is concerned.", "Language": "en", "Citations": "3"},
{"Title": "Incremental and robust learning of subspace representations", "Authors": ["Skocaj D.", "Leonardis A."], "Keywords": ["Incremental learning", "Robust learning", "Subspace learning"], "Date": "2008", "Abstract": "Learning is a fundamental capability of any cognitive system. To enable efficient operation of a cognitive agent in a real-world environment, visual learning has to be a continuous and robust process. In this article, we present a method for subspace learning, which takes these considerations into account. We present an incremental method, which sequentially updates the principal subspace considering weighted influence of individual images as well as individual pixels within an image. We further extend this approach to enable determination of consistencies in the input data and imputation of the inconsistent values using the previously acquired knowledge, resulting in a novel method for incremental, weighted, and robust subspace learning. We demonstrate the effectiveness of the proposed concept in several experiments on learning of object and background representations. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "24"},
{"Title": "FASTKD2 is an RNA-binding protein required for mitochondrial RNA processing and translation", "Authors": ["Popow J.", "Alleaume A.-M.", "Curk T.", "Schwarzl T.", "Sauer S.", "Hentze M.W."], "Keywords": ["ICLIP", "Mendelian disease", "Mitochondria", "Oxidative phosphorylation", "RNA-binding proteins", "Transcript processing"], "Date": "2015", "Abstract": "Mitochondrial RNA processing is an essential step for the synthesis of the components of the electron transport chain in all eukaryotic organisms, yet several aspects of mitochondrial RNA biogenesis and regulation are not sufficiently understood. RNA interactome capture identified several disease-relevant RNA-binding proteins (RBPs) with noncanonical RNAbinding architectures, including all six members of the FASTK (FAS-activated serine/threonine kinase) family of proteins. A mutation within one of these newly assigned FASTK RBPs, FASTKD2, causes a rare form of Mendelian mitochondrial encephalomyopathy. To investigate whether RNA binding of FASTKD2 contributes to the disease phenotype, we identified the RNA targets of FASTKD2 by iCLIP. FASTKD2 interacts with a defined set of mitochondrial transcripts including 16S ribosomal RNA (RNR2) and NADH dehydrogenase subunit 6 (ND6) messenger RNA. CRISPR-mediated deletion of FASTKD2 leads to aberrant processing and expression of RNR2 and ND6 mRNA that encodes a subunit of the respiratory complex I. Metabolic phenotyping of FASTKD2-deficient cells reveals impaired cellular respiration with reduced activities of all respiratory complexes. This work identifies key aspects of the molecular network of a previously uncharacterized, disease-relevant RNAbinding protein, FASTKD2, by a combination of genomic, molecular, and metabolic analyses.", "Language": "en", "Citations": "32"},
{"Title": "Light fountain\u2013a virtually enhanced stone sculpture", "Authors": ["Solina F.", "Meden B."], "Keywords": ["3-D surface model", "art installation", "int art", "Kinect", "OpenFrameworks", "range image", "simulation of running water", "Stone sculpture", "water drops"], "Date": "2017", "Abstract": "The article describes the making of an art piece combining stone sculpture and virtual water. The motivation for this art piece was to enrich the usual static format of a stone sculpture with a dynamic dimension. The dynamic dimension is attained with virtual water droplets running over the stone surface which behave as real water droplets. The 3-D surface of a specially carved stone sculpture is during an exhibition continuously captured by the Kinect sensor. Each water drop out of many thousands, which are introduced into the installation as evenly distributed rain drops, falling over the sculpture, are simulated individually to run over the stone surface following the largest slope. These simulated water drops are projected with a video projector as light points on the surface of the sculpture. An observer can enjoy simultaneously the haptic experience of touching the stone and observing a digitally generated but physically grounded animation.", "Language": "en", "Citations": "1"},
{"Title": "Relational model of temporal data based on 6th normal form Relacijski model vremenskih podataka zasnovan na 6. Normalnoj formi", "Authors": ["Golec D.", "Mahnic V.", "Kovac T."], "Keywords": ["6th Normal Form", "Logical model", "Relation", "Relational modelling", "Temporal data"], "Date": "2017", "Abstract": "This paper brings together two different research areas, i.e. Temporal Data and Relational Modelling. Temporal data is data that represents a state in time while temporal database is a database with built-in support for handling data involving time. Most of temporal systems provide sufficient temporal features, but the relational models are improperly normalized, and modelling approaches are missing or unconvincing. This proposal offers advantages for a temporal database modelling, primarily used in analytics and reporting, where typical queries involve a small subset of attributes and a big amount of records. The paper defines a distinctive logical model, which supports temporal data and consistency, based on vertical decomposition and sixth normal form (6NF). The use of 6NF allows attribute values to change independently of each other, thus preventing redundancy and anomalies. Our proposal is evaluated against other temporal models and super-fast querying is demonstrated, achieved by database join elimination. The paper is intended to help database professionals in practice of temporal modelling.", "Language": "en", "Citations": "0"},
{"Title": "Heterogeneous network decomposition and weighting with text mining heuristics", "Authors": ["Kralj J.", "Robnik-Sikonja M.", "Lavrac N."], "Keywords": ["Centroid classifier", "Heterogeneous information networks", "Network analysis", "Network decomposition", "PageRank", "SVM", "Text mining heuristics"], "Date": "2016", "Abstract": "The paper presents an approach to mining heterogeneous information networks applied to a task of categorizing customers linked in a heterogeneous network of products, categories and customers. We propose a two step methodology to classify the customers. In the first step, the heterogeneous network is decomposed into several homogeneous networks using different connecting nodes. Similarly to the construction of bag-of-words vectors in text mining, we assign larger weights to more important nodes. In the second step, the resulting homogeneous networks are used to classify data either by network propositionalization or label propagation. Because the data set is highly imbalanced we adapt the label propagation algorithm to handle imbalanced data. We perform a series of experiments and compare different heuristics used in the first step of the methodology, as well as different classifiers which can be used in the second step of the methodology.", "Language": "en", "Citations": "0"},
{"Title": "Georeferencing works of literature", "Authors": ["Solina F.", "Ravnik R."], "Keywords": ["Collaborative work", "Georeferences", "KML", "Literature", "Web application"], "Date": "2010", "Abstract": "A pilot web application for collaborative georeferencing of Slovenian works of literature was developed. Problems in georeferencing in general and georeferencing works of literature are discussed. The pilot system can be a model for establishing web sites for georeferencing literature for different languages. The application is based on Google Maps and implemented using PHP, JavaScript and MySQL. The intended users of the system are mainly lovers of literature, travelers and pupils.", "Language": "en", "Citations": "2"},
{"Title": "Cloud integration platform as a concept for integration of applications and systems in farming Obla\u010dna integracijska platforma kot koncept integracije za aplikacije in sisteme v kmetijstvu", "Authors": ["Rupnik R."], "Keywords": ["Cloud integration", "Integration", "Platform"], "Date": "2018", "Abstract": "There are various applications and systems available for farming where each of them cover a niche area and farming processes: ERP (Enterprise Resource Planning) system for farming, application for winegrowers, systems collecting data from various sensors, etc. The problem is that those applications and systems only enable farmers to perform analyses on application or system data. Farmers therefore cannot perform analyses on data of more than one application or system. The EU project AgroIT was defined and executed to solve the problem of standardized integration of applications and systems for farming. The concept of integration of applications and systems is a cloud integration platform where applications and systems do not integrate directly, but through a cloud integration platform based on a standard defined in the project. The paper introduces realization of goals of the AgroIT project and the cloud integration platform as a key achievement of the project.", "Language": "en", "Citations": "1"},
{},
{"Title": "Q2 Prediction of ozone concentrations", "Authors": ["Zabkar J.", "Zabkar R.", "Vladusic D.", "Cemas D.", "Suc D.", "Bratko I."], "Keywords": ["Air pollution", "Ozone concentration prediction model", "Qualitative modelling"], "Date": "2006", "Abstract": "We describe a case study in which we applied Q", "Language": "en", "Citations": "7"},
{"Title": "A case study on multi-modal biometrics in the cloud", "Authors": ["Emersic Z.", "Bule J.", "Zganec-Gros J.", "Struc V.", "Peer P."], "Keywords": ["Biometric-cloud services", "Cloud computing", "Fusion strategies", "Multi-modal biometrics"], "Date": "2014", "Abstract": "Cloud computing is particularly interesting for the area of biometric recognition, where scalability, availability and accessibility are important aspects. In this paper we try to evaluate different strategies for combining existing uni-modal (cloud-based) biometric experts into a multi-biometric cloud-service. We analyze several fusion strategies from the perspective of performance gains, training complexity and resource consumption and discuss the results of our analysis. The experimental evaluation is conducted based on two biometric cloud-services developed in the scope of the Competence Centere CLASS, a face recognition service and a fingerprint recognition service, which are also briefly described in the paper. The presented results are important to researchers and developers working in the area of biometric services for the cloud looking for easy solutions for improving the quality of their services.", "Language": "en", "Citations": "1"},
{"Title": "Mining data from hemodynamic simulations for generating prediction and explanation models", "Authors": ["Radovic M.D.", "Filipovic N.D.", "Bosnic Z.", "Vracar P.", "Kononenko I."], "Keywords": [], "Date": "2010", "Abstract": "Arterial geometry variability is present both within and across individuals. To analyze the influence of geometric parameters on maximal wall shear stress (MWSS) in the human carotid artery bifurcation, the computer simulations were run to generate the data pertaining to this phenomenon. In our work we evaluate various prediction models for modeling relationship between geometric parameters of the carotid bifurcation and the MWSS. The results revealed the highest potential of using the neural network model for this prediction task. The achieved results and generated explanations of the prediction model represent progress in assessment of stroke risk for a given patient's geometry in real time. \u00a9 2010 IEEE.", "Language": "en", "Citations": "4"},
{},
{"Title": "Biomechanical and clinical alterations of the hip joint following femoral neck fracture and implantation of bipolar hip endoprosthesis", "Authors": ["Smrke D.", "Biscevic M.", "Smrke B.U.R.", "Zupan B."], "Keywords": ["Arthroplasty", "Biomechanicanics", "Femoral neck fracture", "Status"], "Date": "2010", "Abstract": "The implantation of a bipolar partial hip endoprosthesis is a treatment of choice for displaced medial femoral neck fracture. We present an experimental study which asses and compare biomechanical and clinical status through period before and after hip fracture and implantation of bipolar partial hip endoprosthesis. This study encompassed 75 patients who suffered from an acute medial femoral neck fracture and were treated with the implantation of a bipolar partial hip endoprosthesis. Their biomechanical status (stress distribution on the hip joint weight bearing area) and clinical status (Harris Hip Score) were estimated for the time prior to the injury and assessed at the follow-up examination that was, on average, carried out 40 months after the operation. Despite ageing, the observed Harris Hip Score at the follow-up examination was higher than that estimated prior to the injury (77.9>69.6;p=0.006). Similarly, the hip stress distribution was reduced (2.7 MPa<2.3 MPa; p=0.001). While this reduction can be attributed to a loss of weight due to late ageing, the principal improvement came from the operative treatment and corresponding restoration of the biomechanical properties of the hip joint. The implantation of a bipolar partial hip endoprosthesis for patients with displaced medial femoral neck fractures improves the biomechanical and clinical features of the hip, what should have on mind during making decision about treatment.", "Language": "en", "Citations": "2"},
{"Title": "Estimating the time complexity of the algorithms by counting the Java bytecode instructions", "Authors": ["Dobravec T."], "Keywords": [], "Date": "2017", "Abstract": "Ranging the selected algorithms with the same theoretical time complexity boundaries can only be done by comparing the behavior of their implementations in the real environment. Timing the algorithms in practice is very difficult since it is hard to ensure a fair and reproducible environment in which implementations can be compared. In this paper we present a system called ALGator that was developed to facilitate the process of testing, comparing and evaluating the algorithms. Besides the time complexity indicators, ALGator also measures the usages of the Java bytecode instructions. We present the usage of the ALGator's JVM indicators for the estimation of the time complexity of selected bytecode instructions. We also present a method to predict the behavior of the simple algorithm for matrix multiplication using the results of the JVM indicators analysis.", "Language": "en", "Citations": "1"},
{"Title": "A trajectory-based analysis of coordinated team activity in a basketball game", "Authors": ["Perse M.", "Kristan M.", "Kovacic S.", "Vuckovic G.", "Pers J."], "Keywords": ["Basketball", "Behavior recognition", "Complex behavior", "Play segmentation", "Semantic description"], "Date": "2009", "Abstract": "This paper proposes a novel, trajectory-based approach to the automatic recognition of complex multi-player behavior in a basketball game. First, a probabilistic play model is applied to the player-trajectory data in order to segment the play into game phases (offense, defense, time out). In this way, both the temporal boundaries of the observed activity and its broader context are obtained. Next, the team's activity is analyzed in more detail by detecting the key elements of basketball play. Following basketball theory, these key elements (starting formation, screen, and move) are the building blocks of basketball play, and therefore their temporal order is used to produce a semantic description of the observed activity. Finally, the activity is recognized by comparing its semantic description with the descriptions of manually defined templates, stored in a database. The effectiveness and robustness of the proposed approach is demonstrated on two championship games and 71 examples of three types of basketball offense. \u00a9 2008 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "81"},
{"Title": "3-Connected planar graphs are 5-distinguishing colorable with two exceptions", "Authors": ["Fijavz G.", "Negami S.", "Sano T."], "Keywords": ["Distinguishing chromatic number", "Distinguishing number", "Planar graphs"], "Date": "2011", "Abstract": "A graph G is said to be d-distinguishing colorable if there is a d-coloring of G such that no automorphism of G except the identity map preserves colors. We shall prove that every 3-connected planar graph is 5-distinguishing colorable except K", "Language": "en", "Citations": "3"},
{"Title": "Severe cardiac autonomic derangement and altered ventricular repolarization pave the way to postoperative atrial fibrillation", "Authors": ["Kalisnik J.M.", "Hrovat E.", "Hrastovec A.", "Avbelj V.", "Zibert J.", "Gersak B."], "Keywords": ["Cardiac autonomic modulation", "Cardiac surgery", "Heart rate variability", "Postoperative atrial fibrillation", "QT interval"], "Date": "2015", "Abstract": "Objective: Postoperative atrial fibrillation (POAF) is a frequent complication after heart surgery. It has been shown that cardiac autonomic derangement plays a significant role in the genesis of atrial fibrillation (AF) and that AF might also be promoted by altered repolarization. Thus, the aim of our study was to determine the levels of cardiac autonomic modulation and repolarization properties in patients developing POAF. Methods: Seventy-nine patients scheduled for aortic and/or coronary artery bypass grafting surgery with cardiopulmonary bypass were enrolled prospectively. High-resolution 20-minute electrocardiogram recordings were obtained day before surgery to determine P, PR, QT, and QTc intervals, as well as linear (time and frequency domain) and nonlinear heart rate variability parameters (fractal dimension and detrended fluctuation analysis). QTc interval was calculated using Framingham correction. Results: Twenty-nine patients developed POAF (AF group), and 50 did not (non-AF group). Groups were similar regarding demographics, surgery type, and perioperative characteristics, except for older age in the AF group. QT and QTc intervals (Framingham) were longer in the AF group [442 (44) vs 422 (28) milliseconds, P = 0.018; and 448 (44) vs 431 (24) milliseconds, P = 0.031 and P = 0.019, respectively]. Time domain heart rate variability parameter PNN50 (percentage of pairs of adjacent NN intervals differing 950 milliseconds) was higher [14% (21%) vs 8% (16%), P = 0.015], and nonlinear parameter detrended fluctuation analysis \u03b12 was lower in the AF group [0.81 (0.21) vs 0.91 (0.20), P = 0.031]. Conclusions: Profound cardiac autonomic derangement, suggestive of parasympathetic excessive modulation, exists preoperatively in patients inclined to POAF after cardiac surgery, whereby parameters PNN50 and \u03b12 differentiated the AFfrom the non-AF group. Prolonged QTc intervals are associated with an increased risk of POAF.", "Language": "en", "Citations": "3"},
{"Title": "Spam filtering using statistical data compression models", "Authors": ["Bratko A.", "Filipic B.", "Cormack G.V.", "Lynam T.R.", "Zupan B."], "Keywords": ["Dynamic Markov compression", "Markov models", "Prediction by partial matching", "Spam filtering", "Text categorization"], "Date": "2006", "Abstract": "Spam filtering poses a special problem in text categorization, of which the defining characteristic is that filters face an active adversary, which constantly attempts to evade filtering. Since spam evolves continuously and most practical applications are based on online user feedback, the task calls for fast, incremental and robust learning algorithms. In this paper, we investigate a novel approach to spam filtering based on adaptive statistical data compression models. The nature of these models allows them to be employed as probabilistic text classifiers based on character-level or binary sequences. By modeling messages as sequences, tokenization and other error-prone preprocessing steps are omitted altogether, resulting in a method that is very robust. The models are also fast to construct and incrementally updateable. We evaluate the filtering performance of two different compression algorithms; dynamic Markov compression and prediction by partial matching. The results of our empirical evaluation indicate that compression models outperform currently established spam filters, as well as a number of methods proposed in previous studies.", "Language": "en", "Citations": "161"},
{"Title": "Statistical approach for forecasting road surface temperature", "Authors": ["Krsmanc R.", "Slak A.S.", "Demsar J."], "Keywords": ["Prediction models", "Road weather information systems (RWIS)", "Road weather station (RWS)", "Stepwise linear regression"], "Date": "2013", "Abstract": "Snow and ice make road conditions and use difficult and represent a major challenge for the winter road maintenance service. Optimizing winter maintenance service and safety thus requires accurate short-term forecasts of the meteorological state of the roads. The most common approach to forecasting road conditions is an energy balance model based on a one-dimensional diffusion equation. Physical models can predict the road surface temperature, which is the most important parameter for determining the road surface condition (e.g. dry, wet, ice, snow). However, such models can show a large degree of error at sites where physical processes are too complex to be simulated correctly. To solve this problem, physical models are often combined with statistical approaches. This paper proposes a purely statistical method for forecasting road surface temperature based on stepwise linear regression analysis with appropriate selection of the input parameters and separate models for different time intervals. The method is tested on data from several Slovenian road weather stations. Its accuracy is comparable to or better than that of physical models. \u00a9 2012 Royal Meteorological Society.", "Language": "en", "Citations": "10"},
{"Title": "Erratum to \"Reduction of symmetric configurations n3\" [Discrete Appl. Math. 99 (1-3) (2000) 401 -411] (DOI:10.1016/S0166-218X(99)00147-X)", "Authors": ["Steffen E.", "Pisanski T.", "Boben M.", "Ravnik N."], "Keywords": ["Configurations", "Cubic graphs", "Incidence structures"], "Date": "2006", "Abstract": "In [H.-G. Carstens, T. Dinski, E. Steffen, Reduction of symmetric configurations n", "Language": "en", "Citations": "2"},
{"Title": "Beyond aphasia: Altered EEG connectivity in Broca's patients during working memory task", "Authors": ["Rutar Gorisek V.", "Zupanc Isoski V.", "Belic A.", "Manouilidou C.", "Koritnik B.", "Bon J.", "Pecaric Meglic N.", "Vrabec M.", "Zibert J.", "Repovs G.", "Zidar J."], "Keywords": ["Broca's aphasia", "Default mode network", "EEG coherence", "Gamma", "Multiple demand network", "Phonological loop", "Synchronized oscillations", "Theta", "Working memory"], "Date": "2016", "Abstract": "Broca's region and adjacent cortex presumably take part in working memory (WM) processes. Electrophysiologically, these processes are reflected in synchronized oscillations. We present the first study exploring the effects of a stroke causing Broca's aphasia on these processes and specifically on synchronized functional WM networks. We used high-density EEG and coherence analysis to map WM networks in ten Broca's patients and ten healthy controls during verbal WM task. Our results demonstrate that a stroke resulting in Broca's aphasia also alters two distinct WM networks. These theta and gamma functional networks likely reflect the executive and the phonological processes, respectively. The striking imbalance between task-related theta synchronization and desynchronization in Broca's patients might represent a disrupted balance between task-positive and WM-irrelevant functional networks. There is complete disintegration of left fronto-centroparietal gamma network in Broca's patients, which could reflect the damaged phonological loop.", "Language": "en", "Citations": "0"},
{"Title": "Social network aided plagiarism detection", "Authors": ["Zrnec A.", "Lavbic D."], "Keywords": [], "Date": "2017", "Abstract": "The prevalence of different kinds of electronic devices and the volume of content on the Web have increased the amount of plagiarism, which is considered an unethical act. If we want to be efficient in the detection and prevention of these acts, we have to improve today's methods of discovering plagiarism. The paper presents a research study where a framework for the improved detection of plagiarism is proposed. The framework focuses on the integration of social network information, information from the Web, and an advanced semantically enriched visualization of information about authors and documents that enables the exploration of obtained data by seeking of advanced patterns of plagiarism. To support the proposed framework, a special software tool was also developed. The statistical evaluation confirmed that the employment of social network analysis and advanced visualization techniques led to improvements in the confirmation and investigation stages of the plagiarism detection process, thereby enhancing the overall efficiency of the plagiarism detection process.", "Language": "en", "Citations": "2"},
{"Title": "A quality management model based on databases and knowledge", "Authors": ["Srdoc A.", "Bratko I.", "Sluga A."], "Keywords": ["DQC model", "Knowledge management", "Knowledge synthesis", "Machine learning", "Quality management", "Ship-repair"], "Date": "2011", "Abstract": "In the paper the results of the doctoral research in which a new knowledgefocused approach to quality management called Deep Quality Concept (DQC) is conceptualised, are presented in short. The main features of the new quality management model developed on that approach also are presented. Particular attention is paid to expert knowledge-especially tacit and deep domain knowledge, i.e. on knowledge that is not only decisive for quality, but also for the competitive advantage of an organisation. Given that such knowledge is hard or even impossible to be formalised by traditional methods, computer concepts-including artificial intelligence (AI) concepts, also are included in the model. The DQC model contains both, i.e. (1) the part concerning development of quality standard i.e. quality award criteria based on the developed approach; and (2) the part concerning implementation of the so obtained standard i.e. award criteria. The main points and potential of the model and approach are validated in the case study by example of delivery time estimate in ship-repair, aimed to get a more transparent assessment and decision structure through the use of machine learning-one of the AI's best known and efficient knowledge acquisition and representing techniques. The application results showed that the proposed approach can contribute significantly to the more reliable quality, particularly in complex and highly dynamic and stochastic domains. That confirmed that computer and AI concepts need to be considered as an integral part of quality management systems, as it is anticipated in the DQC model.", "Language": "en", "Citations": "0"},
{"Title": "Virtual coronary cineangiography", "Authors": ["Lebar Bajec I.", "Trunk P.", "Oseli D.", "Zimic N."], "Keywords": ["Coronary angiography", "Medical training tool", "Stenosis diagnosis", "Stenosis quantification", "Virtual reality"], "Date": "2003", "Abstract": "The mastering of myocardial infarction diagnosis is traditionally composed of laborious trial- and error-based examination of canonical coronary cineangiographies. In the following article we suggest a system that enables the instructor to generate student-specific cases, thus allowing teaching not only the basic feature searching and stenosis evaluation processes, but also the importance of the correct acquisition viewpoint. With the proposal of the development of the Digital Cardiologist intelligent agent we also envisage the possibility of the student's self-tutoring. \u00a9 2003 Elsevier Science Ltd. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "Statistical comparison of classifiers through Bayesian hierarchical modelling", "Authors": ["Corani G.", "Benavoli A.", "Demsar J.", "Mangili F.", "Zaffalon M."], "Keywords": [], "Date": "2017", "Abstract": "Usually one compares the accuracy of two competing classifiers using null hypothesis significance tests. Yet such tests suffer from important shortcomings, which can be overcome by switching to Bayesian hypothesis testing. We propose a Bayesian hierarchical model that jointly analyzes the cross-validation results obtained by two classifiers on multiple data sets. The model estimates more accurately the difference between classifiers on the individual data sets than the traditional approach of averaging, independently on each data set, the cross-validation results. It does so by jointly analyzing the results obtained on all data sets, and applying shrinkage to the estimates. The model eventually returns the posterior probability of the accuracies of the two classifiers being practically equivalent or significantly different.", "Language": "en", "Citations": "5"},
{"Title": "Q2 learning and its application to car modelling", "Authors": ["Vladusic D.", "Suc D.", "Bratko I.", "Rulka W."], "Keywords": [], "Date": "2006", "Abstract": "In this paper we describe an application of Q 2 learning, a recently developed approach to machine learning in numerical domains (uc et al., 20032004) to the automated modelling of a complex, industrially relevant mechanical system - a four wheel suspension and steering system of a car. In this experiment, first a qualitative model of this dynamic system was induced from data, and then this model was reified into a quantitative model. The induced qualitative models enable explanation of relations among the variables in the system and, when reified into quantitative models, enable accurate numerical prediction. Furthermore, the qualitative guidance of the quantitative modelling process leads to predictions that are significantly more accurate than those obtained by state-of-the-art numerical learning methods.", "Language": "en", "Citations": "2"},
{"Title": "GarQ: An efficient scheduling data structure for advance reservations of grid resources", "Authors": ["Sulistio A.", "Cibej U.", "Prasad S.K.", "Buyya R."], "Keywords": ["Advance reservation", "Calendar queue", "Data structure", "Grid computing", "Segment tree"], "Date": "2009", "Abstract": "In Grids, users may require assurance for completing their jobs on shared resources. Such guarantees can only be provided by reserving resources in advance. However, if many reservation requests arrive at a resource simultaneously, the overhead of providing such service due to adding, deleting and searching, will be significant. An efficient data structure for managing these reservations plays an important role in order to minimise the time required for searching available resources, adding and deleting reservations. In this paper, we present new approaches to advance reservation in order to deal with the limitations of the existing data structures, such as Segment Tree and Calendar Queue in similar problems. We propose a Grid advanced reservation Queue (GarQ), which is a new data structure that improves some weaknesses of the aforementioned data structures. We demonstrate the superiority of the proposed structure by conducting a detailed performance evaluation on real workload traces.", "Language": "en", "Citations": "20"},
{"Title": "Data mining based decision support system to support association rules", "Authors": ["Rupnik R.", "Kukar M."], "Keywords": ["Association rules", "Data mining", "Decision support", "Decision support system"], "Date": "2007", "Abstract": "Modern organizations use several types of decision support systems to facilitate decision support. In many cases OLAP based tools are used in the business area enabling multiple views on data and through that a deductive approach to data analysis. Data mining extends the possibilities for decision support by discovering patterns and relationships hidden in data and therefore enabling an inductive approach to data analysis. The use of data mining to facilitate decision support enables new approaches to problem solving. The paper introduces our approach to integration of decision support systems with data mining methods. We introduce a data mining based decision support system designed for business users enabling them to use association rule models to facilitate decision support with only a basic level of knowledge of data mining.", "Language": "en", "Citations": "4"},
{"Title": "A context-aware approach to wireless transmission adaptation", "Authors": ["Pejovic V.", "Belding E.M."], "Keywords": [], "Date": "2011", "Abstract": "Recent advancements in wireless transmission have enabled networks with a high level of physical layer flexibility. Unfortunately, these new opportunities are not harnessed by modern wireless systems. Due to inefficient resource allocation, systems typically encounter problems such as spectrum scarcity, energy depletion or low quality of service. In this paper we consider the problem of physical layer parameter adaptation in a flexible wireless system. We observe that for many practical purposes the acceptable quality of communication depends on the interplay among the packet loss ratio, energy savings and spectrum utilization. We harness this fact and propose a context-aware physical layer parameter adaptation solution, WhiteRate. Our solution adjusts the modulation level, coding scheme and channel width to achieve the communication profile that matches application requirements. We implement WhiteRate in GNUradio and evaluate it in both indoor and outdoor environments. We demonstrate improvements on two important fronts: spectrum utilization and energy efficiency. Moreover, we show that by using WhiteRate, both benefits can be achieved simultaneously. \u00a9 2011 IEEE.", "Language": "en", "Citations": "11"},
{"Title": "Java RMI, RMI tunneling and web services comparison and performance analysis", "Authors": ["Juric M.B.", "Kezmah B.", "Hericko M.", "Rozman I.", "Vezocnik I."], "Keywords": ["Performance", "RMI", "SOAP", "Tunneling", "Web services"], "Date": "2004", "Abstract": "This article compares different approaches for developing Java distributed applications which have to communicate through firewalls and proxies, including RMI over open ports, HTTP-to-port, HTTP-to-CGI, HTTP-to-servlet tunneling and web services. A functional comparison of approaches has been done, as well as a detailed performance analysis with overhead analysis and identification of optimizations. Therefore the paper contributes to the overall understanding of different approaches for developing Java distributed applications in circumstances, where the communication through firewalls and/or proxies is inevitable. The paper also contributes to the understanding of performance related issues.", "Language": "en", "Citations": "23"},
{"Title": "Analysis of experts' quantitative assessment of adolescent basketball players and the role of anthropometric and physiological attributes", "Authors": ["Strumbelj E.", "Erculj F."], "Keywords": ["coaching", "morphology", "motor skills", "performance evaluation", "players' selection", "sports"], "Date": "2014", "Abstract": "In this paper, we investigated two questions: (1) can measurements of anthropometric and physiological attributes substitute for expert assessment of adolescent basketball players, and (2) how much does the quantitative assessment of a player vary among experts? The first question is relevant to the potential simplification of the player selection process. The second question pertains directly to the validity of expert quantitative assessment. Our research was based on data from 148 U14 female and male basketball players. For each player, an array of anthropometric and physiological attributes was recorded, including body height, body mass, BMI, and several motor skill tests. Furthermore, each player's current ability and potential ability were quantitatively evaluated by two different experts from a group of seven experts. Analysis of the recorded data showed that the anthropometric and physiological attributes explained between 15% and 40% of the variance in experts' scores. The primary predictive attributes were speed and agility (for predicting current ability) and body height and growth potential (for predicting potential ability). We concluded that these attributes were not sufficiently informative to act as a substitute for expert assessment of the players' current or potential ability. There is substantial variability in different experts' scores of the same player's ability. However, the differences between experts are mostly in scale, and the relationships between experts' scores are monotonic. That is, different experts rank players on ability very similarly, but their scores are not well calibrated.", "Language": "en", "Citations": "5"},
{"Title": "Computational approaches for the genetic and phenotypic characterization of a Saccharomyces cerevisiae wine yeast collection", "Authors": ["Franco-Duarte R.", "Umek L.", "Zupan B.", "Schuller D."], "Keywords": ["Bayesian classifier", "Ethanol resistance", "Genotype", "Indigenous yeast", "Microsatellite", "Phenotype", "Saccharomyces cerevisiae", "Strain collection", "Winemaking"], "Date": "2009", "Abstract": "Within this study, we have used a set of computational techniques to relate the genotypes and phenotypes of natural populations of Saccharomyces cerevisiae, using allelic information from 11 microsatellite loci and results from 24 phenotypic tests. A group of 103 strains was obtained from a larger S. cerevisiae winemaking strain collection by clustering with self-organizing maps. These strains were further characterized regarding their allelic combinations for 11 microsatellites and analysed in phenotypic screens that included taxonomic criteria (carbon and nitrogen assimilation tests, growth at different temperatures) and tests with biotechnological relevance (ethanol resistance, H", "Language": "en", "Citations": "18"},
{"Title": "An approximate method for filtering out data dependencies with a sufficiently large distance between memory references", "Authors": ["Bulic P.", "Dobravec T."], "Keywords": ["Data dependency", "Multimedia extensions", "SIMD instructions", "Vectorizing compilers"], "Date": "2011", "Abstract": "In this paper we present an approximate algorithm for detecting and filtering data dependencies with a sufficiently large distance between memory references. A sequence of the same operations (typically enclosed in a 'for' loop) can be replaced with a single SIMD operation if the distance between memory references is greater than or equal to the number of data processed in the SIMD register. Some loops that could not be vectorized on traditional vector processors, can still be parallelized for short SIMD execution. There are a number of approximate data-dependence tests that have been proposed in the literature but in all of them data dependency will be assumed when actually there is no such a dependence that could restrict parallelization related to the short SIMD execution model. By examining the properties of linear subscript expressions of possibly conflicting data references, our algorithm gives the green light to the parallelization process if some sufficient conditions regarding the dependence distance are met. Our method is based on the Banerjee test and checks the minimum and maximum distances between memory references within the iteration space rather than searching for the existence of an integer solution to the dependence equation. The proposed method extends the accuracy and applicability of the classical Banerjee test. \u00a9 2009 Springer Science+Business Media, LLC.", "Language": "en", "Citations": "1"},
{},
{"Title": "The moodstripe - An evaluation of a novel visual interface as an alternative for online response gathering", "Authors": ["Pesek M.", "Godec P.", "Poredos M.", "Strle G.", "Guna J.", "Stojmenova E.", "Pogacnik M.", "Marolt M."], "Keywords": ["Gathering feedback efficiency", "Human computer interaction", "System usability score", "User evaluation study", "User interface"], "Date": "2014", "Abstract": "We present an innovative dynamic visual interface, the Mood-Stripe, which provides a continuous-scale, multi-parameter drag-and-drop alternative to the standard n-degree (Likert) scale widgets, commonly used in online evaluation processes. We elaborate on the motivation for the development of the new user input interfaces, and present the results of cross evaluation of the GMail product by using the SUS questionnaire with the standard and the proposed MoodStripe interfaces. The overall goal is to design a more intuitive interface, by reducing the noise and task load inherent in traditional interfaces for standardized user-feedback gathering tests. The results show the MoodStripe interface outperforms the standard scale approach both in terms of intuitiveness and functionality. Additionally, the cross-evaluation of the both approaches shows comparable SUS scores.", "Language": "en", "Citations": "0"},
{"Title": "Automated essay evaluation with semantic analysis", "Authors": ["Zupanc K.", "Bosnic Z."], "Keywords": ["Automated scoring", "Essay evaluation", "Natural language processing", "Semantic attributes", "Semantic feedback"], "Date": "2017", "Abstract": "Essays are considered as the most useful tool to assess learning outcomes, guide students\u2019 learning process and to measure their progress. Manual grading of students\u2019 essays is a time-consuming process, but is nevertheless necessary. Automated essay evaluation represents a practical solution to this task, however, its main weakness is the predominant focus on vocabulary and text syntax, and limited consideration of text semantics. In this work, we propose an extension of existing automated essay evaluation systems by incorporating additional semantic coherence and consistency attributes. We design the novel coherence attributes by transforming sequential parts of an essay into the semantic space and measuring changes between them to estimate coherence of the text. The novel consistency attributes detect semantic errors using information extraction and logic reasoning. The resulting system (named SAGE - Semantic Automated Grader for Essays) provides semantic feedback for the writer and achieves significantly higher grading accuracy compared with 9 other state-of-the-art automated essay evaluation systems.", "Language": "en", "Citations": "14"},
{"Title": "Weighted and robust incremental method for subspace learning", "Authors": ["Skocaj D.", "Leonardis A."], "Keywords": [], "Date": "2003", "Abstract": "Visual learning is expected to be a continuous and robust process, which treats input images and pixels selectively. In this paper we present a method for subspace learning, which takes these considerations into account. We present an incremental method, which sequentially updates the principal subspace considering weighted influence of individual images as well as individual pixels within an image. This approach is further extended to enable determination of consistencies in the input data and imputation of the values in inconsistent pixels using the previously acquired knowledge, resulting in a novel incremental, weighted and robust method for subspace learning.", "Language": "en", "Citations": "122"},
{"Title": "A model for the SaaS-application integration, migration and backup with a common data model approach Model za integracijo, migracijo in varnostno kopiranje aplikacij SaaS prek skupnega podatkovnega modela", "Authors": ["Povse R.", "Juric M.B."], "Keywords": [], "Date": "2014", "Abstract": "The paper addresses the field of cloud computing, more specifically the SaaS (Software-as-a-Service) layer of cloud computing. Although the usage of the SaaS applications has become very popular in recent years, their end-users are still facing certain limitations. In fact, several users leverage more than one SaaS application without any possibility of integrating heterogeneous data. Also, they are often bound to a specific application due to the \"vendor lock-in\" problem and they are not always given the option to maintain data backups outside the provider's data centers. Hence, we address these issues by introducing a model providing a unified platform for (i) preparing data backups, (ii) executing data migration and (iii) performing data integration among different SaaS applications. We evaluate the proposed model by performing a demonstrational implementation.", "Language": "en", "Citations": "0"},
{"Title": "Using Constraint-Based Reasoning for Multi-objective Optimisation of the ENTICE Environment", "Authors": ["Gec S.", "Kimovski D.", "Prodan R.", "Stankovski V."], "Keywords": ["Container Image", "Quality of Service", "Reasoning", "Repository", "Virtual Machine Image"], "Date": "2016", "Abstract": "ENTICE is a set of innovative software services currently being developed to facilitate efficient operations of distributed Virtual Machine and container images (VMI/CI) repositories. Its operation necessitates various decision making for which a solver for Multi-Objective Optimisation (MOO) problems is used. However, the solver is a bottleneck due to its computational complexity. In order to be able to reduce the search space for the solver, we have developed an ontology and corresponding Knowledge Base (KB) that underpins the operation of the ENTICE environment. The Knowledge Base is developed based on the Jena Fuseki technology. To address the problem of computational complexity, constraint based queries and different reasoning mechanisms are applied. The Knowledge Base services are then integrated with other ENTICE services including the MOO solver. It is shown that this approach significantly reduces the computational complexity for the MOO, thus it shortens the optimisation time, and makes it possible to use the MOO for both strategic (decisions that can be made up to one day in advance) and dynamic (decisions requiring response within one minute) decision making possible.", "Language": "en", "Citations": "2"},
{"Title": "Suitability of FRAM method for hazard analysis of ATM functional system of Slovenia control Ltd", "Authors": ["Pielick M.", "Mraz M."], "Keywords": [], "Date": "2017", "Abstract": "The study will explain how the safety regulations are distributed among ANSPs (Air Navigation Service Provider) in EU. We will explain the current procedures and methods applied to safety assessments in domain of air traffic control. We will explain the shortcomings of the commonly used methodology. We will explain the basics of a modern approach to safety assessment by applying systemic methods and provide a summary of known applications in air traffic domain. In the end we will apply a selected method to model a part of Slovenia Control\u2019s ATM functional system.", "Language": "en", "Citations": "0"},
{"Title": "Restricted shortest paths in 2-circulant graphs", "Authors": ["Dobravec T.", "Robic B."], "Keywords": ["\u21131-lattice", "Circulant graphs", "Closest vector problem", "Diophantine equation", "Routing"], "Date": "2009", "Abstract": "Semi-directed 2-circulant graph is a subgraph of an (undirected) 2-circulant graph in which the links of one type (i.e., short or long) are directed while the other links are undirected. The shortest paths in semi-directed circulant graphs are called the restricted shortest paths in 2-circulant graphs. In this paper we show that the problem of finding the restricted shortest paths is equivalent (1) to solving an optimization problem which involves Diophantine equations and (2) to the closest vector problem in a subset of a point lattice of all cyclic paths of the graph. We also present our new, efficient algorithm for constructing the restricted shortest paths which requires O (log n) arithmetic operations. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "Data warehouse for an e-learning platform", "Authors": ["Nebic Z.", "Mahnic V."], "Keywords": ["Data warehouse", "E-learning", "Moodle", "OLAP", "University of Rijeka"], "Date": "2010", "Abstract": "E-learning is becoming more and more prominent at many higher education institutions. While this development has many advantages it also presents the teachers and the administration with new problems and questions, especially related to monitoring and analysis of platform use, including usage statistics, level of adoption at an institution, course activity, among others. We try to answer some of these questions by identifying related indicators and by offering tools that allow analysis and interpretation of results through data warehouse and OLAP technologies, and by building the basis for a future data warehouse that will support decision making. The steps for building a working data warehouse solution and several corresponding analyses are shown using a Moodle e-learning platform used at the University of Rijeka.", "Language": "en", "Citations": "3"},
{"Title": "Particulate matter (PM10) patterns in Europe: An exploratory data analysis using non-negative matrix factorization", "Authors": ["Zibert J.", "Cedilnik J.", "Praznikar J."], "Keywords": ["Non-negative matrix factorization", "Particular matter", "Space-time patterns", "Synoptic situations"], "Date": "2016", "Abstract": "In last decade space-density of monitoring stations increased, in to addition also air pollution modeling made big progress. Using diversity of big data can lead to better knowledge about air pollution at continental scale. The focus of presented study is the data-driven approach using non-negative matrix factorization to provide new insights and to study the characteristic space-time particulate-matter patterns across Europe. We analyzed the PM10 concentrations obtained from 1097 monitoring stations (AirBase data) and the Monitoring Atmospheric Composition and Climate (MACC) modeled fields for a period of 3 years. We distinguished five characteristic patterns obtained from the AirBase data and five patterns from the MACC data. A comparison between the AirBase and MACC data shows a good spatial overlap for the east Europe, central Europe and the Mediterranean patterns. However, it should be noted that an analysis of the MACC data revealed two additional marine patterns: the Celtic and the North Seas. The Po Valley and Balkan patterns were very clearly identified when analyzing the AirBase data. In order to better understand the influence of the synoptic situation on the particulate-matter concentrations the synoptic meteorological situations were additionally analyzed. The cold season, low wind and very stable conditions, which can last for several days, is the most common situation linked to high concentrations of anthropogenic air pollution with particulate matter. In contrast, for the Mediterranean pattern the most common situation (high factor loadings) is observed during the summer period. This pattern also exhibits a clearer annual cycle. A closer look at the sea-salt patterns (Celtic and North Seas) shows low time-series correlations between these two factors. Nevertheless, the physical mechanism is the same: a steep gradient between the cyclone and the anti-cyclone that causes high winds and, consequently, higher sea-salt production.", "Language": "en", "Citations": "4"},
{"Title": "Human skin colour clustering for face detection", "Authors": ["Kovac J.", "Peer P.", "Solina F."], "Keywords": ["2D colour space", "3D colour space", "Automatic detection", "Computer vision", "Face candidates search", "Human face", "Illumination independence", "Skin-colour determination"], "Date": "2003", "Abstract": "Computer vision is one out of many areas that wants to understand the process of human functionality and copy that process with intention to complement human life with intelligent machines. For better human-computer interaction it is necessary for the machine to see people. This can be achieved by employing face detection algorithms, like the one used in the installation \"15 Seconds of Fame\". Mentioned installation unites the areas of modern art and technology. Its algorithm is based on skin colour detection. One of the problems this and similar algorithms have to deal with is sensitivity to the illumination conditions under which the input image is captured. Hence illumination sensitivity influences face detection results. One of the aspects from which we can observe illumination influence is the choice of the proper colour space. Since some colour spaces are designed to eliminate the influence of illumination (brightness) when describing colour of an object, an idea of using such a colour space for skin-colour detection has been taken under consideration and some of the methods have been researched and tested.", "Language": "en", "Citations": "354"},
{"Title": "Web-based real-time LADAR data visualization with multi-user collaboration support", "Authors": ["Bohak C.", "Kim B.H.", "Kim M.Y."], "Keywords": ["Data visualization", "LADAR", "LiDAR", "Point cloud data", "WebGL"], "Date": "2018", "Abstract": "In this paper we present a web-based visualization system developed for visualizing real-time point cloud data obtained from LADAR (or other) sensors. The system allows direct visualization of captured data, visualization of data from database or visualization of preprocessed data (e.g. labeled or classified data). The system allows the concurrent visualization from same or different data-sources on multiple clients in the web browser. Due to the use of modern web technologies the client can also be used on mobile devices. The system is developed using modern client- and server-side web technologies. The system allows connection with an existing LADAR sensor grabber applications through use of UDP sockets. Both server- and client-side parts of the system are modular and allow the integration of newly developed modules and designing a specific work-flow scenarios for target end-user groups. The system allows the interactive visualization of datasets with millions of points as well as streaming visualization with high throughput speeds.", "Language": "en", "Citations": "1"},
{"Title": "Multi-model fitting using particle swarm optimization for 3D perception in robot vision", "Authors": ["Zhou K.", "Zillich M.", "Vincze M.", "Vrecko M.", "Skocaj D."], "Keywords": [], "Date": "2010", "Abstract": "Attention operators based on 2D image cues (such as color, texture) are well known and discussed extensively in the vision literature but are not ideally suited for robotic applications. In such contexts it is the 3D structure of scene elements that makes them interesting or not. We show how a bottom-up exploration mechanism that selects spaces of interest (SOIs) based on scene elements that pop out from planes is used within a larger architecture for a cognitive system. This mechanism simplifies the object localization as single plane detection, which is however not practical when dealing with real scenes that contains objects with complicated structures (e.g. objects in a multi-layer shelf). Therefore, the key work required for this situation is the multi-plane estimation, which is solved in this paper using Particle Swarm Optimization (PSO). \u00a9 2010 IEEE.", "Language": "en", "Citations": "7"},
{"Title": "Nuclear spin relaxation of mesogenic fluids in spherical microcavities", "Authors": ["Vilfan M.", "Vuk M."], "Keywords": [], "Date": "2004", "Abstract": "The characteristics of nuclear spin relaxation of mesogenic fluids in spherical microcavities were studied. It was found that the relaxation was induced by the time modulation of spin interactions as the molecules diffused between the ordered surface layer into the isotropic interior volume and back. The three distinct dispersion regimes shown by the spin-lattice relaxation rate T", "Language": "en", "Citations": "5"},
{"Title": "Search strategies for subgraph isomorphism algorithms", "Authors": ["Cibej U.", "Mihelic J."], "Keywords": ["Search strategy", "Subgraph isomorphism", "Ullmann\u2019s algorithm"], "Date": "2014", "Abstract": "Searching for subgraph isomorphisms is an essential problem in pattern matching. Most of the algorithms use a branch-and-bound method to sequentially assign pattern nodes to compatible nodes in the target graph. It is well known that the order in which nodes are assigned, a so-called search strategy, influences drastically the size of the search space. In this article we investigate the impact of various search strategies on the efficiency of two algorithms, the first being the Ullmann\u2019s algorithm and the second one the recently proposed improvement of Ullmann\u2019s algorithm. From the large set of proposed orders we find the most successful ones by thorough testing on a large database of graphs.", "Language": "en", "Citations": "4"},
{"Title": "Big cloud infrastructures: How green are they? Kako zelena je infrastruktura velikih oblakov?", "Authors": ["Ciglaric M."], "Keywords": [], "Date": "2015", "Abstract": "Addressing environmental issues and implementation of environmental practices are becoming important areas of businesses, government policies and society in the broadest sense. In the last decade the use of information technologies increased tremendously in many areas that positively affect our everyday life and optimize how we work. However with all the positive aspects, IT also comes with a negative impact on the environment. Computer infrastructures and data centers consume huge amounts of electricity for operation and cooling systems. Every unit of energy consumed for processing requires approximately as much energy for cooling. Both processes contribute to emissions of greenhouse gasses. The primary purpose of this analysis is to review recent research in the green IT area and explore the state of the informal drivers and formal initiatives that influence the development of green IT and enable greener data centers. Primarily the pressures come from non-governmental organizations, i.e. Greenpeace and like, and later their warnings get formalized into official recommendations, EU directives and consequently, after some time they find their place in local legislation.", "Language": "en", "Citations": "0"},
{"Title": "ClowdFlows: Online workflows for distributed big data mining", "Authors": ["Kranjc J.", "Orac R.", "Podpecan V.", "Lavrac N.", "Robnik-Sikonja M."], "Keywords": ["Batch processing", "Big data", "Cloud computing", "Data mining platform", "Map-reduce", "Scientific workflows"], "Date": "2017", "Abstract": "The paper presents a platform for distributed computing, developed using the latest software technologies and computing paradigms to enable big data mining. The platform, called ClowdFlows, is implemented as a cloud-based web application with a graphical user interface which supports the construction and execution of data mining workflows, including web services used as workflow components. As a web application, the ClowdFlows platform poses no software requirements and can be used from any modern browser, including mobile devices. The constructed workflows can be declared either as private or public, which enables sharing the developed solutions, data and results on the web and in scientific publications. The server-side software of ClowdFlows can be multiplied and distributed to any number of computing nodes. From a developer's perspective the platform is easy to extend and supports distributed development with packages. The paper focuses on big data processing in the batch and real-time processing mode. Big data analytics is provided through several algorithms, including novel ensemble techniques, implemented using the map-reduce paradigm and a special stream mining module for continuous parallel workflow execution. The batch mode and real-time processing mode are demonstrated with practical use cases. Performance analysis shows the benefit of using all available data for learning in distributed mode compared to using only subsets of data in non-distributed mode. The ability of ClowdFlows to handle big data sets and its nearly perfect linear speedup is demonstrated.", "Language": "en", "Citations": "43"},
{"Title": "Real-Time interactive platform-Agnostic volumetric path tracing in WebGL 2.0", "Authors": ["Lesar Z.", "Bohak C.", "Marolt M."], "Keywords": ["Path tracing", "Volume rendering", "WebGL"], "Date": "2018", "Abstract": "Path tracing has become a de facto standard for photo-realistic rendering due to its conceptual and algorithmic simplicity. Over the last few years, it has been successfully applied to the rendering of participating media, although it has not seen widespread adoption. Most implementations are targeted at speciic platforms or hardware, which makes them diicult to deploy or extend. However, recent advancements in web technologies enable us to access graphics hardware from a web browser in a platform-agnostic manner. Therefore, in this paper, we present an implementation of a state-ofthe-art volumetric path tracer developed in JavaScript using WebGL 2.0. The presented solution supports the use of arbitrary 2D transfer functions and heterogeneous volumetric data, aims to be interactive, platform-agnostic, easily extensible, and runs in real-time - both on desktop and mobile devices.", "Language": "en", "Citations": "2"},
{"Title": "Detecting concept drift in data streams using model explanation", "Authors": ["Demsar J.", "Bosnic Z."], "Keywords": ["Concept drift", "Data stream", "Explanation", "Visualization"], "Date": "2018", "Abstract": "Learning from data streams (incremental learning) is increasingly attracting research focus due to many real-world streaming problems and due to many open challenges, among which is the detection of concept drift \u2013 a phenomenon when the data distribution changes and makes the current prediction model inaccurate or obsolete. Current state-of-the art detection methods can be roughly split into performance monitoring algorithms and distribution comparing algorithms. In this work we propose a novel concept drift detector that can be combined with an arbitrary classification algorithm. The proposed concept drift detector is based on computing multiple model explanations over time and observing the magnitudes of their changes. The model explanation is computed using a methodology that yields attribute-value contributions for prediction outcomes and thus provides insight into the model's decision-making process and enables its transparency. The evaluation has revealed that the methods surpass the baseline methods in terms of concept drift detection, accuracy, robustness and sensitivity. To even further augment interpretability, we visualized the detection of concept drift, enabling macro and micro views of the data.", "Language": "en", "Citations": "7"},
{"Title": "Jumping across biomedical contexts using compressive data fusion", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": [], "Date": "2016", "Abstract": "Motivation: The rapid growth of diverse biological data allows us to consider interactions between a variety of objects, such as genes, chemicals, molecular signatures, diseases, pathways and environmental exposures. Often, any pair of objects - such as a gene and a disease - can be related in different ways, for example, directly via gene-disease associations or indirectly via functional annotations, chemicals and pathways. Different ways of relating these objects carry different semantic meanings. However, traditional methods disregard these semantics and thus cannot fully exploit their value in data modeling. Results: We present Medusa, an approach to detect size-k modules of objects that, taken together, appear most significant to another set of objects. Medusa operates on large-scale collections of heterogeneous datasets and explicitly distinguishes between diverse data semantics. It advances research along two dimensions: it builds on collective matrix factorization to derive different semantics, and it formulates the growing of the modules as a submodular optimization program. Medusa is flexible in choosing or combining semantic meanings and provides theoretical guarantees about detection quality. In a systematic study on 310 complex diseases, we show the effectiveness of Medusa in associating genes with diseases and detecting disease modules. We demonstrate that in predicting gene-disease associations Medusa compares favorably to methods that ignore diverse semantic meanings. We find that the utility of different semantics depends on disease categories and that, overall, Medusa recovers disease modules more accurately when combining different semantics.", "Language": "en", "Citations": "7"},
{"Title": "Managing business rules in enterprises", "Authors": ["Bajec M.", "Krisper M."], "Keywords": ["Business rule", "Business rule approach", "Business rule management", "Information system development"], "Date": "2001", "Abstract": "Business rules are evidently an important business category since they describe how enterprises are doing business. Their value in developing applications that are flexible and amenable to change has made them attractive also within information system (IS) domain. In the paper we describe how business rules can be used as information resources that help in establishing the link between organization's business and its supporting IS. We show a business rule management scenario that integrates the activities for managing the business rule in an organization, and the activities that support business rule management across the IS development life cycle.", "Language": "en", "Citations": "3"},
{"Title": "The Visual Object Tracking VOT2017 Challenge Results", "Authors": ["Kristan M.", "Leonardis A.", "Matas J.", "Felsberg M.", "Pflugfelder R.", "Zajc L.C.", "Vojir T.", "Hager G.", "Lukezic A.", "Eldesokey A.", "Fernandez G.", "Garcia-Martin A.", "Muhic A.", "Petrosino A.", "Memarmoghadam A.", "Vedaldi A.", "Manzanera A.", "Tran A.", "Alatan A.", "Mocanu B.", "Chen B.", "Huang C.", "Xu C.", "Sun C.", "Du D.", "Zhang D.", "Du D.", "Mishra D.", "Gundogdu E.", "Velasco-Salido E.", "Khan F.S.", "Battistone F.", "Subrahmanyam G.R.K.S.", "Bhat G.", "Huang G.", "Bastos G.", "Seetharaman G.", "Zhang H.", "Li H.", "Lu H.", "Drummond I.", "Valmadre J.", "Jeong J.-C.", "Cho J.-I.", "Lee J.-Y.", "Noskova J.", "Zhu J.", "Gao J.", "Liu J.", "Kim J.-W.", "Henriques J.F.", "Martinez J.M.", "Zhuang J.", "Xing J.", "Gao J.", "Chen K.", "Palaniappan K.", "Lebeda K.", "Gao K.", "Kitani K.M.", "Zhang L.", "Wang L.", "Yang L.", "Wen L.", "Bertinetto L.", "Poostchi M.", "Danelljan M.", "Mueller M.", "Zhang M.", "Yang M.-H.", "Xie N.", "Wang N.", "Miksik O.", "Moallem P.", "Pallavi Venugopal M.", "Senna P.", "Torr P.H.S.", "Wang Q.", "Yu Q.", "Huang Q.", "Martin-Nieto R.", "Bowden R.", "Liu R.", "Tapu R.", "Hadfield S.", "Lyu S.", "Golodetz S.", "Choi S.", "Zhang T.", "Zaharia T.", "Santopietro V.", "Zou W.", "Hu W.", "Tao W.", "Li W.", "Zhou W.", "Yu X.", "Bian X.", "Li Y.", "Xing Y.", "Fan Y.", "Zhu Z.", "Zhang Z.", "He Z."], "Keywords": [], "Date": "2017", "Abstract": "The Visual Object Tracking challenge VOT2017 is the fifth annual tracker benchmarking activity organized by the VOT initiative. Results of 51 trackers are presented; many are state-of-the-art published at major computer vision conferences or journals in recent years. The evaluation included the standard VOT and other popular methodologies and a new 'real-time' experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The VOT2017 goes beyond its predecessors by (i) improving the VOT public dataset and introducing a separate VOT2017 sequestered dataset, (ii) introducing a realtime tracking experiment and (iii) releasing a redesigned toolkit that supports complex experiments. The dataset, the evaluation kit and the results are publicly available at the challenge website1.", "Language": "en", "Citations": "69"},
{"Title": "Discrete gradient fields on infinite complexes", "Authors": ["Ayala R.", "Vilches J.A.", "Jerse G.", "Kosta N.M."], "Keywords": ["Critical simplex", "Discrete gradient field", "Gradient path", "Infinite simplicial complex"], "Date": "2011", "Abstract": "The aim of this work is to characterize the discrete gradient vector fields on infinite and locally finite simplicial complexes which are induced by a proper discrete Morse function. This characterization is essentially given by the non-existence of closed trajectories and the absence of a certain kind of incidence between monotonous rays in the given field.", "Language": "en", "Citations": "2"},
{"Title": "Continuous Theta Burst Stimulation Over the Dorsolateral Prefrontal Cortex and the Pre-SMA Alter Drift Rate and Response Thresholds Respectively During Perceptual Decision-Making", "Authors": ["Georgiev D.", "Rocchi L.", "Tocco P.", "Speekenbrink M.", "Rothwell J.C.", "Jahanshahi M."], "Keywords": ["Continuous theta burst stimulation", "DLPFC", "Perceptual decision-making", "Pre-SMA", "Speed-accuracy trade off"], "Date": "2016", "Abstract": "Background The speed-accuracy trade-off (SAT) refers to the balancing of speed versus accuracy during decision-making. SAT is very commonly investigated with perceptual decision-making tasks such as the moving dots task (MDT). The dorsolateral prefrontal cortex (DLPFC) and the pre-supplementary motor area (pre-SMA) are two brain regions considered to be involved in the control of SAT. Objectives/hypotheses The study tested whether the DLPFC and the pre-SMA play an essential role in the control of SAT. We hypothesized that continuous theta burst stimulation (cTBS) over the right DLPFC would primarily alter the rate of accumulation of evidence, whereas stimulation of the pre-SMA would influence the threshold for reaching a decision. Methods Fifteen (5 females; mean age = 30, SD = 5.40) healthy volunteers participated in the study. We used two versions of the MDT and cTBS over the right DLPFC, pre-SMA and sham stimulation. The drift diffusion model was fit to the behavioural data (reaction time and error rate) in order to calculate the drift rate, boundary separation (threshold) and non-decision time. Results cTBS over the right DLPFC decreased the rate of accumulation of evidence (i.e. the drift rate from the diffusion model) in high (0.35 and 0.5) but not in low coherence trials. cTBS over the pre-SMA changed the boundary separation/threshold required to reach a decision on accuracy, but not on speed trials. Conclusions The results suggest for the first time that both the DLPFC and the pre-SMA make essential but distinct contributions to the modulation of SAT.", "Language": "en", "Citations": "12"},
{"Title": "Machine Learning for Predicting Cognitive Diseases: Methods, Data Sources and Risk Factors", "Authors": ["Bratic B.", "Kurbalija V.", "Ivanovic M.", "Oder I.", "Bosnic Z."], "Keywords": ["Alzheimer\u2019s disease", "Cognitive diseases", "Data mining", "Machine learning", "Parkinson\u2019s disease"], "Date": "2018", "Abstract": "Machine learning and data mining approaches are being successfully applied to different fields of life sciences for the past 20 years. Medicine is one of the most suitable application domains for these techniques since they help model diagnostic information based on causal and/or statistical data and therefore reveal hidden dependencies between symptoms and illnesses. In this paper we give a detailed overview of the recent machine learning research and its applications for predicting cognitive diseases, especially the Alzheimer\u2019s disease, mild cognitive impairment and the Parkinson\u2019s disease. We survey different state-of-the-art methodological approaches, data sources and public data, and provide their comparative analysis. We conclude by identifying the open problems within the field that include an early detection of the cognitive diseases and inclusion of machine learning tools into diagnostic practice and therapy planning.", "Language": "en", "Citations": "1"},
{"Title": "A Local Approach to 1-Homogeneous Graphs", "Authors": ["Jurisic A.", "Koolen J."], "Keywords": ["1-homogeneous", "Distance-regular graphs", "Equitable partitions", "Locally strongly-regular"], "Date": "2000", "Abstract": "Let \u0393 be a distance-regular graph with diameter d. For vertices x and y of \u0393 at distance i, 1 \u2264 i \u2264 d, we define the sets C", "Language": "en", "Citations": "21"},
{"Title": "Data backups in the clouds Varnostne kopije podatkov v oblakih", "Authors": ["Zrnec A."], "Keywords": ["Band width", "Cloud", "Cloud computing", "Computer center", "Outsourcing"], "Date": "2011", "Abstract": "In the paper we present a concept of making backups in the cloud. We discuss the current practice of making backup copies of data enabling backups to be stored in a separate location and analyze the possibility of making backups in the cloud. We mainly focus on the economic and performance aspects of using cloud computing for making backup copies of data.", "Language": "en", "Citations": "1"},
{"Title": "The design of intelligent control of a kitchen refrigerator", "Authors": ["Mraz M."], "Keywords": ["ANFIS", "Fuzzy logic", "Modeling", "Refrigerator control", "Simulation"], "Date": "2001", "Abstract": "The article provides an example of how to design an \"intelligent\" digital control for maintaining the temperature at a predefined level in a common kitchen refrigerator. The control works on the basis of modeling a thermostatic appliance and the use of fuzzy logic. Thermostatically simulated and fuzzy controlled model are presented successively. The latter is set-up on the basis of the Sugeno's type of fuzzy rules and the Jang's procedure of learning. MATLAB, SIMULINK and Fuzzy Logic TOOLBOX (FLT) are the programming environments used for realization of the model. The principal aim in designing the control is to assure the fastest and best transition possible from an analogue to digital control of the refrigerating appliance, which represents the basis of a functional expansion demanded by the present market. \u00a9 2001 IMACS. Published by Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "12"},
{"Title": "dictyExpress: A web-based platform for sequence data management and analytics in Dictyostelium and beyond", "Authors": ["Stajdohar M.", "Rosengarten R.D.", "Kokosar J.", "Jeran L.", "Blenkus D.", "Shaulsky G.", "Zupan B."], "Keywords": ["Bioinformatics", "ChIP-seq", "Differential gene expression", "Platform", "RNA-seq", "Visual analytics"], "Date": "2017", "Abstract": "Background: Dictyostelium discoideum, a soil-dwelling social amoeba, is a model for the study of numerous biological processes. Research in the field has benefited mightily from the adoption of next-generation sequencing for genomics and transcriptomics. Dictyostelium biologists now face the widespread challenges of analyzing and exploring high dimensional data sets to generate hypotheses and discovering novel insights. Results: We present dictyExpress (2.0), a web application designed for exploratory analysis of gene expression data, as well as data from related experiments such as Chromatin Immunoprecipitation sequencing (ChIP-Seq). The application features visualization modules that include time course expression profiles, clustering, gene ontology enrichment analysis, differential expression analysis and comparison of experiments. All visualizations are interactive and interconnected, such that the selection of genes in one module propagates instantly to visualizations in other modules. dictyExpress currently stores the data from over 800 Dictyostelium experiments and is embedded within a general-purpose software framework for management of next-generation sequencing data. dictyExpress allows users to explore their data in a broader context by reciprocal linking with dictyBase-a repository of Dictyostelium genomic data. In addition, we introduce a companion application called GenBoard, an intuitive graphic user interface for data management and bioinformatics analysis. Conclusions: dictyExpress and GenBoard enable broad adoption of next generation sequencing based inquiries by the Dictyostelium research community. Labs without the means to undertake deep sequencing projects can mine the data available to the public. The entire information flow, from raw sequence data to hypothesis testing, can be accomplished in an efficient workspace. The software framework is generalizable and represents a useful approach for any research community. To encourage more wide usage, the backend is open-source, available for extension and further development by bioinformaticians and data scientists.", "Language": "en", "Citations": "4"},
{"Title": "LL conflict resolution using the embedded left LR parser", "Authors": ["Slivnik B."], "Keywords": ["Embedded parsing", "Left LR parsing", "LL conflicts"], "Date": "2012", "Abstract": "A method for resolving LL(k) conflicts using small LR(k) parsers (called embedded left LR(k) parsers) is described. An embedded left LR(k) parser is capable of (a) producing the prefix of the left parse of the input string and (b) stopping not on the end-of-file marker but on any string from the set of lookahead strings fixed at the parser generation time. The conditions regarding the termination of the embedded left LR(k) parser if used within LL(k) (and similar) parsers are defined and examined in-depth. It is proved that an LL(k) parser augmented with a set of embedded left LR(k) parsers can parse any deterministic context-free grammar in the same asymptotic time as LR(k) parser. As the embedded left LR(k) parser produces the prefix of the left parse, the LL(k) parser augmented with embedded left LR(k) parsers still produces the left parse and the compiler writer does not need to bother with different parsing strategies during the compiler implementation.", "Language": "en", "Citations": "4"},
{"Title": "Applications of qualitative multi-attribute decision models in health care", "Authors": ["Bohanec M.", "Zupan B.", "Rajkovic V."], "Keywords": ["Decision support", "Hierarchical models", "Multi-attribute decision-making", "Qualitative models"], "Date": "2000", "Abstract": "Hierarchical decision models are a general decision support methodology aimed at the classification or evaluation of options that occur in decision-making processes. They are also important for the analysis, simulation and explanation of options. Decision models are typically developed through the decomposition of complex decision problems into smaller and less complex subproblems; the result of such decomposition is a hierarchical structure that consists of attributes and utility functions. This article presents an approach to the development and application of qualitative hierarchical decision models that is based on DEX, an expert system shell for multi-attribute decision support. The distinguishing characteristics of DEX are the use of qualitative (symbolic) attributes, and 'if-then' decision rules. Also, DEX provides a number of methods for the analysis of models and opinions, such as selective explanation and what-if analysis. We demonstrate the applicability and flexibility of the approach presenting four real-life applications of DEX in health care: assessment of breast cancer risk, assessment of basic living activities in community nursing, risk assessment in diabetic foot care, and technical analysis of radiogram errors. In particular, we highlight and justify the importance of knowledge presentation and option analysis methods for practical decision-making. We further show that, using a recently developed data mining method called HINT, such hierarchical decision models can be discovered from retrospective patient data.", "Language": "en", "Citations": "67"},
{"Title": "Smart contracts for container based video conferencing services: Architecture and implementation", "Authors": ["Gec S.", "Lavbic D.", "Bajec M.", "Stankovski V."], "Keywords": ["Blockchain", "Container", "Monetization", "Smart Contracts", "Video-Conferencing"], "Date": "2019", "Abstract": "Today, container-based virtualization is very popular due to the lightweight nature of containers and the ability to use them flexibly in various heterogeneously composed systems. This makes it possible to collaboratively develop services by sharing various types of resources, such as infrastructures, software and digitalized content. In this work, our home made video-conferencing (VC) system is used to study resource usage optimisation in business context. An application like this, does not provide monetization possibilities to all involved stakeholders including end users, cloud providers, software engineers and similar. Blockchain related technologies, such as Smart Contracts (SC) offer a possibility to address some of these needs. We introduce a novel architecture for monetization of added-value according to preferences of the stakeholders that participate in joint software service offers. The developed architecture facilitates use case scenarios of service and resource offers according to fixed and dynamic pricing schemes, fixed usage period, prepaid quota for flexible usage, division of income, consensual decisions among collaborative service providers, and constrained based usage of resources or services. Our container-based VC service, which is based on the Jitsi Meet Open Source software is used to demonstrate the proposed architecture and the benefits of the investigated use cases.", "Language": "en", "Citations": "0"},
{"Title": "Web services and Java middleware functional and performance analysis for SOA", "Authors": ["Juric M.B.", "Hericko M.", "Welzer T.", "Rozman I.", "Sasa A.", "Krisper M."], "Keywords": ["Java", "Performance", "RMI", "Tunneling", "Web services"], "Date": "2007", "Abstract": "This article focuses on analysis of key middleware technologies for realization of SOA on Java platform. It compares Web services, native Java distributed technology RMI (Remote Method Invocation), and various alternatives which allow communication through firewalls and proxies, including HTTP-to-port, HTTP-to-CGI, and HTTP-to-servlet tunneling. In addition to functional comparison the article presents a detailed performance analysis with overhead analysis and identification of optimizations. The paper contributes to the understanding of functional and performance aspects of distributed middleware technologies for realization of SOA. \u00a9 2007 IEEE.", "Language": "en", "Citations": "4"},
{"Title": "Technical, legal, economic and social aspects of biometrics for cloud computing", "Authors": ["Bule J.", "Peer P."], "Keywords": ["Biometrics", "Cloud biometrics", "Cloud computing", "Development", "Economic gain", "Legislation", "Privacy protection", "Social impact"], "Date": "2014", "Abstract": "This article addresses technical, legal, economic and social aspects of biometrics for cloud computing, featuring application example, gains of such solution, current laws, directives and legislation for biometrics and cloud computing. It is primarily based on Slovenian example due to common general EU legislation in the field of cloud computing and biometrics. Authentication on the Internet is still mainly done using passwords, while biometrics is practically not used. It is commonly known that everything is moving to the cloud and biometrics is not an exception. Amount of biometric data is expected to grow significantly over the next few years and only cloud computing is possible to process such amounts of data. Due to these facts and increasing security needs, we propose and implement the use of biometry as a service in the cloud. A challenge regarding the use of biometric solutions in the cloud is the protection of the privacy of individuals and their personal data. In Slovenia privacy legislation is very strong, it permits usage of biometrics only for very specific reasons, but we predict that big players on the market will change this fact globally. One of the important reasons for that is also the fact that biometrics for cloud computing provides some strong benefits and economic incentives. Proper deployment can provide significant savings. Such solutions could improve people's quality of life in terms of social development, especially in sense of more convenient, safer and reliable identification over multiple government and non-government services.", "Language": "en", "Citations": "0"},
{"Title": "Online kernel density estimation for interactive learning", "Authors": ["Kristan M.", "Skocaj D.", "Leonardis A."], "Keywords": ["Compression", "Hellinger distance", "Kernel density estimation", "Mixture models", "Online learning", "Unlearning", "Unscented transform"], "Date": "2010", "Abstract": "In this paper we propose a Gaussian-kernel-based online kernel density estimation which can be used for applications of online probability density estimation and online learning. Our approach generates a Gaussian mixture model of the observed data and allows online adaptation from positive examples as well as from the negative examples. The adaptation from the negative examples is realized by a novel concept of unlearning in mixture models. Low complexity of the mixtures is maintained through a novel compression algorithm. In contrast to the existing approaches, our approach does not require fine-tuning parameters for a specific application, we do not assume specific forms of the target distributions and temporal constraints are not assumed on the observed data. The strength of the proposed approach is demonstrated with examples of online estimation of complex distributions, an example of unlearning, and with an interactive learning of basic visual concepts. \u00a9 2009 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "24"},
{"Title": "Enhancing upper confidence bounds for trees with temporal difference values", "Authors": ["Vodopivec T.", "Ster B."], "Keywords": [], "Date": "2014", "Abstract": "Upper confidence bounds for trees (UCT) is one of the most popular and generally effective Monte Carlo tree search (MCTS) algorithms. However, in practice it is relatively weak when not aided by additional enhancements. Improving its performance without reducing generality is a current research challenge. We introduce a new domain-independent UCT enhancement based on the theory of reinforcement learning. Our approach estimates state values in the UCT tree by employing temporal difference (TD) learning, which is known to outperform plain Monte Carlo sampling in certain domains. We present three adaptations of the TD(\u03bb) algorithm to the UCT's tree policy and backpropagation step. Evaluations on four games (Gomoku, Hex, Connect Four, and Tic Tac Toe) reveal that our approach increases UCT's level of play comparably to the rapid action value estimation (RAVE) enhancement. Furthermore, it proves highly compatible with a modified all moves as first heuristic, where it considerably outperforms RAVE. The findings suggest that integration of TD learning into MCTS deserves further research, which may form a new class of MCTS enhancements.", "Language": "en", "Citations": "5"},
{"Title": "Art - A perfect testbed for computer vision related research", "Authors": ["Peer P.", "Batagelj B."], "Keywords": [], "Date": "2009", "Abstract": "Contemporary art nowadays tries to exploit modern technology to better address and enlighten specific problems and ideas of our time. Our interest in wider impact of modern technology on society and the interest in contemporary art, brought our attention also to the applicative field of use of computer vision methods in art. This chapter walks us through a few projects, proving that art is definitely a perfect testbed for our research: 15 Seconds of Fame, Dynamic Anamorphosis, Virtual Skiing, Smart Wall, Virtual Dance and Virtual Painter, from face detection, motion following, depth recovery, touchless human-computer interaction to popart, constant eye gaze of a person on the portrait, regardless of where the spectator stands, immersion into different virtual worlds without the need for any special equipment. \u00a9 2009 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "2"},
{"Title": "Modelling the environment of a mobile robot with the embedded flow state machine", "Authors": ["Ster B.", "Dobnikar A."], "Keywords": ["Environment modelling", "Mobile robots", "Navigation", "Prediction", "Reactive/planning approach", "Recurrent neural networks", "Topological modelling"], "Date": "2006", "Abstract": "A type of topological approach to mobile robot navigation is discussed and experimentally evaluated. The environment as experienced by a moving robot is treated as a dynamical system. Simple types of reactive behavior are supplemented with eventual decisions to switch between them. When switching criteria are defined, the system may be described in the form similar to a finite state machine. Since it is embedded in the environment and dependent on the sensory flow of the robot, we introduce the term \"Embedded flow state machine\" (EFSM). We implemented it with a recurrent neural network, trained on a sequence of sensory contents and actions. One of the main virtues of this approach is that no explicit localization is required, since the recurrent neural network holds the state implicitly. The EFSM is applicable to multi-step prediction of sensory information and the travelled distances between decision points, given a sequence of decisions at decision points. Thus, the optimal path to a specified goal can be sought. One of the main issues is, for how many steps ahead the prediction is reliable enough. In other words, is it feasible to perform environment modelling and path planning in this manner? The approach is tested on a miniature mobile robot, equipped with proximity sensors and a color video camera. Decision 'points,' where deviations from the wall-following behavior are allowed, are based on color object recognition. In the case of an experimental environment of medium complexity, this approach was successful.", "Language": "en", "Citations": "2"},
{"Title": "Effective message routing in unstructured peer-to-peer overlays", "Authors": ["Ciglaric M."], "Keywords": [], "Date": "2005", "Abstract": "There is a lack of efficiency in flooding-based unstructured peer-to-peer overlays, where loosely coupled nodes require high local autonomy. Two routing improvements are compared based on answer caching, where the cached metadata facilitates content-based routing of queries. Since peer nodes keep joining and leaving the overlay, a mechanism to keep the metadata valid is analysed. The problem area is reviewed, an overlay network model described, and related message routing issues and the simulation environment explained. Simulation results confirm expectations about the traffic reduction, while the user experience does not deteriorate. \u00a9 IEE, 2005.", "Language": "en", "Citations": "2"},
{"Title": "Computerized segmentation of whole-body bone scintigrams and its use in automated diagnostics", "Authors": ["Sajn L.", "Kukar M.", "Kononenko I.", "Milcinski M."], "Keywords": ["Automatic segmentation", "Image processing", "Machine learning", "Reference point detection", "Whole-body bone scintigraphy"], "Date": "2005", "Abstract": "Bone scintigraphy or whole-body bone scan is one of the most common diagnostic procedures in nuclear medicine used in the last 25 years. Pathological conditions, technically poor image resolution and artefacts necessitate that algorithms use sufficient background knowledge of anatomy and spatial relations of bones in order to work satisfactorily. A robust knowledge based methodology for detecting reference points of the main skeletal regions that is simultaneously applied on anterior and posterior whole-body bone scintigrams is presented. Expert knowledge is represented as a set of parameterized rules which are used to support standard image-processing algorithms. Our study includes 467 consecutive, non-selected scintigrams, which is, to our knowledge the largest number of images ever used in such studies. Automatic analysis of whole-body bone scans using our segmentation algorithm gives more accurate and reliable results than previous studies. Obtained reference points are used for automatic segmentation of the skeleton, which is applied to automatic (machine learning) or manual (expert physicians) diagnostics. Preliminary experiments show that an expert system based on machine learning closely mimics the results of expert physicians. \u00a9 2005 Elsevier Ireland Ltd. All rights reserved.", "Language": "en", "Citations": "18"},
{"Title": "Multiresolution approach to biomedical image segmentation with statistical models of appearance", "Authors": ["Ivekovic S.", "Leonardis A."], "Keywords": [], "Date": "2003", "Abstract": "Structural variability present in biomedical images is known to aggravate the segmentation process. Statistical models of appearance proved successful in exploiting the structural variability information in the learning set to segment a previously unseen medical image more reliably. In this paper we show that biomedical image segmentation with statistical models of appearance can be improved in terms of accuracy and efficiency by a multiresolution approach. We outline two different multiresolution approaches. The first demonstrates a straightforward extension of the original statistical model and uses a pyramid of statistical models to segment the input image on various resolution levels. The second applies the idea of direct coefficient propagation through the Gaussian image pyramid and uses only one statistical model to perform the multiresolution segmentation in a much simpler manner. Experimental results illustrate the scale of improvement achieved by using the multiresolution approaches described. Possible further improvements are discussed at the end. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "1"},
{"Title": "Problem decomposition for behavioural cloning", "Authors": ["Suc D.", "Bratko I."], "Keywords": [], "Date": "2000", "Abstract": "In behavioural cloning of the human operator\u2019s skill, a controller is usually induced directly as a classifier from system\u2019s states into actions. Experience shows that this often results in brittle controllers. In this paper we explore a decomposition of the cloning problem into two learning problems: the learning of operator\u2019s control trajectories and the learning of the system\u2019s dynamics separately. We analyse advantages of such indirect controllers. We give characterization of the learner\u2019s error that is plausible explanation of why this decomposition approach has empirically proved to be usually superior to direct cloning.", "Language": "en", "Citations": "6"},
{"Title": "Real time panoramic depth imaging from multiperspective panoramas using standard cameras", "Authors": ["Peer P.", "Solina F."], "Keywords": ["Computer vision", "Depth image", "Depth sensor", "Mosaicing", "Motion parallax effect", "Multiperspec- tive panoramic image", "Real time", "Reconstruction", "Standard camera", "Stereo vision"], "Date": "2009", "Abstract": "Recently we have presented a system for panoramic depth imaging with a single standard camera. The system is mosaic-based, which means that we use a sin- gle standard rotating camera and assemble the captured images in a multiperspective panoramic image. Due to a setoff of the camera's optical center from the rotational center of the system we are able to capture the motion parallax effect from a single sweep around the rotational center, which enables the stereo reconstruction. One of the problems of such a system is the fact that we cannot generate a stereo pair of im- ages in real time. This chapter presents a possible solution to this problem, which is based on simultaneously using many standard cameras. We perform simulations on real scene images to establish the quality of new sensor results in comparison to results obtained with the old sensor. The goal of the chapter is to reveal whether the new sen- sor can be used for real time capturing of panoramic depth images and consequently for autonomous navigation of a mobile robot in a room. In particular, we focus on the real time generation of panoramic stereo pairs since the calculation of depth images can already be run in real time. The basic panoramic depth imaging system and its real time extension are comprehensively analysed and compared. The analyses reveal a number of interesting properties of the systems. According to the basic system accu- racy we definitely can use the system for autonomous robot localization and navigation tasks. The assumptions made in the real time extension of the basic system are proved to be correct, but the accuracy of the new sensor generally deteriorates in comparison to the basic sensor.", "Language": "en", "Citations": "0"},
{"Title": "Prediction of intended career choice in family medicine using artificial neural networks", "Authors": ["Petek Ster M.", "Svab I.", "Ster B."], "Keywords": ["artificial neural networks", "attitudes", "family medicine", "intended career choice", "Medical students"], "Date": "2015", "Abstract": "Background: Due to the importance of family medicine and a relative shortage of doctors in this discipline, it is important to know how the decision to choose a career in this field is made. Objective: Since this decision is closely linked to students' attitudes towards family medicine, we were interested in identifying those attitudes that predict intended career choice in family medicine. Methods: A cross-sectional study was performed among 316 final-year medical students of the Ljubljana Medical Faculty in Slovenia. The students filled out a 164-item questionnaire, developed based on the European definition of family medicine and the EURACT Educational Agenda, using a seven-point Likert scale containing attitudes towards family medicine. The students also recorded their interest in family medicine on a five-point Likert scale. Attitudes were selected using a feature selection procedure with artificial neural networks that best differentiated between students who are likely and students who are unlikely to become family physicians. Results: Thirty-one out of 164 attitudes predict a career in family medicine, with a classification accuracy of at least 85%. Predictors of intended career choice in family medicine are related to three categories: understanding of the discipline, working in a coherent health care system and person-centredness. The most important predictor is an appreciation of a long-term doctor-patient relationship. Conclusion: Students whose intended career choice is family medicine differ from other students in having more positive attitudes towards family physicians' competences and towards characteristics of family medicine and primary care.", "Language": "en", "Citations": "6"},
{"Title": "Learning hierarchical compositional representations of object structure", "Authors": ["Fidler S.", "Boben M.", "Leonardis A."], "Keywords": [], "Date": "2009", "Abstract": "Visual categorization of objects has captured the attention of the vision community for decades (Dickinson 2008). The increased popularity of the problem witnessed in recent years and the advent of powerful computer hardware have led to a seeming success of categorization approaches on the standard datasets such as Caltech-101 (Fei-Fei et al. 2004). However, the high level of discrepancy between the accuracy of object classification and detection/segmentation (Everingham et al. 2007) suggests that the problem still poses a significant and open challenge. The recent preoccupation with tuning the approaches to specific datasets might have averted attention from the most crucial issue: the representation (Edelman and Intrator 2004). This chapter focuses on what we believe are two central representational design principles: a hierarchical organization of categorical representations, or, more specifically, the principles of hierarchical compositionality and statistical, bottom-up learning. Given images of complex scenes, objects must be inferred from the pixel information through some recognition process. This requires an efficient and robust matching of the internal object representation against the representation produced from the scene. Despite the seemingly effortless performance of human perception, the diversity and the shear number of visual object classes appearing at various scales, 3-D views, and articulations have placed a great obstacle to the task. In fact, it has been shown by Tsotsos (1990) that the unbounded visual search is NP complete; thus, approximate, hierarchical solutions might be the most promising or plausible way to tackle the problem. This line of architecture is also consistent with the findings on biological systems (Rolls and Deco 2002; Connor et al. 2007).", "Language": "en", "Citations": "21"},
{"Title": "New media spaces and new media objects", "Authors": ["Bovcon N.", "Vaupotic A."], "Keywords": ["Computer vision", "Digital video", "Internet of things", "Video installation", "Virtual space"], "Date": "2009", "Abstract": "The paper deals with three artistic examples of humanistic transposition of smart technologies that reshape our reality. Computer vision technology redefines the classical notions of space that we enhabit and repostulates Shakespear's verse \"All the world is a stage.\" New media objects that simultaneously participate in our everyday life and in the \"internet of things\" pose new networked information on the state of their handling. The Forest of Arden, the metaphor used for the new mixed and smart reality, is neither a totality of paradise nor the Arcadia of easy living but a challenge for the scientists, artists and humanists alike. It is a semiotic forest, where the melancholic philosopher Jaques, confronted with people from all classes and with various problems, faces the given notions and tries to make new sense out of them.", "Language": "en", "Citations": "0"},
{"Title": "Simulated predator attacks on flocks: A comparison of tactics", "Authors": ["Demsar J.", "Bajec I.L."], "Keywords": ["Artificial life", "Bird", "Boid", "Flock", "Fuzzy logic", "Predator"], "Date": "2014", "Abstract": "It is not exactly known why birds aggregate in coordinated flocks. The most common hypothesis proposes that the reason is protection from predators. Most of the currently developed examples of individual-based predator-prey models assume predators are attracted to the center of a highly coordinated flock. This proposed attraction of a predator to a flock would appear to be contradictory to an alternate hypothesis that flocks evolved as a protection against predation. In an attempt to resolve this apparent conflict, in this article we use a fuzzy individual-based model to study three attack tactics (attack center, attack nearest, attack isolated) and analyze the success of predation on two types of prey (social and individualistic). Our simulations revealed that social flocking (as opposed to individualistic behavior) is the optimal anti-predatory response to predators attacking mainly isolated individuals. \u00a9 2014 Massachusetts Institute of Technology.", "Language": "en", "Citations": "11"},
{"Title": "Fusion of non-visual modalities into the probabilistic occupancy map framework for person localization", "Authors": ["Mandeljc R.", "Pers J.", "Kristan M.", "Kovacic S."], "Keywords": [], "Date": "2011", "Abstract": "In this paper we investigate the possibilities for fusion of non-visual sensor modalities into state-of-the-art vision-based framework for person detection and localization, the Probabilistic Occupancy Map (POM), with the aim of improving the frame-by-frame localization results in a realistic (cluttered) indoor environment. We point out the aspects that need to be considered when fusing non-visual sensor information into POM and provide a mathematical model for it. We demonstrate the proposed fusion method on the example of multi-camera and radio-based person localization setup. The performance of both systems is evaluated, showing their strengths and weaknesses. We show that localization results may be significantly improved by fusing the information from the radio-based system into the camera-based POM framework using the proposed model. \u00a9 2011 IEEE.", "Language": "en", "Citations": "8"},
{"Title": "Obtaining meteorological data from aircraft with mode-S radars", "Authors": ["Hrastovec M.", "Solina F."], "Keywords": [], "Date": "2013", "Abstract": "The main sources of meteorological data at high altitudes are radiosondes and Aircraft Meteorological Data Relay (AM-DAR) data [1]. Mode-select (Mode-S) radars are capable of getting similar readings from aircraft as AMDAR; however, this source of meteorological data has not been used systematically so far. In this article, we analyze the meteorological data obtained by means of Mode-S radars and propose how this data can be used. The frst prerequisite for acquiring meteorological data with radars is the necessary aircraft onboard sensors that can measure the meteorological data. Next, the aircraft transponder has to fetch the data from the sensors and report them. Mode-S radar confguration further determines whether the radar will request meteorological data from the aircraft. In our experiments, described later, approximately only 6% of responses from aircraft included meteorological information required by the radar. Still, the amount of data collected by means of Mode-S radars is much larger and much cheaper to obtain in comparison with radiosondes. We expect that in the future this percentage of successful responses from aircraft will grow with the modernization of airliner feets because newer aircraft are better equipped with meteorological sensors. \u00a9 2013 IEEE.", "Language": "en", "Citations": "8"},
{"Title": "A program for Progressive chess", "Authors": ["Janko V.", "Guid M."], "Keywords": ["A* algorithm", "Checkmate search", "Chess", "Combinatorial complexity", "Heuristic search", "Heuristics", "Minimax search", "Progressive chess"], "Date": "2016", "Abstract": "In Progressive chess, rather than just making one move per turn, players play progressively longer series of moves. Combinatorial complexity generated by many sequential moves represents a difficult challenge for classic search algorithms. In this article, we present the design of a state-of-the-art program for Progressive chess. The program follows the generally recommended strategy for this game, which consists of three phases: looking for possibilities to checkmate the opponent, playing sequences of generally good moves when checkmate is not available, and preventing checkmates from the opponent. For efficient and effective checkmate search we considered two versions of the A* algorithm, and developed five different heuristics for guiding the search. For finding promising sequences of moves we developed another set of heuristics, and combined the A* algorithm with minimax search, in order to fight the combinatorial complexity. We constructed an opening book, and designed specialized heuristics for playing Progressive chess endgames. An application with a graphical user interface was implemented in order to enable human players to play Progressive chess against the computer, and to use the computer to analyze their games. The program performed excellently in experiments with checkmate search, and won both mini-matches against a human chess master. We also present the findings of self-play experiments between different versions of the program.", "Language": "en", "Citations": "0"},
{"Title": "Generalized cages", "Authors": ["Boben M.", "Jajcay R.", "Pisanski T."], "Keywords": [], "Date": "2015", "Abstract": "Let 2 \u2a7d k", "Language": "no", "Citations": "3"},
{"Title": "Superquadric-based object recognition", "Authors": ["Krivic J.", "Solina F."], "Keywords": ["Part-level object modeling", "Range images", "Superquadrics"], "Date": "2001", "Abstract": "This paper proposes a technique for object recognition using superquadric built models. Superquadrics, which are three dimensional models suitable for part-level representation of objects, are reconstructed from range images using the recover-and-select paradigm. Using an interpretation tree, the presence of an object in the scene from the model database can be hypothesized. These hypotheses are verified by projecting and re-fitting the object model to the range image which at the same time enables a better localization of the object in the scene.", "Language": "en", "Citations": "0"},
{"Title": "Multi-document summarization via Archetypal Analysis of the content-graph joint model", "Authors": ["Canhasi E.", "Kononenko I."], "Keywords": ["Archetypal analysis", "Content-graph joint model", "Document summarization", "Matrix decomposition"], "Date": "2014", "Abstract": "In recent years, algebraic methods, more precisely matrix decomposition approaches, have become a key tool for tackling document summarization problem. Typical algebraic methods used in multi-document summarization (MDS) vary from soft and hard clustering approaches to low-rank approximations. In this paper, we present a novel summarization method AASum which employs the archetypal analysis for generic MDS. Archetypal analysis (AA) is a promising unsupervised learning tool able to completely assemble the advantages of clustering and the flexibility of matrix factorization. In document summarization, given a content-graph data matrix representation of a set of documents, positively and/or negatively salient sentences are values on the data set boundary. These extreme values, archetypes, can be computed using AA. While each sentence in a data set is estimated as a mixture of archetypal sentences, the archetypes themselves are restricted to being sparse mixtures, i.e., convex combinations of the original sentences. Since AA in this way readily offers soft clustering, we suggest to consider it as a method for simultaneous sentence clustering and ranking. Another important argument in favor of using AA in MDS is that in contrast to other factorization methods, which extract prototypical, characteristic, even basic sentences, AA selects distinct (archetypal) sentences and thus induces variability and diversity in produced summaries. Experimental results on the DUC generic summarization data sets evidence the improvement of the proposed approach over the other closely related methods.", "Language": "en", "Citations": "18"},
{"Title": "Integrative clustering by nonnegative matrix factorization can reveal coherent functional groups from gene profile data", "Authors": ["Brdar S.", "Crnojevic V.", "Zupan B."], "Keywords": ["Clustering", "data fusion", "gene profiling", "gene set enrichment", "nonnegative matrix factorization (NMF)"], "Date": "2015", "Abstract": "Recent developments in molecular biology and techniques for genome-wide data acquisition have resulted in abundance of data to profile genes and predict their function. These datasets may come from diverse sources and it is an open question how to commonly address them and fuse them into a joint prediction model. A prevailing technique to identify groups of related genes that exhibit similar profiles is profile-based clustering. Cluster inference may benefit from consensus across different clustering models. In this paper, we propose a technique that develops separate gene clusters from each of available data sources and then fuses them by means of nonnegative matrix factorization. We use gene profile data on the budding yeast S. cerevisiae to demonstrate that this approach can successfully integrate heterogeneous datasets and yield high-quality clusters that could otherwise not be inferred by simply merging the gene profiles prior to clustering.", "Language": "en", "Citations": "2"},
{"Title": "Incremental LDA learning by combining reconstructive and discriminative approaches", "Authors": ["Uray M.", "Skocaj D.", "Roth P.M.", "Bischof H.", "Leonardis A."], "Keywords": [], "Date": "2007", "Abstract": "Incremental subspace methods have proven to enable efficient training if large amounts of training data have to be processed or if not all data is available in advance. In this paper we focus on incremental LDA learning which provides good classification results while it assures a compact data representation. In contrast to existing incremental LDA methods we additionally consider reconstructive information when incrementally building the LDA subspace. Hence, we get a more flexible representation that is capable to adapt to new data. Moreover, this allows to add new instances to existing classes as well as to add new classes. The experimental results show that the proposed approach outperforms other incremental LDA methods even approaching classification results obtained by batch learning.", "Language": "en", "Citations": "25"},
{"Title": "Fuzzy logic as a computational tool for quantitative modelling of biological systems with uncertain kinetic data", "Authors": ["Bordon J.", "Moskon M.", "Zimic N.", "Mraz M."], "Keywords": ["computational biology", "Fuzzy logic", "gene regulatory networks", "modelling and simulation", "ordinary differential equations", "synthetic biology", "uncertain kinetic data"], "Date": "2015", "Abstract": "Quantitative modelling of biological systems has become an indispensable computational approach in the design of novel and analysis of existing biological systems. However, kinetic data that describe the system\u00e2\u20ac\u2122s dynamics need to be known in order to obtain relevant results with the conventional modelling techniques. These data are often hard or even impossible to obtain. Here, we present a quantitative fuzzy logic modelling approach that is able to cope with unknown kinetic data and thus produce relevant results even though kinetic data are incomplete or only vaguely defined. Moreover, the approach can be used in the combination with the existing state-of-the-art quantitative modelling techniques only in certain parts of the system, i.e., where kinetic data are missing. The case study of the approach proposed here is performed on the model of three-gene repressilator.", "Language": "en", "Citations": "8"},
{"Title": "Categorization of Numerical Values for Dex Hierarchical Models", "Authors": ["Znidarsic M.", "Bohanec M.", "Bratko I."], "Keywords": ["Categorization", "Decision support", "Multi-attribute decision models"], "Date": "2003", "Abstract": "DEX is a multi-attribute decision modelling methodology. Its specialty is the use of ordinal data and qualitative utility functions. Numerical attributes must be therefore categorized before use in DEX models. We present the problem of numerical data categorization and propose two methods which simplify and partly automate this task. The methods suggest interval bounds according to the desired number of categories and the preference curve of attribute values. We implemented both methods and made some experiments with typical inputs. The most interesting results are presented and analysed.", "Language": "en", "Citations": "2"},
{"Title": "An iterative logarithmic multiplier", "Authors": ["Babic Z.", "Avramovic A.", "Bulic P."], "Keywords": ["Computer arithmetic", "Digital signal processing", "FPGA", "Logarithmic number system", "Multiplier"], "Date": "2011", "Abstract": "Digital signal processing algorithms often rely heavily on a large number of multiplications, which is both time and power consuming. However, there are many practical solutions to simplify multiplication, like truncated and logarithmic multipliers. These methods consume less time and power but introduce errors. Nevertheless, they can be used in situations where a shorter time delay is more important than accuracy. In digital signal processing, these conditions are often met, especially in video compression and tracking, where integer arithmetic gives satisfactory results. This paper presents a simple and efficient multiplier with the possibility to achieve an arbitrary accuracy through an iterative procedure, prior to achieving the exact result. The multiplier is based on the same form of number representation as Mitchell's algorithm, but it uses different error correction circuits than those proposed by Mitchell. In such a way, the error correction can be done almost in parallel (actually this is achieved through pipelining) with the basic multiplication. The hardware solution involves adders and shifters, so it is not gate and power consuming. The error summary for operands ranging from 8 bits to 16 bits indicates a very low relative error percentage with two iterations only. For the hardware implementation assessment, the proposed multiplier is implemented on the Spartan 3 FPGA chip. For 16-bit operands, the time delay estimation indicates that a multiplier with two iterations can work with a clock cycle more than 150 MHz, and with the maximum relative error being less than 2%. \u00a9 2010 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "30"},
{"Title": "A new refinement method for registration of range images based on segmented data", "Authors": ["Kverh B.", "Leonardis A."], "Keywords": ["CAD models", "Range images", "Registration", "Reverse engineering", "Segmentation"], "Date": "2002", "Abstract": "We present a new method for registration of range images, which is based on the results we obtain from the segmentation process. We need two range images segmented into regions, each of them modeled by a parametric model and the approximation of the transformation between the two range images. Then two sets of corresponding points, one from each range image, are chosen and the transformation between them is computed to further refine the initial approximation of the transformation. The novelty is how we obtain the a corresponding points for the original set of points from the range image. Namely, to obtain them we project set of points from the first image onto geometric parametric models that were recovered in the second range image and viceversa. This way we obtain two sets of corresponding points. Then we compute the transformation between the two sets. Few iterations are required to improve the initial approximation of the transformation. The results have shown a significant improvement in precision of the registration in comparison with traditional approaches.", "Language": "en", "Citations": "2"},
{"Title": "Feature selection based on community detection in feature correlation networks", "Authors": ["Savic M.", "Kurbalija V.", "Bosnic Z.", "Ivanovic M."], "Keywords": ["Alzheimer\u2019s disease", "Community detection", "Feature correlation networks", "Feature selection"], "Date": "2019", "Abstract": "Feature selection is an important data preprocessing step in data mining and machine learning tasks, especially in the case of high dimensional data. In this paper, we propose a novel feature selection method based on feature correlation networks, i.e. complex weighted networks describing the strongest correlations among features in a dataset. The method utilizes community detection techniques to identify cohesive groups of features in feature correlation networks. A subset of features exhibiting a strong association with the class variable is selected according to the identified community structure taking into account the size of feature communities and connections within them. The proposed method is experimentally evaluated on a high dimensional dataset containing signaling protein features related to the diagnosis of Alzheimer\u2019s disease. We compared the performance of seven commonly used classifiers that were trained without feature selection, after feature selection by four variants of our method determined by different community detection techniques, and after feature selection by four widely used state-of-the-art feature selection methods available in the WEKA machine learning library. The results of the experimental evaluation indicate that our method improves the classification accuracy of several classification models while greatly reducing the dimensionality of the dataset. Additionally, our method tends to outperform traditional feature selection methods provided by the WEKA library.", "Language": "en", "Citations": "0"},
{"Title": "Deriving concepts and strategies from chess tablebases", "Authors": ["Guid M.", "Mozina M.", "Sadikov A.", "Bratko I."], "Keywords": [], "Date": "2010", "Abstract": "Complete tablebases, indicating best moves for every position, exist for chess endgames. There is no doubt that tablebases contain a wealth of knowledge, however, mining for this knowledge, manually or automatically, proved as extremely difficult. Recently, we developed an approach that combines specialized minimax search with the argument-based machine learning (ABML) paradigm. In this paper, we put this approach to test in an attempt to elicit human-understandable knowledge from tablebases. Specifically, we semi-automatically synthesize knowledge from the KBNK tablebase for teaching the difficult king, bishop, and knight versus the lone king endgame. \u00a9 2010 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "4"},
{"Title": "Comparison of performance of Web services, WS-Security, RMI, and RMI-SSL", "Authors": ["Juric M.B.", "Rozman I.", "Brumen B.", "Colnaric M.", "Hericko M."], "Keywords": ["Java", "Performance", "RMI", "RMI-SSL", "Web services", "WS-Security"], "Date": "2006", "Abstract": "This article analyses two most commonly used distributed models in Java: Web services and RMI (Remote Method Invocation). The paper focuses on regular (unsecured) as well as on secured variants, WS-Security and RMI-SSL. The most important functional differences are identified and the performance on two operating systems (Windows and Linux) is compared. Sources of performance differences related to the architecture and implementation are identified. The overheads related to the usage of security and the influences of JCE (Java Cryptography Extension) security providers on the performance of secured remote invocations are identified. Finally, the impact of distributed models on design and implementation of distributed applications is identified and guidelines for improving distributed application performance in design and implementation stage are provided. The paper contributes to the understanding of functional and performance related differences between Web services and RMI and their secure variants, WS-Security and RMI-SSL. \u00a9 2005 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "73"},
{"Title": "Semantic approach for multi-objective optimisation of the ENTICE distributed Virtual Machine and container images repository", "Authors": ["Gec S.", "Kimovski D.", "Pascinski U.", "Prodan R.", "Stankovski V."], "Keywords": ["distributed repository", "knowledge", "reasoning", "semantics", "Virtual Machine or container images"], "Date": "2017", "Abstract": "New software engineering technologies facilitate development of applications from reusable software components, such as Virtual Machine and container images (VMI/CIs). Key requirements for the storage of VMI/CIs in public or private repositories are their fast delivery and cloud deployment times. ENTICE is a federated storage facility for VMI/CIs that provides optimisation mechanisms through the use of fragmentation and replication of images and a Pareto Multi-Objective Optimisation (MO) solver. The operation of the MO solver is, however, time-consuming due to the size and complexity of the metadata, specifying various non-functional requirements for the management of VMI/CIs, such as geolocation, operational cost, and delivery time. In this work, we address this problem with a new semantic approach, which uses an ontology of the federated ENTICE repository, knowledge base, and constraint-based reasoning mechanism. Open Source technologies such as Prot\u00e9g\u00e9, Jena Fuseki, and Pellet were used to develop a solution. Two specific use cases, (1) repository optimisation with offline and (2) online redistribution of VMI/CIs, are presented in detail. In both use cases, data from the knowledge base are provided to the MO solver. It is shown that Pellet-based reasoning can be used to reduce the input metadata size used in the optimisation process by taking into consideration the geographic location of the VMI/CIs and the provenance of the VMI fragments. It is shown that this process leads to reduction of the input metadata size for the MO solver by up to 60% and reduction of the total optimisation time of the MO solver by up to 68%, while fully preserving the quality of the solution, which is significant.", "Language": "en", "Citations": "2"},
{"Title": "Implementing content packaging standards", "Authors": ["Kavcic A."], "Keywords": ["Common Cartridge", "content packaging", "e-learning", "interoperability", "SCORM", "standards"], "Date": "2011", "Abstract": "The paper deals with the issue of standards and specifications applied to educational digital content. Content packaging has to comply to standards to assure its interoperability and portability, as well as facilitate search and discovery, accessibility, and (re)use. Two most common packaging standards for educational content are described in the paper: already recognized and widely used ADL SCORM, and a newcomer in the field, IMS Common Cartridge, which successfully overcomes some problems of using modern Web technologies in education. We also describe our experience in implementing these standards, the problems, and advantages of using such packaged content. \u00a9 2011 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Text mining in medicine", "Authors": ["Zitnik S.", "Bajec M."], "Keywords": [], "Date": "2013", "Abstract": "Many medical applications and current ongoing medical research depend on text mining techniques. It is estimated that 90 % of all data is unstructured, such as emails, voice or video records, data streams, and Word documents. In the last decade, the estimated growth of unstructured data is about 62 %, whereas the amount of structured data has grown only by 22 %. In this chapter we therefore overview some methods and tools that enable researchers to automatically retrieve, extract, and integrate unstructured medical data. Due to increasing number of unstructured documents, the automatic text mining methods ease access to relevant data, already conducted research along with its results, and save money by trying to eliminate repeated research experiments. Natural language processing is lately receiving a lot of attention because researchers are trying to adapt techniques from other domains to work on biomedical data. We focus especially on methods from the fields of (1) information retrieval (indexing, searching, and retrieval of relevant documents given an input query), (2) information extraction (automatic extraction of structured data from unstructured sources with the main tasks of named entity recognition, relationship extraction, and coreference resolution), and (3) data integration (data merging and redundancy elimination).", "Language": "en", "Citations": "0"},
{"Title": "Exon Junction Complexes Show a Distributional Bias toward Alternatively Spliced mRNAs and against mRNAs Coding for Ribosomal Proteins", "Authors": ["Hauer C.", "Sieber J.", "Schwarzl T.", "Hollerer I.", "Curk T.", "Alleaume A.-M.", "Hentze M.W.", "Kulozik A.E."], "Keywords": [], "Date": "2016", "Abstract": "The exon junction complex (EJC) connects spliced mRNAs to posttranscriptional processes including RNA localization, transport, and regulated degradation. Here, we provide a comprehensive analysis of bona fide EJC binding sites across the transcriptome including all four RNA binding EJC components eIF4A3, BTZ, UPF3B, and RNPS1. Integration of these data sets permits definition of high-confidence EJC deposition sites as well as assessment of whether EJC heterogeneity drives alternative nonsense-mediated mRNA decay pathways. Notably, BTZ (MLN51 or CASC3) emerges as the EJC subunit that is almost exclusively bound to sites 20\u201324 nucleotides upstream of exon-exon junctions, hence defining EJC positions. By contrast, eIF4A3, UPF3B, and RNPS1 display additional RNA binding sites suggesting accompanying non-EJC functions. Finally, our data show that EJCs are largely distributed across spliced RNAs in an orthodox fashion, with two notable exceptions: an EJC deposition bias in favor of alternatively spliced transcripts and against the mRNAs that encode ribosomal proteins.", "Language": "en", "Citations": "22"},
{"Title": "FuCoLoT \u2013 A Fully-Correlational Long-Term Tracker", "Authors": ["Lukezic A.", "Zajc L.C.", "Vojir T.", "Matas J.", "Kristan M."], "Keywords": [], "Date": "2019", "Abstract": "We propose FuCoLoT \u2013 a Fully Correlational Long-term Tracker. It exploits the novel DCF constrained filter learning method to design a detector that is able to re-detect the target in the whole image efficiently. FuCoLoT maintains several correlation filters trained on different time scales that act as the detector components. A novel mechanism based on the correlation response is used for tracking failure estimation. FuCoLoT achieves state-of-the-art results on standard short-term benchmarks and it outperforms the current best-performing tracker on the long-term UAV20L benchmark by over 19%. It has an order of magnitude smaller memory footprint than its best-performing competitors and runs at 15\u00c2\u00a0fps in a single CPU thread.", "Language": "en", "Citations": "0"},
{"Title": "Predicting mechanical properties of elastomers with neural networks", "Authors": ["Trebar M.", "Susteric Z.", "Lotric U."], "Keywords": ["Mechanical properties of elastomers", "Modelling", "Neural networks"], "Date": "2007", "Abstract": "Despite the existence of a solid theoretical basis interrelating various mechanical properties of elastomers, the complexity of these materials and strong dependence of characteristic material parameters on deformational and temperature conditions cause insuperable difficulties in establishing accurate relations between the crosslinking properties of elastomeric compounds and crosslinked elastomers in practice. Since knowledge of such presumably nonlinear relations would be valuable for several reasons, this work attempts to uncover these relations using methods of soft computing, in particular neural networks. The resulting relations obtained by neural network analysis have proved to be incontestably good and completely in accordance with expectations, thus contributing to proficiency in dealing with elastomeric materials, as well as curtailing possibilities of testing redundancy. \u00a9 2007 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "11"},
{"Title": "Pyramidal tract activation due to subthalamic deep brain stimulation in Parkinson's disease", "Authors": ["Mahlknecht P.", "Akram H.", "Georgiev D.", "Tripoliti E.", "Candelario J.", "Zacharia A.", "Zrinzo L.", "Hyam J.", "Hariz M.", "Foltynie T.", "Rothwell J.C.", "Limousin P."], "Keywords": ["deep brain stimulation (DBS)", "magnetic resonance imaging (MRI)", "neurophysiology", "Parkinson's disease (PD)", "subthalamic nucleus (STN)", "upper motoneuron"], "Date": "2017", "Abstract": "Background: Subthalamic deep brain stimulation (STN-DBS) is an effective treatment for Parkinson's disease (PD), but can have side effects caused by stimulus spread to structures outside the target volume such as the pyramidal tract. Objectives: To assess the relevance of pyramidal tract activation with STN-DBS in PD. Methods: In a multimodal, blinded study in 20 STN-DBS patients, we measured stimulation thresholds for evoking electromyographic activity in orbicularis oris and first dorsal interosseous muscles at each of 150 electrode sites. We also modeled the electric field spread and calculated its overlap with the estimated anatomical location of corticospinal and corticobulbar tracts from primary motor cortex using 3 Tesla MRI probabilistic tractography. Results: Mean resting motor thresholds were significantly lower for the contralateral orbicularis oris (3.5 \u00b1 1.0 mA) compared with ipsilaterally (4.1 \u00b1 1.1 mA) and with the contralateral first dorsal interosseous (4.0 \u00b1 1.2 mA). The modeled volumes of corticobulbar and corticospinal tract activated correlated inversely with the resting motor threshold of the contralateral orbicularis oris and first dorsal interosseous, respectively. Active motor thresholds were significantly lower compared with resting motor thresholds by around 30% to 35% and correlated with the clinically used stimulation amplitude. Backward multiple regression in 12 individuals with a \u201clateral-type\u201d speech showed that stimulation amplitude, levodopa equivalent dose reduction postsurgery, preoperative speech intelligibility, and first dorsal interosseous resting motor thresholds explained 79.9% of the variance in postoperative speech intelligibility. Conclusions: Direct pyramidal tract activation can occur at stimulation thresholds that are within the range used in clinical routine. This spread of current compromises increase in stimulation strengths and is related to the development of side effects such as speech disturbances with chronic stimulation. \u00a9 2017 International Parkinson and Movement Disorder Society.", "Language": "en", "Citations": "16"},
{"Title": "Online routing in convex subdivisions", "Authors": ["Bose P.", "Brodnik A.", "Carlsson S.", "Demaine E.D.", "Fleischer R.", "Lopez-Ortiz A.", "Morin P.", "Ian Munro J."], "Keywords": [], "Date": "2000", "Abstract": "We consider online routing algorithms for finding paths between the vertices of plane graphs. We show (1) there exists a routing algorithm for arbitrary triangulations that has no memory and uses no randomization, (2) no equivalent result is possible for convex subdivisions, (3) there is no competitive online routing algorithm under the Euclidean distance metric in arbitrary triangulations, and (4) there is no competitive online routing algorithm under the link distance metric even when the input graph is restricted to be a Delaunay, greedy, or minimum-weight triangulation.", "Language": "en", "Citations": "26"},
{"Title": "RFID data loggers in fish supply chain traceability", "Authors": ["Trebar M.", "Lotric M.", "Fonda I.", "Pletersek A.", "Kovacic K."], "Keywords": [], "Date": "2013", "Abstract": "Radio frequency identification (RFID) is an innovative and well-recognized technology that supports all kinds of traceability systems in many areas. It becomes very important in the food industry where the electronic systems are used to capture the data in the supply chain. Additionally, RFID data loggers with sensors are available to perform a cold chain optimization for perishable foods. This paper presents the temperature monitoring solution at the box level in the fish supply chain as part of the traceability system implemented with RFID technology. RFID data loggers are placed inside the box to measure the temperature of the product and on the box for measuring ambient temperature. The results show that the system is very helpful during the phases of storage and transportation of fish to provide the quality control. The sensor data is available immediately at the delivery to be checked on the mobile RFID reader and afterwards stored in the traceability systems database to be presented on a web to stakeholders and private consumers. \u00a9 2013 Mira Trebar et al.", "Language": "en", "Citations": "19"},
{"Title": "Using heuristic-search based engines for estimating human skill at chess", "Authors": ["Matej G.", "Ivan B."], "Keywords": [], "Date": "2011", "Abstract": "Establishing heuristic-search based chess programs as appropriate tools for estimating human skill levels at chess may seem impossible due to the following issues: the programs' evaluations and decisions tend to change with the depth of search and with the program used. In this research, we provide an analysis of the differences between heuristic-search based programs in estimating chess skill. We used four different chess programs to perform analyses of large data sets of recorded human decisions, and obtained very similar rankings of skill-based performances of selected chess players using any of these programs at various levels of search. A conclusion is that, given two chess players, all the programs unanimously rank one player to be clearly stronger than the other, or all the programs assess their strengths to be similar. We also repeated our earlier analysis with the program CRAFTY of the human World Chess Champions with currently one of the strongest chess programs, RYBKA 32, and obtained qualitatively very similar results as with CRAFTY. This speaks in favour of computer heuristic search being adequate for estimating skill levels of chess players, despite the above stated issues.", "Language": "en", "Citations": "12"},
{"Title": "Model of complex networks based on citation dynamics", "Authors": ["Subelj L.", "Bajec M."], "Keywords": ["Citation networks", "Clustering", "Complex networks", "Degree mixing", "Graph models"], "Date": "2013", "Abstract": "Complex networks of real-world systems are believed to be controlled by common phenomena, producing structures far from regular or random. These include scale-free degree distributions, small-world structure and assortative mixing by degree, which are also the properties captured by different random graph models proposed in the literature. However, many (non-social) real-world networks are in fact disassortative by degree. Thus, we here propose a simple evolving model that generates networks with most common properties of real-world networks including degree disassortativity. Furthermore, the model has a natural interpretation for citation networks with different practical applications.", "Language": "en", "Citations": "16"},
{"Title": "Room categorization based on a hierarchical representation of space", "Authors": ["Ursic P.", "Tabernik D.", "Boben M.", "Skocaj D.", "Leonardis A.", "Kristan M."], "Keywords": ["Compositional Hierarchies", "Range Data", "Representation of Space", "Room Categorization"], "Date": "2013", "Abstract": "For successful operation in real-world environments, a mobile robot requires an effective spatial model. The model should be compact, should possess large expressive power and should scale well with respect to the number of modelled categories. In this paper we propose a new compositional hierarchical representation of space that is based on learning statistically significant observations, in terms of the frequency of occurrence of various shapes in the environment. We have focused on a two-dimensional space, since many robots perceive their surroundings in two dimensions with the use of a laser range finder or sonar. We also propose a new low-level image descriptor, by which we demonstrate the performance of our representation in the context of a room categorization problem. Using only the lower layers of the hierarchy, we obtain state-of-the-art categorization results in two different experimental scenarios. We also present a large, freely available, dataset, which is intended for room categorization experiments based on data obtained with a laser range finder. \u00a9 2013 Ur\u0161i\u010d et al.", "Language": "en", "Citations": "3"},
{"Title": "Similarity of transcription profiles for genes in gene sets", "Authors": ["Toplak M.", "Curk T.", "Zupan B."], "Keywords": ["association", "BioGRID", "gene sets", "gene transcription profile", "interaction gain", "KEGG"], "Date": "2011", "Abstract": "In gene set focused knowledge-based analysis we assume that genes from the same functional gene set have similar transcription profiles. We compared the distributions of similarity scores of gene transcription profiles between genes from the same gene sets and genes chosen at random. In line with previous research, our results show that transcription profiles of genes from the same gene sets are on average indeed more similar than random transcription profiles, although the differences are slight. We performed the experiments on 35 human cancer data sets, with KEGG pathways and BioGRID interactions as gene set sources. Pearson correlation coefficient and interaction gain were used as association measures. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "Covariate analysis of descriptor-based ear recognition techniques", "Authors": ["Emersic Z.", "Meden B.", "Peer P.", "Struc V."], "Keywords": [], "Date": "2017", "Abstract": "Dense descriptor-based feature extraction techniques represent a popular choice for implementing biometric ear recognition system and are in general considered to be the current state-of-the-art in this area. In this paper, we study the impact of various factors (i.e., head rotation, presence of occlusions, gender and ethnicity) on the performance of 8 state-of-the-art descriptor-based ear recognition techniques. Our goal is to pinpoint weak points of the existing technology and identify open problems worth exploring in the future. We conduct our covariate analysis through identification experiments on the challenging AWE (Annotated Web Ears) dataset and report our findings. The results of our study show that high degrees of head movement and presence of accessories significantly impact the identification performance, whereas mild degrees of the listed factors and other covariates such as gender and ethnicity impact the identification performance only to a limited extent.", "Language": "en", "Citations": "5"},
{"Title": "Security policy conceptual modeling and formalization for networked information systems", "Authors": ["Trcek D."], "Keywords": [], "Date": "2000", "Abstract": "Security in networked information systems is a very complex task that ranges from the level of crypto-primitives over crypto-protocols to the level of organizational matters and legislation. All this is comprised in a so-called security policy, which is often treated as an after-thought. One of the main reasons is the lack of appropriate techniques for conceptual modeling of security policy at early stages of system design. The approach in this paper is based on flow controls as one of the key ingredients for defining a security policy. Consequent security services and security architectures are derived by means of the proposed technique, which also bridges the gap to formal techniques. The result is a formalized output that serves as a basis for further refinement in subsequent stages of the modeling process.", "Language": "en", "Citations": "13"},
{"Title": "Attribute interactions in medical data analysis", "Authors": ["Jakulin A.", "Bratko I.", "Smrke D.", "Demsar J.", "Zupan B."], "Keywords": [], "Date": "2003", "Abstract": "There is much empirical evidence about the success of naive Bayesian classification (NBC) in medical applications of attribute-based machine learning. NBC assumes conditional independence between attributes. In classification, such classifiers sum up the pieces of classrelated evidence from individual attributes, independently of other attributes. The performance, however, deteriorates significantly when the \"interactions\" between attributes become critical. We propose an approach to handling attribute interactions within the framework of \"voting\" classifiers, such as NBC. We propose an operational test for detecting interactions in learning data and a procedure that takes the detected interactions into account while learning. This approach induces a structuring of the domain of attributes, it may lead to improved classifier's performance and may provide useful novel information for the domain expert when interpreting the results of learning. We report on its application in data analysis and model construction for the prediction of clinical outcome in hip arthroplasty. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "38"},
{"Title": "A strong excision theorem for generalised Tate cohomology", "Authors": ["Kosta N.M."], "Keywords": [], "Date": "2005", "Abstract": "We consider the analogue of the fixed point theorem of A. Borel in the context of Tate cohomology. We show that for general compact Lie groups G the Tate cohomology of a G-CW complex X with coefficients in a field of characteristic 0 is in general not isomorphic to the cohomology of the fixed point set, and thus the fixed point theorem does not apply. Instead, the following excision theorem is valid: if X\u2032 is the subcomplex of all G-cells of orbit type G/H where dim H > 0, and y is a ring such that for every finite isotropy group H the order |H| is invertible in V, then \u0124*", "Language": "en", "Citations": "0"},
{"Title": "A quality evaluation framework for mobile learning applications", "Authors": ["Baloh M.", "Zupanc K.", "Kosir D.", "Bosnic Z.", "Scepanovic S."], "Keywords": ["M-learning", "Mobile applications", "Mobile self-study applications", "Quality evaluation framework", "Software quality"], "Date": "2015", "Abstract": "The field of mobile learning is in the process of rapid growth. While we can easily identify characteristics, which separate mobile learning from other forms of learning, a review of the academic field yielded few accepted standards, frameworks and systems, which application developers can reference when building applications. The focus of this paper is to continue the research into quality assessment for mobile learning applications. The paper proposes and analyses a quality assessment framework for mobile learning applications. The framework was adapted from an existing framework, which was developed for mobile learning environments, which support the mentor - learner model of study. The adapted framework is intended to be used with self-study applications - applications, which do not rely on the presence of a teacher or mentor. This framework is evaluated with respect to given quality criteria on a set of 21 mobile learning applications. The resulting ratings are evaluated using Pearson correlation and the Apriori algorithm to discover correlation between evaluated attributes.", "Language": "en", "Citations": "3"},
{"Title": "Scrum in software engineering courses: An outline of the literature", "Authors": ["Mahnic V."], "Keywords": ["Agile methods", "Capstone project", "Problem-based learning", "Scrum", "Software engineering education"], "Date": "2015", "Abstract": "This article provides an outline of the literature dealing with teaching Scrum in software engineering courses. The search of studies in the Scopus database revealed 23 papers that were identified as primary studies relevant to this research. These studies are classified by their main topic and their main results are presented. All studies stress that the teaching of Scrum should not be limited to traditional lectures, but some practical experience should be provided in order to strengthen comprehension and achieve deep learning. For this reason, the use of Scrum in capstone (or similar) projects requiring students to work in teams is the most widely adopted strategy, described in seven studies. Four studies describe the use of simulation games as an alternative to practical project work (and even traditional lectures). Teaching Scrum through a capstone course provides a suitable environment for in-depth analyses of students' perceptions of typical Scrum practices (discussed in four studies), but also opens some pedagogical issues, such as the assessment of individual performance and consideration of students' learning styles (three studies). The remaining studies describe general experience and recommendations, the development of teaching aids and the combination of Scrum with other process models.", "Language": "en", "Citations": "16"},
{"Title": "Developmentally regulated DNA methylation in Dictyostelium discoideum", "Authors": ["Katoh M.", "Curk T.", "Xu Q.", "Zupan B.", "Kuspa A.", "Shaulsky G."], "Keywords": [], "Date": "2006", "Abstract": "Methylation of cytosine residues in DNA plays a critical role in the silencing of gene expression, organization of chromatin structure, and cellular differentiation of eukaryotes. Previous studies failed to detect 5-methylcytosine in Dictyostelium genomic DNA, but the recent sequencing of the Dictyostelium genome revealed a candidate DNA methyltransferase gene (dnmA). The genome sequence also uncovered an unusual distribution of potential methylation sites, CpG islands, throughout the genome. DnmA belongs to the Dnmt2 subfamily and contains all the catalytic motifs necessary for cytosine methyltransferases. Dnmt2 activity is typically weak in Drosophila melanogaster, mouse, and human cells and the gene function in these systems is unknown. We have investigated the methylation status of Dictyostelium genomic DNA with antibodies raised against 5-methylcytosine and detected low levels of the modified nucleotide. We also found that DNA methylation increased during development. We searched the genome for potential methylation sites and found them in retrotransposable elements and in several other genes. Using Southern blot analysis with methylation-sensitive and -insensitive restriction endonucleases, we found that the DIRS retrotransposon and the guaB gene were indeed methylated. We then mutated the dnmA gene and found that DNA methylation was reduced to about 50% of the wild-type level. The mutant cells exhibited morphological defects in late development, indicating that DNA methylation has a regulatory role in Dictyostelium development. Our findings establish a role for a Dnmt2 methyltransferase in eukaryotic development. Copyright \u00a9 2006, American Society for Microbiology. All Rights Reserved.", "Language": "en", "Citations": "43"},
{"Title": "Improving the graph grammar parser of Rekers and Sch\u00fcrr", "Authors": ["Furst L.", "Mernik M.", "Mahnic V."], "Keywords": [], "Date": "2011", "Abstract": "Graph grammars and graph grammar parsers are to visual languages what string grammars and parsers are to textual languages. A graph grammar specifies a set of valid graphs and can thus be used to formalise the syntax of a visual language. A graph grammar parser is a tool for recognising valid programs in such a formally defined visual language. A parser for context-sensitive graph grammars, which have proved to be suitable for formalising real-world visual languages, was developed by Rekers and Sch\u00fcrr. We propose three improvements of this parser. One of them enlarges the class of parsable graph grammars, while the other two increase the parser's computational efficiency. Experimental results show that for some (meaningful) graph grammars, our improvements can enhance the parser's performance by orders of magnitude. The proposed improvements will hopefully increase both the parser's applicability and the interest in visual language parsing in general. \u00a9 2011 The Institution of Engineering and Technology.", "Language": "en", "Citations": "8"},
{"Title": "Disease Containment Strategies based on Mobility and Information Dissemination", "Authors": ["Lima A.", "De Domenico M.", "Pejovic V.", "Musolesi M."], "Keywords": [], "Date": "2015", "Abstract": "Human mobility and social structure are at the basis of disease spreading. Disease containment strategies are usually devised from coarse-grained assumptions about human mobility. Cellular networks data, however, provides finer-grained information, not only about how people move, but also about how they communicate. In this paper we analyze the behavior of a large number of individuals in Ivory Coast using cellular network data. We model mobility and communication between individuals by means of an interconnected multiplex structure where each node represents the population in a geographic area (i.e., a sous-pr\u00e9fecture, a third-level administrative region). We present a model that describes how diseases circulate around the country as people move between regions. We extend the model with a concurrent process of relevant information spreading. This process corresponds to people disseminating disease prevention information, e.g., hygiene practices, vaccination campaign notices and other, within their social network. Thus, this process interferes with the epidemic. We then evaluate how restricting the mobility or using preventive information spreading process affects the epidemic. We find that restricting mobility does not delay the occurrence of an endemic state and that an information campaign might be an effective countermeasure.", "Language": "en", "Citations": "25"},
{"Title": "Polycyclic configurations", "Authors": ["Boben M.", "Pisanski T."], "Keywords": ["Configurations", "Graphs"], "Date": "2003", "Abstract": "Polycyclic configurations constitute a generalization of the well-known class of cyclic configurations. They admit a concise description via voltage graphs over cyclic groups. Polycyclic (v", "Language": "en", "Citations": "35"},
{"Title": "Bacterial discrimination by dictyostelid amoebae reveals the complexity of ancient interspecies interactions", "Authors": ["Nasser W.", "Santhanam B.", "Miranda E.R.", "Parikh A.", "Juneja K.", "Rot G.", "Dinh C.", "Chen R.", "Zupan B.", "Shaulsky G.", "Kuspa A."], "Keywords": [], "Date": "2013", "Abstract": "Background Amoebae and bacteria interact within predator-prey and host-pathogen relationships, but the general response of amoeba to bacteria is not well understood. The amoeba Dictyostelium discoideum feeds on, and is colonized by, diverse bacterial species, including Gram-positive [Gram(+)] and Gram-negative [Gram(-)] bacteria, two major groups of bacteria that differ in structure and macromolecular composition. Results Transcriptional profiling of D. discoideum revealed sets of genes whose expression is enriched in amoebae interacting with different species of bacteria, including sets that appear specific to amoebae interacting with Gram(+) or with Gram(-) bacteria. In a genetic screen utilizing the growth of mutant amoebae on a variety of bacteria as a phenotypic readout, we identified amoebal genes that are only required for growth on Gram(+) bacteria, including one that encodes the cell-surface protein gp130, as well as several genes that are only required for growth on Gram(-) bacteria, including one that encodes a putative lysozyme, AlyL. These genes are required for parts of the transcriptional response of wild-type amoebae, and this allowed their classification into potential response pathways. Conclusions We have defined genes that are critical for amoebal survival during feeding on Gram(+), or Gram(-), bacteria that we propose form part of a regulatory network that allows D. discoideum to elicit specific cellular responses to different species of bacteria in order to optimize survival. \u00a9 2013 Elsevier Ltd.", "Language": "en", "Citations": "25"},
{"Title": "iCLIP identifies novel roles for SAFB1 in regulating RNA processing and neuronal function", "Authors": ["Rivers C.", "Idris J.", "Scott H.", "Rogers M.", "Lee Y.-B.", "Gaunt J.", "Phylactou L.", "Curk T.", "Campbell C.", "Ule J.", "Norman M.", "Uney J.B."], "Keywords": ["HnRNP", "ICLIP", "Long non-coding RNA", "MiRNA", "NCAM1", "Neuronal", "RNA", "SAFB1", "Splicing"], "Date": "2015", "Abstract": "Background: SAFB1 is a RNA binding protein implicated in the regulation of multiple cellular processes such as the regulation of transcription, stress response, DNA repair and RNA processing. To gain further insight into SAFB1 function we used iCLIP and mapped its interaction with RNA on a genome wide level. Results: iCLIP analysis found SAFB1 binding was enriched, specifically in exons, ncRNAs, 3' and 5' untranslated regions. SAFB1 was found to recognise a purine-rich GAAGA motif with the highest frequency and it is therefore likely to bind core AGA, GAA, or AAG motifs. Confirmatory RT-PCR experiments showed that the expression of coding and non-coding genes with SAFB1 cross-link sites was altered by SAFB1 knockdown. For example, we found that the isoform-specific expression of neural cell adhesion molecule (NCAM1) and ASTN2 was influenced by SAFB1 and that the processing of miR-19a from the miR-17-92 cluster was regulated by SAFB1. These data suggest SAFB1 may influence alternative splicing and, using an NCAM1 minigene, we showed that SAFB1 knockdown altered the expression of two of the three NCAM1 alternative spliced isoforms. However, when the AGA, GAA, and AAG motifs were mutated, SAFB1 knockdown no longer mediated a decrease in the NCAM1 9-10 alternative spliced form. To further investigate the association of SAFB1 with splicing we used exon array analysis and found SAFB1 knockdown mediated the statistically significant up- and downregulation of alternative exons. Further analysis using RNAmotifs to investigate the frequency of association between the motif pairs (AGA followed by AGA, GAA or AAG) and alternative spliced exons found there was a highly significant correlation with downregulated exons. Together, our data suggest SAFB1 will play an important physiological role in the central nervous system regulating synaptic function. We found that SAFB1 regulates dendritic spine density in hippocampal neurons and hence provide empirical evidence supporting this conclusion. Conclusions: iCLIP showed that SAFB1 has previously uncharacterised specific RNA binding properties that help coordinate the isoform-specific expression of coding and non-coding genes. These genes regulate splicing, axonal and synaptic function, and are associated with neuropsychiatric disease, suggesting that SAFB1 is an important regulator of key neuronal processes.", "Language": "en", "Citations": "6"},
{"Title": "Permutation routing in double-loop networks: Design and empirical evaluation", "Authors": ["Dobravec T.", "Robic B.", "Zerovink J."], "Keywords": ["Algorithm design", "Double-loop network", "Permutation routing", "Simulation"], "Date": "2003", "Abstract": "A double-loop network is an undirected graph whose nodes are integers 0,1, . . . , n - 1 and each node u is adjacent to four nodes u \u00b1 h", "Language": "en", "Citations": "8"},
{"Title": "Using COBIT indicators for measuring scrum-based software development", "Authors": ["Mahnic V.", "Zabkar N."], "Keywords": ["Agile software development", "AGIT", "COBIT", "IT balanced scorecard", "IT indicators", "IT performance measurement", "Scrum"], "Date": "2008", "Abstract": "The aim of this paper is to determine the level of compliance of AGIT model, developed during our previous research for measuring Scrum-based software development, with the information systems auditing criteria. For this purpose we use COBIT model. After a short introduction of Scrum, AGIT and COBIT, we perform comparison analysis of their indicators for software development. Then we upgrade AGIT model with the selected COBIT indicators. In order to improve the clarity of the model, we present its structure using IT Balanced Scorecard. Finally we suggest possible further research.", "Language": "en", "Citations": "11"},
{"Title": "Qualitatively faithful quantitative prediction", "Authors": ["Sue D.", "Vladusic D.", "Bratko I."], "Keywords": [], "Date": "2003", "Abstract": "In this paper we describe a case study in which we applied an approach to qualitative machine learning to induce, from system's behaviour data, a qualitative model of a complex, industrially relevant mechanical system (a car wheel suspension system). The induced qualitative model enables nice causal interpretation of the relations in the modelled system. Moreover, we also show that the qualitative model can be used to guide the quantitative modelling process leading to numerical predictions that may be considerably more accurate than those obtained by state-of-the-art numerical modelling methods. This idea of combining qualitative and quantitative machine learning for system identification is in this paper carried out in two stages: (1) induction of qualitative constraints from system's behaviour data, and (2) induction of a numerical regression function that both respects the qualitative constraints and fits the training data numerically. We call this approach Q", "Language": "en", "Citations": "8"},
{"Title": "Towards automated cooking process", "Authors": ["Jazbec A.", "Mraz M.", "Bajec I.L.", "Zimic N."], "Keywords": ["Automated cooking process", "Fuzzy control system", "Intelligent cooking", "Sound based control"], "Date": "2007", "Abstract": "This paper presents a new approach towards the intelligent cooking process based on the correlation of the sound pressure in the cooking pan and the temperature of the pan's interior. When captured from the cover's handle the degree of correlation between the sound pressure and the interior's temperature is grater than the correlation between the temperature inside the cover's handle and the interior's temperature. With this new non-invasive approach (i.e., one that does not physically alter neither the pan nor the pan's contents), we achieved the automated cooking process. The main benefits are the minimization of the time spent behind the kitchen range and less power consumption. \u00a9 2007 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "Maximal simultaneously nilpotent sets of matrices over antinegative semirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Antinegative semiring", "Nilpotent matrix", "Simultaneous nilpotence"], "Date": "2016", "Abstract": "We study the simultaneously nilpotent index of a simultaneously nilpotent set of matrices over an antinegative commutative semiring S. We find an upper bound for this index and give some characterizations of the simultaneously nilpotent sets when this upper bound is met. In the special case of antinegative semirings with all zero divisors nilpotent, we also find a bound on the simultaneously nilpotent index for all nonmaximal simultaneously nilpotent sets of matrices and establish their cardinalities in case of a finite S.", "Language": "en", "Citations": "0"},
{"Title": "A framework for robust and incremental self-localization of a mobile robot", "Authors": ["Jogan M.", "Artac M.", "Skocaj D.", "Leonardis A."], "Keywords": [], "Date": "2003", "Abstract": "In this contribution we present a framework for an embodied robotic system that is capable of appearance-based self-localization. Specifically, we concentrate on the issues of robustness, flexibility, and scalability of the system. The framework presented is based on a panoramic eigenspace model of the environment. Its main feature is that it allows for simultaneous localization and map building using an incremental learning algorithm. Further, both the learning and the training processes are designed in a way to achieve robustness and adaptability to changes in the environment. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "7"},
{"Title": "Probabilistic segmentation and labeling of ethnomusicological field recordings", "Authors": ["Marolt M."], "Keywords": [], "Date": "2009", "Abstract": "The paper presents a method for segmentation and labeling of ethnomusicological field recordings. Field recordings are integral documents of folk music performances and typically contain interviews with performers intertwined with actual performances. As these are live recordings of amateur folk musicians, they may contain interruptions, false starts, environmental noises or other interfering factors. Our goal was to design a robust algorithm that would approximate manual segmentation of field recordings. First, short audio fragments are classified into one of the following categories: speech, solo singing, choir singing, instrumental or bell chiming performance. Then, a set of candidate segment boundaries is obtained by observing how the energy of the signal and its content change, and finally the recording is segmented with a probabilistic model that maximizes the posterior probability of segments given a set of candidate segment boundaries with their probabilities and prior knowledge of lengths of segments belonging to different categories. Evaluation of the algorithm on a set of field recordings from the Ehtnomuse archive is presented. \u00a9 2009 International Society for Music Information Retrieval.", "Language": "en", "Citations": "9"},
{"Title": "Simplified design of the Speech Recognition System Enostavnej\u0161a zasnova sistema za razpoznavanje govora", "Authors": ["Rozman R."], "Keywords": [], "Date": "2013", "Abstract": "Disadvantages of the currently used Speech Recognition Systems (SRSs) and alternative ways of their evolution are presented and discussed. In our opinion, SRSs are rather static structures with a lot of predefined knowledge that is built into them upon their creation, and usually remaining unchanged or unadapted during the recognition process. Several possible ways of increasing the amount of the dynamic, automatically learned knowledge in the next generation of SRSs are discussed; this is of a particular importance for under-resourced languages. A group of SRSs, i.e. compact SRSs with a limited vocabulary based on the Neural Network as an acoustic model, is of a particular interest. Its structure is more compatible with the recent developments in the field of distributed and parallel processing. Two experimental systems are presented and tested on a simple phoneme recognition task. One system is a fairly complete SRS based on the Neural Network as an acoustic model and Viterbi search as a time model. The second system is much simpler using only the Neural Network as an acoustic model. It will support our further research in this field.", "Language": "en", "Citations": "1"},
{"Title": "Quantitative analysis of separate and combined performance of local searcher and genetic algorithm", "Authors": ["Djordjevic M.", "Brodnik A."], "Keywords": ["Genetic algorithms", "Grafted genetic algorithm", "Memetic algorithms (MA)", "Traveling Salesman Problem (TSP)"], "Date": "2011", "Abstract": "In this paper an environment is established for a quantitative analysis of separate and combined performance of local searchers and standard genetic algorithm. Well researched and controlled Euclidean Travelling Salesman Problem examines the impact of grafting a 2-opt based local searcher into the standard genetic algorithm for solving the Travelling Salesman Problem with Euclidean distance. Standard genetic algorithms are known to be rather slow, while 2-opt search applied to the Travelling Salesman Problem quickly gives results that are far from optimal. We propose a strategy to graft a 2-opt local searcher into a genetic algorithm, after recombination, to optimize each offspring's genomes. Genetic algorithm provides new search areas, while 2-opt improves convergence. We tested our algorithm on examples from TSPLIB and proved that this method combines good qualities from both methods applied, significantly outperforming each of them.", "Language": "en", "Citations": "2"},
{"Title": "Towards symbolic mining of images with association rules: Preliminary results on textures", "Authors": ["Bevk M.", "Kononenko I."], "Keywords": [], "Date": "2006", "Abstract": "This paper presents new textural features which are based on association rules. We give a texture representation, which is an appropriate formalism, that allows straightforward application of association rules algorithms. This representation has several good properties like invariance to global lightness and invariance to rotation. Association rules capture structural and statistical information and are very convenient to identify the structures that occur most frequently and have the most discriminative power. The results from our experiments show that this representation gives comparable results to standard texture descriptions and better results than general image descriptions. \u00a9 2006-IOS Press and the authors.", "Language": "en", "Citations": "8"},
{"Title": "15 Seconds of fame - An interactive, computer-vision based art installation", "Authors": ["Solina F.", "Peer P.", "Batagelj B.", "Juvan S."], "Keywords": [], "Date": "2002", "Abstract": "\"15 seconds of fame\" is an interactive art installation which elevates the face of a randomly selected gallery visitor for 15 seconds into a \"work of art\". The installation was inspired by Andy Warhol's statement that \"In the future everybody will be world famous for fifteen minutes\" as well as by the pop-art style of his works. The installation consists of a computer with a flat-panel monitor, a digital camera and proprietary software that can detect human faces in images and graphically transform them. In this paper we present the technical background of the installation, in particular, how computer vision techniques were applied in this art installation.", "Language": "en", "Citations": "22"},
{"Title": "Human Tra2 proteins jointly control a CHEK1 splicing switch among alternative and constitutive target exons", "Authors": ["Best A.", "James K.", "Dalgliesh C.", "Hong E.", "Kheirolahi-Kouhestani M.", "Curk T.", "Xu Y.", "Danilenko M.", "Hussain R.", "Keavney B.", "Wipat A.", "Klinck R.", "Cowell I.G.", "Cheong Lee K.", "Austin C.A.", "Venables J.P.", "Chabot B.", "Santibanez Koref M.", "Tyson-Capper A.", "Elliott D.J."], "Keywords": [], "Date": "2014", "Abstract": "Alternative splicing - the production of multiple messenger RNA isoforms from a single gene - is regulated in part by RNA binding proteins. While the RBPs transformer2 alpha (Tra2\u03b1) and Tra2\u03b2 have both been implicated in the regulation of alternative splicing, their relative contributions to this process are not well understood. Here we find simultaneous - but not individual - depletion of Tra2\u03b1 and Tra2\u03b2 induces substantial shifts in splicing of endogenous Tra2\u03b2 target exons, and that both constitutive and alternative target exons are under dual Tra2\u03b1 -Tra2\u03b2 control. Target exons are enriched in genes associated with chromosome biology including CHEK1, which encodes a key DNA damage response protein. Dual Tra2 protein depletion reduces expression of full-length CHK1 protein, results in the accumulation of the DNA damage marker \u03b3H2AX and decreased cell viability. We conclude Tra2 proteins jointly control constitutive and alternative splicing patterns via paralog compensation to control pathways essential to the maintenance of cell viability.", "Language": "en", "Citations": "23"},
{"Title": "Role of user models in adaptive hypermedia systems", "Authors": ["Kavcic Alenka"], "Keywords": [], "Date": "2000", "Abstract": "Different approaches to user modeling in adaptive hypermedia systems are described. The role of a user model is presented together with the various approaches to user modeling in adaptive hypermedia systems. Further, user modeling in different educational adaptive hypermedia systems is discussed.", "Language": "en", "Citations": "23"},
{"Title": "Argument-based machine learning", "Authors": ["Bratko I.", "Mozina M.", "Zabkar J."], "Keywords": ["Argumentation", "CN2", "Inductive logic programming", "Machine learning", "Rule learning"], "Date": "2006", "Abstract": "In this paper, some recent ideas will be presented about making machine learning (ML) more effective through mechanisms of argumentation. In this sense, argument-based machine learning (ABML) is defined as a refinement of the usual definition of ML. In ABML, some learning examples are accompanied by arguments, that are expert's reasons for believing why these examples are as they are. Thus ABML provides a natural way of introducing domain-specific prior knowledge in a way that is different from the traditional, general background knowledge. The task of ABML is to find a theory that explains the \"argumented\" examples by making reference to the given reasons. ABML, so defined, is motivated by the following advantages in comparison with standard learning from examples: (1) arguments impose constraints over the space of possible hypotheses, thus reducing search complexity, and (2) induced theories should make more sense to the expert. Ways of realising ABML by extending some existing ML techniques are discussed, and the aforementioned advantages of ABML are demonstrated experimentally. \u00a9 Springer-Verlag Berlin Heidelberg 2006.", "Language": "en", "Citations": "2"},
{"Title": "Characterizing the RNA targets and position-dependent splicing regulation by TDP-43", "Authors": ["Tollervey J.R.", "Curk T.", "Rogelj B.", "Briese M.", "Cereda M.", "Kayikci M.", "Konig J.", "Hortobagyi T.", "Nishimura A.L.", "Zupunski V.", "Patani R.", "Chandran S.", "Rot G.", "Zupan B.", "Shaw C.E.", "Ule J."], "Keywords": [], "Date": "2011", "Abstract": "TDP-43 is a predominantly nuclear RNA-binding protein that forms inclusion bodies in frontotemporal lobar degeneration (FTLD) and amyotrophic lateral sclerosis (ALS). The mRNA targets of TDP-43 in the human brain and its role in RNA processing are largely unknown. Using individual nucleotide-resolution ultraviolet cross-linking and immunoprecipitation (iCLIP), we found that TDP-43 preferentially bound long clusters of UG-rich sequences in vivo. Analysis of RNA binding by TDP-43 in brains from subjects with FTLD revealed that the greatest increases in binding were to the MALAT1 and NEAT1 noncoding RNAs. We also found that binding of TDP-43 to pre-mRNAs influenced alternative splicing in a similar position-dependent manner to Nova proteins. In addition, we identified unusually long clusters of TDP-43 binding at deep intronic positions downstream of silenced exons. A substantial proportion of alternative mRNA isoforms regulated by TDP-43 encode proteins that regulate neuronal development or have been implicated in neurological diseases, highlighting the importance of TDP-43 for the regulation of splicing in the brain. \u00a9 2011 Nature America, Inc. All rights reserved.", "Language": "en", "Citations": "537"},
{"Title": "On quality of different annotation sources for gene expression analysis", "Authors": ["Mulas F.", "Curk T.", "Bellazzi R.", "Zupan B."], "Keywords": [], "Date": "2009", "Abstract": "Mining of biomedical data increasingly relies on utility of knowledge repositories. In gene expression analysis, these are often used for gene labeling with an assumption that similarly annotated genes have similar expression profiles. In the paper we use this assumption to craft a method with which we scored six different annotation sources (e.g., Gene Ontology, PubMed, and MeSH annotations) for their utility in gene expression data analysis. Experiments show that the sources that include manual curation perform well and, for instance, score better than automatic annotation from gene-related PubMed abstracts. We also show that there is no clear winner, pointing at the need for methods that could successfully integrate annotations from different sources. \u00a9 2009 Springer Berlin Heidelberg.", "Language": "en", "Citations": "4"},
{"Title": "A toolkit for modelling and simulating data Grids: An extension to GridSim", "Authors": ["Sulistio A.", "Cibej U.", "Venugopal S.", "Robic B.", "Buyya R."], "Keywords": ["Data grids", "Grid computing", "Grid simulation"], "Date": "2008", "Abstract": "Data Grids are an emerging technology for managing large amounts of distributed data. This technology is highly anticipated by scientific communities, such as in the area of astronomy and high-energy physics, because their experiments generate massive amounts of data which need to be shared and analysed. Since it is not feasible to test different usages on real testbeds, it is easier to use simulations as a means of studying complex scenarios. This paper presents our work on incorporating data Grids features as an extension to GridSim, a computational Grid simulator. The extension provides essential building blocks for simulating various data Grids scenarios. Moreover, it is designed to be easily extended. This approach makes it easy to try various strategies and to add functionalities to suit the needs of other communities. This paper also gives a detailed description of the design and usage examples demonstrating the versatility of this tool.", "Language": "en", "Citations": "143"},
{"Title": "Software accessibility: Recommendations and guidelines", "Authors": ["Kavcic A."], "Keywords": ["Accessibility", "Accessibility guidelines", "People with disabilities", "Software accessibility", "Standards", "Usability"], "Date": "2005", "Abstract": "The paper deals with problems that people with special needs have when using Information Technology products. Many software developers are not even aware of the problem, although people with disabilities nowadays represent a considerably market share. In recent times, more accessible software design is encouraged also by the national laws. The aim of this paper is to point out the difficulties and obstacles that most software applications pose to the disabled people and to seek for the guidelines on accessible software design. \u00a92005 IEEE.", "Language": "en", "Citations": "14"},
{"Title": "Hierarchical spatial model for 2D range data based room categorization", "Authors": ["Ursic P.", "Leonardis A.", "Skocaj D.", "Kristan M."], "Keywords": [], "Date": "2016", "Abstract": "The next generation service robots are expected to co-exist with humans in their homes. Such a mobile robot requires an efficient representation of space, which should be compact and expressive, for effective operation in real-world environments. In this paper we present a novel approach for 2D ground-plan-like laser-range-data-based room categorization that builds on a compositional hierarchical representation of space, and show how an additional abstraction layer, whose parts are formed by merging partial views of the environment followed by graph extraction, can achieve improved categorization performance. A new algorithm is presented that finds a dictionary of exemplar elements from a multi-category set, based on the affinity measure defined among pairs of elements. This algorithm is used for part selection in new layer construction. Room categorization experiments have been performed on a challenging publicly available dataset, which has been extended in this work. State-of-the-art results were obtained by achieving the most balanced performance over all categories.", "Language": "en", "Citations": "3"},
{"Title": "Discovery of genetic networks through abduction and qualitative simulation", "Authors": ["Zupan B.", "Bratko I.", "Demsar J.", "Juvan P.", "Kuspa A.", "Halter J.A.", "Shaulsky G."], "Keywords": [], "Date": "2007", "Abstract": "GenePath is an automated system for reasoning about genetic networks, wherein a set of genes have various influences on one another and on a biological outcome. It acts on a set of experiments in which genes are knocked-out or overexpressed, and the outcome of interest is evaluated. Implemented in Prolog, GenePath uses abductive inference to elucidate network constraints based on background knowledge and experimental results. Two uses of the system are demonstrated: synthesis of a consistent network from abduced constraints, and qualitative reasoning-based approach that generates a set of networks consistent with the data. In practice, as illustrated by an example on aggregation of a soil amoeba Dictyostelium discoideum, a combination of constraint satisfaction and qualitative reasoning produces a small set of plausible networks. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "2"},
{"Title": "Deep sequencing of virus-derived small interfering RNAs and RNA from viral particles shows highly similar mutational landscapes of a plant virus population", "Authors": ["Kutnjak D.", "Rupar M.", "Gutierrez-Aguirre I.", "Curk T.", "Kreuze J.F.", "Ravnikar M."], "Keywords": [], "Date": "2015", "Abstract": "RNA viruses exist within a host as a population of mutant sequences, often referred to as quasispecies. Within a host, sequences of RNA viruses constitute several distinct but interconnected pools, such as RNA packed in viral particles, double-stranded RNA, and virus-derived small interfering RNAs. We aimed to test if the same representation of within-host viral population structure could be obtained by sequencing different viral sequence pools. Using ultradeep Illumina sequencing, the diversity of two coexisting Potato virus Y sequence pools present within a plant was investigated: RNA isolated from viral particles and virus-derived small interfering RNAs (the derivatives of a plant RNA silencing mechanism). The mutational landscape of the within-host virus population was highly similar between both pools, with no notable hotspots across the viral genome. Notably, all of the singlenucleotide polymorphisms with a frequency of higher than 1.6% were found in both pools. Some unique single-nucleotide polymorphisms (SNPs) with very low frequencies were found in each of the pools, with more of them occurring in the small RNA (sRNA) pool, possibly arising through genetic drift in localized virus populations within a plant and the errors introduced during the amplification of silencing signal. Sequencing of the viral particle pool enhanced the efficiency of consensus viral genome sequence reconstruction. Nonhomologous recombinations were commonly detected in the viral particle pool, with a hot spot in the 3' untranslated and coat protein regions of the genome. We stress that they present an important but often overlooked aspect of virus population diversity.", "Language": "en", "Citations": "28"},
{"Title": "Commutativity preservers via maximal centralizers", "Authors": ["Dolinar G.", "Guterman A.", "Kuzma B.", "Oblak P."], "Keywords": ["Centralizers", "General preservers of commutativity", "Matrix algebra"], "Date": "2014", "Abstract": "Bijective maps on matrices over arbitrary fields with suffciently may elements which preserve commutativity in both direction are classified.", "Language": "en", "Citations": "3"},
{"Title": "A balanced mixture of antagonistic pressures promotes the evolution of parallel movement", "Authors": ["Demsar J.", "Strumbelj E.", "Bajec I.L."], "Keywords": [], "Date": "2016", "Abstract": "A common hypothesis about the origins of collective behaviour suggests that animals might live and move in groups to increase their chances of surviving predator attacks. This hypothesis is supported by several studies that use computational models to simulate natural evolution. These studies, however, either tune an ad-hoc model to 'reproduce' collective behaviour, or concentrate on a single type of predation pressure, or infer the emergence of collective behaviour from an increase in prey density. In nature, prey are often targeted by multiple predator species simultaneously and this might have played a pivotal role in the evolution of collective behaviour. We expand on previous research by using an evolutionary rule-based system to simulate the evolution of prey behaviour when prey are subject to multiple simultaneous predation pressures. We analyse the evolved behaviour via prey density, polarization, and angular momentum. Our results suggest that a mixture of antagonistic external pressures that simultaneously steer prey towards grouping and dispersing might be required for prey individuals to evolve dynamic parallel movement.", "Language": "en", "Citations": "4"},
{"Title": "Teaching with Open-Source Robotic Manipulator", "Authors": ["Cehovin Zajc L.", "Rezelj A.", "Skocaj D."], "Keywords": ["Education", "Evaluation", "Open-hardware", "Open-source", "Robotic manipulator"], "Date": "2019", "Abstract": "In this paper we present and evaluate the usage of an open-source robotic manipulator platform, that we have developed, in the context of various educational scenarios that we have conducted. The system was tested in multiple diverse learning scenarios, ranging from a summer school for primary-school students, to the course at the university level study. We show that the introduction of the system in the educational process improves the motivation as well as acquired knowledge of the participants.", "Language": "en", "Citations": "0"},
{"Title": "HINMINE: heterogeneous information network mining with information retrieval heuristics", "Authors": ["Kralj J.", "Robnik-Sikonja M.", "Lavrac N."], "Keywords": ["Centroid classifier", "Heterogeneous information networks", "Imbalanced data", "Information retrieval", "label propagation", "Network analysis", "Network decomposition", "Personalized PageRank", "SVM", "Text mining heuristics"], "Date": "2018", "Abstract": "The paper presents an approach to mining heterogeneous information networks by decomposing them into homogeneous networks. The proposed HINMINE methodology is based on previous work that classifies nodes in a heterogeneous network in two steps. In the first step the heterogeneous network is decomposed into one or more homogeneous networks using different connecting nodes. We improve this step by using new methods inspired by weighting of bag-of-words vectors mostly used in information retrieval. The methods assign larger weights to nodes which are more informative and characteristic for a specific class of nodes. In the second step, the resulting homogeneous networks are used to classify data either by network propositionalization or label propagation. We propose an adaptation of the label propagation algorithm to handle imbalanced data and test several classification algorithms in propositionalization. The new methodology is tested on three data sets with different properties. For each data set, we perform a series of experiments and compare different heuristics used in the first step of the methodology. We also use different classifiers which can be used in the second step of the methodology when performing network propositionalization. Our results show that HINMINE, using different network decomposition methods, can significantly improve the performance of the resulting classifiers, and also that using a modified label propagation algorithm is beneficial when the data set is imbalanced.", "Language": "en", "Citations": "4"},
{"Title": "Test of the interactive orientation method on the example of the triglav glacier Test uporabnosti interaktivne metode orientacije na primeru posnetkov triglavskega ledenika", "Authors": ["Cekada M.T.", "Strumbelj E.", "Jakovac A."], "Keywords": ["Digital elevation model", "Interactive photo orientation method", "Laser scanning", "Remote sensing"], "Date": "2007", "Abstract": "The interactive method of orientation between 2D photographs and a 3D height model presented with a point cloud is described. The orientation parameters of the 2D photograph (rotations and location of the camera) are derived using the interactive searching for the best fit of the projected 3D height model points on the image. This method requires good operator's knowledge of the photograph and its details and above all a lot of time. The most suitable photographs for the interactive method of orientation are the panoramic ones, with removed distortions. On the photograph there should be many easily found targets or details (edges...). The method was developed for the error search in the laser scanning point cloud (R\u00f6nnholm et al., 2003a, 2003b). In this paper a test of the usefulness of the interactive method of orientation on the example of Horizont nonmetric panoramic photographs of the Triglav glacier is presented. Because of the lack of good details seen on the photographs and in the 3D model, we derived only approximate values for the orientation parameters. Unfortunately this is not enough for conducting measurements of height difference out of the Horizont photograph. If correct values of orientation were available, the height differences between the 3D model and photographs could be measured. This would be done with shifts of the 3D point images on two Horizont photographs, so that they would fit the photographs better. The 3D model presents the state of the Triglav glacier in the year 2005. The Horizont photographs present the state of the glacier in each month from 1976 on.", "Language": "en", "Citations": "1"},
{"Title": "Fast Image-Based Obstacle Detection from Unmanned Surface Vehicles", "Authors": ["Kristan M.", "Kenk V.S.", "Kovacic S.", "Pers J."], "Keywords": ["Autonomous surface vehicles", "Gaussian mixture models", "Markov random fields (MRFs)", "obstacle-map estimation"], "Date": "2016", "Abstract": "Obstacle detection plays an important role in unmanned surface vehicles (USVs). The USVs operate in a highly diverse environments in which an obstacle may be a floating piece of wood, a scuba diver, a pier, or a part of a shoreline, which presents a significant challenge to continuous detection from images taken on board. This paper addresses the problem of online detection by constrained, unsupervised segmentation. To this end, a new graphical model is proposed that affords a fast and continuous obstacle image-map estimation from a single video stream captured on board a USV. The model accounts for the semantic structure of marine environment as observed from USV by imposing weak structural constraints. A Markov random field framework is adopted and a highly efficient algorithm for simultaneous optimization of model parameters and segmentation mask estimation is derived. Our approach does not require computationally intensive extraction of texture features and comfortably runs in real time. The algorithm is tested on a new, challenging, dataset for segmentation, and obstacle detection in marine environments, which is the largest annotated dataset of its kind. Results on this dataset show that our model outperforms the related approaches, while requiring a fraction of computational effort.", "Language": "en", "Citations": "22"},
{"Title": "The encoding complexity of two dimensional range minimum data structures", "Authors": ["Brodal G.S.", "Brodnik A.", "Davoodi P."], "Keywords": [], "Date": "2013", "Abstract": "In the two-dimensional range minimum query problem an input matrix A of dimension m \u00d7 n, m \u2264 n, has to be preprocessed into a data structure such that given a query rectangle within the matrix, the position of a minimum element within the query range can be reported. We consider the space complexity of the encoding variant of the problem where queries have access to the constructed data structure but can not access the input matrix A, i.e. all information must be encoded in the data structure. Previously it was known how to solve the problem with space O(mn min {m,log n}) bits (and with constant query time), but the best lower bound was \u03a9(mn log m) bits, i.e. leaving a gap between the upper and lower bounds for non-quadratic matrices. We show that this space lower bound is optimal by presenting an encoding scheme using O(mn log m) bits. We do not consider query time. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "5"},
{"Title": "Next-Generation Communication Systems for PPDR: The SALUS Perspective", "Authors": ["Marques H.", "Pereira L.", "Rodriguez J.", "Mantas G.", "Sousa B.", "Fonseca H.", "Cordeiro L.", "Palma D.", "Barbatsalou K.", "Simoes P.", "Monteiro E.", "Nyanyo A.", "Wickson P.", "Bouwers B.", "Kolundzija B.", "Olcan D.", "Zerbib D.", "Brouet J.", "Lasserre P.", "Galiotos P.", "Chrysikos T.", "Jelenc D.", "Kos J.", "Trcek D.", "Ladas A.", "Weerasinghe N.", "Adigun O.", "Politis C.", "Muller W."], "Keywords": ["Emergency communications security", "Long-term evolution (LTE)", "Migration roadmaps", "Public Protection and Disaster Relief (PPDR)", "SALUS project", "Terrestrial Trunked Radio (TETRA)", "TETRAPOL technology", "Wi-Fi networks", "Wireless local area networks (WLANs)", "Wireless technologies"], "Date": "2015", "Abstract": "Public Protection and Disaster Relief (PPDR) agencies in European member states currently rely on digital Private Mobile Radio (PMR) networks for mission critical operations. PMR networks are based on two main standards for Europe: Terrestrial Trunked Radio (TETRA) and TETRAPOL. These networks provide secure and resilient mobile voice services, as well as basic data services. However, these traditional PMR networks show substantial limitations, when matched against modern requirements of PPDR agencies, including broadcast communications, dynamic secure groups, secure roaming, and emerging safety and security applications. Moreover, there are significant interoperability constraints when using multiple technologies, such as inter-technology coverage limitations, which can result in ineffective management of emergency events, both at national level as well as in cross-border regions.", "Language": "en", "Citations": "0"},
{"Title": "Predictive data mining in clinical medicine: Current issues and guidelines", "Authors": ["Bellazzi R.", "Zupan B."], "Keywords": ["Clinical medicine", "Data analysis", "Data mining", "Data mining process", "Predictive models"], "Date": "2008", "Abstract": "Background: The widespread availability of new computational methods and tools for data analysis and predictive modeling requires medical informatics researchers and practitioners to systematically select the most appropriate strategy to cope with clinical prediction problems. In particular, the collection of methods known as 'data mining' offers methodological and technical solutions to deal with the analysis of medical data and construction of prediction models. A large variety of these methods requires general and simple guidelines that may help practitioners in the appropriate selection of data mining tools, construction and validation of predictive models, along with the dissemination of predictive models within clinical environments. Purpose: The goal of this review is to discuss the extent and role of the research area of predictive data mining and to propose a framework to cope with the problems of constructing, assessing and exploiting data mining models in clinical medicine. Methods: We review the recent relevant work published in the area of predictive data mining in clinical medicine, highlighting critical issues and summarizing the approaches in a set of learned lessons. Results: The paper provides a comprehensive review of the state of the art of predictive data mining in clinical medicine and gives guidelines to carry out data mining studies in this field. Conclusions: Predictive data mining is becoming an essential instrument for researchers and clinical practitioners in medicine. Understanding the main issues underlying these methods and the application of agreed and standardized procedures is mandatory for their deployment and the dissemination of results. Thanks to the integration of molecular and clinical data taking place within genomic medicine, the area has recently not only gained a fresh impulse but also a new set of complex problems it needs to address. \u00a9 2006 Elsevier Ireland Ltd. All rights reserved.", "Language": "en", "Citations": "421"},
{"Title": "Context-aware mobile application model", "Authors": ["Rupnik R.", "Krisper M."], "Keywords": ["Context", "Context-aware mobile application", "Context-awareness", "Mobile application"], "Date": "2004", "Abstract": "The information society demands an increasingly higher productivity, which can be achieved by using applications in the state of mobility. Mobile applications represent a new application model which enables information support in the state of mobility. We first discuss mobile applications in the context of the information society and define their scope and limitations. Then we introduce a classical mobile application model and context-aware mobile application model. The classical mobile application model is a pull application model which enables the user to run a mobile application on demand. The context-aware mobile application model enables the triggering of mobile applications depending on a particular situation in the information system which determines informational needs of a mobile user.", "Language": "en", "Citations": "2"},
{"Title": "Rapid ontology development", "Authors": ["Labic D.", "Krisper M."], "Keywords": ["Business oriented approach", "Ontology", "Ontology evaluation", "Rapid ontology development", "Semantic Web"], "Date": "2010", "Abstract": "The applications of ontologies are mainly restricted to academia while successful employment in business environments is rare. Existing methodologies are complex and require technical expertise on language syntax and knowledge of the process. We propose Rapid Ontology Development (ROD) approach for ontology construction, where steps are constantly evaluated and the user is guided throughout the process. Following ROD approach the required technical knowledge for ontology modeling is minimized and the process doesn't end with the last successful iteration, but continues with post-development activities of using ontology as a functional component in other systems. \u00a9 2010 The authors and IOS Press. All rights reserved.", "Language": "en", "Citations": "3"},
{"Title": "Computational trust management, QAD, and its applications", "Authors": ["Trcek D."], "Keywords": ["e-services", "human factor", "management", "modeling", "simulation", "trust"], "Date": "2014", "Abstract": "Trust is an important factor for successful e-commerce and e-media applications. However, these media inherently disable many ordinary communication channels and means, and affect trust forming factors. Therefore cyber environment requires additional support when it comes to trust. This is also one key reason why computational trust management methods are being developed now for some fifteen years, while another key reason is to enable better decision making through mathematical modeling and simulations in other areas. These methods are grounded on certain premises, which are analyzed in this paper. On this basis, Qualitative assessment dynamics (QAD for short) is presented that complements the above methods. As opposed to other methods, it is aligned with certain principles of human reasoning. Therefore it further extends the scope of other computational trust management technologies that are typically concerned with artificial ways of reasoning, while QAD gives a basis also for applications in ordinary environments where humans are involved. By using this methodology, experimental work will be presented, applied to the area of organizations and human factor management.", "Language": "en", "Citations": "1"},
{"Title": "Rbfox2-coordinated alternative splicing of Mef2d and Rock2 controls myoblast fusion during myogenesis", "Authors": ["Singh R.K.", "Xia Z.", "Bland C.S.", "Kalsotra A.", "Scavuzzo M.A.", "Curk T.", "Ule J.", "Li W.", "Cooper T.A."], "Keywords": [], "Date": "2014", "Abstract": "Alternative splicing plays important regulatory roles during periods of physiological change. During development, a large number of genes coordinately express protein isoform transitions regulated by alternative splicing; however, the mechanisms that coordinate splicing and the functional integration of the resultant tissue-specific protein isoforms are typically unknown. Here we show that the conserved Rbfox2 RNA binding protein regulates 30% of the splicing transitions observed during myogenesis and is required for the specific step of myoblast fusion. Integration of Rbfox2-dependent splicing outcomes from RNA-seq with Rbfox2 iCLIP data identified Mef2d and Rock2 as Rbfox2 splicing targets. Restored activities of Mef2d and Rock2 rescued myoblast fusion in Rbfox2-depleted cultures, demonstrating functional cooperation of protein isoforms generated by coordinated alterative splicing. The results demonstrate that coordinated alternative splicing by a single RNA binding protein modulates transcription (Mef2d) and cell signaling (Rock2) programs to drive tissue-specific functions (cell fusion) to promote a developmental transition. \u00a9 2014 Elsevier Inc.", "Language": "en", "Citations": "47"},
{"Title": "LLLR parsing: A combination of LL and LR parsing", "Authors": ["Slivnik B."], "Keywords": ["Left parse", "LL parsing", "LR languages"], "Date": "2016", "Abstract": "A new parsing method called LLLR parsing is defined and a method for producing LLLR parsers is described. An LLLR parser uses an LL parser as its backbone and parses as much of its input string using LL parsing as possible. To resolve LL conflicts it triggers small embedded LR parsers. An embedded LR parser starts parsing the remaining input and once the LL conflict is resolved, the LR parser produces the left parse of the substring it has just parsed and passes the control back to the backbone LL parser. The LLLR(k) parser can be constructed for any LR(k) grammar. It produces the left parse of the input string without any backtracking and, if used for a syntax-directed translation, it evaluates semantic actions using the top-down strategy just like the canonical LL(k) parser. An LLLR(k) parser is appropriate for grammars where the LL(k) conflicting nonterminals either appear relatively close to the bottom of the derivation trees or produce short substrings. In such cases an LLLR parser can perform a significantly better error recovery than an LR parser since the most part of the input string is parsed with the backbone LL parser. LLLR parsing is similar to LL(\u2217) parsing except that it (a) uses LR(k) parsers instead of finite automata to resolve the LL(k) conflicts and (b) does not perform any backtracking.", "Language": "en", "Citations": "0"},
{"Title": "Comparison of software repositories for their usability in software process reconstruction", "Authors": ["Jankovic M.", "Bajec M."], "Keywords": [], "Date": "2015", "Abstract": "Software development process is like any other business process composed of activities carried out by process participants in order to achieve a certain goal. In contrast to a typical business process that is relatively deterministic and thus repeatable, software processes are much more dynamic in nature and dependent on a number of circumstances. This explains why actual software development practice in organizations defer from what these organizations prescribe within their adopted software development methods. The research that is reported in this paper aims at analyzing the suitability of software repositories to support de facto software process reconstruction. We examine most common utility tools that are used in software development and analyze the information they capture (we do that for a number of open source and commercial projects). We than suggest what would be a reasonable level of documentation for a software process so that this information would adequately facilitate project managers and developers at their work. Finally, based on our findings, we provide guidelines on how organizations should use software repositories to support the process reconstruction.", "Language": "en", "Citations": "3"},
{"Title": "A voice-driven Web browser for blind people", "Authors": ["Vesnicer B.", "Zibert J.", "Dobrisek S.", "Pavesic N.", "Mihelic F."], "Keywords": [], "Date": "2003", "Abstract": "A small self-voicing Web browser designed for blind users is presented. The Web browser was built from the GTK Web browser Dillo, which is a free software project in terms of the GNU general public license. Additional functionality has been introduced to this original browser in form of different modules. The browser operates in two different modes, browsing mode dialogue mode. In browsing mode user navigates through structure of Web pages using mouse and/or keyboard. When in dialogue mode, the dialogue module offers different actions the user chooses between them using either keyboard or spoken-commands which are recognized by the speech-recognition module. The content of the page is presented to the user by screen-reader module which uses text-tospeech module for its output. The browser is capable of displaying all common Web pages that do not contain frames, java or flash animations. However, the best performance is achieved when pages comply with the recommendations set by the WAI. The browser has been developed in Linux operating system and later ported toWindows 9x/ME/NT/2000/XP platform. Currently it is being tested by members of the Slovenian blind people society. Any suggestions or wishes from them will be considered for inclusion in future versions of the browser.", "Language": "en", "Citations": "8"},
{"Title": "The development of emergent properties in massive multi-agent systems", "Authors": ["Privosnik M.", "Marolt M."], "Keywords": ["Decentralization", "Emergence", "Evolutionary Computation", "Multi-Agent Systems"], "Date": "2001", "Abstract": "Emergent behavior results in complex systems. Emergent behavior can be observed in various systems - a colony of ants, an economy, a brain or a large network of computers - these are all systems with complex emergent behavior. The behavior of a complex system as a whole emerges in a highly non-linear manner from the behaviors of the low level constituent elementary units. This makes traditional linear analysis difficult, if not impossible. In this paper I will focus on emergent properties of massive multi-agent systems. I will present how these properties can be utilized to model global behavior of the system.", "Language": "en", "Citations": "0"},
{"Title": "Subgroup discovery in data sets with multi-dimensional responses", "Authors": ["Umek L.", "Zupan B."], "Keywords": ["classification", "European social survey", "hierarchical clustering", "multiple responses", "Subgroup discovery", "subgroup scoring"], "Date": "2011", "Abstract": "Most of the present subgroup discovery approaches aim at finding subsets of attribute-value data with unusual distribution of a single output variable. In general, real-life problems may be described with richer, multi-dimensional descriptions of the outcome. The discovery task in such domains is to find subsets of data instances with similar outcome description that are separable from the rest of the instances in the input space. We have developed a technique that directly addresses this problem and uses a combination of agglomerative clustering to find subgroup candidates in the space of output attributes, and predictive modeling to score and describe these candidates in the input attribute space. Experiments with the proposed method on a set of synthetic and on a real social survey data set demonstrate its ability to discover relevant and interesting subgroups from the data with multi-dimensional fesponses. \u00a9 2011 IOS Press and the authors. All rights reserved.", "Language": "en", "Citations": "9"},
{"Title": "Dynamic branch prediction and control speculation", "Authors": ["Silc J.", "Ungerer T.", "Robic B."], "Keywords": ["branch prediction", "high performance", "prototype", "simulation", "superscalar processor"], "Date": "2007", "Abstract": "Branch prediction schemes have become an integral part of today\u2019s superscalar processors. They are one of the key issues in enhancing the performance of processors. Pipeline stalls due to conditional branches are one of the most significant impediments to realise the performance potential of superscalar processors. Many schemes for branch prediction, that can effectively and accurately predict the outcome of branch instructions have been proposed. In this paper, an overview of some dynamic branch prediction schemes for superscalar processors are presented. \u00a9 2007 Inderscience Enterprises Ltd.", "Language": "en", "Citations": "3"},
{"Title": "Distributed rendering of voxelized lidar data Porazdeljeno upodabljanje vokseliziranih podatkov lidar", "Authors": ["Lunar M.", "Bohak C.", "Marolt M."], "Keywords": ["LiDAR", "Orthophoto", "Point cloud", "Ray tracing", "Rendering", "Voxels"], "Date": "2016", "Abstract": "In this paper, a system for rendering voxel (3D pixel)-based worlds is presented. The system enables the generation of such worlds from LiDAR and orthophoto data by discretizing LiDAR data with a resolution of 1 m. In world generation, the embedded basic semantic descriptors f LiDAR data are used, enhanced with additional classes acquired from colour information in geospatially aligned orthophoto images. The system allows local use as well as web-based use and supports multi-core and multi-processor hardware, as well as the clustering of multiple instances for faster rendering. We also present the results (renderings of selected areas) and describe other possible uses of the presented system.", "Language": "en", "Citations": "0"},
{"Title": "Specification and verification of knowledge in a multi-agent system", "Authors": ["Bagic M.", "Ciglaric M."], "Keywords": ["Intelligent agent", "Multi-agent system", "Protocol verification", "Specification"], "Date": "2007", "Abstract": "Verification of epistemic properties, i.e. properties involving knowledge of a multi-agent system (MAS) is a great challenge because there are still many open problems especially with the choice of a proper model for MAS specification and then, with the proper technique to verify the system. In this paper we propose our model for MAS specification as the extended labelled transition system (Milner's Process Algebra), namely Mixed Transition System (MTS). We support our approach with the case study of specification and verification of FIPA Subscribe Scenario, a protocol for inter-agent communication.", "Language": "en", "Citations": "1"},
{"Title": "Fuzzifying the thoughts of animats", "Authors": ["Bajec I.L.", "Zimic N.", "Mraz M."], "Keywords": [], "Date": "2003", "Abstract": "In this article we present a fuzzy logic based method for the construction of thoughts of artificial animals (animats). Due to the substantial increase of the processing power of personal computers in the last decade there was a notable progress in the field of animat construction and simulation. Regardless of the achieved results, the coding of the animat's behaviour is very inaccurate and can, to someone not familiar with common physics variables like speed, acceleration, banking, etc., seem like pure black magic. Our leading hypothesis is, that by using linguistic programming based on common sense, unclear and even partially contradictory knowledge of dynamics, we can achieve comparable, if not better, simulation results. We begin the article with the basics of animats, continue with their fuzzyfication and end with the presentation and comparison of simulation results.", "Language": "en", "Citations": "9"},
{"Title": "Studying agile software estimation techniques: The design of an empirical study with students", "Authors": ["Pozenel M.", "Mahnie V."], "Keywords": ["Agile software development", "Capstone course", "Empirical studies", "Planning poker", "Scrum", "Software engineering education", "Team estimation game"], "Date": "2016", "Abstract": "Empirical studies with students (ESWS) are often viewed sceptically because of questionable external validity. However, if conducted in an adequate way, they can provide at least some preliminary evidence regarding new processes, methods and tools that are constantly proposed for possible use in software development. In order to maximise research and pedagogical value, the requirements that research and pedagogy place on valid ESWSs can be found in the scientific literature. In this article, the design of an ESWS is described that strictly follows a checklist for integrating ESWSs with research and teaching goals. The aim of the study is to compare two software effort estimation techniques: planning poker and the team estimation game. For each checklist item, there is a description of how it is considered in the study design. It is shown that the study can be incorporated into the software engineering capstone course seamlessly, without hindering the achievement of teaching goals. Since there is almost no evidence about the team estimation game in the scientific literature, it is expected that the study will help filling this gap and provide valuable results for researchers and industry.", "Language": "en", "Citations": "3"},
{"Title": "Estimation of individual prediction reliability using sensitivity analysis of regression models", "Authors": ["Bosnic Z."], "Keywords": ["Correction of predictions", "Prediction accuracy", "Prediction error", "Predictions", "Regression", "Sensitivity analysis"], "Date": "2008", "Abstract": "The paper is the extended abstract of dissertation which is concerned with the estimation of reliability for the individual predictions of regression models (in contrast to estimating the accuracy of the whole model) and with the use of sensitivity analysis in that area. The dissertation studies the ways of optimal reliability estimate selection among 9 studied reliability estimates and evaluates the methodology on large number of standard benchmark domain as well as on real domains.", "Language": "en", "Citations": "0"},
{"Title": "Creatinine, Neutrophil Gelatinase-Associated Lipocalin, and Cystatin C in Determining Acute Kidney Injury After Heart Operations Using Cardiopulmonary Bypass", "Authors": ["Kalisnik J.M.", "Hrovat E.", "Hrastovec A.", "Zibert J.", "Jerin A.", "Skitek M.", "Santarpino G.", "Klokocovnik T."], "Keywords": ["Acute kidney injury", "Cardiopulmonary bypass", "Cystatin C", "Neutrophil gelatinase-associated lipocalin"], "Date": "2017", "Abstract": "Acute kidney injury (AKI) represents frequent complication after cardiac surgery using cardiopulmonary bypass (CPB). In the hope to enhance earlier more reliable characterization of AKI, we tested the utility of neutrophil gelatinase-associated lipocalin (NGAL) and cystatin C (CysC) in addition to standard creatinine for early determination of AKI after cardiac surgery using CPB. Forty-one patients met the inclusion criteria. Arterial blood samples collected after induction of general anesthesia were used as baseline, further sampling occurred at CPB termination, 2 h after CPB, on the first and second day after surgery. According to AKIN classification 18 patients (44%) developed AKI (AKI1-2 groups) and 23 (56%) did not (non-AKI group). Groups were similar regarding demographics and operative characteristics. CysC levels differed already preoperatively (non-AKI vs. AKI2; P = 0.045; AKI1 vs. AKI2; P = 0.011), while postoperatively AKI2 group differed on the first day and AKI1 on the second regarding non-AKI group (P = 0.004; P = 0.021, respectively). NGAL and creatinine showed significant difference already 2 h after CPB between groups AKI2 and non-AKI and later on the first postoperative day between groups AKI1 and AKI2 (P = 0.028; P = 0.014, respectively). This study shows similar performance of early plasma creatinine and NGAL in patients with preserved preoperative renal function. It demonstrates that creatinine, as well as NGAL, differentiate subsets of patients developing AKI of clinically more advanced grade early after 2 h, also when used single and uncombined.", "Language": "en", "Citations": "3"},
{"Title": "Automated detection of transient ST-segment episodes in 24h electrocardiograms", "Authors": ["Smrdel A.", "Jager F."], "Keywords": ["Ambulatory ECG", "Automatic transient ST-segment episode detection", "Karhune-Lo\u00e8ve transform", "Non-ischaemic", "Performance evaluation", "ST-segment changes", "Time domain"], "Date": "2004", "Abstract": "A novel automated system is presented for improved detection of transient ischaemic and heart rate-related ST-segment episodes in 'real-world' 24h ambulatory ECG data. Using a combination of traditional time-domain and Karhunen-Lo\u00e8ve transform-based approaches, the detector derives QRS complex and ST-segment morphology feature vectors and, by mimicking human examination of feature-vector time series and their trends, tracks the time-varying ST-segment reference level owing to clinically unimportant, non-ischaemic causes, such as slow drifts, axis shifts and conduction changes. The detector estimates the slowly varying ST-segment level trend, identifies step changes in the time series and subtracts the ST-segment reference level thus obtained from the ST-segment level to obtain the ST-segment deviation time series, which are suitable for detection of ST-segment episodes. The detector was developed using the Long-term ST database containing 24 h ambulatory ECG records with human-expert annotated transient ischaemic and heart rate-related ST-segment episodes. The average ST episode detection sensitivity/positive predictivity obtained when using the annotations of the annotation protocol B of the database were 78.9%/80.7%. Evaluation of the detector using the European Society of Cardiology ST-T database as a test database showed average ST episode detection sensitivity/positive predictivity of 81.3%/89.2%, which are better performances, comparable with those of the systems being developed using the European database. \u00a9 IFMBE: 2004.", "Language": "en", "Citations": "40"},
{"Title": "Supplementary services in telecommunication next generation networks", "Authors": ["Aljaz T.", "Brodnik A."], "Keywords": [], "Date": "2004", "Abstract": "Over the next few years the Public-Switched Telephone Network (PSTN) will evolve into Next-Generation Networks (NGNs), in which data and voice will share a common packet-switched network. The NGN introduces new concepts and networking protocols for the 'everything over IP' strategy. The Softswitch, the Media Gateway and the Signaling Gateway are the basic elements of the current NGN architecture. In this paper we will show that these elements do not meet the requirements of some SS7 ISUP supplementary services that are widely used in telecommunication networks. The major drawbacks of the current NGN implementations that are based on those services will be investigated. To overcome these drawbacks we propose a new element in the NGN architecture, called Multi-Service Mediator (MSM) and describe an interworking scenario with other currently defined NGN elements, especially with the Softswitch, We investigate requirenments for controling and management of MSM in NGN. Additionaly we propose building block for simulation of NGN architecture, including MSM.", "Language": "en", "Citations": "1"},
{"Title": "How to exploit multiprocessing features of the SGI Origin 200", "Authors": ["Bulic P.", "Gustin V."], "Keywords": [], "Date": "2000", "Abstract": "In this paper we present some approaches to writing parallel codes on the SGI Origin 200 multiprocessor. In practice, the largest piece of the code-parallelization process is still left to the programmer, with some help from the compiler and the operating system. With the SGI Origin system, in particular, the programmer has to find the pieces of code which can be done in parallel, and the synchronization is also the responsibility of the programmer. The problem we focused on in writing parallel codes, is the synchronization of multiprocessor access to the same memory location. In this paper, some principles of synchronization are described and analyzed. We show how the execution time depends on each of these approaches. The results of testing the microbenchmark indicate that synchronization performs very poorly under the heavy traffic. We show, that in such cases it is better to leave our code to execute in serial on a single processor.", "Language": "en", "Citations": "0"},
{"Title": "On diameter of the commuting graph of a full matrix algebra over a finite field", "Authors": ["Dolzan D.", "Kokol Bukovsek D.", "Kuzma B.", "Oblak P."], "Keywords": ["Commuting graph", "Finite field", "Matrix algebra"], "Date": "2016", "Abstract": "It is shown that the commuting graph of a matrix algebra over a finite field has diameter at most five if the size of the matrices is not a prime nor a square of a prime. It is further shown that the commuting graph of even-sized matrices over finite field has diameter exactly four. This partially proves a conjecture stated by Akbari, Mohammadian, Radjavi, and Raja [Linear Algebra Appl. 418 (2006) 161-176].", "Language": "en", "Citations": "6"},
{"Title": "Assessing software complexity from UML using fractal complexity measure", "Authors": ["Podgorelec V.", "Hericko M.", "Juric M.B."], "Keywords": [], "Date": "2004", "Abstract": "The possibility of predicting complexity of a software system early in the development process would be of great benefit for estimating complexity and associated effort of the implemented system. In the paper we present an experiment to predict the complexity of a software system using U ML model. Based on the previous work on a metrics, where it has been shown that the fractal complexity measure a can be used to assess the complexity of a software system, and on the proposition that a is somehow related to the amount of intellectual energy built into the software artifacts, we tested a hypothesis that by measuring the complexity of UML model of a software system the complexity of the system's implementation can be foreseen. \u00a9 2004 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Learning long-term chess strategies from databases", "Authors": ["Sadikov A.", "Bratko I."], "Keywords": ["Chess da-ta-ba-ses", "Chess endgames", "Computer chess", "Long-term Strategy", "Machine learning"], "Date": "2006", "Abstract": "We propose an approach to the learning of long-term plans for playing chess endgames. We assume that a computer-generated database for an endgame is available, such as the king and rook vs. king, or king and queen vs. king and rook endgame. For each position in the endgame, the database gives the \"value\" of the position in terms of the minimum number of moves needed by the stronger side to win given that both sides play optimally. We propose a method for automatically dividing the endgame into stages characterised by different objectives of play. For each stage of such a game plan, a stage-specific evaluation function is induced, to be used by minimax search when playing the endgame. We aim at learning playing strategies that give good insight into the principles of playing specific endgames. Games played by these strategies should resemble human expert's play in achieving goals and subgoals reliably, but not necessarily as quickly as possible.", "Language": "en", "Citations": "4"},
{"Title": "Mobility-aware cross-layer routing for peer-to-peer networks", "Authors": ["Deokate B.", "Lal C.", "Trcek D.", "Conti M."], "Keywords": ["Cross-layer routing", "Dynamic topology", "MANET", "OMNET++", "Overlay networks", "Route lifetime", "Simulations"], "Date": "2019", "Abstract": "In remote locations, the best way to establish a communication infrastructure is to deploy Peer-to-Peer (P2P) network over Mobile Ad-hoc Network (MANET). However, the node mobility and efficient cross-layer communication between the overlay and MANET routing protocols remains a challenge for reliable data transmission. In this paper, we propose a novel Mobility Aware Cross-layer Routing approach for Peer-to-Peer Networks (MACARON) over MANET. MACARON provides reliable communication and high lifetime routes to support efficient data transmission in the network. For this purpose, it uses cross-layer communications to share routing updates between MANET and overlay routing protocols. MACARON provides guaranteed routes with low path stretch by maintaining O\u02dc(n) routing entries per node, where n is the number of nodes in the network. It provides scalability without using any landmark directory to store routing state, and it can effectively handle moderate mobility by using Last Encounter Routing (LER) protocol.", "Language": "en", "Citations": "0"},
{"Title": "Unfolding communities in large complex networks: Combining defensive and offensive label propagation for core extraction", "Authors": ["Subelj L.", "Bajec M."], "Keywords": [], "Date": "2011", "Abstract": "Label propagation has proven to be a fast method for detecting communities in large complex networks. Recent developments have also improved the accuracy of the approach; however, a general algorithm is still an open issue. We present an advanced label propagation algorithm that combines two unique strategies of community formation, namely, defensive preservation and offensive expansion of communities. The two strategies are combined in a hierarchical manner to recursively extract the core of the network and to identify whisker communities. The algorithm was evaluated on two classes of benchmark networks with planted partition and on 23 real-world networks ranging from networks with tens of nodes to networks with several tens of millions of edges. It is shown to be comparable to the current state-of-the-art community detection algorithms and superior to all previous label propagation algorithms, with comparable time complexity. In particular, analysis on real-world networks has proven that the algorithm has almost linear complexity, O(m", "Language": "en", "Citations": "79"},
{"Title": "Impact of subthalamic deep brain stimulation frequency on upper limb motor function in Parkinson's disease", "Authors": ["Momin S.", "Mahlknecht P.", "Georgiev D.", "Foltynie T.", "Zrinzo L.", "Hariz M.", "Zacharia A.", "Limousin P."], "Keywords": ["Bradykinesia", "brain stimulation", "d.", "deep", "Parkinson's disease", "rigidity", "subthalamic nucleus", "tremor"], "Date": "2018", "Abstract": "Background: Whilst changes in the frequency of subthalamic deep brain stimulation (STN-DBS) have been proposed to improve control of tremor or axial motor features in Parkinson's disease (PD), little is known about the effects of frequency changes on upper limb motor function, particularly bradykinesia. Objective: To investigate the acute effects of various STN-DBS frequencies (40-160 Hz, 40 Hz intervals) on upper limb motor function. Methods: We carried out a randomised, double-blind study on 20 PD patients with chronic STN-DBS using the Simple and Assembly components of the Purdue Pegboard (PP) test and a modified upper limb version of the UPDRS-III (UL-UPDRS-III). Results: There was no significant effect of frequency on bradykinesia on the Simple PP task or the UL-UPDRS-III. There was an effect of frequency on the Assembly PP score when comparing all frequencies (p = 0.019) and between 80 Hz and 130 Hz (p = 0.007), with lower frequencies yielding a better performance. Rigidity and Tremor scores were significantly reduced with higher (80 Hz) compared to lower (40 Hz) frequencies. Conclusions: Our findings suggest that a wide range of frequencies are efficacious in improving acute upper-limb motor function. Reducing the frequency of stimulation down to 80 Hz is safe and has a similar clinical effect to higher frequencies. Therefore, a wider range of frequencies are available when it comes adjusting patients' acute settings without the risk of worsening bradykinesia.", "Language": "en", "Citations": "2"},
{"Title": "Qualitative and quantitative analysis and comparison of Java distributed architectures", "Authors": ["Rozman I.", "Juric M.B.", "Golob I.", "Hericko M."], "Keywords": ["Java", "Performance", "RMI", "Web services"], "Date": "2006", "Abstract": "In this article we have undertaken a qualitative and quantitative comparison of common approaches used to develop distributed solutions in Java: RMI and Web services for regular unsecured communication, RMI-SSL and WS-Security for secure communication and authentication, and HTTP-to-port and HTTP-to-CGI/servlet tunnelling for RMI communication through firewalls and proxies. We have performed a functional comparison that helps with the selection of the most appropriate approach. We have also carried out a detailed performance analysis with the identification of major bottlenecks, identification of design and implementation guidelines for distributed applications, and specification of optimizations for distributed middleware. This article contributes to the understanding of different approaches for developing Java distributed applications, provides detailed performance analysis, presents design and implementation guidelines, and identifies the major performance overheads. Copyright \u00a9 2006 John Wiley & Sons, Ltd.", "Language": "en", "Citations": "1"},
{"Title": "A framework for reengineering software development methods", "Authors": ["Bajee M.", "Rupnik R.", "Krisper M."], "Keywords": ["Method engineering", "Software development methods", "Software process improvement"], "Date": "2006", "Abstract": "Empirical investigations clearly show that in contrast to the theory which implies that the use of methods in software development is beneficial, their use in practice is actually very low. Several reasons have been identified recently as contributing to this rather paradoxical situation. Most importantly: the sociotechnical inappropriateness of a prescribed method to the company's actual ways of working, and the rigidity, i.e. inability of a prescribed method to be adapted to a particular problem. In this paper, we present a framework that deals with these issues. Its purpose is to help software development companies to reengineer their software development processes by establishing formalized and flexible methods based on the actual ways of working in that companies. The framework has been developed in cooperation with software development companies. \u00a9 2006 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Gene Prioritization by Compressive Data Fusion and Chaining", "Authors": ["Zitnik M.", "Nam E.A.", "Dinh C.", "Kuspa A.", "Shaulsky G.", "Zupan B."], "Keywords": [], "Date": "2015", "Abstract": "Data integration procedures combine heterogeneous data sets into predictive models, but they are limited to data explicitly related to the target object type, such as genes. Collage is a new data fusion approach to gene prioritization. It considers data sets of various association levels with the prediction task, utilizes collective matrix factorization to compress the data, and chaining to relate different object types contained in a data compendium. Collage prioritizes genes based on their similarity to several seed genes. We tested Collage by prioritizing bacterial response genes in Dictyostelium as a novel model system for prokaryote-eukaryote interactions. Using 4 seed genes and 14 data sets, only one of which was directly related to the bacterial response, Collage proposed 8 candidate genes that were readily validated as necessary for the response of Dictyostelium to Gram-negative bacteria. These findings establish Collage as a method for inferring biological knowledge from the integration of heterogeneous and coarsely related data sets.", "Language": "en", "Citations": "7"},
{"Title": "The impact of audio segmentation to speaker tracking in broadcast news data", "Authors": ["Zibert J."], "Keywords": ["Audio indexing", "Audio segmentation", "Speaker clustering", "Speaker tracking"], "Date": "2008", "Abstract": "A system for speaker tracking in broadcast-news audio data is presented. The process of speaker tracking in continuous audio streams involves several processing tasks and is therefore treated as a multistage process. The main building blocks of such system include the components for audio segmentation, speech detection, speaker clustering and speaker identification. Our system was developed by implementing the most recent published methods in each component of the system, whereas we focused mainly on the component for audio segmentation for being considered as one of the most critical components of such systems. Two alternative approaches of speaker change detection and speaker clustering are explored and their impacts to the overall speaker-tracking performance are evaluated. The evaluation experiments were performed on broadcast-news audio data with a speaker-tracking system capable of detecting 41 target speakers. The comparison of the evaluation results of different versions of the speaker-tracking system indicates the importance of the tasks in the audio-segmentation module and provides valuable insights into how the system works.", "Language": "en", "Citations": "0"},
{"Title": "Open-source robotic manipulator and sensory platform", "Authors": ["Cehovin Zajc L.", "Rezelj A.", "Skocaj D."], "Keywords": ["Augmented reality", "Computer vision", "Education", "Open-hardware", "Open-source", "Robotic manipulator"], "Date": "2018", "Abstract": "We present an open-source robotic platform for educational use that integrates multiple levels of interaction through the use of additional vision sensor. The environment can be used in virtual, augmented-reality and real-robot modes, enabling smooth transition from a virtual robot manipulator to a real one. We describe the main aspects of our platform that ensure low production costs and encourage openness of both its hardware and software. The main goal of our work was to create a viable low-cost robotic manipulator platform alternative for the university level courses in intelligent robotics, however, the application domain is very broad.", "Language": "en", "Citations": "1"},
{"Title": "A feature selection method based on feature correlation networks", "Authors": ["Savic M.", "Kurbalija V.", "Ivanovic M.", "Bosnic Z."], "Keywords": ["Alzheimer\u2019s disease", "Community detection", "Feature correlation networks", "Feature selection"], "Date": "2017", "Abstract": "Feature selection is an important data preprocessing step in data mining and machine learning tasks, especially in the case of high dimensional data. In this paper we present a novel feature selection method based on complex weighted networks describing the strongest correlations among features. The method relies on community detection techniques to identify cohesive groups of features. A subset of features exhibiting a strong association with the class feature is selected from each identified community of features taking into account the size of and connections within the community. The proposed method is evaluated on a high dimensional dataset containing signaling protein features related to the diagnosis of Alzheimer\u2019s disease. We compared the performance of seven widely used classifiers that were trained without feature selection, with correlation-based feature selection by a state-of-the-art method provided by the WEKA tool, and with feature selection by four variants of our method determined by four different community detection techniques. The results of the evaluation indicate that our method improves the classification accuracy of several classification models while drastically reducing the dimensionality of the dataset. Additionally, one variant of our method outperforms the correlation-based feature selection method implemented in WEKA.", "Language": "en", "Citations": "3"},
{"Title": "The electrophysiological correlates of the working memory subcomponents: evidence from high-density EEG and coherence analysis", "Authors": ["Rutar Gorisek V.", "Belic A.", "Manouilidou C.", "Koritnik B.", "Repovs G.", "Bon J.", "Zibert J.", "Zidar J."], "Keywords": ["Dorsolateral prefrontal cortex", "Electroencephalographic coherence", "Episodic buffer", "Executive function", "Theta synchronization", "Working memory"], "Date": "2015", "Abstract": "Synchronization between prefrontal (executive) and posterior (association) cortices seems a plausible mechanism for temporary maintenance of information. However, while EEG studies reported involvement of (pre)frontal midline structures in synchronization, functional neuroimaging elucidated the importance of lateral prefrontal cortex (PFC) in working memory (WM). Verbal and spatial WM rely on lateralized subsystems (phonological loop and visuospatial sketchpad, respectively), yet only trends for hemispheric dissociation of networks supporting rehearsal of verbal and spatial information were identified by EEG. As oscillatory activity is WM load dependent, we applied an individually tailored submaximal load for verbal (V) and spatial (S) task to enhance synchronization in the relevant functional networks. To map these networks, we used high-density EEG and coherence analysis. Our results imply that the synchronized activity is limited to highly specialized areas that correspond well with the areas identified by functional neuroimaging. In both V and S task, two independent networks of theta synchronization involving dorsolateral PFC of each hemisphere were revealed.\u00a0In V task, left prefrontal and left parietal areas were functionally coupled in gamma frequencies. Theta synchronization thus provides the necessary interface for storage and manipulation of information, while left-lateralized gamma synchronization could represent the EEG correlate of the phonological loop.", "Language": "en", "Citations": "2"},
{"Title": "Analysis and comparison of web services security standards", "Authors": ["Laznik J.", "Juric M.B.", "Hericko M."], "Keywords": ["Security standards", "SOAP", "Web services", "XML"], "Date": "2005", "Abstract": "Web services are one of the most important emerging technologies these days. Many of the analysis predict that web services will reach its peak in 2005. One of the most important areas that need to be covered is security, since web services are intended to be used in open and usually very hostile environments. In this article we will show current situation in security area. We will describe the most important security standards that are used today and their alternatives. We will describe digital signature, encryption, SOAP-Sec, WS-Security, SAML, XACML and XKMS. These standards do not provide full security. They are only one of the building blocks that makes web services more secure to use. The security area lately evolved so much, that web service security standards can help other technologies or as we can say that web services can work with other technologies in synergy.", "Language": "en", "Citations": "0"},
{"Title": "The evaluation of Qualitative Assessment Dynamics (QAD) methodology for managing trust in pervasive computing environments", "Authors": ["Zupancic E.", "Trcek D."], "Keywords": ["human behavior modeling", "pervasive computing", "Qualitative Assessment Dynamics", "trust management"], "Date": "2011", "Abstract": "Environment of pervasive computing fosters interactions between people and devices. The effective collaboration among existing entities cannot exist without trust. For this reason, computationally supported trust management systems are necessary part of such environment. In this paper, we evaluated the Qualitative Assessment Dynamics (QAD) methodology for managing trust, based on simulations. We proposed different scenarios with service requesters and service providers, where some of them provide unsatisfying services. Further, we performed two set of experiments. The first set of experiments demonstrates the effect of introducing trust management system into environment. QAD methodology defines different operators that model an agent's behavior. The second set of experiments shows the effects of QAD operators. We analyzed the behavior of agents modeled with different operators and compared the decisions they made. \u00a9 2011 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "Advanced detection of ST segment episodes in 24-hour ambulatory ECG data by automated tracking of transient ST segment reference level", "Authors": ["Smrdel A.", "Jager F."], "Keywords": [], "Date": "2002", "Abstract": "Using the Long-Term ST Database, we developed and evaluated an advanced algorithm for automated detection of transient ST segment episodes in \"real-world\" 24-hour ambulatory data. To successfully detect transient ST change episodes, the algorithm automatically tracks the time-varying ST segment reference level due to clinically not important non-ischemic causes and subtracts it from the ST segment level. Evaluating of the algorithm using reference annotations of the protocol B of the database yielded gross ST episode detection sensitivity and positive predictivity of approximately 75%.", "Language": "en", "Citations": "4"},
{"Title": "Solving the logistic problems with optimal resource assignment using fuzzy logic methods", "Authors": ["Moskon M.", "Novak S.", "Medeot M.", "Bajec I.L.", "Zimic N.", "Mraz M."], "Keywords": ["fuzzy assignment", "fuzzy logic", "Hungarian algorithm", "logistics"], "Date": "2013", "Abstract": "Fuzzy approach of optimal resource assignment regarding the given demands in the scope of transportation will be presented in the article. The basis of the research is the crisp solution which is also presented in the article. The basic solution was upgraded in order to be able to handle vaguely (i.e. fuzzily) defined requirements and resource properties. The optimal resource configuration is calculated with the aid of Hungarian algorithm which uses the data calculated with the fuzzy methods for its inputs. The approach described is presented on the example of a military convoy formation. Copyright \u00a9 2011 John Wiley & Sons, Ltd.", "Language": "en", "Citations": "0"},
{"Title": "Use of qualitative constraints in modelling of the Lake Glums\u00f8", "Authors": ["Vladusic D.", "Kompare B.", "Bratko I."], "Keywords": ["Lake Glums\u00f8", "Machine learning", "Numerical prediction", "Qualitative reasoning"], "Date": "2007", "Abstract": "This paper describes modelling of time behaviour of phytoplankton and zooplankton in the Danish lake Glums\u00f8 with a recently developed approach to machine learning in numerical domains, called Q", "Language": "en", "Citations": "0"},
{"Title": "Statistical comparisons of classifiers over multiple data sets", "Authors": ["Demsar J."], "Keywords": ["Comparative studies", "Friedman test", "Multiple comparisons tests", "Statistical methods", "Wilcoxon signed ranks test"], "Date": "2006", "Abstract": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.", "Language": "en", "Citations": "5271"},
{"Title": "Panoramic depth imaging: Single standard camera approach", "Authors": ["Peer P.", "Solina F."], "Keywords": ["Mosaicing", "Panoramic image", "Reconstruction", "Stereo vision"], "Date": "2002", "Abstract": "In this paper we present a panoramic depth imaging system. The system is mosaic-based which means that we use a single rotating camera and assemble the captured images in a mosaic. Due to a setoff of the camera's optical center from the rotational center of the system we are able to capture the motion parallax effect which enables stereo reconstruction. The camera is rotating on a circular path with a step defined by the angle, equivalent to one pixel column of the captured image. The equation for depth estimation can be easily extracted from the system geometry. To find the corresponding points on a stereo pair of panoramic images the epipolar geometry needs to be determined. It can be shown that the epipolar geometry is very simple if we are doing the reconstruction based on a symmetric pair of stereo panoramic images. We get a symmetric pair of stereo panoramic images when we take symmetric pixel columns on the left and on the right side from the captured image center column. Epipolar lines of the symmetrical pair of panoramic images are image rows. The search space on the epipolar line can be additionaly constrained. The focus of the paper is mainly on the system analysis. Results of the stereo reconstruction procedure and quality evaluation of generated depth images are quite promissing. The system performs well for reconstruction of small indoor spaces. Our finall goal is to develop a system for automatic navigation of a mobile robot in a room.", "Language": "en", "Citations": "19"},
{"Title": "H-spaces, semiperfect rings and self-homotopy equivalences", "Authors": ["Franetic D.", "Pavesic P."], "Keywords": [], "Date": "2011", "Abstract": "We use the theory of semiperfect rings to derive decomposition theorems for H- and co-H-spaces, generalizing the results of Wilkerson. The results are then used to prove reducibility of self-homotopy equivalences for arbitrary p-local H- and co-H-spaces. \u00a9 Copyright Royal Society of Edinburgh 2011.", "Language": "en", "Citations": "3"},
{"Title": "Improved error recovery in generated LR parsers", "Authors": ["Slivnik B.", "Vilfan B."], "Keywords": ["Error recovery and reporting", "LR parsing"], "Date": "2004", "Abstract": "A new method for error recovery in LR parsers is described. An error recovery routine based on this new method can be generated automatically by a parser generator as a part of an LR parser. Based on the result that a viable suffix from which the unread part of the input is derived can be computed in certain states of an LR parser, the new method uses the viable suffix to discard the erroneous part of the input and to synchronize the parser stack with the rest of the input afterwards. Thus it resembles a simple but efficient error recovery method used by LL and other predictive parsers. It is proved that all states suitable for this kind of error recovery can automatically be identified by a parser generator.", "Language": "en", "Citations": "2"},
{"Title": "proDEX - A DSS tool for environmental decision-making", "Authors": ["Znidarsic M.", "Bohanec M.", "Zupan B."], "Keywords": ["Agricultural sciences", "Environmental science", "Knowledge and agent technology", "Knowledge engineering"], "Date": "2006", "Abstract": "Environmental concepts are becoming common and ever more important parts of decision support models, which are a vital part of decision support systems. proDEX is a software tool for use of decision support models that are based on extended DEX methodology for qualitative multi-criteria decision modelling and support. The supported modelling methodology and the software features are adjusted to environmental modelling needs. \u00a9 2006 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "18"},
{"Title": "A comparison of various linear and non-linear signal processing techniques to separate uterine EMG records of term and pre-term delivery groups", "Authors": ["Fele-Zorz G.", "Kavsek G.", "Novak-Antolic Z.", "Jager F."], "Keywords": ["Electrohysterogram", "Linear signal processing techniques", "Non-linear signal processing techniques", "Pre-term delivery prediction", "Uterine EMG"], "Date": "2008", "Abstract": "Various linear and non-linear signal-processing techniques were applied to three-channel uterine EMG records to separate term and pre-term deliveries. The linear techniques were root mean square value, peak and median frequency of the signal power spectrum and autocorrelation zero crossing; while the selected non-linear techniques were estimation of the maximal Lyapunov exponent, correlation dimension and calculating sample entropy. In total, 300 records were grouped into four groups according to the time of recording (before or after the 26th week of gestation) and according to the total length of gestation (term delivery records - pregnancy duration \u226537 weeks and pre-term delivery records - pregnancy duration <37 weeks). The following preprocessing band-pass Butterworth filters were tested: 0.08-4, 0.3-4, and 0.3-3 Hz. With the 0.3-3 Hz filter, the median frequency indicated a statistical difference between those term and pre-term delivery records recorded before the 26th week (p = 0.03), and between all term and all pre-term delivery records (p = 0.012). With the same filter, the sample entropy indicated statistical differences between those term and pre-term delivery records recorded before the 26th week (p = 0.035), and between all term and all pre-term delivery records (p = 0.011). Both techniques also showed noticeable differences between term delivery records recorded before and after the 26th week (p \u2264 0.001). \u00a9 International Federation for Medical and Biological Engineering 2008.", "Language": "en", "Citations": "100"},
{"Title": "Matrix factorization-based data fusion for gene function prediction in baker's yeast and slime mold", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": ["Data fusion", "Gene function prediction", "Gene ontology annotation", "Matrix factorization", "Membrane protein", "Ribosomal protein"], "Date": "2014", "Abstract": "The development of effective methods for the characterization of gene functions that are able to combine diverse data sources in a sound and easily-extendible way is an important goal in computational biology. We have previously developed a general matrix factorization-based data fusion approach for gene function prediction. In this manuscript, we show that this data fusion approach can be applied to gene function prediction and that it can fuse various heterogeneous data sources, such as gene expression profiles, known protein annotations, interaction and literature data. The fusion is achieved by simultaneous matrix tri-factorization that shares matrix factors between sources. We demonstrate the effectiveness of the approach by evaluating its performance on predicting ontological annotations in slime mold D. discoideum and on recognizing proteins of baker's yeast S. cerevisiae that participate in the ribosome or are located in the cell membrane. Our approach achieves predictive performance comparable to that of the state-of-the-art kernel-based data fusion, but requires fewer data preprocessing steps.", "Language": "en", "Citations": "13"},
{"Title": "A web-service for object detection using hierarchical models", "Authors": ["Tabernik D.", "Cehovin L.", "Kristan M.", "Boben M.", "Leonardis A."], "Keywords": [], "Date": "2013", "Abstract": "This paper proposes an architecture for an object detection system suitable for a web-service running distributed on a cluster of machines. We build on top of a recently proposed architecture for distributed visual recognition system and extend it with the object detection algorithm. As sliding-window techniques are computationally unsuitable for web-services we rely on models based on state-of-the-art hierarchical compositions for the object detection algorithm. We provide implementation details for running hierarchical models on top of a distributed platform and propose an additional hypothesis verification step to reduce many false-positives that are common in hierarchical models. For a verification we rely on a state-of-the-art descriptor extracted from the hierarchical structure and use a support vector machine for object classification. We evaluate the system on a cluster of 80 workers and show a response time of around 10 seconds at throughput of around 60 requests per minute. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "Evaluation of technologies for business process automation", "Authors": ["Pusnik M.", "Juric M.B.", "Rozman I."], "Keywords": ["Business process automation technologies", "Decision models", "ebXML", "RosettaNet", "XLANG"], "Date": "2002", "Abstract": "The importance of process automation for B2B (business to business) collaboration is rising. The efforts are directed towards automating business processes and forming a global electronic market. In this paper we present and evaluate the three most important technologies for business process automation: ebXML (Electronic Business XML-eXtensible Markup Language), RosettaNet and XLANG. They differ in terms of features, quality and serviceability. We analyze, compare and evaluate those technologies from the perspective of SME (small and medium enterprises). Based on the comparison we define a multi-criteria decision model with twenty parameters and the corresponding weights, we evaluate the alternatives and define a utility function, which helps us to select the most technology. The contributions of this paper are the in-depth evaluation of technologies and the definition of a multi-criteria decision model.", "Language": "en", "Citations": "3"},
{"Title": "Incremental PCA for on-line visual learning and recognition", "Authors": ["Artac M.", "Jogan M.", "Leonardis A."], "Keywords": [], "Date": "2002", "Abstract": "The methods for visual learning that compute a space of eigenvectors by Principal Component Analysis (PCA) traditionally require a batch computation step. Since this leads to potential problems when dealing with large sets of images, several incremental methods for the computation of the eigenvectors have been introduced. However, such learning cannot be considered as an on-line process, since all the images are retained until the final step of computation of space of eigenvectors, when their coefficients in this subspace are computed. In this paper we propose a method that allows for simultaneous learning and recognition. We show that we can keep only the coefficients of the learned images and discard the actual images and still are able to build a model of appearance that is fast to compute and open-ended. We performed extensive experimental testing which showed that the recognition rate and reconstruction accuracy are comparable to those obtained by the batch method. \u00a9 2002 IEEE.", "Language": "en", "Citations": "97"},
{"Title": "Combinatorial algorithm for counting small induced graphs and orbits", "Authors": ["Hocevar T.", "Demsar J."], "Keywords": [], "Date": "2017", "Abstract": "Graphlet analysis is an approach to network analysis that is particularly popular in bioinformatics. We show how to set up a system of linear equations that relate the orbit counts and can be used in an algorithm that is significantly faster than the existing approaches based on direct enumeration of graphlets. The approach presented in this paper presents a generalization of the currently fastest method for counting 5-node graphlets in bioinformatics. The algorithm requires existence of a vertex with certain properties; we show that such vertex exists for graphlets of arbitrary size, except for complete graphs and a cycle with four nodes, which are treated separately. Empirical analysis of running time agrees with the theoretical results.", "Language": "en", "Citations": "1"},
{"Title": "Tight Distance-Regular Graphs", "Authors": ["Jurisic A.", "Koolen J.", "Terwilliger P."], "Keywords": ["Distance-regular graph", "Equality", "Homogeneous", "Locally strongly-regular parameterization", "Tight graph"], "Date": "2000", "Abstract": "We consider a distance-regular graph \u0393 with diameter d \u2265 3 and eigenvalues k = \u03b8", "Language": "en", "Citations": "44"},
{"Title": "Contractible subgraphs, Thomassen's conjecture and the dominating cycle conjecture for snarks", "Authors": ["Broersma H.", "Fijavz G.", "Kaiser T.", "Kuzel R.", "Ryjacek Z.", "Vrana P."], "Keywords": ["Contractible graph", "Cubic graph", "Dominating cycle", "Hamiltonian graph", "Line graph", "Snark"], "Date": "2008", "Abstract": "We show that the conjectures by Matthews and Sumner (every 4-connected claw-free graph is Hamiltonian), by Thomassen (every 4-connected line graph is Hamiltonian) and by Fleischner (every cyclically 4-edge-connected cubic graph has either a 3-edge-coloring or a dominating cycle), which are known to be equivalent, are equivalent to the statement that every snark (i.e.\u00a0a cyclically 4-edge-connected cubic graph of girth at least five that is not 3-edge-colorable) has a dominating cycle. We use a refinement of the contractibility technique which was introduced by Ryj\u00e1\u010dek and Schelp in 2003 as a common generalization and strengthening of the reduction techniques by Catlin and Veldman and of the closure concept introduced by Ryj\u00e1\u010dek in 1997. \u00a9 2007 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "10"},
{"Title": "Uterine electromyography during active phase compared with latent phase of labor at term", "Authors": ["Trojner Bregar A.", "Lucovnik M.", "Verdenik I.", "Jager F.", "Gersak K.", "Garfield R.E."], "Keywords": ["active labor", "electrohysterography", "latent labor", "power density spectrum", "Uterine electromyography"], "Date": "2016", "Abstract": "\n                            Introduction In a prospective study in a tertiary university hospital we wanted to determine whether uterine electromyography (EMG) can differentiate between the active and latent phase of labor. Material and methods Thirty women presenting at \u226537\n                            ", "Language": "en", "Citations": "5"},
{"Title": "Improvement of the direct-marketing business process by using data mining", "Authors": ["Rupnik R."], "Keywords": ["Data mining", "Direct marketing", "Methodology"], "Date": "2013", "Abstract": "The direct-marketing business process applied in Slovenian publishing company was inefficient because of inadequate procedure used to create a list of potential customers for a marketing campaign. Considering the nature of the problem, data mining was selected to solve the problem. The company's direct marketing business process was renovated based on the CRISP-DM methodology and by using data mining for decision-making elements of the process. The paper represents a methodology enabling implementation of data mining into the business processes and its use in a direct marketing business process for a Slovenian publishing company.", "Language": "en", "Citations": "0"},
{"Title": "An approximate logarithmic squaring circuit with error compensation for DSP applications", "Authors": ["Avramovic A.", "Babic Z.", "Raic D.", "Strle D.", "Bulic P."], "Keywords": ["Approximate squaring", "Arithmetic circuits", "Computer arithmetic", "Digital signal processing", "Logarithm approximation", "Logic synthesis"], "Date": "2014", "Abstract": "The squaring function is one of the frequently used arithmetic functions in DSP, so an approximation of the squaring function is acceptable as long as this approximation corrupts the bits that are already corrupted by noise, and does not degrade application\u00d7\u00a92014 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "4"},
{"Title": "Autonomous discovery of abstract concepts by a robot", "Authors": ["Bratko I."], "Keywords": ["autonomous discovery", "discovery of abstract concepts", "inductive logic programming", "predicate invention", "robot learning"], "Date": "2011", "Abstract": "In this paper we look at the discovery of abstract concepts by a robot autonomously exploring its environment and learning the laws of the environment. By abstract concepts we mean concepts that are not explicitly observable in the measured data, such as the notions of obstacle, stability or a tool. We consider mechanisms of machine learning that enable the discovery of abstract concepts. Such mechanisms are provided by the logic based approach to machine learning called Inductive Logic Programming (ILP). The feature of predicate invention in ILP is particularly relevant. Examples of actually discovered abstract concepts in experiments are described. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "Selecting features for object detection using an AdaBoost-compatible evaluation function", "Authors": ["Furst L.", "Fidler S.", "Leonardis A."], "Keywords": ["AdaBoost", "Feature selection", "Object detection"], "Date": "2008", "Abstract": "This paper addresses the problem of selecting features in a visual object detection setup where a detection algorithm is applied to an input image represented by a set of features. The set of features to be employed in the test stage is prepared in two training-stage steps. In the first step, a feature extraction algorithm produces a (possibly large) initial set of features. In the second step, on which this paper focuses, the initial set is reduced using a selection procedure. The proposed selection procedure is based on a novel evaluation function that measures the utility of individual features for a certain detection task. Owing to its design, the evaluation function can be seamlessly embedded into an AdaBoost selection framework. The developed selection procedure is integrated with state-of-the-art feature extraction and object detection methods. The presented system was tested on five challenging detection setups. In three of them, a fairly high detection accuracy was effected by as few as six features selected out of several hundred initial candidates. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "16"},
{"Title": "Gender and affect recognition based on GMM and GMM-UBM modeling with relevance MAP estimation", "Authors": ["Gajsek R.", "Zibert J.", "Justin T.", "Struc V.", "Vesnicer B.", "Mihelic F."], "Keywords": ["Affect recognition", "Emotion recognition", "Gender recognition", "GMM-UBM", "MAP"], "Date": "2010", "Abstract": "The paper presents our efforts in the Gender Sub-Challenge and the Affect Sub-Challenge of the INTERSPEECH 2010 Paralin-guistic Challenge. The system for the Gender Sub-Challenge is based on modeling the Mel-Frequency Cepstrum Coefficients using Gaussian mixture models, building a separate model for each of the gender categories. For the Affect Sub-Challenge we propose a modeling schema where a universal background model is first trained an all the training data and then, employing the maximum a posteriori estimation criteria, a new feature vector of means is produced for each particular sample. The feature set used is comprised of low level descriptors from the baseline system, which in our case are split into four subsets, and modeled by its own model. Predictions from all subsystems are fused using the sum rule fusion. Aside from the baseline regression procedure, we also evaluated the Support Vector Regression and compared the performance. Both systems achieve higher recognition results on the development set compared to baseline, but in the Affect Sub-Challenge our system's cross correlation is lower than that of the baseline system, although the mean linear error is slightly superior. In the Gender Sub-Challenge the unweighted average recall on the test set is 82.84%, and for the Affect Sub-Challenge the cross-correlation on the test set is 0.39 with mean linear error of 0.143. \u00a9 2010 ISCA.", "Language": "en", "Citations": "15"},
{"Title": "On the maximum number of cliques in a graph embedded in a surface", "Authors": ["Dujmovic V.", "Fijavz G.", "Joret G.", "Sulanke T.", "Wood D.R."], "Keywords": [], "Date": "2011", "Abstract": "This paper studies the following question: given a surface \u03c3 and an integer n, what is the maximum number of cliques in an n-vertex graph embeddable in \u03c3? We characterise the extremal graphs for this question, and prove that the answer is between 8(n-\u03c9)+2", "Language": "en", "Citations": "10"},
{"Title": "Computational complexity and parallelization of the meshless local Petrov-Galerkin method", "Authors": ["Trobec R.", "Sterk M.", "Robic B."], "Keywords": ["Computational complexity", "FDM", "FEM", "Meshless methods", "MLPG", "Parallel algorithms"], "Date": "2009", "Abstract": "The computational complexity of the meshless local Petrov-Galerkin method (MLPG) has been analyzed and compared with the finite difference (FDM) and finite element methods (FEM) from the user point of view. Theoretically, MLPG is the most complex of the three methods. Experimental results show that MLPG, with appropriately selected integration order and dimensions of support and quadrature domains, achieves similar accuracy to that of FEM. The measurements of parallel complexity and speed-up indicate that parallel MLPG scales well on larger systems. The normalized computational complexity makes FEM the best choice. MLPG remains competitive if human assistance is needed for meshing. \u00a9 2008 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "39"},
{"Title": "Using systems dynamics for human resources management in information systems security", "Authors": ["Trcek D."], "Keywords": ["Cybernetics", "Human resource strategies", "Systems theory"], "Date": "2006", "Abstract": "Purpose - To enable quantitative and qualitative modelling of information systems security management that takes into account technology and human factor. Design/methodology/approach - The approach is based on systems dynamics and it is done in two phases. In the first phase two basic qualitative models are developed, while in the second phase a possibility to further develop them into quantitative models is studied. Findings - Appropriate approach to IS security management requires addressing \"hard\" and \"soft\" factors. Further, to enable quantitative study of such systems, which are highly non-linear, exact analytical (mathematically rigorous) treatment is close to impossible. Thus, computer simulations have to be used. One appropriate methodological answer to the above requirements is systems (business) dynamics. Research limitations/implications - Research limitations are partially related to system dynamics, which operates on an aggregates level. This prevents or makes harder study of phenomena at the micro level, from where the above-mentioned aggregates emerge. Further, many sub-areas need further standardisation to enable more realistic simulations - one such case is security policy standardisation and quantification. Similar holds true for threats/vulnerabilities and related taxonomies. Practical implications - The research presents one of first steps in the direction that could provide quantitative models for effective IS security policy management in organisations. Originality/value - The research presents two models, one for risk management and the other, which is a generic model that identifies basic variables that have to be addressed for IS security management. Further, findings can be used for security awareness courses. \u00a9 Emerald Group Publishing Limited.", "Language": "en", "Citations": "9"},
{"Title": "Iris-based human verification system: A research prototype", "Authors": ["Vrcek G.", "Peer P."], "Keywords": ["Biometric systems", "Eye anatomy", "Feature extraction", "Gabor wawelets", "Hamming distance", "Iris", "Verification"], "Date": "2009", "Abstract": "The biometric person authentication technique based on the human iris is well studied to be applied in any access control system requiring a high level of security. In this paper a system for personal verification based on iris patterns is presented. This includes a problem of biometric systems, required knowledge of the anatomy of the human eye, what is verification based on the iris patterns and algorithm operating principle. In the middle part, we present the verification algorithm, which consists of image processing to obtain iris information, iris normalization, feature extraction, and person verification. Gabor filter was used for feature extraction. From such result iris bit template sequence is encoded. Then the Hamming distance is calculated from the iris template, which gives the estimate of the match in the verification process. The results of the algorithm were obtained on CASIA iris database, where a decision threshold for the Hamming distance was set to 0.427, which gives 0% false acceptance rate and 11.584% false rejection rate on the test set. As we were developing a prototype system with the goal to build a foundation for possible improvements and for a real test system, we were very satisfied with the achieved results. \u00a92009 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "Heterogeneous computing architecture for fast detection of SNP-SNP interactions", "Authors": ["Sluga D.", "Curk T.", "Zupan B.", "Lotric U."], "Keywords": ["CUDA", "Genome-wide association studies", "Graphic processing unit", "Intel Xeon Phi", "Many Integrated Core coprocessor", "SNP-SNP interactions"], "Date": "2014", "Abstract": "Background: The extent of data in a typical genome-wide association study (GWAS) poses considerable computational challenges to software tools for gene-gene interaction discovery. Exhaustive evaluation of all interactions among hundreds of thousands to millions of single nucleotide polymorphisms (SNPs) may require weeks or even months of computation. Massively parallel hardware within a modern Graphic Processing Unit (GPU) and Many Integrated Core (MIC) coprocessors can shorten the run time considerably. While the utility of GPU-based implementations in bioinformatics has been well studied, MIC architecture has been introduced only recently and may provide a number of comparative advantages that have yet to be explored and tested.Results: We have developed a heterogeneous, GPU and Intel MIC-accelerated software module for SNP-SNP interaction discovery to replace the previously single-threaded computational core in the interactive web-based data exploration program SNPsyn. We report on differences between these two modern massively parallel architectures and their software environments. Their utility resulted in an order of magnitude shorter execution times when compared to the single-threaded CPU implementation. GPU implementation on a single Nvidia Tesla K20 runs twice as fast as that for the MIC architecture-based Xeon Phi P5110 coprocessor, but also requires considerably more programming effort.Conclusions: General purpose GPUs are a mature platform with large amounts of computing power capable of tackling inherently parallel problems, but can prove demanding for the programmer. On the other hand the new MIC architecture, albeit lacking in performance reduces the programming effort and makes it up with a more general architecture suitable for a wider range of problems. \u00a9 2014 Sluga et al.; licensee BioMed Central Ltd.", "Language": "en", "Citations": "17"},
{"Title": "New components of the Dictyostelium PKA pathway revealed by Bayesian analysis of expression data", "Authors": ["Parikh A.", "Huang E.", "Dinh C.", "Zupan B.", "Kuspa A.", "Subramanian D.", "Shaulsky G."], "Keywords": [], "Date": "2010", "Abstract": "Background: Identifying candidate genes in genetic networks is important for understanding regulation and biological function. Large gene expression datasets contain relevant information about genetic networks, but mining the data is not a trivial task. Algorithms that infer Bayesian networks from expression data are powerful tools for learning complex genetic networks, since they can incorporate prior knowledge and uncover higher-order dependencies among genes. However, these algorithms are computationally demanding, so novel techniques that allow targeted exploration for discovering new members of known pathways are essential.Results: Here we describe a Bayesian network approach that addresses a specific network within a large dataset to discover new components. Our algorithm draws individual genes from a large gene-expression repository, and ranks them as potential members of a known pathway. We apply this method to discover new components of the cAMP-dependent protein kinase (PKA) pathway, a central regulator of Dictyostelium discoideum development. The PKA network is well studied in D. discoideum but the transcriptional networks that regulate PKA activity and the transcriptional outcomes of PKA function are largely unknown. Most of the genes highly ranked by our method encode either known components of the PKA pathway or are good candidates. We tested 5 uncharacterized highly ranked genes by creating mutant strains and identified a candidate cAMP-response element-binding protein, yet undiscovered in D. discoideum, and a histidine kinase, a candidate upstream regulator of PKA activity.Conclusions: The single-gene expansion method is useful in identifying new components of known pathways. The method takes advantage of the Bayesian framework to incorporate prior biological knowledge and discovers higher-order dependencies among genes while greatly reducing the computational resources required to process high-throughput datasets. \u00a9 2010 Parikh et al; licensee BioMed Central Ltd.", "Language": "en", "Citations": "5"},
{"Title": "On dependence analysis for SIMD enhanced processors", "Authors": ["Bulic P.", "Gustin V."], "Keywords": [], "Date": "2005", "Abstract": "There are a number of data dependence tests that have been proposed in the literature. In each test there is a different trade-off between accuracy and efficiency. The most widely used approximate data dependence tests are the Banerjee inequality and the GCD test. In this paper we consider parallelization for microprocessors with a multimedia extension (the short SIMD execution model). For the short SIMD parallelism extraction it is essential that, if dependency exists, then the distance between memory references is greater than or equal to the number of data processed in the SIMD register. This implies that some loops that could not be vectorized on traditional vector processors can still be parallelized for the short SIMD execution. In all of these tests the parallelization would be prohibited when actually there is no parallelism restriction relating to the short SIMD execution model. In this paper we present a new, fast and accurate data dependence test (called D-test) for array references with linear subscripts, which is used in a vectorizing compiler for microprocessors with a multimedia extension. The presented test is suitable for use in a dependence analyzer that is organized as a series of tests, progressively increasing in accuracy, as a replacement for the GCD or Banerjee tests. \u00a9 Springer-Verlag Berlin Heidelberg 2005.", "Language": "en", "Citations": "1"},
{"Title": "Corpus vs. Lexicon supervision in morphosyntactic tagging: The case of Slovene", "Authors": ["Ljubesic N.", "Erjavec T."], "Keywords": ["Evaluation", "Part-of-speech tagging", "Slavic languages"], "Date": "2016", "Abstract": "In this paper we present a tagger developed for inflectionally rich languages for which both a training corpus and a lexicon are available. We do not constrain the tagger by the lexicon entries, allowing both for lexicon incompleteness and noisiness. By using the lexicon indirectly through features we allow for known and unknown words to be tagged in the same manner. We test our tagger on Slovene data, obtaining a 25% error reduction of the best previous results both on known and unknown words. Given that Slovene is, in comparison to some other Slavic languages, a well-resourced language, we perform experiments on the impact of token (corpus) vs. type (lexicon) supervision, obtaining useful insights in how to balance the effort of extending resources to yield better tagging results.", "Language": "en", "Citations": "5"},
{"Title": "Why is rule learning optimistic and how to correct it", "Authors": ["Mozina M.", "Demsar J.", "Zabkar J.", "Bratko I."], "Keywords": [], "Date": "2006", "Abstract": "In their search through a huge space of possible hypotheses, rule induction algorithms compare estimations of qualities of a large number of rules to find the one that appears to be best. This mechanism can easily find random patterns in the data which will - even though the estimating method itself may be unbiased (such as relative frequency) - have optimistically high quality estimates. It is generally believed that the problem, which eventually leads to overfilling, can be alleviated by using m-estimate of probability. We show that this can only partially mend the problem, and propose a novel solution to making the common rule evaluation functions account for multiple comparisons in the search. Experiments on artificial data sets and data sets from the UCI repository show a large improvement in accuracy of probability predictions and also a decent gain in AUC of the constructed models. \u00a9 Springer-Verlag Berlin Heidelberg 2006.", "Language": "en", "Citations": "12"},
{"Title": "Glioblastoma in patients over 70 years of age", "Authors": ["Smrdel U.", "Vidmar M.S.", "Smrdel A."], "Keywords": ["age group over 70 years", "elderly", "glioblastoma", "prognostic factors", "treatment"], "Date": "2018", "Abstract": "Glioblastoma has in last 20 years seen the steady increase of incidence, which is most prominent in the group of older patients. These older than 70 years have significantly poorer prognosis than other patients and are considered a distinct group of glioblastoma patients. Modified prognostic factors are being used in these patients and this information is lately supplemented with the genetic and epigenetic information on tumour. The therapy is now often tailored accordingly. The aim of our study was to analyse the current treatment of the glioblastoma patients over 70 years of age to determine the impact of clinical prognostic factors. Among patients treated at the Institute of Oncology Ljubljana between 1997 and 2015, we found that 207 were older than 70 years. We analysed their survival, clinical prognostic factors (age, performance status) treatment modalities (extent of surgery, radiation dose, chemotherapy). Median survival of patients older than 70 years was 5.3 months which was statistically significant inferior to the survival of younger patients (p < 0.001). The clinical prognostic factors that influenced survival the most were performance status (p < 0.001), extent of surgical resection (p < 0.001), addition of temozolomide (p < 0.001) and addition of radiotherapy (p = 0.006). Patients receiving concomitant radiochemotherapy with temozolomide followed by adjuvant temozolomide, had same median survival as patients receiving adjuvant temozolomide after completion of radiotherapy. The increase of the number of older patients with glioblastoma corresponds to the increase in the life expectancy but in Slovenia also to the increased availability of diagnostic procedures. Clinical prognostic markers are helpful in decision on the aggressiveness of treatment. Radiotherapy and temozolomide have the biggest impact on survival, but the radiotherapy dose seems to be of secondary importance. In selected patients, chemotherapy alone might be sufficient to achieve an optimal effect. Patients that were fitter, had more aggressive surgery, and received temozolomide fared the best. The scheduling of the temozolomide seems to have limited impact on survival as in our study, there was no difference weather patients received temozolomide concomitant with radiotherapy or after the radiotherapy. Thus far, our findings corroborate the usefulness of recursive partitioning analysis (RPA) classes in clinical decisions.", "Language": "en", "Citations": "1"},
{"Title": "Practical evaluation of stateful NAT64/DNS64 translation", "Authors": ["Skoberne N.", "Ciglaric M."], "Keywords": ["Domain name system", "IP networks", "Network address translation", "Next-generation networking", "Protocols"], "Date": "2011", "Abstract": "It is often suggested that the approach to IPv6 transition is dual-stack deployment; however, it is not feasible in certain environments. As Network Address Translation-Protocol Translation (NAT-PT) has been deprecated, stateful NAT64 and DNS64 RFCs have been published, supporting only IPv6-to-IPv4 translation scenario. Now the question of usability in the real world arises. In this paper, we systematically test a number of widely used application-layer network protocols to find out how well they traverse Ecdysis, the first open source stateful NAT64 and DNS64 implementation. We practically evaluated 18 popular protocols, among them HTTP, RDP, MSNP, and IMAP, and discuss the shortcomings of such translations that might not be apparent at first sight. \u00a9 2011 AECE.", "Language": "en", "Citations": "10"},
{"Title": "An adaptive coupled-layer visual model for robust visual tracking", "Authors": ["Cehovin L.", "Kristan M.", "Leonardis A."], "Keywords": [], "Date": "2011", "Abstract": "This paper addresses the problem of tracking objects which undergo rapid and significant appearance changes. We propose a novel coupled-layer visual model that combines the target's global and local appearance. The local layer in this model is a set of local patches that geometrically constrain the changes in the target's appearance. This layer probabilistically adapts to the target's geometric deformation, while its structure is updated by removing and adding the local patches. The addition of the patches is constrained by the global layer that probabilistically models target's global visual properties such as color, shape and apparent local motion. The global visual properties are updated during tracking using the stable patches from the local layer. By this coupled constraint paradigm between the adaptation of the global and the local layer, we achieve a more robust tracking through significant appearance changes. Indeed, the experimental results on challenging sequences confirm that our tracker outperforms the related state-of-the-art trackers by having smaller failure rate as well as better accuracy. \u00a9 2011 IEEE.", "Language": "en", "Citations": "76"},
{"Title": "Illumination insensitive recognition using eigenspaces", "Authors": ["Bischof H.", "Wildenauer H.", "Leonardis A."], "Keywords": [], "Date": "2004", "Abstract": "Variations in illumination can have a dramatic effect on the appearance of an object in an image. In this paper, we propose how to deal with illumination variations in eigenspace methods. We demonstrate that the eigenimages obtained by a training set under a single illumination condition (ambient light) can be used for recognition of objects taken under different illumination conditions. The major idea is to incorporate a gradient based filter bank into the eigenspace recognition framework. We show that the eigenimage coefficients are invariant to linear filtering (input and eigenimages are filtered with same filters). To achieve further illumination insensitivity we devised a robust procedure for coefficient recovery. The proposed approach has been extensively evaluated on a set of 4932 images and the results were compared to other approaches. \u00a9 2004 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "23"},
{"Title": "Quality of classification explanations with PRBF", "Authors": ["Robnik-Sikonja M.", "Kononenko I.", "Strumbelj E."], "Keywords": ["Classification explanation", "Model comprehensibility", "Model explanation", "Model visualization", "Probabilistic RBF networks", "Quality of explanation"], "Date": "2012", "Abstract": "Recently two general methods for explaining classification models and their predictions have been introduced. Both methods are based on an idea that importance of a feature or a group of features in a specific model can be estimated by simulating lack of knowledge about the values of the feature(s). For the majority of models this requires an approximation by averaging over all possible feature values. A probabilistic radial basis function network (PRBF) is one of the models where such approximation is not necessary and therefore offers a chance to evaluate the quality of approximation by comparing it to the exact solution.We present both explanation methods and demonstrate their behavior with PRBF. The explanations make individual decisions of classifiers transparent and allow inspection and visualization of otherwise opaque models.We empirically compare the quality of explanations based on marginalization of the Gaussian distribution (the exact method) and explanation with averaging over all feature values (the approximation). The results show that the approximation method and the exact solution give very similar results, which increases the confidence in the explanation methodology also for other classification models. \u00a9 2012 Elsevier B.V.", "Language": "en", "Citations": "2"},
{"Title": "Evaluating coherence of essays using sentence-similarity networks", "Authors": ["Zupanc K.", "Savic M.", "Bosnic Z.", "Ivanovic M."], "Keywords": ["Automated Essay Evaluation", "Networks", "Semantic Space"], "Date": "2017", "Abstract": "The main weakness of automated essay evaluation systems is their predominant focus on vocabulary and text syntax, while consideration of text semantics is often neglected. In this work, we propose several new attributes for measuring coherence and consistency of essays that are based on a network representation of essays. In this representation, nodes represent sentences and links reflect similarity between them. We evaluated the proposed attributes on a benchmark dataset showing that their integration into a state-of-the-art system for essay evaluation indicates a potential for improvement of predictive performance.", "Language": "en", "Citations": "1"},
{"Title": "Textual features for corpus visualization using correspondence analysis", "Authors": ["Petrovic S.", "Basic B.D.", "Morin A.", "Zupan B.", "Chauchat J.-H."], "Keywords": ["Correspondence analysis", "Letter n-grams", "Text mining", "Text visualization", "Word n-grams"], "Date": "2009", "Abstract": "Explorative data analysis in text mining essentially relies on effective visualization techniques which can expose hidden relationships among documents and reveal correspondence between documents and their features. In text mining, the documents are most often represented by feature vectors of very high dimensions, requiring dimensionality reduction to obtain visual projections in two- or three-dimensional space. Correspondence analysis is an unsupervised approach that allows for construction of low-dimensional projection space with simultaneous placement of both documents and features, making it ideal for explorative analysis in text mining. Its present use, however, has been limited to word-based features. In this paper, we investigate how this particular document representation compares to the representation with letter n-grams and word n-grams, and find that these alternative representations yield better results in separating documents of different class. We perform our experimental analysis on a bilingual Croatian-English parallel corpus, allowing us to additionally explore the impact of features in different languages on the quality of visualizations. \u00a9 2009 IOS Press. All rights reserved.", "Language": "en", "Citations": "6"},
{"Title": "Predicting patient's long-term clinical status after hip arthroplasty using hierarchical decision modelling and data mining", "Authors": ["Zupan B.", "Demsar J.", "Smrke D.", "Bozikov K.", "Stankovski V.", "Bratko I.", "Beck J.R."], "Keywords": ["Data Mining", "Harris Hip Score", "Hierarchical Decision Models", "Hip Arthroplasty", "Prognostic Models"], "Date": "2001", "Abstract": "Construction of a prognostic model is presented for the long-term outcome after femoral neck fracture treatment with implantation of hip endoprosthesis. While the model is induced from the follow-up data, we show that the use of additional expert knowledge is absolutely crucial to obtain good predictive accuracy. A schema is proposed where domain knowledge is encoded as a hierarchical decision model of which only a part is induced from the data while the rest is specified by the expert. Although applied to hip endoprosthesis domain, the proposed schema is general and can be used for the construction of other prognostic models where both follow-up data and human expertise is available.", "Language": "en", "Citations": "9"},
{"Title": "Extended symbolic mining of textures with association rules", "Authors": ["Kononenko I.", "Bevk M."], "Keywords": ["Artex algorithm", "Association rules", "Image mining", "Meta-association rules", "Multi-resolution", "Texture description"], "Date": "2009", "Abstract": "The association rules algorithms can be used for describing textures if an appropriate texture representation formalism is used. This representation has several good properties like invariance to global lightness and invariance to rotation. Association rules capture structural and statistical information and are very convenient to identify the structures that occur most frequently and have the most discriminative power. This paper presents the extended textural features which are based on association rules. We extend the basic textural features of our algorithm ArTex in three ways, using (a) various interestingness measures, (b) the multi-resolution approach, and (c) the meta-parameters. Results of our experiments show that the extended representation improves the utility of basic textural features and often gives better results, comparable to standard texture descriptions.", "Language": "en", "Citations": "1"},
{"Title": "Visualization-based cancer microarray data classification analysis", "Authors": ["Mramor M.", "Leban G.", "Demsar J.", "Zupan B."], "Keywords": [], "Date": "2007", "Abstract": "Motivation: Methods for analyzing cancer microarray data often face two distinct challenges: the models they infer need to perform well when classifying new tissue samples while at the same time providing an insight into the patterns and gene interactions hidden in the data. State-of-the-art supervised data mining methods often cover well only one of these aspects, motivating the development of methods where predictive models with a solid classification performance would be easily communicated to the domain expert. Results: Data visualization may provide for an excellent approach to knowledge discovery and analysis of class-labeled data. We have previously developed an approach called VizRank that can score and rank point-based visualizations according to degree of separation of data instances of different class. We here extend VizRank with techniques to uncover outliers, score features (genes) and perform classification, as well as to demonstrate that the proposed approach is well suited for cancer microarray analysis. Using VizRank and radviz visualization on a set of previously published cancer microarray data sets, we were able to find simple, interpretable data projections that include only a small subset of genes yet do clearly differentiate among different cancer types. We also report that our approach to classification through visualization achieves performance that is comparable to state-of-the-art supervised data mining techniques. \u00a9 The Author 2007. Published by Oxford University Press. All rights reserved.", "Language": "en", "Citations": "44"},
{"Title": "Computational approaches in synthetic and systems biology", "Authors": ["Moskon M.", "Bordon J.", "Mraz M.", "Zimic N.", "Petroni M."], "Keywords": ["Computational design", "Computational modelling", "Computer simulation", "Network inference", "Parameter estimation techniques", "Performance evaluation", "Sensitivity analysis", "Stability analysis"], "Date": "2014", "Abstract": "Computational biology is an emerging scientific field which employs computational methods in the study of various biological systems. This chapter presents a review of methods that have been introduced to the fields of synthetic and systems biology in recent years. Approaches presented mainly rely on the establishment of computational models. These models allow us to observe the behaviour of a certain biological system in a given environment. Exact kinetic data that describe underlying dynamics are usually necessary to establish accurate computational models. Kinetic data are on the other hand hard or even impossible to obtain experimentally in some cases. Parameter estimation techniques that also rely on computational approaches can be used to accurately evaluate missing kinetic data. With the establishment of computational models, computational analyses can be conducted, such as performance, robustness, sensitivity or stability analysis. These techniques can be used further on to reduce the amount of experimental work and enable straightforward design of novel biological systems in the context of synthetic biology.", "Language": "en", "Citations": "0"},
{"Title": "From illustrations to an interactive art installation", "Authors": ["Pavlin E.", "Elsner Z.", "Jagodnik T.", "Batagelj B.", "Solina F."], "Keywords": ["Art therapy", "Computer games", "Human-computer interface design", "Severe learning difficulties"], "Date": "2015", "Abstract": "Purpose \u2013 The purpose of this paper is to set an example of how people with severe learning difficulties could be more integrated into our society. Design/methodology/approach \u2013 The installation consists of puzzles in the form of a specially designed table with an integrated touch screen. As the visual templates for the puzzles serve pictures painted by a person with severe learning difficulties. The pieces of the puzzles are manipulated directly by the player on the touch screen presenting an intuitive and easily learned user interface. Findings \u2013 The framework for the work was a creation of an interactive art installation in the form of a game where users assemble puzzles on a touch monitor, housed in a specially designed table. Paintings by a person with severe learning difficulty served as visual templates for the puzzles. The pieces of the puzzles can be manipulated directly by the user on a touch screen presenting an intuitive and easily learned user interface, which stimulates the learning of fine motor skills and encourages practice, thus making it suitable for persons with severe learning difficulties in an art therapy setting. Practical implications \u2013 As the work has the format of an interactive art installation, this enables it to gain publicity through exhibitions in art galleries. Social implications \u2013 The installation demonstrates how people with severe learning difficulties can be integrated into the broader society. At the same time, these people are encouraged to use modern computer information technology, which is becoming a necessity also for this group of users. Ethical issues regarding how this group of people can get involved in such work are also discussed. Originality/value \u2013 Combining the habituation of people with severe learning difficulties with computer technology in the form of a game, and framing the whole process as a fine art undertaking, to win the public recognition, is a novelty in addressing the needs of these people.", "Language": "en", "Citations": "2"},
{"Title": "Stage prediction of embryonic stem cell differentiation from genome-wide expression data", "Authors": ["Zagar L.", "Mulas F.", "Garagna S.", "Zuccotti M.", "Bellazzi R.", "Zupan B."], "Keywords": [], "Date": "2011", "Abstract": "Motivation: The developmental stage of a cell can be determined by cellular morphology or various other observable indicators. Such classical markers could be complemented with modern surrogates, like whole-genome transcription profiles, that can encode the state of the entire organism and provide increased quantitative resolution. Recent findings suggest that such profiles provide sufficient information to reliably predict the cell's developmental stage. Results: We use whole-genome transcription data and several data projection methods to infer differentiation stage prediction models for embryonic cells. Given a transcription profile of an uncharacterized cell, these models can then predict its developmental stage. In a series of experiments comprising 14 datasets from the Gene Expression Omnibus, we demonstrate that the approach is robust and has excellent prediction ability both within a specific cell line and across different cell lines. \u00a9 The Author 2011. Published by Oxford University Press. All rights reserved.", "Language": "en", "Citations": "13"},
{"Title": "Modelling In-Store Consumer Behaviour Using Machine Learning and Digital Signage Audience Measurement Data", "Authors": ["Ravnik R.", "Solina F.", "Zabkar V."], "Keywords": [], "Date": "2014", "Abstract": "Audience adaptive digital signage is a new emerging technology, where public broadcasting displays adapt their content to the audience demographic and temporal features. The collected audience measurement data can be used as a unique basis for statistical analysis of viewing patterns, interactive display applications and also for further research and observer modelling. Here, we use machine learning methods on real-world digital signage viewership data to predict consumer behaviour in a retail environment, especially oriented towards the purchase decision process and the roles in purchasing situations. A case study is performed on data from a small retail shop where demographic and audience data of 1294 store customers were collected, manually verified and analysed. Among all customers, 246 store customers were involved in a buying process that resulted in an actual purchase. Comparison of different machine learning methods shows that by using support vector machines we can predict with 88.6% classification accuracy whether a customer will actually make a purchase, which outperforms classification accuracy of a baseline (majority) classifier by 7.5%. A similar approach can also be used to predict the roles of an individual in the purchase decision process. We show that by extending the audience measurement dataset with additional heuristic features, the support vector machines classifier on average improves the classification accuracy of a baseline classifier by 15%.", "Language": "en", "Citations": "2"},
{"Title": "Estimation of regressor reliability", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": ["Prediction error", "Prediction reliability", "Regression", "Reliability estimates", "Transduction", "Transductive reasoning"], "Date": "2008", "Abstract": "The use of common regression models usually confronts us with a hindrance that we have no information on the reliability of the individual output prediction. Using a modified transductive principle, we propose three reliability estimates that solve this problem. The testing results indicate that our reliability estimates significantly correlate with the prediction error.", "Language": "en", "Citations": "4"},
{"Title": "Improving matrix factorization recommendations for examples in cold start", "Authors": ["Ocepek U.", "Rugelj J.", "Bosnic Z."], "Keywords": ["Cold start", "Imputation", "Matrix factorization", "Missing values", "Recommender systems"], "Date": "2015", "Abstract": "Recommender systems suggest items of interest to users based on their preferences (i.e. previous ratings). If there are no ratings for a certain user or item, it is said that there is a problem of a cold start, which leads to unreliable recommendations. We propose a novel approach for alleviating the cold start problem by imputing missing values into the input matrix. Our approach combines local learning, attribute selection, and value aggregation into a single approach; it was evaluated on three datasets and using four matrix factorization algorithms. The results showed that the imputation of missing values significantly reduces the recommendation error. Two tested methods, denoted with 25-FR-ME-<sup>\u2217</sup> and 10-FR-ME-<sup>\u2217</sup>, significantly improved performance of all tested matrix factorization algorithms, without the requirement to use a different recommendation algorithm for the users in the cold start state.", "Language": "en", "Citations": "20"},
{"Title": "Localization of Facial Landmarks in Depth Images Using Gated Multiple Ridge Descent", "Authors": ["Krizaj J.", "Emersic Z.", "Dobrisek S.", "Peer P.", "Struc V."], "Keywords": [], "Date": "2018", "Abstract": "A novel method for automatic facial landmark localization is presented. The method builds on the supervised descent framework, which was shown to successfully localize landmarks in the presence of large expression variations and mild occlusions, but struggles when localizing landmarks on faces with large pose variations. We propose an extension of the supervised descent framework that trains multiple descent maps and results in increased robustness to pose variations. The performance of the proposed method is demonstrated on the Bosphorus, the FRGC and the UND data sets for the problem of facial landmark localization from 3D data. Our experimental results show that the proposed method exhibits increased robustness to pose variations, while retaining high performance in the case of expression and occlusion variations.", "Language": "en", "Citations": "0"},
{"Title": "Modelling operator's skill by machine learning", "Authors": ["Bratko I."], "Keywords": ["Behavioural cloning", "Control system synthesis", "Human-centred design", "Machine learning", "Skill modelling", "Skill reconstruction", "Skill-based systems"], "Date": "2000", "Abstract": "Controlling complex dynamic systems requires skills that operators often cannot completely describe, but can demonstrate. This paper describes some research into the transfer of human control skill into an automatic controller. Controllers are generated from examples of control traces. This process can be aided by techniques of Machine Learning (ML), and is also called \"behavioural cloning\". The paper gives a review of ML-based approaches to behavioural cloning, representative experiments, and an assessment of the results. Some recent work is discussed, including the extraction of the operator's subconscious sub-goals and the use of qualitative representations. It is argued that the key to success is a suitable representation and decomposition of the machine learning problem involved. \u00a9 2000 Univ of Zagreb.", "Language": "en", "Citations": "0"},
{"Title": "A mid-level melody-based representation for calculating audio similarity", "Authors": ["Marolt M."], "Keywords": ["Melody-based representation", "Mid-level representation", "Music similarity", "Searching audio"], "Date": "2006", "Abstract": "We propose a mid-level melody-based representation that incorporates melodic, rhythmic and structural aspects of a music signal and is useful for calculating audio similarity measures. Most current approaches to music similarity use either low-level signal features, such as MFCCs that mostly capture timbral characteristics of music and contain little semantic information, or require symbolic representations, which are difficult to obtain from audio signals. The proposed mid-level representation is our attempt to bridge the gap between audio and symbolic domains by providing an integrated melodic, rhythmic and structural representation of music signals. The representation is based on a set of melodic fragments extracted from prominent melodic lines, it is beat-synchronous, which makes it independent of tempo variations and contains information on repetitions of short melodic phrases within the analyzed piece. We show how it can be calculated automatically from polyphonic audio signals and demonstrate its use for discovering melodic similarities between songs. We present results obtained by using the representation for finding different interpretations of songs in a music collection. \u00a9 2006 University of Victoria.", "Language": "en", "Citations": "27"},
{"Title": "Conservative visual learning for object detection with minimal hand labeling effort", "Authors": ["Roth P.", "Grabner H.", "Skocaj D.", "Bischof H.", "Leonardis A."], "Keywords": [], "Date": "2005", "Abstract": "We present a novel framework for unsupervised training of an object detection system. The basic idea is to (1) exploit a huge amount of unlabeled video data by being very conservative in selecting training examples; and (2) to start with a very simple object detection system and using generative and discriminative classifiers in an iterative co-training fashion to arrive at increasingly better object detectors. We demonstrate the framework on a surveillance task where we learn a person detector. We start with a simple moving object classifier and proceed with robust PCA (on shape and appearance) as a generative classifier which in turn generates a training set for a discriminative AdaBoost classifier. The results obtained by AdaBoost are again filtered by PCA which produces an even better training set. We demonstrate that by using this approach we avoid hand labeling training data and still achieve a state of the art detection rate. \u00a9 Springer-Verlag Berlin Heidelberg 2005.", "Language": "en", "Citations": "10"},
{"Title": "dictyExpress: A Dictyostelium discoideum gene expression database with an explorative data analysis web-based interface", "Authors": ["Rot G.", "Parikh A.", "Curk T.", "Kuspa A.", "Shaulsky G.", "Zupan B."], "Keywords": [], "Date": "2009", "Abstract": "Background: Bioinformatics often leverages on recent advancements in computer science to support biologists in their scientific discovery process. Such efforts include the development of easy-to-use web interfaces to biomedical databases. Recent advancements in interactive web technologies require us to rethink the standard submit-and-wait paradigm, and craft bioinformatics web applications that share analytical and interactive power with their desktop relatives, while retaining simplicity and availability. Results: We have developed dictyExpress, a web application that features a graphical, highly interactive explorative interface to our database that consists of more than 1000 Dictyostelium discoideum gene expression experiments. In dictyExpress, the user can select experiments and genes, perform gene clustering, view gene expression profiles across time, view gene co-expression networks, perform analyses of Gene Ontology term enrichment, and simultaneously display expression profiles for a selected gene in various experiments. Most importantly, these tasks are achieved through web applications whose components are seamlessly interlinked and immediately respond to events triggered by the user, thus providing a powerful explorative data analysis environment. Conclusion: dictyExpress is a precursor for a new generation of web-based bioinformatics applications with simple but powerful interactive interfaces that resemble that of the modern desktop. While dictyExpress serves mainly the Dictyostelium research community, it is relatively easy to adapt it to other datasets. We propose that the design ideas behind dictyExpress will influence the development of similar applications for other model organisms. \u00a9 2009 Rot et al; licensee BioMed Central Ltd.", "Language": "en", "Citations": "47"},
{"Title": "Researching dictionary needs of language users through social media: A semi-automatic approach", "Authors": ["Cibej J.", "Holdt S.A."], "Keywords": ["Automatic extraction", "Facebook", "Language problems", "Lexicographical user research", "Slovene", "Social media"], "Date": "2018", "Abstract": "With the rise of digital media in the last decades, many language-related discussions have found home on various fora and social media such as Facebook, where users can participate in a shared-interest group to discuss language use, problems and resources. The posts in these groups are formulated by language users as a genuine response to a specific disruption in language use and offer an empirical starting point for studying language problems. We propose an automatic approach to extracting questions from language-related Facebook groups and describe the procedure in consecutive steps. We also address the issues of copyright, privacy and ethical constraints, and propose ways to overcome them. We present the extraction method on a case of two Slovene language-related Facebook groups: Za vsaj pribliz\u02c7no pravilno rabo slovens\u02c7cine and Drus\u02c7tvo ljubiteljskih pravopisarjev in slovnicarjev. Both groups allow users to discuss language-related problems and find answers to their questions within the community. Our first extraction from these groups yielded approximately 1,900 posts (written by approximately 500 users) and 13,000 comments (posted by more than 900 users), providing ample material that can be analyzed to reveal the users' most frequent language problems.", "Language": "en", "Citations": "0"},
{"Title": "Mobile robot localization using an incremental eigenspace model", "Authors": ["Artac M.", "Jogan M.", "Leonardis A."], "Keywords": ["On-line visual learning", "PCA updating", "Repetitive learning", "Robot localization", "View-based robot localization"], "Date": "2002", "Abstract": "When using appearance-based recognition for self-localization of mobile robots, the images obtained during the exploration of the environment need to be efficiently stored in the memory. PCA offers means for representing the images in a low-dimensional subspace, which allows for efficient matching and recognition. For active exploration it is necessary to use an incremental method for the computation of the subspace. While such methods have been considered before, only the on-line construction of eigenvectors has been addressed. Representations of the images in the subspace were computed only after the final subspace had been built, requiring that all the images were kept in the memory. In this paper we propose to use an incremental PCA algorithm with the updating of partial image representations in a way that allows the robot to discard the acquired images immediately after the update. Such a model is open-ended, meaning that we can easily update it with new images. We show that the performance of the proposed method is comparable to the performance of the batch method in terms of compression, computational cost and the precision of localization. We also show that by applying the repetitive learning, the subspace converges to that constructed with the batch method.", "Language": "en", "Citations": "63"},
{"Title": "Use of NFC and QR code identification in an electronic ticket system for public transport", "Authors": ["Finzgar L.", "Trebar M."], "Keywords": [], "Date": "2011", "Abstract": "The evolution of modern communication technologies and widespread use of mobile phones enable their common use in numerous everyday applications. This paper describes the implementation of a system, which enables the use of phones for acquiring electronic public transport ticket. QR codes and RFID tags are used for registering passenger at the beginning and at the end of their journeys. The development of mobile applications was done by using two modern technologies, mobile operating system Android and Near Field Communication (NFC). A passenger can see the amounts charged for a ticket and other information, like departures. Moreover, a conductor can use the Android application to check the validity of the tickets. Beside the possibility of getting an electronic ticket, which is stored on the phone, a website was built to enable new users of this system to register. An additional website was designed for administrators to see the data on the journeys of passengers and to analyse statistics results. \u00a9 2011 University of Split.", "Language": "en", "Citations": "27"},
{"Title": "Identification of evolutionarily conserved exons as regulated targets for the splicing activator Tra2\u03b2 in development", "Authors": ["Grellscheid S.", "Dalgliesh C.", "Storbeck M.", "Best A.", "Liu Y.", "Jakubik M.", "Mende Y.", "Ehrmann I.", "Curk T.", "Rossbach K.", "Bourgeois C.F.", "Stevenin J.", "Grellscheid D.", "Jackson M.S.", "Wirth B.", "Elliott D.J."], "Keywords": [], "Date": "2011", "Abstract": "Alternative splicing amplifies the information content of the genome, creating multiple mRNA isoforms from single genes. The evolutionarily conserved splicing activator Tra2\u03b2 (Sfrs10) is essential for mouse embryogenesis and implicated in spermatogenesis. Here we find that Tra2\u03b2 is up-regulated as the mitotic stem cell containing population of male germ cells differentiate into meiotic and post-meiotic cells. Using CLIP coupled to deep sequencing, we found that Tra2\u03b2 binds a high frequency of exons and identified specific G/A rich motifs as frequent targets. Significantly, for the first time we have analysed the splicing effect of Sfrs10 depletion in vivo by generating a conditional neuronal-specific Sfrs10 knock-out mouse (Sfrs10 ", "Language": "en", "Citations": "48"},
{"Title": "Automatization of the stream mining process", "Authors": ["Subelj L.", "Bosnic Z.", "Kukar M.", "Bajec M."], "Keywords": ["data mining", "expert system", "stream mining"], "Date": "2014", "Abstract": "The problem this paper addresses is related to Data Stream Mining and its automatization within Information Systems. Our aim is to show that the expertise which is usually provided by data and data mining experts and is crucial for problems of this kind can be successfully captured and computerized. To this end we observed data mining experts at work and in discussion with them coded their knowledge in a form of an expert system. The evaluation over four different datasets confirms the automatization of the stream mining process is possible and can produce results comparable to those achieved by data mining experts. \u00a9 2014 Springer International Publishing.", "Language": "en", "Citations": "0"},
{"Title": "Commuting graphs of matrices over semirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Commuting graph", "Matrices", "Semiring"], "Date": "2011", "Abstract": "We calculate diameters and girths of commuting graphs of the set of all nilpotent matrices over a semiring, the group of all invertible matrices over a semiring, and the full matrix semiring. \u00a9 2010 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "16"},
{"Title": "Collective pairwise classification for multi-way analysis of disease and drug data", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": ["Collective classification", "Drug-drug interactions", "Drug-target interactions", "Gene-disease network", "Multi-relational learning", "Symptoms-disease network", "Three-way model"], "Date": "2016", "Abstract": "Interactions between drugs, drug targets or diseases can be predicted on the basis of molecular, clinical and genomic features by, for example, exploiting similarity of disease pathways, chemical structures, activities across cell lines or clinical manifestations of diseases. A successful way to better understand complex interactions in biomedical systems is to employ collective relational learning approaches that can jointly model diverse relationships present in multiplex data.We propose a novel collective pairwise classification approach for multi-way data analysis. Our model leverages the superiority of latent factor models and classifies relationships in a large relational data domain using a pairwise ranking loss. In contrast to current approaches, our method estimates probabilities, such that probabilities for existing relationships are higher than for assumed-to-be-negative relationships. Although our method bears correspondence with the maximization of non-differentiable area under the ROC curve, we were able to design a learning algorithm that scales well on multi-relational data encoding interactions between thousands of entities.We use the new method to infer relationships from multiplex drug data and to predict connections between clinical manifestations of diseases and their underlying molecular signatures. Our method achieves promising predictive performance when compared to state-of-the-art alternative approaches and can make \u201ccategory-jumping\u201d predictions about diseases from genomic and clinical data generated far outside the molecular context.", "Language": "en", "Citations": "7"},
{"Title": "Machine learning applied to quality management-A study in ship repair domain", "Authors": ["Srdoc A.", "Bratko I.", "Sluga A."], "Keywords": ["Deep quality concept", "Delivery time estimate", "Dock works", "Knowledge acquisition", "Quality management"], "Date": "2007", "Abstract": "The awareness about the importance of knowledge within the quality management community is increasing. For example, the Malcolm Baldrige Criteria for Performance Excellence recently included knowledge management into one of its categories. However, the emphasis in research related to knowledge management is mostly on knowledge creation and dissemination, and not knowledge formalisation process. On the other hand, identifying the expert knowledge and experience as crucial for the output quality, especially in dynamic industries with high share of incomplete and unreliable information such as ship repair, this paper argues how important it is to have such knowledge formalised. The paper demonstrates by example of delivery time estimate how for that purpose the deep quality concept (DQC)-a novel knowledge-focused quality management framework, and machine learning methodology could be effectively used. In the concluding part of the paper, the accuracy of the obtained prediction models is analysed, and the chosen model is discussed. The research indicates that standardisation of problem domain notions and expertly designed databases with possible interface to machine learning algorithms need to be considered as an integral part of any quality management system in the future, in addition to conventional quality management concepts. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "9"},
{"Title": "One year later: The effect of the Bologna reform on algorithms and data structures course teaching", "Authors": ["Rozanc I."], "Keywords": ["ADS course", "Bologna reform", "Examination data", "P-value", "Statistic analysis", "Z-test"], "Date": "2011", "Abstract": "The introduction of Bologna reform caused changes in structure, performance and grading of university courses. We present our experience with porting the old \"pre-Bologna\" course on basic algorithms and data structures to the new one fulfilling the Bologna criteria. The key differences are a more practical lectures performance, more individual practical work, use of modern communication teaching tools and important emphasis on practice assessment. After the first year of teaching the new course we compared the collected examination results with the \"pre-Bologna\" results. After careful selection and necessary preparations we present a statistical test of the old and new examination results samples to prove that new results are synchronized with old ones.", "Language": "en", "Citations": "2"},
{"Title": "Qualitative trust model with a configurable method to aggregate ordinal data", "Authors": ["Jelenc D.", "Trcek D."], "Keywords": ["Data aggregation", "Multi-agent system", "Ordinal", "Qualitative", "Trust"], "Date": "2014", "Abstract": "Trust models are mechanisms that allow agents to build trust without relying on a trusted central authority. Our goal was to develop a trust model that would operate with values that humans easily understand and manipulate: qualitative and ordinal values. The result is a trust model that computes trust from experiences created in interactions and from opinions obtained from third-party agents. The trust model, termed qualitative trust model (QTM), uses qualitative and ordinal values for assessing experiences, expressing opinions and estimating trust. We treat such values appropriately; we never convert them to numbers, but merely use their relative order. To aggregate a collection of such values, we propose an aggregation method that is based on comparing distributions and show some of its properties; the method can be used in other domains and can be seen as an alternative to median and similar methods. To cope with lying agents, QTM estimates trustworthiness in opinion providers with a modified version of the weighted majority algorithm, and additionally combines trustworthiness with social links between agents; such links are obtained implicitly by observing how agents provide opinions about each other. Finally, we compare QTM against a set of well-known trust models and demonstrate that it consistently performs well and on par with other quantitative models, and in many cases even outperforms them, particularly when the number of direct experiences is low. \u00a9 2013 The Author(s).", "Language": "en", "Citations": "1"},
{"Title": "The integration of classical artistic media in a smart space prototype", "Authors": ["Bovcon N.", "Solina F.", "Batagelj B.", "Vaupotic A.", "Dezeljin D."], "Keywords": ["Avatar", "Computer vision", "Digital animation", "Digital video", "Human-centered human-computer interaction design", "Interactive installation", "Multiple-screen video projection", "New media art", "Smart space", "Synthetic realism"], "Date": "2009", "Abstract": "In the mixed reality of the computer installation Presence, which functions as a prototype for a smart space, the visitor is placed in the position of a person in audience with the king. The interaction with a digital avatar is structured according to the rules of social behaviours and following the script of the Shakespeare's play. The paper explains different aspects of the conceptualisation of an interdisciplinary collaboration between artists and computer engineers. \u00a9 2009 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "A framework and tool-support for reengineering software development methods", "Authors": ["Bajec M.", "Vavpotic D."], "Keywords": ["Computer aided method engineering (came) tools", "Method adaptation and extension techniques", "Situation factors and suitability", "Situational method engineering", "Software development method", "Software process", "Software process improvement"], "Date": "2008", "Abstract": "The purpose of the research described in this paper is to propose a framework and supporting tools that will help software companies to establish formalised methods that will be technically and socially sound with their needs. Following the framework the companies can asses and improve their existing ways of working, capture them into formalised methods and continuously enrich them based on the past development experiences. Furthermore, the formalised methods that are designed based on the suggested framework are flexible and can be automatically adjusted by the supporting tools to suite circumstances of a particular project or team. This paper describes the framework philosophy and its tool support. \u00a9 2008 Institute of Mathematics and Informatics.", "Language": "en", "Citations": "11"},
{"Title": "A model for resistance management in IT projects and programs", "Authors": ["Vrhovec S.", "Rupnik R."], "Keywords": ["Information systems", "Program management", "Project management", "Resistance management", "Resistance to change"], "Date": "2011", "Abstract": "Modern organizations change frequently. They try to implement changes rapidly and effectively. Change usually involves change in information systems (IS) due to its importance in organizations. When implementing larger IS changes in IT projects or programs, resistance to change is an important factor of a possible project failure. Resistance management does not only help organizations to implement changes more rapidly and efficiently but is also an important source of innovation in the change process. Also, the probability of making the right changes increases when resistance is managed well. In this paper we briefly present resistance and resistance management, propose a resistance-management model and position resistance management within the PMI project-management standard.", "Language": "en", "Citations": "8"},
{"Title": "Machine learning model for aircraft performances", "Authors": ["Hrastovec M.", "Solina F."], "Keywords": [], "Date": "2014", "Abstract": "This paper presents new idea how trajectory calculations could be improved in order to match real flights better. Exact trajectory calculation is important for future of air traffic control, because it is one of the enablers for safe traffic increase. Methods used to calculate trajectories are based on aircraft types and their performances mainly. However, we believe that there are many other influencing factors which should be taken into account. We collect available data about flights and store them into a multi-dimensional database. Knowledge accumulated in this database is the basis for aircraft performances prediction using machine learning methods. In that way the prediction is not based on aircraft type alone, but also on other attributes like aerodrome of departure, destination and operator. There attributes indirectly imply to procedures, operator's best practices, local airspace characteristics, etc. and enable us to make better predictions of aircraft performances. Predictions in this case are not static but tailored to every particular flight.", "Language": "en", "Citations": "6"},
{"Title": "The influence of a sustained multifaceted approach to improve antibiotic prescribing in Slovenia during the past decade: Findings and implications", "Authors": ["Furst J.", "Cizman M.", "Mrak J.", "Kos D.", "Campbell S.", "Coenen S.", "Gustafsson L.L.", "Furst L.", "Godman B."], "Keywords": ["antibiotic resistance", "antibiotic use", "drug utilization", "multifaceted interventions", "policies", "rational use of antibiotics", "Slovenia"], "Date": "2015", "Abstract": "Introduction: Rising antibiotic resistance has become an increasing public health problem. There is a well-established correlation between antibiotic consumption and antimicrobial resistance. Consequently, measures to rationalize the prescribing of antibiotics should reduce the resistant strains. Following a 24% increase in antibiotic consumption at the end of the 1990s, multiple activities were designed and introduced by the Health Insurance Institute of Slovenia (ZZZS) and other organizations in Slovenia at the end of 1999. These activities reduced the antibiotic consumption by 18.7% by 2002. These measures have continued. Objective: To study changes in antibiotic utilization from 1995 to 2012 alongside the multiple interventions and their consequences, including changes in resistance patterns. Methods: This was a retrospective observational study involving all patients dispensed at least one ZZZS prescription for an antibiotic in Slovenia. Utilization was expressed in defined daily doses per thousand inhabitants per day. Multifaceted interventions were conducted over time involving all key stakeholder groups, that is, the Ministry of Health, ZZZS, physician groups and patients. These included comprehensive communication programs as well as prescribing restrictions for a number of antibiotics and classes. Results: From 1999 to 2012, antibiotic consumption decreased by 2-9% per year, with an overall decrease of 31%. There were also appreciable structural changes. Overall antibiotic utilization and the utilization of 7 out of 10 antibiotics significantly decreased after multiple interventions. The resistance of Streptococcus pneumoniae to penicillin decreased in line with decreased utilization. However, its resistance to macrolides increased from 5.4 to 21% despite halving of its utilization. The resistance of Escherichia coli to fluoroquinolones doubled from 10 to 21% despite utilization decreasing by a third. Expenditures on antibiotics decreased by 53%. Conclusion: Multiple demand-side measures introduced following increased utilization significantly decreased subsequent antibiotic utilization and associated costs. However, there was variable impact on antibiotic resistance. Additional targeted activities are planned to further reduce antibiotic prescribing and resistance.", "Language": "en", "Citations": "27"},
{"Title": "Combining learning style models and alleviating the new user problem in learning recommender systems", "Authors": ["Ocepek U.", "Rugelj J.", "Serbec I.N.", "Bosnic Z."], "Keywords": ["Adaptive learning environments", "Learning styles models", "New user problem", "Recommender systems"], "Date": "2016", "Abstract": "Adaptive computer-based multimedia learning environments support the idea that people learn better and more deeply when appropriate images (i.e., animations, video, static graphics) are included with text or narration. Adaptive learning environments mostly support only traditional concepts of learning. Development in the field of cognitive science now indicates the need for design and development of e-learning systems that are based on constructivist learning approach. Recommender systems, which suggest items of interest to a student based on his properties, preferences, and activities, can be the appropriate solution. In this paper, we present a concept of such a learning recommender system that combines knowledge from pedagogy and recommender systems, and analyze how combining of four different learning style models (cognitive styles, epistemic styles, hemispheric styles, and perceiving styles) influences the choosing preferred types of multimedia materials. We present a decision model intended for predicting an appropriate multimedia type of learning material for each individual student. The results show that students prefer well-structured learning texts with color discrimination, and that the hemispheric learning style model is the most important criterion in deciding student preferences for different multimedia learning materials. In the second part of our research, we describe an approach for improving recommendation performance in environments where the system has no prior information about learners. As a combination of findings of our research, we outline the concept of a learning system and explain its possible positive effects in practice.", "Language": "en", "Citations": "0"},
{"Title": "Adiabatic pipelining: A key to ternary computing with quantum dots", "Authors": ["Pecar P.", "Ramsak A.", "Zimic N.", "Mraz M.", "Lebar Bajec I."], "Keywords": [], "Date": "2008", "Abstract": "The quantum-dot cellular automaton (QCA), a processing platform based on interacting quantum dots, was introduced by Lent in the mid-1990s. What followed was an exhilarating period with the development of the line, the functionally complete set of logic functions, as well as more complex processing structures, however all in the realm of binary logic. Regardless of these achievements, it has to be acknowledged that the use of binary logic is in computing systems mainly the end result of the technological limitations, which the designers had to cope with in the early days of their design. The first advancement of QCAs to multi-valued (ternary) processing was performed by Lebar Bajec et al, with the argument that processing platforms of the future should not disregard the clear advantages of multi-valued logic. Some of the elementary ternary QCAs, necessary for the construction of more complex processing entities, however, lead to a remarkable increase in size when compared to their binary counterparts. This somewhat negates the advantages gained by entering the ternary computing domain. As it turned out, even the binary QCA had its initial hiccups, which have been solved by the introduction of adiabatic switching and the application of adiabatic pipeline approaches. We present here a study that introduces adiabatic switching into the ternary QCA and employs the adiabatic pipeline approach to successfully solve the issues of elementary ternary QCAs. What is more, the ternary QCAs presented here are sizewise comparable to binary QCAs. This in our view might serve towards their faster adoption. \u00a9 IOP Publishing Ltd.", "Language": "en", "Citations": "19"},
{"Title": "Minimum Degree and Graph Minors", "Authors": ["Fijavz G.", "Wood D.R."], "Keywords": ["forbidden minors", "graph minors", "minimum degree"], "Date": "2008", "Abstract": "A graph G is a minor minimal minimum degree graph (MMMD) if \u03b4 (H) < \u03b4 (G) for every proper minor H of G. We (i) determine all complete multipartite MMMD graphs and show that (ii) every small k-regular graph is a MMMD graph. Intuitively it seems that MMMD graphs are highly connected. Countering that (iii) we show that MMMD graphs may have a rich block-structure. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "1"},
{"Title": "Application for sexually transmitted infection risk assessment", "Authors": ["Ajanovic A.", "Konda J.", "Fele-Zorz G.", "Gradisek A.", "Gams M.", "Peterlin A.", "Pocivavsek K.", "Maticic M."], "Keywords": ["Awareness", "Health education", "Prevention and health", "Sexually transmitted infections", "Web application", "Website"], "Date": "2017", "Abstract": "We present a web application to detect risks related to sexually transmitted infections (STIs). The application works as a questionnaire about sexual behaviour and risk factors for STIs and, based on the answers, calculates the risk of being infected. The application also works as an informational tool with educating about STIs and prevention. It uses a combination of approaches from computer science and psychology to deliver a usable, clean interface with which the user feels safe.", "Language": "en", "Citations": "1"},
{"Title": "LRTA* works much better with pessimistic heuristics", "Authors": ["Sadikov A.", "Bratko I."], "Keywords": [], "Date": "2008", "Abstract": "Recently we showed that under very reasonable conditions, incomplete, real-time search methods like RTA* work better with pessimistic heuristic functions than with optimistic, admissible heuristic functions of equal quality. The use of pessimistic heuristic functions results in higher percentage of correct decisions and in shorter solution lengths. We extend this result to learning RTA* (LRTA*) and demonstrate that the use of pessimistic instead of optimistic (or mixed) heuristic functions of equal quality results in much faster learning process at the cost of just marginally worse quality of converged solutions.", "Language": "en", "Citations": "1"},
{"Title": "Bilingual lexicon extraction from comparable corpora for closely related languages", "Authors": ["Fiser D.", "Ljubesic N."], "Keywords": [], "Date": "2011", "Abstract": "In this paper we present a knowledge-light approach to extract a bilingual lexicon for closely related languages from comparable corpora. While in most related work an existing dictionary is used to translate context vectors, we take advantage of the similarities between languages instead and build a seed lexicon from words that are identical in both languages and then further extend it with context-based cognates and translations of the most frequent words. We also use cognates for reranking translation candidates obtained via context similarity and extract translation equivalents for all content words, not just nouns as in most related work. The results are very encouraging, suggesting that other similar languages could benefit from the same approach. By enlarging the seed lexicon with cognates and translations of the most frequent words and by cognate-based reranking of translation candidates we were able to improve the average baseline precision from 0.592 to 0.797 on the mean reciprocal rank for the ten top-ranking translation candidates for nouns, verbs and adjectives with a 46% recall on the gold standard of 1000 random entries from a traditional dictionary.", "Language": "en", "Citations": "10"},
{"Title": "Combining vector quantization and ant-colony algorithm for mesh-partitioning", "Authors": ["Silc J.", "Korosec P.", "Robic B."], "Keywords": [], "Date": "2004", "Abstract": "We present heuristic mesh-partitioning method build on the ant-colony algorithm (ACA) in order to improve the quality of the mesh partitions. The method focuses on improving the initial partition that is submitted to the ACA. The method is experimentally compared with the well-known mesh-partitioning programs pMETIS 4.0 and Chaco 2.0. \u00a9 Springer-Verlag Berlin Heidelberg 2004.", "Language": "en", "Citations": "3"},
{"Title": "Decomposition of a complex fuzzy controller for the truck-and-trailer reverse parking problem", "Authors": ["Zimic N.", "Mraz M."], "Keywords": ["Decomposition", "Fuzzy control", "Fuzzy systems", "Hierarchical fuzzy controller", "Truck-and-trailer parking"], "Date": "2006", "Abstract": "The use of fuzzy logic has, in the last twenty years, become standard practice in the field of control. The reason lies in the fuzzy logic's ability to relatively quickly transfer uncertain experience and knowledge about the observed object's behaviour into the process of decision making. Nevertheless, one of the biggest problems that arises when using a fuzzy approach is the large number of fuzzy rules that have to be processed in order to produce one decision (i.e. one control output). The number of rules in a fuzzy controller primarily originates from the number of input variables that are entering the decision process and one possible solution for decreasing it is to use the method of decomposition. Its main goal is to implement the equivalent control functionality with a hierarchy of simpler fuzzy controllers. Their main characteristic is a lower number of input variables, which as a consequence leads to a smaller number of fuzzy rules. In our paper we apply the decomposition approach to the classical complex control case of the Truck-and-Trailer (T&T) reverse parking control problem. In such cases the implementation of control using only one fuzzy controller is very complex and the existing solutions, in some details, even deviate from the classical fuzzy approach. Our solution is, on the other hand, based only on the uncertain knowledge about the behaviour of the T&T driver and the results achieved are even better than those achieved by using the existing solutions. \u00a9 2006 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "16"},
{"Title": "Ergonomic trust management in pervasive computing environments - Qualitative assessment dynamics", "Authors": ["Trcek D."], "Keywords": ["Ergonomic IT solutions", "Pervasive computing", "Qualitative algebra", "Trust management"], "Date": "2010", "Abstract": "Pervasive computing research is mainly focused on technology as such, while more human-centric issues are being addressed only recently. One such issue is trust and its management is getting crucial for further and wider acceptance of pervasive computing solutions. In IT domain trust was first addressed some fifteen years ago, but the suggested approaches were actually addressing security. After a few years, more advanced and rational agents assuming methodologies emerged that were based on Bayesian statistics. These were followed by Dempster-Shafer theory of evidence and its derivatives, e.g. subjective algebra, together with some game theoretic attempts. However, trust is a manifestation of (frequently nonrational) reasoning and assessment processes; it has to be treated in line with this fact and adequately supported in interactive computing environments. This requires a commonly accepted and agreed methodology for trust research to enable scientifically sound evaluations of proposed methodologies, which is the first motivation behind this paper - in this area we are lacking experimental verification of developed trust management technologies to check how well they meet users' mental models and expectations. On this basis a cognitive ergonomics focused methodology, called qualitative assessment dynamics (QAD), has been developed, being the second motivation behind this paper. QAD complements existing trust management methodologies. \u00a92010 IEEE.", "Language": "en", "Citations": "4"},
{"Title": "Cardiac autonomic regulation and PR interval determination for enhanced atrial fibrillation risk prediction after cardiac surgery", "Authors": ["Kalisnik J.M.", "Avbelj V.", "Vratanar J.", "Santarpino G.", "Gersak B.", "Fischlein T.", "Trobec R.", "Zibert J."], "Keywords": ["Cardiac autonomic regulation", "Cardiac surgery", "Heart rate variability", "High-resolution electrocardiography", "Postoperative atrial fibrillation", "PR interval"], "Date": "2019", "Abstract": "Background: Changes in cardiac autonomic regulation and P-wave characteristics are associated with the occurrence of atrial fibrillation. The purpose of this study was to evaluate whether combined preoperative non-invasive determination of cardiac autonomic regulation and PR interval allows for the identification of patients at risk of new-onset atrial fibrillation after cardiac surgery. Methods: RR, PR and QT intervals, and linear and non-linear heart rate variability parameters from 20 min high-resolution electrocardiographic recordings were determined one day before surgery in 150 patients on chronic beta blockers undergoing elective coronary artery bypass grafting, aortic valve replacement, or both, electively. Results: Thirty-one patients (21%)developed postoperative atrial fibrillation. In the atrial fibrillation group, more arterial hypertension, a greater age, a higher EuroSCORE II, a higher heart rate variability index (pNN50: 9 \u00b1 20 vs. 4 \u00b1 10, p = 0.050), a short PR interval (156 \u00b1 23 vs. 173 \u00b1 31 ms; p = 0.011), and a reduced short-term scaling exponent of the detrended fluctuation analysis (DFA1, 0.96 \u00b1 0.36 vs. 1.11 \u00b1 0.30 ms; p = 0.032)were found compared to the sinus rhythm group. Logistic regression modeling confirmed PR interval, DFA1 and age as the strongest preoperative predictors of postoperative atrial fibrillation (area under the receiver operating characteristic curve = 0.804). Conclusions: Patients developing atrial fibrillation after cardiac surgery presented with severe cardiac autonomic derangement and a short PR interval preoperatively. The observed state characterizes both altered heart rate regulation and arrhythmic substrate and is strongly related to an increased risk of postoperative atrial fibrillation.", "Language": "en", "Citations": "0"},
{"Title": "Handling numeric attributes with ant colony based classifier for medical decision making", "Authors": ["Piculin M.", "Robnik-Sikonja M."], "Keywords": ["Ant Colony Optimization", "Ant-Miner", "Classification", "Medical data mining", "Numeric attributes", "Rule learning"], "Date": "2014", "Abstract": "In data mining many datasets are described with both discrete and numeric attributes. Most Ant Colony Optimization based classifiers can only deal with discrete attributes and need a pre-processing discretization step in case of numeric attributes. We propose an adaptation of AntMiner+ for rule mining which intrinsically handles numeric attributes. We describe the new approach and compare it to the existing algorithms. The proposed method achieves comparable results with existing methods on UCI datasets, but has advantages on datasets with strong interactions between numeric attributes. We analyse the effect of parameters on the classification accuracy and propose sensible defaults. We describe application of the new method on a real world medical domain which achieves comparable results with the existing method. \u00a9 2014 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "3"},
{"Title": "Learning predictive qualitative models with pad\u00e9", "Authors": ["Zabkar J.", "Mozina M.", "Bratko I.", "Demsar J."], "Keywords": ["Machine learning", "Qualitative modelling"], "Date": "2011", "Abstract": "Qualitative models are similar to regression models, except that instead of numerical predictions they provide insight into how a change of a certain input variable affects the output within a context of other inputs. Although people usually reason qualitatively, machine learning has mostly ignored this type of model. We present a new approach to learning qualitative models from numerical data. We describe Pad\u00e9, a suite of methods for estimating partial derivatives of unknown sampled target functions. We show how to build qualitative models using standard machine learning algorithms by replacing the output variable with signs of computed derivatives. Experiments show that the developed methods are quite accurate, scalable to high number of dimensions and robust with regard to noise.", "Language": "en", "Citations": "0"},
{"Title": "Conceptual passages - Sre\u010do dragan's new media art projects", "Authors": ["Bovcon N."], "Keywords": ["Closed-circuit video installation", "Computer vision", "Computer-generated model", "Conceptual plan of entering", "Enunciation", "Techno-image", "Techno-modified gaze", "Techno-performance", "Virtual reality"], "Date": "2008", "Abstract": "The paper introduces several explorations in new media art by Slovene artist Sre\u010do Dragan. The main aspects of his work are the institutions of techno-modified gaze and techno-performance. He explored the institution of techno-modified gaze in the 1990s, when several modes of mixed reality were tested by means of invented conceptual platforms for entering virtual reality and connecting it back to the movement in the gallery space. Internet was used as a new bearer of the tele-present places and of computer-generated reconstructions of architectural spaces. Techno-modified gaze is reflected in the techno-image, in its \"conceptual plan of entering\". The second institution, techno-performance, operating by means of computer vision involves the user at the point of his/her enunciation or display of a determining personal characteristic. It refers to the early happenings, e.g. group communication in a \"closed-circuit\" video installation. It addresses the user as a task for rearranging his/her mental archive in relation to the physical experiences and cultural background.", "Language": "en", "Citations": "2"},
{"Title": "Fast optimization of non-negative matrix tri-factorization", "Authors": ["Copar A.", "Zupan B.", "Zitnik M."], "Keywords": [], "Date": "2019", "Abstract": "Non-negative matrix tri-factorization (NMTF) is a popular technique for learning low-dimensional feature representation of relational data. Currently, NMTF learns a representation of a dataset through an optimization procedure that typically uses multiplicative update rules. This procedure has had limited success, and its failure cases have not been well understood. We here perform an empirical study involving six large datasets comparing multiplicative update rules with three alternative optimization methods, including alternating least squares, projected gradients, and coordinate descent. We find that methods based on projected gradients and coordinate descent converge up to twenty-four times faster than multiplicative update rules. Furthermore, alternating least squares method can quickly train NMTF models on sparse datasets but often fails on dense datasets. Coordinate descent-based NMTF converges up to sixteen times faster compared to well-established methods.", "Language": "en", "Citations": "0"},
{"Title": "Cluster analysis of particulate matter (PM10) and black carbon (BC) concentrations", "Authors": ["Zibert J.", "Praznikar J."], "Keywords": ["Black carbon", "Cluster analysis", "Data clustering", "Data-driven analysis", "PM10", "Port of Koper"], "Date": "2012", "Abstract": "The monitoring of air-pollution constituents like particulate matter (PM10) and black carbon (BC) can provide information about air quality and the dynamics of emissions. Air quality depends on natural and anthropogenic sources of emissions as well as the weather conditions. For a one-year period the diurnal concentrations of PM10 and BC in the Port of Koper were analysed by clustering days into similar groups according to the similarity of the BC and PM10 hourly derived day-profiles without any prior assumptions about working and non-working days, weather conditions or hot and cold seasons. The analysis was performed by using k-means clustering with the squared Euclidean distance as the similarity measure. The analysis showed that 10 clusters in the BC case produced 3 clusters with just one member day and 7 clusters that encompasses more than one day with similar BC profiles. Similar results were found in the PM10 case, where one cluster has a single-member day, while 7 clusters contain several member days. The clustering analysis revealed that the clusters with less pronounced bimodal patterns and low hourly and average daily concentrations for both types of measurements include the most days in the one-year analysis. A typical day profile of the BC measurements includes a bimodal pattern with morning and evening peaks, while the PM10 measurements reveal a less pronounced bimodality. There are also clusters with single-peak day-profiles. The BC data in such cases exhibit morning peaks, while the PM10 data consist of noon or afternoon single peaks. Single pronounced peaks can be explained by appropriate cluster wind speed profiles. The analysis also revealed some special day-profiles. The BC cluster with a high midnight peak at 30/04/2010 and the PM10 cluster with the highest observed concentration of PM10 at 01/05/2010 (208.0 \u03bcg m ", "Language": "en", "Citations": "7"},
{"Title": "Towards quantitative risk management for next generation networks", "Authors": ["Starc I.", "Trcek D."], "Keywords": ["computer security", "economics of security", "risk management", "security measurement", "security metrics"], "Date": "2012", "Abstract": "While user dependence on ICT is rising and the information security situation is worsening at an alarming rate, IT industry is not able to answer accurately and in time questions like \"How secure is our information system?\" Consequently, information security risk management is reactive and is lagging behind incidents. To overcome this problem, risk management paradigm has to change from reactive to active and from qualitative to quantitative. In this section, we present a computerized risk management approach that enables active risk management and is aligned with the leading initiative to make security measurable and manageable. Furthermore, we point out qualitative methods deficiencies and argue about the importance of use of quantitative over qualitative methods in order to improve accuracy of information security feedback information. Finally, we present two quantitative metrics, used together in the model, and enabling a quantitative risk assessment and support risk treatment decision making. \u00a9 2012 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "0"},
{"Title": "Noncommuting graphs of matrices over semirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Matrices", "Noncommuting graph", "Semiring"], "Date": "2011", "Abstract": "We study diameters and girths of noncommuting graphs of semirings. For a noncommutative semiring that is either multiplicatively or additively cancellative, we find the diameter and the girth of its noncommuting graph and prove that it is Hamiltonian. Moreover, we find diameters and girths of noncommuting graphs of all nilpotent matrices over a semiring, all invertible matrices over a semiring, all noninvertible matrices over a semiring, and the full matrix semiring. In nearly all cases we prove that diameters are less than or equal to 2 and girths are less than or equal to 3, except in the case of 2\u00d72 nilpotent matrices. \u00a9 2010 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "UbiTtention: Smart & ambient notification and attention management", "Authors": ["Voit A.", "Henze N.", "Poppinga B.", "Gehring S.", "Weber D.", "Okoshi T.", "Bohmer M.", "Pejovic V."], "Keywords": ["Alerts", "Ambient Interfaces", "Attention", "Notifications"], "Date": "2016", "Abstract": "Users of digital devices are increasingly confronted with a tremendous amount of notifications that appear on multiple devices and screens in their environment. Today many users own different ubiquitous devices such as a smartphone, a tablet, a notebook and a smartwatch. If an email client is installed on every device an incoming email produces up to four notifications - one on each device. In the future, we will receive notifications from an increasing number of ubiquitous devices. Therefore, we need smart attention management for incoming notifications as well as novel ways to present and interact with notifications. One way for a less interrupting attention management could be the use of ambient representations of incoming notifications. This workshop brings together researchers and practitioners from academia and industry to explore how the flood of notifications on different computing devices and in smart environments can be managed, to avoid information overload.", "Language": "en", "Citations": "8"},
{"Title": "Improving the evaluation of software development methodology adoption and its impact on enterprise performance", "Authors": ["Vavpotic D.", "Hovelja T."], "Keywords": ["Enterprise Performance", "Evaluation Approach", "SDM Adoption", "Software Development Methodologies"], "Date": "2012", "Abstract": "Although the literature studying software development methodologies (SDMs) lists several significant positive effects of the deployment of SDMs, investments into SDMs by the enterprises remain relatively limited. Strategic investments decisions, such as SDMs investments, are mostly taken with the goal of improving enterprise performance. In this paper a model for evaluation of the adoption of SDMs that focuses on the abovementioned SDMs impact on enterprise performance is proposed. The model was empirically tested in four case studies in software development small and medium enterprises (SMEs) in Slovenia. The case studies confirmed that the use of the proposed model enabled SMEs to improve SDMs related investment and adoption decisions and enabled SMEs to invest their limited resources in the most productive and competitive way. The case study experience with the proposed model suggests that its use would also bring similar benefits to larger software development enterprises.", "Language": "en", "Citations": "11"},
{"Title": "Epistasis analysis with global transcriptional phenotypes", "Authors": ["Van Driessche N.", "Demsar J.", "Booth E.O.", "Hill P.", "Juvan P.", "Zupan B.", "Kuspa A.", "Shaulsky G."], "Keywords": [], "Date": "2005", "Abstract": "Classical epistasis analysis can determine the order of function of genes in pathways using morphological, biochemical and other phenotypes. It requires knowledge of the pathway's phenotypic output and a variety of experimental expertise and so is unsuitable for genome-scale analysis. Here we used microarray profiles of mutants as phenotypes for epistasis analysis. Considering genes that regulate activity of protein kinase A in Dictyostelium, we identified known and unknown epistatic relationships and reconstructed a genetic network with microarray phenotypes alone. This work shows that microarray data can provide a uniform, quantitative tool for large-scale genetic network analysis.", "Language": "en", "Citations": "84"},
{"Title": "A generalisation of model selection criteria", "Authors": ["Kverh B.", "Leonardis A."], "Keywords": ["Minimum description length", "Model selection", "Parametric models", "Range images", "Reverse engineering", "Segmentation"], "Date": "2004", "Abstract": "In this article we generalise some of the existing model selection criteria used in statistics and computer vision. Model selection criteria are mostly used to decide which model is more appropriate for explaining a specific data set. We adapt these criteria in a way that they can be used for the evaluation of models fitted to different data sets and for the evaluation of sets of models. We found this adaption especially useful in situations where we have a set of models with domains that may overlap and where we need to find an optimal subset of models for explaining the whole data set. Adapted model selection criteria were then succesfully used in range image segmentation application (reverse engineering), based on the recover-and-select paradigm.", "Language": "en", "Citations": "4"},
{"Title": "LMS in the pre-school education program", "Authors": ["Kljun M.", "Brodnik A.", "Starcic A.I."], "Keywords": [], "Date": "2006", "Abstract": "Traditional learning environments are today placed face to face with Information and Communication Technologies (ICT) which gradually ambush visible place in education. For this reason it is important for both students and tutors to be capable to use new methods of supplying and receiving the knowledge. One of possible methods are online Learning Management Systems (LMS) which incorporate principles of social constructionism which is based on the idea that we all better acquire new knowledge within a social process when communicating facts to others and base new knowledge on already known facts. We introduced such online learning environment to a first year students at the Faculty of Education with no previous knowledge or little knowledge of ICT usage. We tried to find out how much time a future preschool teachers need to gain satisfying level of digital literacy and competences for learning and teaching skills with Information and Educational Technology (IET).", "Language": "en", "Citations": "1"},
{"Title": "Capturing the mood: Evaluation of the moodstripe and moodgraph interfaces", "Authors": ["Pesek M.", "Godec P.", "Poredos M.", "Strle G.", "Guna J.", "Stojmenova E.", "Pogacnik M.", "Marolt M."], "Keywords": ["color perception", "mood estimation", "music information retrieval", "questionnaire evaluation"], "Date": "2014", "Abstract": "This study presents an evaluation of two interfaces for gathering user feedback in online surveys. We evaluated the intuitiveness, usability and time complexity of the proposed interfaces in comparison to the more standard approaches. Over 900 users first participated in an online survey exploring the influence of mood on their emotional responses to music and colors. We included several new interfaces in this survey, so after it was completed, users were asked to complete a second survey where they evaluated various aspects of the interfaces. Our analysis shows reduced time complexity and increased intuitiveness of the new interfaces, compared to standard approaches, resulting in lower mental difficulty and frustration of participants.", "Language": "en", "Citations": "3"},
{"Title": "Robust localization using eigenspace of spinning-images", "Authors": ["Jogan M.", "Leonardis A."], "Keywords": [], "Date": "2000", "Abstract": "Under in-plane rotations of a panoramic camera, the information content of a panoramic image is, in general, preserved. However, different representations that can be derived have important implications on further processing, e.g. for appearance-based localisation. We discuss several approaches based on different representations that have been proposed and evaluate them from different points-of-view, in particular, we argue that most of them are not suitable for robust localization under partially occluded views. In this paper we propose a representation-eigenspace of spinning-images-which enables a straightforward application of the robust estimation of eigenimage coefficients which is directly related to the localization.", "Language": "en", "Citations": "43"},
{"Title": "Employing artificial neural networks and regression in analysis on knowledge about sweet potato (Ipomoea batatas L.) in Slovenia", "Authors": ["Kunstelj N.", "Znidarcic D.", "Ster B."], "Keywords": ["Artificial neural networks", "Questionnaire", "Regression", "Slovenia", "Sweet potato"], "Date": "2013", "Abstract": "This article analyses factors affecting the reputation of sweet potato (Ipomoea batatas L.) among people in Slovenia. The inquiry, which included 7 general questions and 19 questions in particular about sweet potato, was completed by 712 respondents. The aim was to find out which factors impact the knowledge about sweet potato, the relations between answers to various questions regarding sweet potato features and willingness of people to know, to buy and to grow it. The methods applied were the Radial basis function neural networks and multiple linear and logistic regressions. It was established that persons with agricultural education are experts and know sweet potato best. Persons from large families are also familiar with it, but to a smaller degree. The answers to 8 questions about sweet potato features were very consistent, since we found out that every answer can be predicted with 98% probability (on the basis of the answers to the other 7 questions). Significant covariates in regression show that the most likely persons to know/buy/grow sweet potato are the people with agricultural education. Older persons are more interested in curative features of sweet potato, while younger and better educated believe in stronger nutritional values. Female respondents are more likely to grow sweet potato than men. Net income also influences willingness to buy sweet potato, because people living with children are more likely to be willing to attend free lectures about sweet potato.", "Language": "en", "Citations": "5"},
{"Title": "Information extraction from receipts using machine learning", "Authors": ["Karlovcec M."], "Keywords": ["Classification", "Dataset", "Information extraction", "Labeling", "Machine learning"], "Date": "2011", "Abstract": "Automated information extraction from receipts can help us to easier organize our expenses. Approach to using Machine learning algorithms for information extraction is introduced. Datasets are generated using the developed application which enables labeling of textual documents. Results of applying the Machine learning algorithms on the datasets show usefulness of the approach.", "Language": "en", "Citations": "0"},
{"Title": "Improving speech recognition robustness using non-standard windows", "Authors": ["Rozman R.", "Kodek D.M."], "Keywords": ["Robustness", "Speech processing", "Speech recognition", "Windowing"], "Date": "2003", "Abstract": "Windowing problem of the short-time frequency analysis in Speech recognition systems (SRS) is considered. Design possibilities for different nonstandard window sequences are presented. Traditional \"digital filtering\" approach to the design of finite window sequences with linear and nonlinear phase response is examined. Since human hearing is relatively insensitive to phase distortions of speech signal, some other ideas of alternative windows with nonlinear phase response are also investigated. Two most promising design methods for nonlinear phase windows are discussed. Practical performance comparison of such windows with the Hamming window on two real SRS is presented. They show that the nonstandard window sequences can contribute to greater SRS robustness. An additional research on non-standard windows and parametrization process as a whole is suggested.", "Language": "en", "Citations": "5"},
{"Title": "Feasibility of spirography features for objective assessment of motor function in Parkinson's disease", "Authors": ["Sadikov A.", "Groznik V.", "Mozina M.", "Zabkar J.", "Nyholm D.", "Memedi M.", "Bratko I.", "Georgiev D."], "Keywords": ["Movement disorder", "Objective monitoring", "Parkinson's disease", "Spirography", "Spirography features", "Visualisation"], "Date": "2017", "Abstract": "Objective Parkinson's disease (PD) is currently incurable, however proper treatment can ease the symptoms and significantly improve the quality of life of patients. Since PD is a chronic disease, its efficient monitoring and management is very important. The objective of this paper was to investigate the feasibility of using the features and methodology of a spirography application, originally designed to detect early Parkinson's disease (PD) motoric symptoms, for automatically assessing motor symptoms of advanced PD patients experiencing motor fluctuations. More specifically, the aim was to objectively assess motor symptoms related to bradykinesias (slowness of movements occurring as a result of under-medication) and dyskinesias (involuntary movements occurring as a result of over-medication). Materials and methods This work combined spirography data and clinical assessments from a longitudinal clinical study in Sweden with the features and pre-processing methodology of a Slovenian spirography application. The study involved 65 advanced PD patients and over 30,000 spiral-drawing measurements over the course of three years. Machine learning methods were used to learn to predict the \u201ccause\u201d (bradykinesia or dyskinesia) of upper limb motor dysfunctions as assessed by a clinician who observed animated spirals in a web interface. The classification model was also tested for comprehensibility. For this purpose a visualisation technique was used to present visual clues to clinicians as to which parts of the spiral drawing (or its animation) are important for the given classification. Results Using the machine learning methods with feature descriptions and pre-processing from the Slovenian application resulted in 86% classification accuracy and over 0.90 AUC. The clinicians also rated the computer's visual explanations of its classifications as at least meaningful if not necessarily helpful in over 90% of the cases. Conclusions The relatively high classification accuracy and AUC demonstrates the usefulness of this approach for objective monitoring of PD patients. The positive evaluation of computer's explanations suggests the potential use of this methodology in a decision support setting.", "Language": "en", "Citations": "5"},
{"Title": "Maximum exploratory equivalence in trees", "Authors": ["Furst L.", "Ibej U.C.", "Mihelic J."], "Keywords": [], "Date": "2015", "Abstract": "Many practical problems are modeled with networks and graphs. Their exploration is of significant importance, and several graph-exploration algorithms already exist. In this paper, we focus on a type of vertex equivalence, called exploratory equivalence, which has a great potential to speed up such algorithms. It is an equivalence based on graph automorphisms and can, for example, help us in solving the subgraph isomorphism problem, which is a well-known NP-hard problem. In particular, if a given pattern graph has nontrivial automorphisms, then each of its nontrivial exploratory equivalent classes gives rise to a set of constraints to prune the search space of solutions. In the paper, we define the maximum exploratory equivalence problem. We show that the defined problem is at least as hard the graph isomorphism problem. Additionally, we present a polynomial-time algorithm for solving the problem when the input is restricted to tree graphs. Furthermore, we show that for trees, a maximum exploratory equivalent partition leads to a globally optimal set of subgraph isomorphism constraints, whereas this is not necessarily the case for general graphs.", "Language": "en", "Citations": "1"},
{"Title": "Towards complex event aware services as part of SOA", "Authors": ["Potocnik M.", "Juric M.B."], "Keywords": ["Complex event processing", "event driven architecture", "service oriented architecture", "web services"], "Date": "2014", "Abstract": "Complex Event Processing (CEP) has so far been implemented in technology and vendor-specific manner. Introducing CEP concepts to the Service Oriented Architecture (SOA) provides an opportunity to enhance the capabilities of SOA. We define a model that supports the CEP usage in SOA where the actual pattern recognition can be done by any external CEP Engine. We define a new service type - a Complex Event Aware (CEA) service that automatically reacts to complex events specified in its interface. The proposed model includes a CEP Manager that provides centralized management of complex events and, through its pluggable adapters, communicates with CEA Services and CEP Engines. It includes a CEP Registry and a CEP Repository enabling versioning and reuse of complex event types, and a CEP Dispatcher providing publish/subscribe communication framework. We design a generic XML schema for abstract complex event type definition and propose new extensions for Service Component Architecture (SCA) and Web Services Description Language (WSDL) specifications, which enable definitions of complex event types and complex event sinks in the CEA Service interface. As a proof-of-concept, we develop a prototype implementation for the largest national telecommunication provider and in the real-world scenario show the advantages of the proposed model.", "Language": "en", "Citations": "10"},
{"Title": "Extremal matrix centralizers", "Authors": ["Dolinar G.", "Guterman A.", "Kuzma B.", "Oblak P."], "Keywords": ["Centralizers", "Matrix algebra"], "Date": "2013", "Abstract": "Matrices with maximal or minimal centralizers are classified over fields with sufficiently large orders. \u00a9 2012 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "13"},
{"Title": "Internet based art installations", "Authors": ["Solina F."], "Keywords": [], "Date": "2000", "Abstract": "An overview of some internet-related computer applications which were used in several art presentations and art installations on the Internet is given. The technical solutions range from the design of typical hypertext contents combining text and images, creation of virtual environments, sending of life video images over the Internet and controlling remote robotic devices over the Internet. These technical solutions were successfully used for presentations of classical fine arts on the Internet as well as for creation of contemporary art installations.", "Language": "en", "Citations": "6"},
{"Title": "Individual prediction reliability estimates in classification and regression", "Authors": ["Pevec D.", "Bosnic Z.", "Kononenko I."], "Keywords": [], "Date": "2012", "Abstract": "Current machine learning algorithms perform well in many problem domains, but in risk-sensitive decision making - for example, in medicine and finance - experts do not rely on common evaluation methods that provide overall assessments of models because such techniques do not provide any information about single predictions. This chapter summarizes the research areas that have motivated the development of various approaches to individual prediction reliability. Based on these motivations, the authors describe six approaches to reliability estimation: inverse transduction, local sensitivity analysis, bagging variance, local cross-validation, local error modelling, and density-based estimation. Empirical evaluation of the benchmark datasets provides promising results, especially for use with decision and regression trees. The testing results also reveal that the reliability estimators exhibit different performance levels when used with different models and in different domains. The authors show the usefulness of individual prediction reliability estimates in attempts to predict breast cancer recurrence. In this context, estimating prediction reliability for individual predictions is of crucial importance for physicians seeking to validate predictions derived using classification and regression models. \u00a9 2012, IGI Global.", "Language": "en", "Citations": "1"},
{"Title": "On the origin and features of an evolved boolean model for subcellular signal transduction systems", "Authors": ["Ster B.", "Avbelj M.", "Jerala R.", "Dobnikar A."], "Keywords": ["Genetic algorithms", "Regression", "Simulation", "Subcellular networks"], "Date": "2011", "Abstract": "In this paper we deal with the evolved Boolean model of the subcellular network for a hypothetical subcellular task that performs some of the basic cellular functions. The Boolean network is trained with a genetic algorithm and the obtained results are analyzed. We show that the size of the evolved Boolean network relates strongly to the task, that the number of output combinations is decreased, which is in concordance with the biological (measured) networks, and that the number of non-canalyzing inputs is increased, which indicates its specialization to the task. We conclude that the structure of the evolved network is biologically relevant, since it incorporates properties of evolved biological systems. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "2"},
{"Title": "System for active video observation over the Internet", "Authors": ["Batagelj B.", "Peer P.", "Solina F."], "Keywords": ["live video", "network camera control", "panoramic view", "user interface"], "Date": "2002", "Abstract": "We have designed a system with an intuitive user interface for remote camera control and image-based queries over the Internet. While searching for present solutions we realized the importance of a well-designed user interface. We developed a system, which enables remote observation and remote control of the JVC network camera over the Internet. The user interface is based on the combination of live video and a static panoramic view of a remote location. It provides a complete overview of a remote location and significantly simplifies the control over the Internet. By interactively moving a rectangular frame in the panoramic picture, the user locally selects the new direction of the camera. Visual summaries of activities at the observed location can be generated as well as custom queries with a simple user interface over the Internet.", "Language": "en", "Citations": "0"},
{"Title": "Small triangle-free configurations of points and lines", "Authors": ["Boben M.", "Grunbaum B.", "Pisanski T.", "Zitnik A."], "Keywords": [], "Date": "2006", "Abstract": "In the paper we show that all combinatorial triangle-free configurations for v \u2264 18 are geometrically realizable. We also show that there is a unique smallest astral (18", "Language": "en", "Citations": "12"},
{"Title": "Designing content-driven intelligent notification mechanisms for mobile applications", "Authors": ["Mehrotra A.", "Musolesi M.", "Hendley R.", "Pejovic V."], "Keywords": ["Context-Aware Computing", "Interruptibility", "Mobile Sensing", "Notifications"], "Date": "2015", "Abstract": "An increasing number of notifications demanding the smartphone user's attention, often arrive at an inappropriate moment, or carry irrelevant content. In this paper we present a study of mobile user interruptibility with respect to notification content, its sender, and the context in which a notification is received. In a real-world study we collect around 70,000 instances of notifications from 35 users. We group notifications according to the applications that initiated them, and the social relationship between the sender and the receiver. Then, by considering both content and context information, such as the current activity of a user, we discuss the design of classifiers for learning the most opportune moment for the delivery of a notification carrying a specific type of information. Our results show that such classifiers lead to a more accurate prediction of users' interruptibility than an alternative approach based on user-defined rules of their own interruptibility.", "Language": "en", "Citations": "58"},
{"Title": "FragViz: Visualization of fragmented networks", "Authors": ["Stajdohar M.", "Mramor M.", "Zupan B.", "Demsar J."], "Keywords": [], "Date": "2010", "Abstract": "Background: Researchers in systems biology use network visualization to summarize the results of their analysis. Such networks often include unconnected components, which popular network alignment algorithms place arbitrarily with respect to the rest of the network. This can lead to misinterpretations due to the proximity of otherwise unrelated elements.Results: We propose a new network layout optimization technique called FragViz which can incorporate additional information on relations between unconnected network components. It uses a two-step approach by first arranging the nodes within each of the components and then placing the components so that their proximity in the network corresponds to their relatedness. In the experimental study with the leukemia gene networks we demonstrate that FragViz can obtain network layouts which are more interpretable and hold additional information that could not be exposed using classical network layout optimization algorithms.Conclusions: Network visualization relies on computational techniques for proper placement of objects under consideration. These algorithms need to be fast so that they can be incorporated in responsive interfaces required by the explorative data analysis environments. Our layout optimization technique FragViz meets these requirements and specifically addresses the visualization of fragmented networks, for which standard algorithms do not consider similarities between unconnected components. The experiments confirmed the claims on speed and accuracy of the proposed solution. \u00a9 2010 \u0160tajdohar et al; licensee BioMed Central Ltd.", "Language": "en", "Citations": "3"},
{"Title": "Automated chess tutor", "Authors": ["Sadikov A.", "Mozina M.", "Guid M.", "Krivec J.", "Bratko I."], "Keywords": [], "Date": "2007", "Abstract": "While recently the strength of chess-playing programs has grown immensely, their capability of explaining in human understandable terms why some moves are good or bad has enjoyed little attention. Progress towards programs with an ability to provide intelligent commentary on chess games, either played by a program or by a human, has been negligible in comparison with the progress concerning playing strength. The typical style of a program's \"comments\" (in terms of the best variations and their numerical scores) is of little use to a human who wants to learn important concepts behind the variations. In this paper, we present some core mechanisms for automated commenting in terms of relevant goals to be achieved or preserved in a given position. By combining these mechanisms with an actual chess engine we were able to transform this engine into a chess tutor/annotator that is capable of generating rather intelligent commentary. The main advantages of our work over related approaches are: (a) it has the ability to act as a tutor for the whole game of chess, and (b) it has a relatively solid chess understanding and is thus able to adequately comment on positional aspects. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "2"},
{"Title": "Fuzzy urban traffic signal control - An overview", "Authors": ["Malej A.", "Brodnik A."], "Keywords": ["Distributed control", "Fuzzy logic", "Hybrid systems", "Traffic signal control"], "Date": "2007", "Abstract": "In the last years, there have been many attempts made to improve urban traffic signal control systems as this is one of the most cost-effective ways to improve the traffic flow through a network of intersections. Besides the traditional methods for traffic signal control that are usually based on a traffic flow model, the development of new systems started to consider the various emerging technologies including artificial intelligence. Application of metaheuristic methods has proven to be worth of being researched. One of them is particularly fuzzy logic when combined with methods for system optimization. It offers a greater degree of flexibility, adaptability and above all the possibility of handling uncertain information and dealing with conflicting situations. We made an overview of the research that has been done so far in this area.", "Language": "en", "Citations": "3"},
{"Title": "Automatic digitization of pluviograph strip charts", "Authors": ["Jaklic A.", "Sajn L.", "Derganc G.", "Peer P."], "Keywords": ["automatic extraction", "computer vision", "digitization", "pluviograph", "rainfall gauge"], "Date": "2016", "Abstract": "An algorithm for automatic digitization of pluviograph strip charts is presented. The rainfall signal is incrementally extracted from the scanned image of a strip chart by combining the moving average method and the curve edge following method. The mechanical properties of float-based rain gauge were used as constraints in the algorithm design. The algorithm was tested on 58 strip chart images. The comparison between the data derived from the algorithm and the data from the Slovenian Environment Agency shows that the algorithm produces an accurate rainfall time series except for the charts that contain ink smudges. Thus, the algorithm is well suited as a main component of an interactive system that would enable visual inspection of the detected rainfall curve and its possible adjustment.", "Language": "en", "Citations": "3"},
{"Title": "Assessment of machine learning reliability methods for quantifying the applicability domain of QSAR regression models", "Authors": ["Toplak M.", "Mocnik R.", "Polajnar M.", "Bosnic Z.", "Carlsson L.", "Hasselgren C.", "Demsar J.", "Boyer S.", "Zupan B.", "Staislring J."], "Keywords": [], "Date": "2014", "Abstract": "The vastness of chemical space and the relatively small coverage by experimental data recording molecular properties require us to identify subspaces, or domains, for which we can confidently apply QSAR models. The prediction of QSAR models in these domains is reliable, and potential subsequent investigations of such compounds would find that the predictions closely match the experimental values. Standard approaches in QSAR assume that predictions are more reliable for compounds that are \"similar\" to those in subspaces with denser experimental data. Here, we report on a study of an alternative set of techniques recently proposed in the machine learning community. These methods quantify prediction confidence through estimation of the prediction error at the point of interest. Our study includes 20 public QSAR data sets with continuous response and assesses the quality of 10 reliability scoring methods by observing their correlation with prediction error. We show that these new alternative approaches can outperform standard reliability scores that rely only on similarity to compounds in the training set. The results also indicate that the quality of reliability scoring methods is sensitive to data set characteristics and to the regression method used in QSAR. We demonstrate that at the cost of increased computational complexity these dependencies can be leveraged by integration of scores from various reliability estimation approaches. The reliability estimation techniques described in this paper have been implemented in an open source add-on package (https://bitbucket.org/biolab/ orange-reliability) to the Orange data mining suite. \u00a9 2014 American Chemical Society.", "Language": "en", "Citations": "18"},
{"Title": "Matrix formulation of the multilayered perceptron with a denoising unit", "Authors": ["Lotric U.", "Dobnikar A."], "Keywords": ["Denoising", "Neural networks", "Wavelet multiresolution analysis"], "Date": "2003", "Abstract": "A denoising unit, based on the wavelet multiresolution analysis, is integrated into the multilayered perceptron. It is shown that by its mathematical formulation the denoising unit is an analogy to the layers of the multilayered perceptron. Denoising is treated as a part of the model and the same cost function is used for setting all the free parameters, i. e., those of the denoising unit and those of the multilayered perceptron. It is illustrated that the proposed model outmatches the multilayered perceptron in noisy time-series prediction problems.", "Language": "en", "Citations": "2"},
{"Title": "Estimation of prediction reliability in regression based on a transductive approach", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": [], "Date": "2005", "Abstract": "The use of common regression models usually confronts us with a hindrance that we have no information on the output prediction reliability. Although the methods for evaluation of predictor's quality are widely used, they provide a quality estimate for the predictor as a whole and give no information about reliability of the individual predictions. In the past work these reliabilities were most commonly derived from various distributions observed on the learning set. As an alternative, our approach is based on a modified transductive principle. The testing results indicate that our reliability estimates significantly correlate with the prediction error in most tests. Copyright \u00a9 IICAI 2005.", "Language": "en", "Citations": "2"},
{"Title": "MFAM: Multiple frequency adaptive model-based indoor localization method", "Authors": ["Tuta J.", "Juric M.B."], "Keywords": ["Adaptive localization", "IEEE 802.11ah", "Indoor positioning", "Model-based localization", "Multi-frequency localization", "Propagation modeling"], "Date": "2018", "Abstract": "This paper presents MFAM (Multiple Frequency Adaptive Model-based localization method), a novel model-based indoor localization method that is capable of using multiple wireless signal frequencies simultaneously. It utilizes indoor architectural model and physical properties of wireless signal propagation through objects and space. The motivation for developing multiple frequency localization method lies in the future Wi-Fi standards (e.g., 802.11ah) and the growing number of various wireless signals present in the buildings (e.g., Wi-Fi, Bluetooth, ZigBee, etc.). Current indoor localization methods mostly rely on a single wireless signal type and often require many devices to achieve the necessary accuracy. MFAM utilizes multiple wireless signal types and improves the localization accuracy over the usage of a single frequency. It continuously monitors signal propagation through space and adapts the model according to the changes indoors. Using multiple signal sources lowers the required number of access points for a specific signal type while utilizing signals, already present in the indoors. Due to the unavailability of the 802.11ah hardware, we have evaluated proposed method with similar signals; we have used 2.4 GHz Wi-Fi and 868 MHz HomeMatic home automation signals. We have performed the evaluation in a modern two-bedroom apartment and measured mean localization error 2.0 to 2.3 m and median error of 2.0 to 2.2 m. Based on our evaluation results, using two different signals improves the localization accuracy by 18% in comparison to 2.4 GHz Wi-Fi-only approach. Additional signals would improve the accuracy even further. We have shown that MFAM provides better accuracy than competing methods, while having several advantages for real-world usage.", "Language": "en", "Citations": "2"},
{"Title": "The Singular Bivariate Quartic Tracial Moment Problem", "Authors": ["Bhardwaj A.", "Zalar A."], "Keywords": ["Affine linear transformations", "Flat extensions", "Moment matrix", "Non-commutative polynomial", "Truncated moment problem"], "Date": "2018", "Abstract": "The (classical) truncated moment problem, extensively studied by Curto and Fialkow, asks to characterize when a finite sequence of real numbers indexes by words in commuting variables can be represented with moments of a positive Borel measure \u03bc on R", "Language": "en", "Citations": "0"},
{"Title": "Are e-commerce users defenceless?", "Authors": ["Trampus M.", "Ciglaric M.", "Pancur M.", "Vidmar T."], "Keywords": ["Attack", "E-commerce", "Run time modification", "Security"], "Date": "2003", "Abstract": "We are interested in new ways of threats and attack on the e-commerce. The server side of e-commerce platform is usually very well protected and secured. Unfortunately, this is not true for the client side. End users are usually undereducated in the field of computer security. They use Internet clients such as Web browsers and e-mail programs to do their e-commerce business. Their platform that is used to run these programs can hardly be trusted. This paper focuses on the attacks on system and application infrastructure. The main idea of our approach is to take advantage of existing applications and attack them while they are executing. We analyze the steps that need to be taken in such attacks and point out the properties of the applications and execution environments that can be exploited. To demonstrate the findings, we present two case studies of such attacks. The first exploits a Web browser which uses SSL (Secure Sockets Layer) and the second an e-mail client which uses digital signatures. In both cases we are able to successfully perform the attack which escapes the end user's notice. In the final part of the paper we present a possible defence against such attack together with our work on a security enforcement system.", "Language": "en", "Citations": "1"},
{"Title": "A wireless sensor network for real-time monitoring of the living and working environment Brez\u017ei\u010dni sistem za spremljanje \u017eivljenjskega in delovnega okolja v realnem \u010dasu", "Authors": ["Cesnovar R.", "Spetic A."], "Keywords": [], "Date": "2015", "Abstract": "The indoor environment significantly affects our health, comfort and productivity. However in everyday life, we monitor indoor temperature and ignore other factors that can be affected and can tell us a lot about the indoor environment. That is why we developed CubeSensors as one of the first devices on the market providing instant and continuos monitoring of the important indoor factors: temperature, humidity, noise, illumination, air quality and barometric pressure. By affecting these factors and monitoring the changes, we can analyse the quality of indoor living, define the factors most affecting the quality of living in a particular room and decide what can we do about them. The device was designed for a continuous operation and with the end consumer in mind. By collecting the indoor data, we created a platform enabling us to monitor indoor environment and providing a potential for smart building management, like heating systems, HVAC systems, lights, managing windows, doors and shades, ventilation etc. Using the combined monitoring system, detailed indoor environment data is collected and different environmental factors in the same room are correlated.", "Language": "en", "Citations": "0"},
{"Title": "Feature extraction and shape representation of ambulatory electrocardiogram using the Karhunen-Lo\u00e8ve transform", "Authors": ["Jager F."], "Keywords": ["Ambulatory electrocardiogram", "Feature extraction", "Karhunen-Lo\u00e8ve transform", "Kernel-approximation method", "Shape representation"], "Date": "2002", "Abstract": "We developed a new approach to feature extraction and shape representation of QRS complex and ST segment pattern vectors of ambulatory electrocardiograms using the Karhunen-Lo\u00e8ve transform. We describe applying the Karhune-Lo\u00e8ve transform to the ambulatory electrocardiogram, derivation of robust covariance matrices using kernel-approximation method and with noisy patterns excluded, a method to estimate sufficient and necessary QRS complex and ST segment Karhunen-Lo\u00e8ve feature-vector dimensionality, and a technique to derive and represent time series of feature vectors.", "Language": "en", "Citations": "4"},
{"Title": "Beyond Standard Benchmarks: Parameterizing Performance Evaluation in Visual Object Tracking", "Authors": ["Zajc L.C.", "Lukezic A.", "Leonardis A.", "Kristan M."], "Keywords": [], "Date": "2017", "Abstract": "Object-to-camera motion produces a variety of apparent motion patterns that significantly affect performance of short-term visual trackers. Despite being crucial for designing robust trackers, their influence is poorly explored in standard benchmarks due to weakly defined, biased and overlapping attribute annotations. In this paper we propose to go beyond pre-recorded benchmarks with post-hoc annotations by presenting an approach that utilizes omnidirectional videos to generate realistic, consistently annotated, short-term tracking scenarios with exactly parameterized motion patterns. We have created an evaluation system, constructed a fully annotated dataset of omnidirectional videos and generators for typical motion patterns. We provide an in-depth analysis of major tracking paradigms which is complementary to the standard benchmarks and confirms the expressiveness of our evaluation approach.", "Language": "en", "Citations": "3"},
{"Title": "On basic embeddings of compacta into the plane", "Authors": ["Mramor-Kosta N.", "Trenklbrova E."], "Keywords": [], "Date": "2003", "Abstract": "A compactum K \u2208 \u211d", "Language": "pt", "Citations": "2"},
{"Title": "Predictive control of rubber mixing process based on neural network models", "Authors": ["Bratina M.", "Susteric Z.", "Ster B.", "Lottic U.", "Dobnikar A."], "Keywords": ["Mixing", "Neural networks", "Predictive control", "Rubber"], "Date": "2009", "Abstract": "The work presents an attempt to establish a closed-loop control system for mixing rubber compounds in internal mixers by means of soft computing methods in form of neural networks and methods of information theory. As the basic physical aspect to be controlled is chosen the course of compound viscosity during the mixing process, since it comprises both the features of the mixed materials and those of processing. Besides, viscosity is proportional to the rotor torque which is a continuously measurable quantity. Since too many factors influence viscosity, or rotor torque, to be modeled analytically, neural networks, together with information theory, have proved to be suitable method to create efficient means for viscosity course redirection in the case of deviations, some of which being unavoidable despite measures. Thus an on-line predictive control system is formed, enabling compound production of more uniform quality.", "Language": "en", "Citations": "0"},
{"Title": "Selective topological approach to mobile robot navigation with recurrent neural networks", "Authors": ["Vodopivec T.", "Ster B."], "Keywords": ["Mobile robot", "Motion planning", "Recurrent neural networks", "Topological modelling"], "Date": "2015", "Abstract": "In this paper, we use a special architecture of recurrent neural networks (RNNs) to enhance a topological approach to mobile robot navigation using RNNs. This architecture selectively latches presumably relevant input information and ignores presumably irrelevant input information. Simple types of reactive behaviour are supplemented with random decisions to switch between them at decision points. The RNN is trained on a sequence of sensory contents and actions. This well-known approach is applicable to multi-step prediction of sensory information and the travelled distances between decision points, given a sequence of decisions at decision points. Thus, the optimal path to a specified goal can be sought. A problem of this approach is that due to inherent inability to design a perfect reactive behaviour, unwanted situations may appear, such as redundant decision points, unreliable switching among behaviours. We demonstrate that the applied type of RNN lowers the impact of faulty decision points and thus improves the prediction.", "Language": "en", "Citations": "0"},
{"Title": "Idempotent matrices over antirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Antiring", "Conjugate action", "Idempotent matrix", "Linear preserver", "Semiring"], "Date": "2009", "Abstract": "We study the idempotent matrices over a commutative antiring. We give a characterization of idempotent matrices by digraphs. We study the orbits of conjugate action and find the cardinality of orbits of basic idempotents. Finally, we prove that invertible, linear and idempotent preserving operators on n \u00d7 n matrices over entire antirings are exactly conjugate actions for n \u2265 3. We also give a complete characterization of the 2 \u00d7 2 case. \u00a9 2009 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "5"},
{"Title": "Fast training of recurrent neural networks with the LRP reinforcement scheme Hitro u\u010denje rekurentnih nevronskih mre\u017e s korekcijsko shemo LRP", "Authors": ["Ster B."], "Keywords": ["Finite state automata", "Gradient learning", "Heuristic algorithms", "Learning automata", "Recurrent neural networks"], "Date": "2008", "Abstract": "Recurrent neural networks (RNNs) [3] are neural networks containing feedback connections which create loops and enable the functionality of memorizing. RNNs are able to solve time-dependent tasks, such as modeling dynamical systems, predicting time-series [4], learning sequences, induction of finite state automata [1, 2], etc. Two algorithms for training recurrent neural networks were compared, i.e. the standard gradient RTRL (Real Time Recurrent Learning) algorithm and heuristic LRP (Linear Reward-Penalty) algorithm, which is otherwise used as a reinforcement scheme in learning automata [5]. The RTRL algorithm [8] may be used in two variants: the batch RTRL, where the same sequence of input/output samples is applied continually for calculation of gradients and consequently for updating the weights, and the on-line RTRL, where the weight updating is done after each presented sample. The results on different variants of the delayed XOR(r) task at r=2, 3, 4 (see Figure 2 for r = 2 and Figure 3 for r = 3) and different RNN sizes show that LRP operates much faster than the batch RTRL (Table 1). The course of the mean squared error (MSE) of the batch RTRL and of the LRP algorithm is also quite different. Figure 4 shows the course of the MSE for different sizes of RNN. The batch RTRL requires a long time before the error begins decreasing; as it finally happens, it decreases quickly and smoothly. The MSE of LRP decreases more monotonically, but on the other hand much less smoothly. By using a moving window, LRP was also compared to the on-line RTRL. LRP was of a comparable speed in RNNs of up to 40 neurons, but much faster in larger RNNs (Table 2).", "Language": "en", "Citations": "0"},
{"Title": "Adaptive radial basis decomposition by learning vector quantization", "Authors": ["Ster B.", "Dobnikar A."], "Keywords": ["Decomposition", "Function approximation", "Mobile robot", "RBF neural network", "Reinforcement learning"], "Date": "2003", "Abstract": "A method for function approximation in reinforcement learning settings is proposed. The action-value function of the Q-learning method is approximated by the radial basis function neural network and learned by the gradient descent. Those radial basis units that are unable to fit the local action-value function exactly enough are decomposed into new units with smaller widths. The local temporal-difference error is modelled by a two-class learning vector quantization algorithm, which approximates distributions of the positive and of the negative error and provides the centers of the new units. This method is especially convenient in cases of smooth value functions with large local variation in certain parts of the state space, such that non-uniform placement of basis functions is required. In comparison with four related methods, it has the smallest requirements of basis functions when achieving a comparable accuracy.", "Language": "en", "Citations": "4"},
{"Title": "Assessing the effectiveness of real-world network simplification", "Authors": ["Blagus N.", "Subelj L.", "Bajec M."], "Keywords": ["Complex networks", "Merging", "Network simplification", "Sampling", "Simplification effectiveness"], "Date": "2014", "Abstract": "Many real-world networks are large, complex and thus hard to understand, analyze or visualize. Data about networks are not always complete, their structure may be hidden, or they may change quickly over time. Therefore, understanding how an incomplete system differs from a complete one is crucial. In this paper, we study the changes in networks submitted to simplification processes (i.e., reduction in size). We simplify 30 real-world networks using six simplification methods and analyze the similarity between the original and simplified networks based on the preservation of several properties, for example, degree distribution, clustering coefficient, betweenness centrality, density and degree mixing. We propose an approach for assessing the effectiveness of the simplification process to define the most appropriate size of simplified networks and to determine the method that preserves the most properties of original networks. The results reveal that the type and size of original networks do not affect the changes in the networks when submitted to simplification, whereas the size of simplified networks does. Moreover, we investigate the performance of simplification methods when the size of simplified networks is 10% that of the original networks. The findings show that sampling methods outperform merging ones, particularly random node selection based on degree and breadth-first sampling. \u00a9 2014 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "4"},
{"Title": "Time-series modeling using information-theory techniques Modeliranje \u010dasovnih vrst z metodami teorije informacij", "Authors": ["Bratina M.", "Dobnikar A.", "Lotric U."], "Keywords": ["Information theory", "Neural networks", "Pre-processing", "Time-series modeling"], "Date": "2009", "Abstract": "The paper analyzes the possibility of using measures originating from the information theory in modeling dynamic systems from time series. Our modeling was based on a multilayered perceptron neural network into which several information measures were integrated. The presented information measures are based on the Shannon's definitions of entropy and the divergence given in Eqs. (1) and (2). To simplify the extremely computationally intensive methods, the Renyi's counterparts defined by Eqs. (6) and (7) are preferred instead. For example, the information potential given in (9) can be calculated even without integrations. Besides the information potential, the Renyi's approximations of the measures presented by Eqs. (3) and (5) are very useful in modeling. The measures originating from the information theory were included in two ways: in the data preprocessing and as the criterion function during the process of neural network learning. The basic aim of preprocessing is to reduce the number of inputs to the model and/or reformulate them and thus improve the model performance and generalization ability. Besides classical methods for feature extraction, including the appropriate number of the last values in a time series that results in the best model (H1), the greedy algorithm for finding the combination of inputs, enabling the best model performance (H2) or making a linear combination of original inputs using the principal component analysis (PCA), two methods based on the information theory were tested: the independent component analysis (ICA) and the maximally discriminative projection (MDP). The neural network learning is usually based on minimizing the mean square error \u03b5(E). As an alternative, maximizing information potential V", "Language": "en", "Citations": "0"},
{"Title": "Multiobjective discovery of human-like driving strategies", "Authors": ["Dovgan E.", "Sodnik J.", "Bratko I.", "Filipic B."], "Keywords": ["Driving strategy", "Fuel consumption", "Human driving", "Multiobjective optimization", "Traveling time"], "Date": "2017", "Abstract": "Human driving models aim at producing human-like driving strategies by mimicking the behavior of drivers. Drivers optimize several objectives when traveling along a route, such as the traveling time and the fuel consumption. However, these objectives are not taken into account when building human driving models. To overcome this shortcoming, we designed a two-level Multiobjective Optimization algorithm for discovering Human-like Driving Strategies (MOHDS) that combines the human driving models with the optimization of the traveling time and the fuel consumption. Consequently, MOHDS enables to simultaneously mimic human driving behavior and optimize relevant driving objectives. MOHDS was tested on a two-lane rural route and compared to the existing approaches for human driving modeling. The results show that, unlike the existing approaches, MOHDS finds the driving strategies with various tradeoffs between the objectives.", "Language": "en", "Citations": "0"},
{"Title": "A self-adaptive model-based Wi-Fi indoor localization method", "Authors": ["Tuta J.", "Juric M.B."], "Keywords": ["Indoor positioning", "Propagation model", "Received signal strength (RSS)", "Self-adaptive", "Wi-Fi localization"], "Date": "2016", "Abstract": "This paper presents a novel method for indoor localization, developed with the main aim of making it useful for real-world deployments. Many indoor localization methods exist, yet they have several disadvantages in real-world deployments\u2014some are static, which is not suitable for long-term usage, some require costly human recalibration procedures, and others require special hardware such as Wi-Fi anchors and transponders. Our method is self-calibrating and self-adaptive thus maintenance free and based on Wi-Fi only. We have employed two well-known propagation models\u2014free space path loss and ITU models\u2014which we have extended with additional parameters for better propagation simulation. Our self-calibrating procedure utilizes one propagation model to infer parameters of the space and the other to simulate the propagation of the signal without requiring any additional hardware beside Wi-Fi access points, which is suitable for real-world usage. Our method is also one of the few model-based Wi-Fi only self-adaptive approaches that do not require the mobile terminal to be in the access-point mode. The only input requirements of the method are Wi-Fi access point positions, and positions and properties of the walls. Our method has been evaluated in single-and multi-room environments, with measured mean error of 2-3 and 3-4 m, respectively, which is similar to existing methods. The evaluation has proven that usable localization accuracy can be achieved in real-world environments solely by the proposed Wi-Fi method that relies on simple hardware and software requirements.", "Language": "en", "Citations": "8"},
{},
{"Title": "Capability Maturity Model (CMM) application in small software organizations Uporaba modela CMM v majhnih organizacijah za razvoj programske opreme", "Authors": ["Rozanc I.", "Mahnic V."], "Keywords": ["Capability Maturity Model (CMM)", "Personal Software Process (PSP)", "Software process improvement", "Team Software Process (TSP)"], "Date": "2003", "Abstract": "CMM (Figure 1) is a successful but difficult to use quality model. As such it is appropriate especially for large software organizations. However, as both big and small organizations have similar problems in the software development process, CMM principles seem to be ideal for software improvement in organizations of any size, too. It is therefore a good idea to implement CMM in small organizations, too. In the paper we present three ways to adapt CMM to small organizations. We can use one of the two existing process models built on CMM principles. The Personal Software Process (PSP) defines the process for an individual engineer (Figure 2) in a software organization. The Team Software Process (TSP) upgrades PSP and defines the process for a team (Figure 3). In the third solution we propose adaptation of CMM that includes PSP and TSP as well as characteristics of small organizations. An adaptive software process (Figure 4) fulfilling requirements of CMM level 3 is described and a measurement model is proposed in order to achieve CMM level 4.", "Language": "en", "Citations": "0"},
{"Title": "Markov random field model for segmenting large populations of lipid vesicles from micrographs", "Authors": ["Zupanc J.", "Drobne D.", "Ster B."], "Keywords": ["Cell segmentation", "Image processing", "Image segmentation", "Lipid vesicle", "Markov random field", "Nanoparticles"], "Date": "2011", "Abstract": "Giant unilamellar lipid vesicles, artificial replacements for cell membranes, are a promising tool for in vitro assessment of interactions between products of nanotechnologies and biological membranes. However, the effect of nanoparticles can not be derived from observations on a single specimen, vesicle populations should be observed instead. We propose an adaptation of the Markov random field image segmentation model which allows detection and segmentation of numerous vesicles in micrographs. The reliability of this model with different lighting, blur, and noise characteristics of micrographs is examined and discussed. Moreover, the automatic segmentation is tested on micrographs with thousands of vesicles and the result is compared to that of manual segmentation. The segmentation step presented is part of a methodology we are developing for bio-nano interaction assessment studies on lipid vesicles. \u00a9 2011 Informa Healthcare USA, Inc.", "Language": "en", "Citations": "5"},
{"Title": "The computational beauty of flocking: Boids revisited", "Authors": ["Bajec I.L.", "Zimic N.", "Mraz M."], "Keywords": ["Animat", "Artificial life", "Bird", "Boid", "Flock", "Moore automaton"], "Date": "2007", "Abstract": "Artificial-life research was founded in the mid-1980s. It promotes the idea of the bottom-up research approach, where only the basic units of a situation and their local interaction are modelled, and then the system is left to evolve. However, the notable progress of the processing power of personal computers, evident in the last two decades, has had little influence on the ways the basic units (artificial animals or animats) are constructed. This impacts largely on the applicability of the methods in other research fields. Our field of choice is the modelling of bird flocks. This area was at its peak in the late 1980s when Craig W. Reynolds presented the first and most influential model-the boids. In spite of his many following works no formal definition has ever been presented. This might be the reason why a second generation of flocking models is still awaited. In this article we make a step forward, all in view of allowing for the development of the second-generation models. We present an artificial animal construction framework that has been obtained as a generalization of the existing bird flocking models, but is not limited to them. The article thus presents a formal definition of the framework and gives an example of its use. In the latter the framework is employed to present a formalization of Reynolds's boids.", "Language": "en", "Citations": "9"},
{"Title": "Explanation and reliability of prediction models: The case of breast cancer recurrence", "Authors": ["Strumbelj E.", "Bosnic Z.", "Kononenko I.", "Zakotnik B.", "Kuhar C.G."], "Keywords": ["Breast cancer", "Classification explanation", "Data mining", "Machine learning", "Prediction reliability"], "Date": "2010", "Abstract": "In this paper, we describe the first practical application of two methods, which bridge the gap between the non-expert user and machine learning models. The first is a method for explaining classifiers' predictions, which provides the user with additional information about the decision-making process of a classifier. The second is a reliability estimation methodology for regression predictions, which helps the users to decide to what extent to trust a particular prediction. Both methods are successfully applied to a novel breast cancer recurrence prediction data set and the results are evaluated by expert oncologists. \u00a9 2009 Springer-Verlag London Limited.", "Language": "en", "Citations": "23"},
{"Title": "Adding discriminative power to a generative hierarchical compositional model using histograms of compositions", "Authors": ["Tabernik D.", "Leonardis A.", "Boben M.", "Skocaj D.", "Kristan M."], "Keywords": ["Discriminative features", "Feature sharing", "Hierarchical compositional model", "HoC", "LHOP"], "Date": "2015", "Abstract": "Abstract In this paper we identify two types of problems with excessive feature sharing and the lack of discriminative learning in hierarchical compositional models: (a) similar category misclassifications and (b) phantom detections in background objects. We propose to overcome those issues by fully utilizing a discriminative features already present in the generative models of hierarchical compositions. We introduce descriptor called histogram of compositions to capture the information important for improving discriminative power and use it with a classifier to learn distinctive features important for successful discrimination. The generative model of hierarchical compositions is combined with the discriminative descriptor by performing hypothesis verification of detections produced by the hierarchical compositional model. We evaluate proposed descriptor on five datasets and show to improve the misclassification rate between similar categories as well as the misclassification rate of phantom detections on backgrounds. Additionally, we compare our approach against a state-of-the-art convolutional neural network and show to outperform it under significant occlusions.", "Language": "en", "Citations": "2"},
{"Title": "A tool for measurement of innovation newness and adoption in tourism firms", "Authors": ["Krizaj D.", "Brodnik A.", "Bukovec B."], "Keywords": ["Adoption of innovations", "Innovation taxonomy", "Tourism innovation"], "Date": "2014", "Abstract": "The paper focuses on the newness characteristic of realized innovations and their adoption in tourism firms. For that purpose it investigates three research problems: (i) measurement of newness level and adoption of tourism innovations; (ii) definition of tourism innovations taxonomy (needed for the measurement); and (iii) statistical analysis of innovations' adoption in tourism destinations (result of the measurement). The main aim of the research was to develop and validate the tool used for such measurements. The tool should help researchers and managers in tracking and benchmarking how innovative tourism firms are. \u00a9 2012 John Wiley & Sons, Ltd.", "Language": "en", "Citations": "30"},
{"Title": "Wireless corner: RFID-based traceability along the food-production chain", "Authors": ["Rajo-Iglesias E.", "Cuinas I.", "Newman R.", "Trebar M.", "Catarinucci L.", "Melcon A.A."], "Keywords": ["food technology", "from farm to fork", "Radiofrequency identification", "RFID", "supply chain management", "traceability", "wireless sensor networks"], "Date": "2014", "Abstract": "This contribution explains and analyzes the use of RFID (radio-frequency identification) for defining a complete traceability system applied to the food-production chain. The paper contains a summary of the actual work developed to test the ability of radio technologies to perform traceability at different food companies in a variety of sectors: wine, fish, and meat. Each pilot experience is explained, with special emphasis on the radio segment implemented by RFID technologies and sensors, whether connected by wired or as elements of a wireless sensor network. The application of the new RFID-based system at the three investigated sectors, and the return on investment that the companies could obtain by its usage, are the core of the paper. \u00a9 2014 IEEE.", "Language": "en", "Citations": "10"},
{"Title": "A measure for a balanced workload and its extremal values", "Authors": ["Govorcin J.", "Skrekovski R.", "Vukasinovic V.", "Vukicevic D."], "Keywords": ["Betweenness centrality", "Centrality measures", "Social networks"], "Date": "2016", "Abstract": "In order to measure the extent to which the distribution of workload between actors in the network can be equalized, a degree-weighted measure for a balanced workload based on betweenness centrality is introduced. The goal of this study is to determine the extremal values of the introduced measure, as well as the graph structures where the extremal values are attained. Several real world networks were used for evaluation of the new invariant. The obtained results are used for statistical comparison with standard measures of centrality to demonstrate validity of the introduced measure.", "Language": "en", "Citations": "0"},
{"Title": "Toll-like receptor 4 senses oxidative stress mediated by the oxidation of phospholipids in extracellular vesicles", "Authors": ["Maneek-Keber M.", "Frank-Bertoncelj M.", "Hafner-Bratkovie I.", "Smole A.", "Zorko M.", "Pirher N.", "Hayer S.", "Kralj-Iglie V.", "Rozman B.", "Ilc N.", "Horvat S.", "Jerala R."], "Keywords": [], "Date": "2015", "Abstract": "Oxidative stress produced in response to infection or sterile injury activates the innate immune response. We found that extracellular vesicles (EVs) isolated from the plasma of patients with rheumatoid arthritis or secreted from cells subjected to oxidative stress contained oxidized phospholipids that stimulated cells expressing Toll-like receptor 4 (TLR4) in a manner dependent on its co-receptor MD-2. EVs from healthy subjects or reconstituted synthetic EVs subjected to limited oxidation gained the ability to stimulate TLR4-expressing cells, whereas prolonged oxidation abrogated this property. Furthermore, we found that 15-lipoxygenase generated hydro(pero)xylated phospholipids that stimulated TLR4-expressing cells. Molecular modeling suggested that the mechanism of activation of TLR4 by oxidized phospholipids in EVs was structurally similar to that of the TLR4 ligand lipopolysaccharide (LPS). This was supported by experiments showing that EV-mediated stimulation of cells required MD-2, that mutations that block LPS binding to TLR4 abrogated the stimulatory effect of EVs, and that EVs induced TLR4 dimerization. On the other hand, analysis of gene expression profiles showed that genes encoding factors that resolve inflammation were more abundantly expressed in responses to EVs than in response to LPS. Together, these data suggest that EVs act as an oxidative stress-induced endogenous danger signal that underlies the pervasive role of TLR4 in inflammatory diseases.", "Language": "en", "Citations": "34"},
{"Title": "An expert system for detecting automobile insurance fraud using social network analysis", "Authors": ["Subelj L.", "Furlan S.", "Bajec M."], "Keywords": ["Assessment propagation", "Automobile insurance", "Fraud detection", "Link analysis", "Social network analysis"], "Date": "2011", "Abstract": "The article proposes an expert system for detection, and subsequent investigation, of groups of collaborating automobile insurance fraudsters. The system is described and examined in great detail, several technical difficulties in detecting fraud are also considered, for it to be applicable in practice. Opposed to many other approaches, the system uses networks for representation of data. Networks are the most natural representation of such a relational domain, allowing formulation and analysis of complex relations between entities. Fraudulent entities are found by employing a novel assessment algorithm, Iterative Assessment Algorithm (IAA), also presented in the article. Besides intrinsic attributes of entities, the algorithm explores also the relations between entities. The prototype was evaluated and rigorously analyzed on real world data. Results show that automobile insurance fraud can be efficiently detected with the proposed system and that appropriate data representation is vital. \u00a9 2010 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "45"},
{"Title": "Ant-inspired query routing performance in dynamic peer-to-peer networks", "Authors": ["Ciglaric M.", "Vidmar T."], "Keywords": [], "Date": "2006", "Abstract": "P2P Networks are highly dynamic structures since their nodes - peer users keep joining and leaving continuously. In the paper, we study the effects of network change rates on query routing efficiency. First, the problem background is described and abstract system model is defined. The system characteristics and behavior are analyzed and abstracted with a set of measurable metrics. The paper studies Mute query routing protocol and compares its behavior to previously suggested routing protocols. The chosen routing technique makes use of cached metadata from previous answer messages (analogy to ants laying feromone). The paper also discusses mechanisms for broken path detection and metadata maintenance. Further, simulations in various dynamic network environments are presented and discussed: the degree of network dynamics varies from one node departure and node join per ten queries generated to five node departures and joins per one generated query. Several metrics are used to clarify the protocol behavior even with high rate of node departures, but it is shown that above a certain threshold it literally breaks down and exhibits considerable efficiency degradation. \u00a9 2006 IEEE.", "Language": "en", "Citations": "4"},
{"Title": "FTsim: A 3D tool for teaching automation concepts", "Authors": ["Ile N.", "Lotric U."], "Keywords": ["Automation", "Control", "Education", "Simulation", "Visualization"], "Date": "2018", "Abstract": "To improve efficiency and reduce costs, industry rapidly adopts the concepts promoted by Industry 4.0 paradigm. For the future engineers it is important to meet these concepts already during their study. To better prepare them for their careers, we have included some of the new concepts in the Process automation course. With a small group of students, we have developed a 3D visualization and simulation tool, which other students can use instead of real devices. The presented 3D simulator FTsim is built using Unity 3D game engine and mimics the behaviour of three Fischertechnik training models. The simulator succeeds to imitate the physical devices to sufficient detail and allows students to heavily reduce the time spent with a physical teaching platform, yet still devise robust applications. We have experienced very positive feedback from the students, including bug reports and improvement requests. The simulator is based on freely available tools and can be easily expanded to bigger models, other teaching platforms, or even to industrial set-ups.", "Language": "en", "Citations": "0"},
{"Title": "Audience measurement of digital signage: Quantitative study in real-world environment using computer vision", "Authors": ["Ravnik R.", "Solina F."], "Keywords": ["audience measurement", "computer vision", "digital signage", "quantitative study"], "Date": "2013", "Abstract": "We present a quantitative study of digital signage audience measurement using computer vision. We developed a camera-enhanced digital signage display that acquires audience measurement metrics with computer vision algorithms. Temporal metrics of a person's dwell time, display in-view time and attention time are extracted. The system also determines demographic metrics of the gender and age group. The digital signage display was deployed in a real-world environment of a clothing boutique, where demographic and viewership data of 1294 store customers were recorded, manually verified and analysed. The analysis shows that 35% of customers specifically looked-at the display, having the average attention time of 0.7 s. Interestingly, the attention time was substantially higher for men (1.2 s) than for women (0.4 s). Age group comparison reveals that children (1-14 years) are the most responsive to the digital signage. Finally, the analysis shows that the average attention time is significantly higher when displaying the dynamic content (0.9 s) when compared with the static content (0.6 s). \u00a9 2013 The Author.", "Language": "en", "Citations": "18"},
{"Title": "Predicting the outcome of head-up tilt test using heart rate variability and baroreflex sensitivity parameters in patients with vasovagal syncope", "Authors": ["Klemenc M.", "Strumbelj E."], "Keywords": ["Baroreceptor sensitivity", "Head-up tilt test", "Heart rate variability", "Vasovagal syncope"], "Date": "2015", "Abstract": "Purpose: The aim of the study was to investigate whether a statistical model could be used for an early prediction of the head-up tilt test (HUTT) outcome from heart rate variability (HRV) and baroreflex sensitivity (BRS) data obtained during early stages of the HUTT. Methods: A modified Italian protocol was used for HUTT in 105 patients with a previous history of vasovagal syncope. Beat-to-beat heart rate and blood pressure were continuously recorded. Fast Fourier transformation was used for spectral analysis of HRV and a sequence technique for measuring the BRS. Results: Linear statistical models based on HRV and BRS data from the first 15\u00a0min of HUTT were no more accurate than always naively predicted majority class that a syncope will occur (average model out-of-sample accuracy 56.2\u00a0\u00b1\u00a05.1\u00a0% vs. majority class relative frequency 54.2\u00a0%). Even when HRV and BRS data from the first 30\u00a0min were used in the model, we did not obtain any predictions of meaningful practical value (75.0\u00a0\u00b1\u00a05.1\u00a0% accuracy vs. 72.2\u00a0% majority class). Conclusions: While there are discernible and meaningful differences between HUTT-P and HUTT-N subjects, they are not sufficient to discriminate between the two groups and predict a syncope early in the HUTT. The results might improve with a larger set of subjects; however, we can conclude that it is not likely that syncope predictions of practical value can be obtained from aggregate HRV spectral analysis and BRS values.", "Language": "en", "Citations": "4"},
{"Title": "Web-enabled knowledge-based analysis of genetic data", "Authors": ["Juvan P.", "Zupan B.", "Demsar J.", "Bratko I.", "Halter J.A.", "Kuspa A.", "Shaulsky G."], "Keywords": [], "Date": "2001", "Abstract": "We present a web-based implementation of GenePath, an intelligent assistant tool for data analysis in functional genomics. GenePath considers mutant data and uses expert-defined patterns to find gene-to-gene or gene-to-outcome relations. It presents the results of analysis as genetic networks, wherein a set of genes has various influence on one another and on a biological outcome. In the paper, we particularly focus on its web-based interface and explanation mechanisms.", "Language": "en", "Citations": "3"},
{"Title": "C2SLDS: A WSN-based perishable food shelf-life prediction and LSFO strategy decision support system in cold chain logistics", "Authors": ["Qi L.", "Xu M.", "Fu Z.", "Mira T.", "Zhang X."], "Keywords": ["Cold chain logistics", "Decision support system", "Perishable food", "Shelf-life", "Wireless sensor network"], "Date": "2014", "Abstract": "Temperature monitoring, shelf-life visibility and Least Shelf-life First Out (LSFO) stock strategy are important contents in perishable food cold chain logistics for both cold chain managers and workers in order to reduce quality and economic losses. This paper illustrates a wireless sensor network (WSN) based integrated Cold Chain Shelf Life Decision Support System (C", "Language": "en", "Citations": "43"},
{"Title": "Modelling the effects of environmental conditions on apparent photosynthesis of Stipa bromoides by machine learning tools", "Authors": ["Dalaka A.", "Kompare B.", "Robnik-Sikonja M.", "Sgardelis S.P."], "Keywords": ["Automatic model construction", "Machine learning", "P-I curves", "Regression trees"], "Date": "2000", "Abstract": "Apparent leaf photosynthesis of the grass Stipa bromoides was measured in the field in two sites of Northern Greece. For predicting apparent photosynthesis from irradiance, temperature and relative air humidity data, we applied and compared two modelling approaches: ordinary statistical modelling and automatic model construction based on machine learning procedures. Ordinary statistical models were constructed based on background knowledge concerning the response of photosynthesis to irradiance. A Michaelis-Menten type light saturation curve was selected among six candidate models and was extended to include air temperature effects. A bell-shaped function of temperature was substituted for the parameter describing the asymptotic maximum photosynthesis. The final model accounted for 67.3% of data variation and was further improved by splitting the data set by experimental site. Site-specific differences were detected regarding the half saturation constant for light and the optimal temperature for photosynthesis. Automatic model construction produced a number of regression trees that enabled a detailed but simple description of the way irradiance, temperature and relative humidity affect photosynthesis. Photosynthesis increases with increasing irradiance, temperature affects photosynthesis when irradiance is close to saturation levels and relative humidity has an effect when both irradiance and temperature are high. There is a threshold value of relative humidity (at about 35%), below which photosynthesis is independent of irradiance within the observed range, decreasing with increasing temperature when temperature is high (> 25\u00b0C) and increasing with increasing relative humidity when temperature is low (< 25\u00b0C). The machine learning tools we used provide a very powerful modelling alternative to ordinary curve fitting methods. Their major advantages are the flexibility to select between accuracy and generality and their robustness against outliers and mixtures of differential responses. The models are transparent and easily interpreted. They seem to be able to handle quite complex dependencies among attributes, not requiring prior expert knowledge. (C) 2000 Elsevier Science B.V.", "Language": "en", "Citations": "10"},
{"Title": "Multiple eigenspaces by MDL", "Authors": ["Leonardis A.", "Bischof H."], "Keywords": [], "Date": "2000", "Abstract": "In this paper we propose a novel approach to constructing multiple eigenspaces from a set of training images based on the MDL principle. The main idea is to systematically build a redundant set of eigenspaces, which are treated as hypotheses that are then subject to a selection procedure. The selection procedure, based on the MDL principle, selects the final resulting set of eigenspaces as an optimal representation of the training set. We have tested the proposed method on a number of standard image sets, and the significance of the approach with respect to the recognition rate has been clearly demonstrated. \u00a9 2000 IEEE.", "Language": "en", "Citations": "5"},
{"Title": "Histogram remapping as a preprocessing step for robust face recognition", "Authors": ["Struc V.", "Zibert J.", "Pavesic N."], "Keywords": ["Face recognition", "Histogram equalization", "Histogram remapping", "Preprocessing tehniques"], "Date": "2009", "Abstract": "Image preprocessing techniques represent an essential part of a face recognition systems, which has a great impact on the performance and robustness of the recognition procedure. Amongst the number of techniques already presented in the literature, histogram equalization has emerged as the dominant preprocessing technique and is regularly used for the task of face recognition. With the property of increasing the global contrast of the facial image while simultaneously compensating for the illumination conditions present at the image acquisition stage, it represents a useful preprocessing step, which can ensure enhanced and more robust recognition performance. Even though, more elaborate normalization techniques, such as the multiscale retinex technique, isotropic and anisotropic smoothing, have been introduced to field of face recognition, they have been found to be more of a complement than a real substitute for histogram equalization. However, by closer examining the characteristics of histogram equalization, one can quickly discover that it represents only a specific case of a more general concept of histogram remapping techniques (which may have similar characteristics as histogram equalization does). While histogram equalization remapps the histogram of a given facial image to a uniform distribution, the target distribution could easily be replaced with an arbitrary one. As there is no theoretical justification of why the uniform distribution should be preferred to other target distributions, the question arises: how do other (non-uniform) target distributions influence the face recognition process and are they better suited for the recognition task. To tackle this issues, we present in this paper an empirical assessment of the concept of histogram remapping with the following target distributions: the uniform, the normal, the lognormal and the exponential distribution. We perform comparative experiments on the publicly available XM2VTS and YaleB databases and conclude that similar or even better recognition results that those ensured by histogram equalization can be achieved when other (non-uniform) target distribution are considered for the histogram remapping. This enhanced performance, however, comes at a price, as the nonuniform distributions rely on some parameters which have to be trained or selected appropriately to achieve the optimal performance.", "Language": "en", "Citations": "29"},
{"Title": "Modeling ischemia with finite elements and automated machine learning", "Authors": ["Robnik-Sikonja M.", "Radovic M.", "Dorovic S.", "Andelkovic-Cirkovic B.", "Filipovic N."], "Keywords": ["Automatic model selection", "Cardiac ischemia", "Data mining", "Finite element modeling"], "Date": "2018", "Abstract": "The main purpose of this study was to noninvasively detect and localize ischemic cardiac disease using the finite element method (FEM) in combination with machine learning approach. The forward FEM simulations of cardiac ischemia in different heart segments enabled the creation of a virtual database which consisted of corresponding body surface potentials. Two sets of experiments were performed on the database in order to select the best method for determining the existence of ischemia and then to predict the location of ischemia. Using Auto-WEKA and R caret package, several machine learning algorithms were tested. The best first phase model returned the classification accuracy of 95.3%, while the best second phase model determined the correct ischemia location with 95% accuracy. Considerable modeling and computational time are needed to create a training database and perform training, but once trained, the models will instantly return results. Thus, the main advantage of the proposed approach to ischemic detection and localization is a real-time availability of results and a novel, two-phase design which guides the selection of an adequate treatment.", "Language": "en", "Citations": "0"},
{"Title": "Facilitating information system development with Panoramic view on data", "Authors": ["Lavbic D.", "Lajovic I.", "Krisper M."], "Keywords": ["Associative thinking", "Object recognition", "Rapid application development", "Software development"], "Date": "2010", "Abstract": "The increasing amount of information and the absence of an effective tool for assisting users with minimal technical knowledge lead us to use associative thinking paradigm for implementation of a software solution - Panorama. In this study, we present object recognition process, based on context + focus information visualization techniques, as a foundation for realization of Panorama. We show that user can easily define data vocabulary of selected domain that is furthermore used as the application framework. The purpose of Panorama approach is to facilitate software development of certain problem domains by shortening the Software Development Life Cycle with minimizing the impact of implementation, review and maintenance phase. Our approach is focused on using and updating data vocabulary by users without extensive programming skills. Panorama therefore facilitates traversing through data by following associations where user does not need to be familiar with the query language, the data structure and does not need to know the problem domain fully. Our approach has been verified by detailed comparison to existing approaches and in an experiment by implementing selected use cases. The results confirmed that Panorama fits problem domains with emphasis on data oriented rather than ones with process oriented aspects. In such cases the development of selected problem domains is shortened up to 25%, where emphasis is mainly on analysis, logical design and testing, while omitting physical design and programming, which is performed automatically by Panorama tool.", "Language": "en", "Citations": "5"},
{"Title": "Comparison of attacks on IPv4 and IPv6 protocols", "Authors": ["Ciglaric M.", "Krevl A.", "Pancur M."], "Keywords": ["Attack", "IPv6", "Network security", "Spoofing"], "Date": "2009", "Abstract": "The paper overviews the main security-related network attacks and gives the attackers' motivation. Since the paper focuses on the differences between IPv4 and IPv6, the attacks are grouped accordingly. The paper gives an overview of sniffing, application layer attacks, rogue devices, man-in-the-middle attacks and flooding; inquiring, unauthorized access, fragmentation and header manipulation, IP spoofing, ARP and DHCP attacks, broadcast amplification attacks (smurf), routing attacks, viruses and worms. For each of them, a general execution idea and implementation are discussed in both protocols and recommendations for detection or prevention are given.", "Language": "en", "Citations": "0"},
{"Title": "Reinforcement learning and genetic regulatory network reconstruction", "Authors": ["Ster B.", "Dobnikar A."], "Keywords": ["canalysing Boolean functions", "genetic regulatory network", "reinforcement learning"], "Date": "2013", "Abstract": "Many different models of genetic regulatory networks (GRN) exist, but most of them are focused on off-line processing, so that important features of real networks, like adaptive and non-stationary characterare missed. Interdisciplinary insight into the area of self-organization within the living organisms has caused some interesting new thoughts, and the suggested model is among them. Based on reinforcement learning of the Boolean network with random initial structure, the model is searching for a specialized network, that agrees with experimentally obtained data from the real GRN. With some experiments of real biological networks we investigate its behaviour. \u00a9 2013 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "0"},
{"Title": "Part-level object recognition", "Authors": ["Krivic J.", "Solina F."], "Keywords": ["part-level object modelling", "range images", "superquadrics"], "Date": "2002", "Abstract": "This paper proposes a technique for object recognition using superquadric built models. Superquadrics, which are three dimensional models suitable for part-level representation of objects, are reconstructed from range images using the recover- and-select paradigm. Using an interpretation tree, the presence of an object in the scene from the model database can be hypothesized. These hypotheses are verified by projecting and refitting the object model to the range image which at the same time enables a better localization of the object in the scene.", "Language": "en", "Citations": "1"},
{"Title": "A hardware/software view of CUDA Strojni in programski vidiki arhitekture CUDA", "Authors": ["Dobravec T.", "Bulic P."], "Keywords": ["CUDA", "Graphics processing units", "Parallel programming"], "Date": "2010", "Abstract": "Extended abstract. This paper presents a hardware/software view of CUDA. A Graphical Processing Unit (GPU) has up to 480 streaming processors (SP), organized as up to 30 streaming multiprocessors (SM). Each SM has 8 or 16 SPs [2, 3]. SM is an independent processing unit. SM consists of a Fetch/Issue unit (Figure 1) and an execution unit (Figure 2). SM uses two different clock domains for the Fetch/Issue unit and the execution unit, respectively. SM manages and executes threads in groups of 32 parallel threads called warps. An issued warp executes in the execution unit as a set of 32 threads over four processor cycles (Figure 3). Besides a shared memory and a register set for each SM, each SM has a uniform access to a global memory. The global memory (up to 4GB DDR3 SDRAM) is connected to SMs via a 384-bit wide bus and is used for comunication between CPU and GPU and for sharing data among threads in different SMs. From the programmers point of view, CUDA is a device which can be used through a parallel programming model. The code is split into the so called kernels which are executed in up to several thousand parallel and/or asynchronous threads. Parallel threads can cooperate using synchronization mechanisms and shared memory, while asynchronous threads can only use a common global memory.", "Language": "en", "Citations": "1"},
{"Title": "Decision support system to support the solving of classification problems in telecommunications", "Authors": ["Rupnik R."], "Keywords": ["Classification", "Data mining", "Decision support", "Decision support system", "Knowledge discovery", "Telecommunications"], "Date": "2009", "Abstract": "Traditional techniques of data analysis do not enable the solution of all kind of problems and for that reason they have become insufficient. This caused a new interdisciplinary field of data mining to arise, encompassing both classical statistical, and modern machine learning techniques to support the data analysis and knowledge discovery from data. Data mining methods are powerful in dealing with large quantities of data, but on the other hand they are difficult to master by business users to facilitate decision support. In this paper we introduce our approach to integration of decision support system with data mining method called classification. We discuss the role of data mining to facilitate decision support, the use of classification method in decision support system, discuss applied approaches and introduce a data mining decision support system called DMDSS (Data Mining Decision Support System). We also present some obtained results and plans for future development.", "Language": "en", "Citations": "1"},
{"Title": "A Novel Performance Evaluation Methodology for Single-Target Trackers", "Authors": ["Kristan M.", "Matas J.", "Leonardis A.", "Vojir T.", "Pflugfelder R.", "Fernandez G.", "Nebehay G.", "Porikli F.", "Cehovin L."], "Keywords": ["model-free tracking", "Performance analysis", "single-target tracking", "tracker evaluation datasets", "tracker evaluation methodology", "tracker evaluation system"], "Date": "2016", "Abstract": "This paper addresses the problem of single-target tracker performance evaluation. We consider the performance measures, the dataset and the evaluation system to be the most important components of tracker evaluation and propose requirements for each of them. The requirements are the basis of a new evaluation methodology that aims at a simple and easily interpretable tracker comparison. The ranking-based methodology addresses tracker equivalence in terms of statistical significance and practical differences. A fully-annotated dataset with per-frame annotations with several visual attributes is introduced. The diversity of its visual properties is maximized in a novel way by clustering a large number of videos according to their visual attributes. This makes it the most sophistically constructed and annotated dataset to date. A multi-platform evaluation system allowing easy integration of third-party trackers is presented as well. The proposed evaluation methodology was tested on the VOT2014 challenge on the new dataset and 38 trackers, making it the largest benchmark to date. Most of the tested trackers are indeed state-of-the-art since they outperform the standard baselines, resulting in a highly-challenging benchmark. An exhaustive analysis of the dataset from the perspective of tracking difficulty is carried out. To facilitate tracker comparison a new performance visualization technique is proposed.", "Language": "en", "Citations": "118"},
{"Title": "Spaces with high topological complexity", "Authors": ["Franc A.", "Pavesic P."], "Keywords": [], "Date": "2014", "Abstract": "By a formula of Farber, the topological complexity TC(X) of a (p - 1)-connected m-dimensional CW-complex X is bounded above by (2m + 1)/p + 1. We show that the same result holds for the monoidal topological complexity TC ", "Language": "en", "Citations": "0"},
{"Title": "Planning smooth and obstacle-avoiding B-spline paths for autonomous mining vehicles", "Authors": ["Berglund T.", "Brodnik A.", "Jonsson H.", "Staffanson M.", "Soderkvist I."], "Keywords": ["Articulated vehicle", "Autonomous Guided Vehicle (AGV)", "B-spline curve", "Derivative of curvature", "Mining industry", "Motion-planning", "Nonlinear optimization", "Obstacle-avoidance", "Path-planning", "Safety margin", "Smoothness", "Travel time"], "Date": "2010", "Abstract": "We study the problem of automatic generation of smooth and obstacle-avoiding planar paths for efficient guidance of autonomous mining vehicles. Fast traversal of a path is of special interest. We consider four-wheel four-gear articulated vehicles and assume that we have an a priori knowledge of the mine wall environment in the form of polygonal chains. Computing quartic uniform B-spline curves, minimizing curvature variation, staying at least at a proposed safety margin distance from the mine walls, we plan high speed paths. We present a study where our implementations are successfully applied on eight path-planning cases arising from real-world mining data provided by the Swedish mining company Luossavaara-Kiirunavaara AB (LKAB). The results from the study indicate that our proposed methods for computing obstacle-avoiding minimum curvature variation B-splines yield paths that are substantially better than the ones used by LKAB today. Our simulations show that, with an average 32.13%, the new paths are faster to travel along than the paths currently in use. Preliminary results from the production at LKAB show an overall 5%-10% decrease in the total time for an entire mining cycle. Such a cycle includes both traveling, ore loading, and unloading. Note to Practitioners-This article was motivated by the problem of how to automatically produce high quality drive-paths for autonomous transportation vehicles in mines. The vehicles are heavy (> 100 tonnes) but are still expected to run at speeds up to 20 km/h to be productive. To reach these speeds without damaging the steering gear and the mechanics of the vehicles, their paths need to be smooth. It turns out that visual inspection is often insufficient to produce a path with high smoothness. We suggest a method for computing paths, requiring an a priori knowledge about the environment, that minimizes the amount of steering needed. The computed paths are safe as they guarantee no collisions between the vehicle and the tunnel wall. We present a study of eight cases based on real-world application data from the Swedish mining company Luossavaara- Kiirunavaara AB (LKAB) showing that, with an average of 32.13%, our paths are faster to travel along than the paths currently in use. \u00a9 2009 IEEE.", "Language": "en", "Citations": "69"},
{"Title": "Theoretical and Empirical Analysis of ReliefF and RReliefF", "Authors": ["Robnik-Sikonja M.", "Kononenko I."], "Keywords": ["Attribute evaluation", "Classification", "Feature selection", "Regression", "Relief algorithm"], "Date": "2003", "Abstract": "Relief algorithms are general and successful attribute estimators. They are able to detect conditional dependencies between attributes and provide a unified view on the attribute estimation in regression and classification. In addition, their quality estimates have a natural interpretation. While they have commonly been viewed as feature subset selection methods that are applied in prepossessing step before a model is learned, they have actually been used successfully in a variety of settings, e.g., to select splits or to guide constructive induction in the building phase of decision or regression tree learning, as the attribute weighting method and also in the inductive logic programming. A broad spectrum of successful uses calls for especially careful investigation of various features Relief algorithms have. In this paper we theoretically and empirically investigate and discuss how and why they work, their theoretical and practical properties, their parameters, what kind of dependencies they detect, how do they scale up to large number of examples and features, how to sample data for them, how robust are they regarding the noise, how irrelevant and redundant attributes influence their output and how different metrics influences them.", "Language": "en", "Citations": "1345"},
{"Title": "Early roman barge from the ljubljanica river at Sinja Gorica Zgodnjerimska ladja iz ljubljanice pri Sinji Gorici", "Authors": ["Eric M.", "Gaspari A.", "Cufar K.", "Solina F.", "Verbic T."], "Keywords": ["Early roman barge", "Nauportus (vrhnika)", "Photogrammetric 3D model", "Roman period", "Sinja gorica", "The ljubljanica river", "Underwater archaeology"], "Date": "2014", "Abstract": "Preventive underwater archaeological surveying in the bed of the Ljubljanica River, conducted at Sinja Gorica in 2008, revealed the remains of an Early Roman wooden barge from the beginning of the 1st century AD. Detailed documentation of the 4.5m long and 2.8m wide section of the boat followed in October 2012 and included photogrammetric three-dimensional modelling. The construction characteristics and size revealed a boat of the Mediterranean shipbuilding tradition, with an elongated oval shape and a flat bottom and vertical sides, constructed using the shell-first technique and planks fastened with iron clamps, while the hull was reinforced with floor-timbers in a manner not yet published in the relevant literature. The barge, made mostly of beech wood, was built soon after AD 3 according to the dendrochronological analysis. The wood is very poorly preserved. The barge was presumably used to transport cargo between Nauportus and Emona.", "Language": "en", "Citations": "6"},
{"Title": "Crawl and crowd to bring machine translation to under-resourced languages", "Authors": ["Toral A.", "Espla-Gomis M.", "Klubicka F.", "Ljubesic N.", "Papavassiliou V.", "Prokopidis P.", "Rubino R.", "Way A."], "Keywords": ["Crowdsourcing", "Statistical machine translation", "Web crawling"], "Date": "2017", "Abstract": "We present a widely applicable methodology to bring machine translation (MT) to under-resourced languages in a cost-effective and rapid manner. Our proposal relies on web crawling to automatically acquire parallel data to train statistical MT systems if any such data can be found for the language pair and domain of interest. If that is not the case, we resort to (1) crowdsourcing to translate small amounts of text (hundreds of sentences), which are then used to tune statistical MT models, and (2) web crawling of vast amounts of monolingual data (millions of sentences), which are then used to build language models for MT. We apply these to two respective use-cases for Croatian, an under-resourced language that has gained relevance since it recently attained official status in the European Union. The first use-case regards tourism, given the importance of this sector to Croatia\u2019s economy, while the second has to do with tweets, due to the growing importance of social media. For tourism, we crawl parallel data from 20 web domains using two state-of-the-art crawlers and explore how to combine the crawled data with bigger amounts of general-domain data. Our domain-adapted system is evaluated on a set of three additional tourism web domains and it outperforms the baseline in terms of automatic metrics and/or vocabulary coverage. In the social media use-case, we deal with tweets from the 2014 edition of the soccer World Cup. We build domain-adapted systems by (1) translating small amounts of tweets to be used for tuning by means of crowdsourcing and (2) crawling vast amounts of monolingual tweets. These systems outperform the baseline (Microsoft Bing) by 7.94 BLEU points (5.11 TER) for Croatian-to-English and by 2.17 points (1.94 TER) for English-to-Croatian on a test set translated by means of crowdsourcing. A complementary manual analysis sheds further light on these results.", "Language": "en", "Citations": "3"},
{"Title": "Using asymmetric windows in automatic speech recognition", "Authors": ["Rozman R.", "Kodek D.M."], "Keywords": ["Asymmetric windows", "Automatic speech recognition", "Robustness", "Short Time Fourier Transform", "Windowing"], "Date": "2007", "Abstract": "This paper considers the windowing problem of the short-time frequency analysis that is used in speech recognition systems (SRS). Since human hearing is relatively insensitive to short-time phase distortion of the speech signal there is no apparent reason for the use of symmetric windows which give a linear phase response. Furthermore, phase information is usually completely disregarded in SRS. This should be contrasted with the well-known fact that relaxation of the linearity constraint on window phase results in a better magnitude response and shorter time delay. These observations form a strong argument in favor of the research presented in this paper. First, a general overview of the role that windows play in the frequency analysis stage of SRS is presented. Important properties for speech recognition are highlighted and potential advantages of asymmetric windows are presented. Among them the shorter time delay and the better magnitude response are most important. Two possible design methods for asymmetric windows are discussed. Since little is known about window influence on SRS performance the design methods are first considered from a frequency analysis point of view. This is followed by practical evaluations on real SRS. Expectations were confirmed by the results. The proposed asymmetric windows increased the robustness of elementary, isolated and connected speech recognition on a variety of adverse test conditions. This is particularly true for the case of a combination of additive and low pass convolutional distortions. Further research on asymmetric windows and on the parameterization process as a whole is suggested. \u00a9 2007 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "15"},
{"Title": "Early machine learning research in ljubljana", "Authors": ["Kononenko I."], "Keywords": ["Decision trees", "Machine learning", "Naive Bayesian classifier", "ReliefF"], "Date": "2018", "Abstract": "We describe early machine learning research in Ljubljana, motivated by medical diagnostic problems, in the areas of building decision trees with Assistant, the development of Na\u00efve and Semi-Na\u00efve Bayesian classifier and its explanations of individual predictions, and the development of ReliefF and RReliefF algorithms for non-myopic evaluation of attributes in classification and regression, respectively. Povzetek: V \u010dlanku opi\u0161emo zgodnje raziskave na podro\u010dju strojnega u\u010denja v Ljubljani, ki so bile motivirane z medicinskimi diagnosti\u010dnimi problemi. Razvili smo sistem Asistent za gradnjo odlo\u010ditvenih dreves, naivni in delno naivni Bayesov klasifikator in metodo razlage njunih napovedi ter algoritma ReliefF in RReliefF za nekratkovidno ocenjevanje atributov v klasifikaciji in regresiji.", "Language": "sl", "Citations": "0"},
{"Title": "Flexible-attribute problems", "Authors": ["Mihelic J.", "Robic B."], "Keywords": ["Algorithms", "Approximability", "Flexibility", "NP-hard", "Uncertainty"], "Date": "2010", "Abstract": "Problems with significant input-data uncertainty are very common in practical situations. One approach to dealing with this uncertainty is called scenario planning, where the data uncertainty is represented with scenarios. A scenario represents a potential realization of the important parameters of the problem. In this paper we present a new approach to coping with data uncertainty, called the flexibility approach. Here a problem is described as a set of interconnected simple scenarios. The idea is to find a solution for each scenario such that, after a change in scenario, transforming from one solution to the other one is not expensive. We define two versions of flexibility and hence two versions of the problem, which are called the sum-flexible-attribute problem and the max-flexible-attribute problem. For both problems we prove the NP-hardness as well as the non-approximability. We present polynomial time algorithms for solving the two problems to optimality on trees. Finally, we discuss the possible applications and generalizations of the new approach. \u00a9 Springer Science+Business Media, LLC 2009.", "Language": "en", "Citations": "2"},
{"Title": "Sense classification of shallow discourse relations with focused RNNs", "Authors": ["Weiss G.", "Bajec M."], "Keywords": [], "Date": "2018", "Abstract": "Understanding the sense of discourse relations between segments of text is essential to truly comprehend any natural language text. Several automated approaches have been suggested, but all rely on external resources, linguistic feature engineering, and their processing pipelines are built from substantially different models. In this paper, we introduce a novel system for sense classification of shallow discourse relations (FR system) based on focused recurrent neural networks (RNNs). In contrast to existing systems, FR system consists of a single end-to-end trainable model for handling all types and senses of discourse relations, requires no feature engineering or external resources, is language-independent, and can be applied at the word and even character levels. At its core, we present our novel generalization of the focused RNNs layer, the first multi-dimensional RNN-attention mechanism for constructing text/argument embeddings. The filtering/gating RNN enables downstream RNNs to focus on different aspects of the input sequence and project it into several embedding subspaces. These argument embeddings are then used to perform sense classification. FR system has been evaluated using the official datasets and methodology of CoNLL 2016 Shared Task. It does not fall a lot behind state-of-the-art performance on English, the most researched and supported language, but it outperforms existing best systems by 2.5% overall results on the Chinese blind dataset.", "Language": "en", "Citations": "0"},
{"Title": "A visualization and user interface framework for heterogeneous distributed environments", "Authors": ["Mahnic M.", "Skocaj D."], "Keywords": [], "Date": "2012", "Abstract": "Systems that require complex computations are frequently implemented in a distributed manner. Such systems are often split into components where each component is employed to perform a specific type of processing. The components of a system may be implemented in different programming languages because some languages are more suited for expressing and solving certain kinds of problems. The user of the system must have a way to monitor the state of individual components and also to modify their execution parameters through a user interface while the system is running. The distributed execution and programming language diversity represent a problem for the development of graphic user interfaces. \u00a9 2012 MIPRO.", "Language": "en", "Citations": "0"},
{"Title": "A model of the RS memory-cell realization in biological systems Model realizacije funkcionalosti RS-pomnilne celice v biolo\u0161kem sistemu", "Authors": ["Moskon M.", "Ciglic M.", "Jerala R.", "Zimic N.", "Mraz M."], "Keywords": ["Genetic regulatory network", "Memorizing in biological systems", "RS memory-cell", "Synthetic biology", "Unconventional computing"], "Date": "2009", "Abstract": "In the past eighty years computer structures that represent the basic logic primitives in the construction of computer systems have been based on electronic components. Recently, there has been a strong need noted to have them minimised and to make their response-time faster [1]. To achieve the goal, alternative information-processing platforms are being looked for. Biological systems are certainly one of the possible alternative processing platforms of the future [2]. Dynamics of these circuits is based on the presence (respectively absence) of specific proteins, i.e. transcriptional factors. By manipulating these circuits with genetic engineering (i.e. DNA modifications), their desired behaviour can be achieved. Here we present the basic concepts of these systems on the example of the RS memory-cell for which we developed a mathematical model based on Ordinary Differential Equations (ODEs). We also made a simulation model in the Matlab/Simulink environment (Figures 7 and 8). We simulate the behaviour of our circuit in ideal circumstances (Figure 9) and in those which are an approximation of a real environment (Figure 10). The promising simulation results indicating that realization in a living tissue is possible will be addressed in our future work.", "Language": "en", "Citations": "0"},
{"Title": "Iterative joint extraction of entities, relationships and coreferences from text sources", "Authors": ["Zitnik S.", "Bajec M."], "Keywords": [], "Date": "2015", "Abstract": "Machine understanding of textual documents has been challenging since the early computer era. Since the information extraction research field emerged it has inferred multiple natural language processing tasks, such as named entities recognition, relationships extraction and coreference resolution. Even though for the purpose of the end-to-end information extraction all of the three tasks are crucial, existing work has been focusing merely on one specific task at the time or at best on their connection in a pipeline. In this paper we introduce a novel iterative and joint information extraction system that interconnects all the three tasks together using iterative feature functions which use the advantage of the intermediate extractions. Furthermore, we introduce a special transformation of data into skip-mention sequences to enable the extraction of relations and coreferences using fast first-order graphical models. Additionally, the system uses an ontology as its knowledge source, as a list of inferred extraction rules, and as a data schema of extracted results. Experimental results show that the accuracy of extractions improves after each iteration. In particular, our model obtained a 15% error reduction on named entity recognition over individual models.", "Language": "en", "Citations": "0"},
{"Title": "Transductive reliability estimation for kernel based classifiers", "Authors": ["Tzikas D.", "Kukar M.", "Likas A."], "Keywords": [], "Date": "2007", "Abstract": "Estimating the reliability of individual classifications is very important in several applications such as medical diagnosis. Recently, the transductive approach to reliability estimation has been proved to be very efficient when used with several machine learning classifiers, such as Naive Bayes and decision trees. However, the efficiency of the transductive approach for state-of-the art kernel-based classifiers was not considered. In this work we deal with this problem and apply the transductive reliability methodology with sparse kernel classifiers, specifically the Support Vector Machine and Relevance Vector Machine. Experiments with medical and bioinformatics datasets demonstrate better performance of the transductive approach for reliability estimation compared to reliability measures obtained directly from the output of the classifiers. Furthermore, we apply the methodology in the problem of reliable diagnostics of the coronary artery disease, outperforming the expert physicians' standard approach. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "2"},
{"Title": "Sampling promotes community structure in social and information networks", "Authors": ["Blagus N.", "Subelj L.", "Weiss G.", "Bajec M."], "Keywords": ["Communities", "Complex networks", "Modules", "Network sampling", "Node group structure"], "Date": "2015", "Abstract": "Any network studied in the literature is inevitably just a sampled representative of its real-world analogue. Additionally, network sampling is lately often applied to large networks to allow for their faster and more efficient analysis. Nevertheless, the changes in network structure introduced by sampling are still far from understood. In this paper, we study the presence of characteristic groups of nodes in sampled social and information networks. We consider different network sampling techniques including random node and link selection, network exploration and expansion. We first observe that the structure of social networks reveals densely linked groups like communities, while the structure of information networks is better described by modules of structurally equivalent nodes. However, despite these notable differences, the structure of sampled networks exhibits stronger characterization by community-like groups than the original networks, irrespective of their type and consistently across various sampling techniques. Hence, rich community structure commonly observed in social and information networks is to some extent merely an artifact of sampling.", "Language": "en", "Citations": "12"},
{"Title": "Generation of a clustering ensemble based on a gravitational self-organising map", "Authors": ["Ilc N.", "Dobnikar A."], "Keywords": ["Cluster analysis", "Clustering-ensemble generation", "Experimental comparison", "Gravitational algorithm", "Self-organising map"], "Date": "2012", "Abstract": "Clustering-ensemble methods have emerged recently as an effective approach to the problem of clustering, which is one of the fundamental data-analysis tools. Data clustering with an ensemble involves two steps: generation of the ensemble with single-clustering methods and the combination of the obtained solutions to produce a final consensus partition of the data. In this paper we first propose a novel clustering method, based on Kohonen's self-organising map and gravitational algorithm, and, second, investigate its performance in the generation of a clustering ensemble. The proposed method is able to discover clusters of complex shapes and determines the number of clusters automatically. Furthermore, its stochastic nature is beneficial in the construction of a diverse ensemble of partitions. Promising results of the presented method were obtained in comparison with three, relevant, single-clustering algorithms over artificial and real data sets. \u00a9 2012 Elsevier B.V.", "Language": "en", "Citations": "12"},
{"Title": "On graphs with complete multipartite \u03bc-graphs", "Authors": ["Jurisic A.", "Munemasa A.", "Tagami Y."], "Keywords": ["\u03bc-graph", "Complete multipartite graph", "Distance-regular graph", "Generalized quadrangle", "Local graph", "Regular point"], "Date": "2010", "Abstract": "Juri\u0161i\u0107 and Koolen proposed to study 1-homogeneous distance-regular graphs, whose\u03bc-graphs (that is, the graphs induced on the common neighbours of two vertices at distance 2) are complete multipartite. Examples include the Johnson graph J (8, 4), the halved 8-cube, the known generalized quadrangle of order (4, 2), an antipodal distance-regular graph constructed by T. Meixner and the Patterson graph. We investigate a more general situation, namely, requiring the graphs to have complete multipartite \u03bc-graphs, and that the intersection number \u03b1 exists, which means that for a triple (x, y, z) of vertices in \u0393, such that x and y are adjacent and z is at distance 2 from x and y, the number \u03b1 (x, y, z) of common neighbours of x, y and z does not depend on the choice of a triple. The latter condition is satisfied by any 1-homogeneous graph. Let K", "Language": "en", "Citations": "1"},
{"Title": "Robust recognition using eigenimages", "Authors": ["Leonardis A.", "Bischof H."], "Keywords": [], "Date": "2000", "Abstract": "The basic limitations of the standard appearance-based matching methods using eigenimages are nonrobust estimation of coefficients and inability to cope with problems related to outliers, occlusions, and varying background. In this paper we present a new approach which successfully solves these problems. The major novelty of our approach lies in the way the coefficients of the eigenimages are determined. Instead of computing the coefficients by a projection of the data onto the eigenimages, we extract them by a robust hypothesize-and-test paradigm using subsets of image points. Competing hypotheses are then subject to a selection procedure based on the Minimum Description Length principle. The approach enables us not only to reject outliers and to deal with occlusions but also to simultaneously use multiple classes of eigenimages.", "Language": "en", "Citations": "203"},
{"Title": "SWITCH-ing from Multi-Tenant to Event-Driven Videoconferencing Services", "Authors": ["Trnkoczy J.", "Pascinski U.", "Gec S.", "Stankovski V."], "Keywords": [], "Date": "2017", "Abstract": "Full mesh is the most commonly used networking topology in Web Real-Time Communication (WebRTC) based videoconferencing (VC) applications, however, due to its inherently poor scaling capability it is not appropriate for multi-party VC with many participants. Solutions based on centralized media server infrastructures are used to leverage the scaling problem. Service providers adopting centralized approach need to ensure good resource utilization to lower the price, and at the same time provide good Quality of Experience (QoE) to the end users. In practice, even with today's advanced cloud technologies, these two conflicting goals are difficult to achieve simultaneously. In order to tackle this complex problem, we propose an innovative event-driven model, that differs from the traditional multi-Tenant service provisioning model. In this work, the architecture and implementation of a WebRTC event-driven multi-party VC, based on Software as a Service (SaaS) principles is presented. A prototype was developed on top of Docker containers and Kubernetes container orchestration technologies, which in our opinion represent key enabling technologies fostering the migration from multi-Tenant towards event-driven architectures. The technology readiness to support such time-critical applications is evaluated. The initial results suggest that although there are some trade-offs in terms of performance/resource consumption, our fully functional prototype allows for on-The-fly media server instance creation and destruction in arbitrary cloud provider infrastructure with still acceptable application usability.", "Language": "en", "Citations": "2"},
{"Title": "Searching for pattern graphs using a search plan in the presence of automorphisms Iskanje vzor\u010dnih grafov s pomo\u010djo iskalnega na\u010drta ob prisotnosti avtomorfizmov", "Authors": ["Furst L.", "Cibej U.", "Mihelic J."], "Keywords": [], "Date": "2018", "Abstract": "The subgraph isomorphism problem, the goal of which is to find the occurrences of a given pattern graph in a given host graph, is, owing to the pervasiveness of large networks, becoming increasingly important. However, the problem is NP-complete, and the search efficiency may also be negatively affected by symmetries in the pattern graph. In this paper, we present an algorithm for solving the subgraph isomorphism problem using a search plan, a sequence of instructions for a systematic traversal of the pattern graph. The presented algorithm pays attention to the symmetries in the pattern graph and thus performs more efficiently than its straightforward counterpart which merely follows the search plan instructions in all possible ways. By testing our algorithm on artificial and real-world graphs, we empirically confirm its advantage over the naive approach and answer several research questions.", "Language": "en", "Citations": "0"},
{"Title": "Analysis of multi-agent activity using petri nets", "Authors": ["Perse M.", "Kristan M.", "Pers J.", "Music G.", "Vuckovic G.", "Kovacic S."], "Keywords": ["Activity recognition and evaluation", "Basketball analysis", "Multi-agent activity analysis", "Trajectories"], "Date": "2010", "Abstract": "This paper presents the use of place/transition petri nets (PNs) for the recognition and evaluation of complex multi-agent activities. The PNs were built automatically from the activity templates that are routinely used by experts to encode domain-specific knowledge. The PNs were built in such a way that they encoded the complex temporal relations between the individual activity actions. We extended the original PN formalism to handle the propagation of evidence using net tokens. The evaluation of the spatial and temporal properties of the actions was carried out using trajectory-based action detectors and probabilistic models of the action durations. The presented approach was evaluated using several examples of real basketball activities. The obtained experimental results suggest that this approach can be used to determine the type of activity that a team has performed as well as the stage at which the activity ended. \u00a9 2009 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "16"},
{"Title": "Bounded face-width forces K7-minors in orientable surfaces", "Authors": ["Fijavz G."], "Keywords": ["Face-width", "Graph embeddings", "Graph minors", "Orientable surfaces"], "Date": "2015", "Abstract": "Let G be a graph embedded in a nonspherical orientable surface with face-width \u2265 19. We prove that G contains a minor isomorphic to K", "Language": "en", "Citations": "0"},
{"Title": "Gene network inference by fusing data from diverse distributions", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": [], "Date": "2015", "Abstract": "Motivation: Markov networks are undirected graphical models that are widely used to infer relations between genes from experimental data. Their state-of-the-art inference procedures assume the data arise from a Gaussian distribution. High-throughput omics data, such as that from next generation sequencing, often violates this assumption. Furthermore, when collected data arise from multiple related but otherwise nonidentical distributions, their underlying networks are likely to have common features. New principled statistical approaches are needed that can deal with different data distributions and jointly consider collections of datasets. Results: We present FuseNet, a Markov network formulation that infers networks from a collection of nonidentically distributed datasets. Our approach is computationally efficient and general: given any number of distributions from an exponential family, FuseNet represents model parameters through shared latent factors that define neighborhoods of network nodes. In a simulation study, we demonstrate good predictive performance of FuseNet in comparison to several popular graphical models. We show its effectiveness in an application to breast cancer RNA-sequencing and somatic mutation data, a novel application of graphical models. Fusion of datasets offers substantial gains relative to inference of separate networks for each dataset. Our results demonstrate that network inference methods for non-Gaussian data can help in accurate modeling of the data generated by emergent high-throughput technologies.", "Language": "en", "Citations": "11"},
{"Title": "Mosaic-based panoramic depth imaging with a single standard camera", "Authors": ["Peer P.", "Solina F."], "Keywords": [], "Date": "2001", "Abstract": "In this article we present a panoramic depth imaging system. The system is mosaic-based which means that we use a single rotating camera and assemble the captured images in a mosaic. Due to a, setoff of the camera's optical center from the rotational center of the system we are able to capture the motion parallax effect which enables the stereo reconstruction. The camera is rotating on a circular path with the step defined by an angle, equivalent to one column of the captured image. The equation for depth estimation can be easily extracted from system geometry. To find the corresponding points on a stereo pair of panoramic images the epipolar geometry needs to be determined. It can be shown that the epipolar geometry is very simple if we are doing the reconstruction based on a symmetric pair of stereo panoramic images. We get a symmetric pair of stereo panoramic images when we take symmetric columns on the left and on the right side from the captured image center column. Epipolar lines of the symmetrical pair of panoramic images are image rows. We focused mainly on the system analysis. Results of the stereo reconstruction procedure and quality evaluation of generated depth images are quite promising. The system performs well in the reconstruction of small indoor spaces. Our final goal is to develop a system for automatic navigation of a mobile robot in a room.", "Language": "en", "Citations": "4"},
{"Title": "An integral framework for information systems security management", "Authors": ["Treek D."], "Keywords": ["Information systems", "Methodological framework", "Security management"], "Date": "2003", "Abstract": "Business use of Internet has exposed security as one of the key-factors for successful online competition. Contemporary management of E-business security involves various approaches in different areas, ranging from technology to organizational issues and legislation. These approaches are often isolated, while management of security requires an integrated approach. This article presents an attempt at management of E-business systems security that is based on integrating existing approaches in a balanced way. To foster practical use of the conceptual model in this paper, brief background knowledge in related areas is given.", "Language": "en", "Citations": "40"},
{"Title": "Augmented Coaching Ecosystem for Non-obtrusive Adaptive Personalized Elderly Care on the basis of Cloud-Fog-Dew computing paradigm", "Authors": ["Gordienko Yu.", "Stirenko S.", "Alienin O.", "Skala K.", "Sojat Z.", "Rojbi A.", "Lopez Benito J.R.", "Artetxe Gonzalez E.", "Lushchyk U.", "Sajn L.", "Llorente Coto A.", "Jervan G."], "Keywords": [], "Date": "2017", "Abstract": "The concept of the augmented coaching ecosystem for non-obtrusive adaptive personalized elderly care is proposed on the basis of the integration of new and available ICT approaches. They include multimodal user interface (MMUI), augmented reality (AR), machine learning (ML), Internet of Things (IoT), and machine-to-machine (M2M) interactions. The ecosystem is based on the Cloud-Fog-Dew computing paradigm services, providing a full symbiosis by integrating the whole range from low level sensors up to high level services using integration efficiency inherent in synergistic use of applied technologies. Inside of this ecosystem, all of them are encapsulated in the following network layers: Dew, Fog, and Cloud computing layer. Instead of the spaghetti connections, mosaic of buttons, puzzles of output data, etc., the proposed ecosystem provides the strict division in the following dataflow channels: consumer interaction channel, machine interaction channel, and caregiver interaction channel. This concept allows to decrease the physical, cognitive, and mental load on elderly care stakeholders by decreasing the secondary human-to-human (H2H), human-to-machine (H2M), and machine-to-human (M2H) interactions in favor of M2M interactions and distributed Dew Computing services environment. It allows to apply this non-obtrusive augmented reality ecosystem for effective personalized elderly care to preserve their physical, cognitive, mental and social well-being.", "Language": "en", "Citations": "7"},
{"Title": "A Bayesian hierarchical latent trait model for estimating rater bias and reliability in largescale performance assessment", "Authors": ["Zupanc K.", "Strumbelj E."], "Keywords": [], "Date": "2018", "Abstract": "We propose a novel approach to modelling rater effects in scoring-based assessment. The approach is based on a Bayesian hierarchical model and simulations from the posterior distribution. We apply it to large-scale essay assessment data over a period of 5 years. Empirical results suggest that the model provides a good fit for both the total scores and when applied to individual rubrics. We estimate the median impact of rater effects on the final grade to be \u00b1 2 points on a 50 point scale, while 10% of essays would receive a score at least \u00b1 5 different from their actual quality. Most of the impact is due to rater unreliability, not rater bias.", "Language": "en", "Citations": "1"},
{},
{"Title": "Towards automated scyphistoma census in underwater imagery: A useful research and monitoring tool", "Authors": ["Vodopivec M.", "Mandeljc R.", "Makovec T.", "Malej A.", "Kristan M."], "Keywords": ["Automated counting", "Convolutional neural networks", "Jellyfish", "Scyphozoa"], "Date": "2018", "Abstract": "Manual annotation and counting of entities in underwater photographs is common in many branches of marine biology. With a marked increase of jellyfish populations worldwide, understanding the dynamics of the polyp (scyphistoma) stage of their life-cycle is becoming increasingly important. In-situ studies of polyp population dynamics are scarce due to small size of the polyps and tedious manual work required to annotate and count large numbers of items in underwater photographs. We devised an experiment which shows a large variance between human annotators, as well as in annotations made by the same annotator. We have tackled this problem, which is present in many areas of marine biology, by developing a method for automated detection and counting. Our polyp counter (PoCo) uses a two-stage approach with a fast detector (Aggregated Channel Features) and a precise classifier consisting of a pre-trained Convolutional Neural Network and a Support Vector Machine. PoCo was tested on a year-long image dataset and performed with accuracy comparable to human annotators but with 70-fold reduction in time. The algorithm can be used in many marine biology applications, vastly reducing the amount of manual labor and enabling processing of much larger datasets. The source code is freely available on GitHub.", "Language": "en", "Citations": "0"},
{"Title": "15 seconds of fame - An interactive, computer-vision based art installation", "Authors": ["Batagelj B.", "Solina F.", "Peer P."], "Keywords": ["Color filters", "Computer vision application", "Face detection"], "Date": "2004", "Abstract": "\"15 seconds of fame\" is an interactive installation which every 15 seconds generates a new pop-art portrait of a randomly selected person from the audience. The installation was inspired by Andy Warhol's ironical statement that \"In the future everybody will be famous for 15 minutes\". The installation detects human faces in digital images of people who are standing in front of the installation. Pop-art portraits are then generated from randomly chosen faces in the audience by applying randomly selected filters. These portraits are shown in 15 second intervals on the flat-panel computer monitor which is framed as a painting. Electronic copies of each displayed portrait can be ordered by e-mail.", "Language": "en", "Citations": "3"},
{"Title": "Inferring network infrastructural behaviour during disasters", "Authors": ["Gilani Z.", "Sathiaseelan A.", "Crowcroft J.", "Pejovic V."], "Keywords": [], "Date": "2016", "Abstract": "An unexpected increase in natural disasters has prompted a large interest in governments and organisations to utilise ICT for many different purposes such as preparation, impact mitigation, loss reduction and relief efforts. This paper presents initial work on studying disaster scenarios from device level perspective to characterise network infrastructural behaviour during extraordinary situations. We find connectivity challenges during disasters and observe sharp decline of quality metrics and loss of station quantity between ordinary and extraordinary time periods. We also make distinctions between usual and unusual behaviour seen during ordinary and extraordinary situations.", "Language": "en", "Citations": "3"},
{"Title": "Improvements to Ullmann's Algorithm for the Subgraph Isomorphism Problem", "Authors": ["Cibej U.", "Mihelic J."], "Keywords": ["algorithm", "experimental evaluation", "graph patterns", "Subgraph isomorphism"], "Date": "2015", "Abstract": "The subgraph isomorphism problem is one of the most important problems for pattern recognition in graphs. Its applications are found in many different disciplines, including chemistry, medicine, and social network analysis. Because of the NP-completeness of the problem, the existing exact algorithms exhibit an exponential worst-case running time. In this paper, we propose several improvements to the well-known Ullmann's algorithm for the problem. The improvements lower the time consumption as well as the space requirements of the algorithm. We experimentally demonstrate the efficiency of our improvement by comparing it to another set of improvements called FocusSearch, as well as other state-of-the-art algorithms, namely VF2 and LAD.", "Language": "en", "Citations": "5"},
{"Title": "Integrating data mining and decision support through data mining based decision support system", "Authors": ["Rupnik R.", "Kukar M.", "Krisper M."], "Keywords": [], "Date": "2007", "Abstract": "The information overload caused by the massive influx of raw data has caused traditional data analysis to become insufficient. This caused a new interdisciplinary field of data mining to arise, encompassing both classical statistical, and modern machine learning tools to support the data analysis and knowledge discovery from databases. While data mining methods are powerful in dealing with large quantities of data, they are often difficult to master by business users to facilitate decision support. In this paper we introduce our approach to integration of decision support system with data mining methods. We discuss the role of data mining to facilitate decision support, the role of data mining in decision support systems, discuss underlying ideas and applied approaches and introduce a data mining decision support system called DMDSS (Data Mining Decision Support System). We present some obtained results and their beneficial use as expected by the business users.", "Language": "en", "Citations": "22"},
{"Title": "Computer vision based reliability control for electromechanical dice gambling machine", "Authors": ["Lapanja Iztok", "Mraz Miha", "Zimic Nikolaj"], "Keywords": [], "Date": "2000", "Abstract": "In our paper we present a complete overview of a reliability control module for an electro-mechanical dice gambling machine based on computer vision principles. We particularly discuss the fundamental, heavily utilized function of color difference, which forms a basis for fast, efficient and parameterized chroma-key applications. We also explain the dice location estimation solution and the final template matching phase of number detection. In the conclusion we discuss some issues and problems which are still left open but should be solved by the project team.", "Language": "en", "Citations": "8"},
{"Title": "Characterization of the Patterson graph", "Authors": ["Brouwer A.E.", "Jurisic A.", "Koolen J.H."], "Keywords": ["Locally polar space", "Suzuki sporadic group"], "Date": "2007", "Abstract": "In this note we show that there is a unique distance-regular graph with intersection array {280, 243, 144, 10 ; 1, 8, 90, 280}. \u00a9 2008 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "Anticipatory mobile computing: A survey of the state of the art and research challenges", "Authors": ["Pejovic V.", "Musolesi M."], "Keywords": ["Anticipatory computing", "Context-aware systems", "Mobile sensing"], "Date": "2015", "Abstract": "Today's mobile phones are far from the mere communication devices they were 10 years ago. Equipped with sophisticated sensors and advanced computing hardware, phones can be used to infer users' location, activity, social setting, and more. As devices become increasingly intelligent, their capabilities evolve beyond inferring context to predicting it, and then reasoning and acting upon the predicted context. This article provides an overview of the current state of the art in mobile sensing and context prediction paving the way for full-fledged anticipatory mobile computing. We present a survey of phenomena that mobile phones can infer and predict, and offer a description of machine learning techniques used for such predictions. We then discuss proactive decision making and decision delivery via the user-device feedback loop. Finally, we discuss the challenges and opportunities of anticipatory mobile computing.", "Language": "en", "Citations": "86"},
{},
{"Title": "Development of Slovenian broadcast news speech database", "Authors": ["Zibert J.", "Mihelic F."], "Keywords": [], "Date": "2004", "Abstract": "The paper reviews the development of a new Slovenian broadcast news speech database. The database consists of audio, video and annotation transcripts of about 34 hours of television daily news program captured from the public TV station RTVSLO. The paper addresses issues concerning transcription and annotation of the collected data, provides information on content analysis and basic statistics of the collected material and reports about preliminary evaluation of automatic segmentation.", "Language": "en", "Citations": "14"},
{"Title": "An experiment in Robot discovery with ILP", "Authors": ["Leban G.", "Zabkar J.", "Bratko I."], "Keywords": [], "Date": "2008", "Abstract": "We describe an experiment in the application of ILP to autonomous discovery in a robotic domain. An autonomous robot is performing experiments in its world, collecting data and formulating predictive theories about this world. In particular, we are interested in the robot's \"gaining insights\" through predicate invention. In the first experimental scenario in a pushing blocks domain, the robot discovers the notion of objects' movability. The second scenario is about discovering the notion of obstacle. We describe experiments with a simulated robot, as well as an experiment with a real robot when robot's observations contain noise. \u00a9 Springer-Verlag Berlin Heidelberg 2008.", "Language": "en", "Citations": "11"},
{"Title": "Beyond dataflow", "Authors": ["Robic B.", "Silc J.", "Ungerer T."], "Keywords": ["Coarse-grain dataflow", "Computer architecture", "Hybrid von neumann/dataflow", "Micro dataflow", "Risc dataflow", "Superscalar microprocessor", "Survey", "Threaded dataflow"], "Date": "2000", "Abstract": "This paper presents some recent advanced dataflow architectures. While the dataflow concept offers the potential of high performance, the performance of an actual dataflow implementation can be restricted by a limited number of functional units, limited memory bandwidth, and the need to associatively match pending operations with available functional units. Since the early 1970s, there have been significant developments in both fundamental research and practical realizations of dataflow models of computation. In particular, there has been active research and development in multithreaded architectures that evolved from the dataflow model. Also some other techniques for combining control-flow and dataflow emerged, such as coarse-grain dataflow, dataflow with complex machine operations, RISC dataflow, and micro dataflow. These developments have also had certain impact on the conception of highperformance superscalar processors in the \"post-RISC\" era.", "Language": "en", "Citations": "3"},
{"Title": "Modeling basketball play-by-play data", "Authors": ["Vracar P.", "Strumbelj E.", "Kononenko I."], "Keywords": ["Decision tree", "Forecasting", "Logistic regression", "Markov process", "NBA"], "Date": "2016", "Abstract": "We present a methodology for generating a plausible simulation of a basketball match between two distinct teams as a sequence of team-level play-by-play in-game events. The methodology facilitates simple inclusion into any expert system and decision-making process that requires the performance evaluation of teams under various scenarios. Simulations are generated using a random walk through a state space whose states represent the in-game events of interest. The main idea of our approach is to extend the state description to capture the current context in the progression of a game. Apart from the in-game event label, the extended state description also includes game time, the points difference, and the opposing teams' characteristics. By doing so, the model's transition probabilities become conditional on a broader game context (and not solely on the current in-game event), which brings several advantages: it provides a means to infer the teams' specific behavior in relation to their characteristics, and to mitigate the intrinsic non-homogeneity of the progression of a basketball game (which is especially evident near the end of the game). To simplify the modeling of the transition distribution, we factorize it into terms that can be estimated with separate models. We applied the presented methodology to three seasons of National Basketball Association (NBA) games. Empirical evaluation shows that the proposed model outperforms the state-of-the-art in terms of forecasting accuracy and in terms of the plausibility of the generated simulations.", "Language": "en", "Citations": "9"},
{"Title": "SSERBC 2017: Sclera segmentation and eye recognition benchmarking competition", "Authors": ["Das A.", "Pal U.", "Ferrer M.A.", "Blumenstein M.", "Stepec D.", "Rot P.", "Emersic Z.", "Peer P.", "Struc V.", "Kumar S.V.A.", "Harish B.S."], "Keywords": [], "Date": "2017", "Abstract": "This paper summarises the results of the Sclera Segmentation and Eye Recognition Benchmarking Competition (SSERBC 2017). It was organised in the context of the International Joint Conference on Biometrics (IJCB 2017). The aim of this competition was to record the recent developments in sclera segmentation and eye recognition in the visible spectrum (using iris, sclera and peri-ocular, and their fusion), and also to gain the attention of researchers on this subject. In this regard, we have used the Multi-Angle Sclera Dataset (MASD version 1). It is comprised of2624 images taken from both the eyes of 82 identities. Therefore, it consists of images of 164 (82\u00d72) eyes. A manual segmentation mask of these images was created to baseline both tasks. Precision and recall based statistical measures were employed to evaluate the effectiveness of the segmentation and the ranks of the segmentation task. Recognition accuracy measure has been employed to measure the recognition task. Manually segmented sclera, iris and peri-ocular regions were used in the recognition task. Sixteen teams registered for the competition, and among them, six teams submitted their algorithms or systems for the segmentation task and two of them submitted their recognition algorithm or systems. The results produced by these algorithms or systems reflect current developments in the literature of sclera segmentation and eye recognition, employing cutting edge techniques. The MASD version 1 dataset with some of the ground truth will be freely available for research purposes. The success of the competition also demonstrates the recent interests of researchers from academia as well as industry on this subject.", "Language": "en", "Citations": "3"},
{"Title": "Reliable diagnostics for coronary artery disease", "Authors": ["Kukar M.", "Groselj C."], "Keywords": [], "Date": "2002", "Abstract": "In the past decades Machine Learning tools have been successfully used in several medical diagnostic problems. While they often significantly outperform expert physicians (in terms of diagnostic accuracy, sensitivity, and specificity), they are mostly not being used in practice. One reason for this is that it is difficult to obtain an unbiased estimation of diagnosis' reliability. We discuss how reliability of diagnoses is assessed in medical decision making and propose a general framework for reliability estimation in Machine Learning, based on transductive inference. We compare our approach with the usual Machine Learning probabilistic approach as well as with classical stepwise diagnostic process where the reliability of diagnosis is presented as its post-test probability. The proposed transductive approach is evaluated in a practical problem of clinical diagnosis of the coronary artery disease. Significant improvements over existing techniques are achieved.", "Language": "en", "Citations": "7"},
{"Title": "Dopaminergic medication alters auditory distractor processing in Parkinson's disease", "Authors": ["Georgiev D.", "Jahanshahi M.", "Dreo J.", "Cus A.", "Pirtosek Z.", "Repovs G."], "Keywords": ["Event-related potentials", "Executive functions", "Movement disorders", "P3", "Parkinson's disease (PD)", "Visual and auditory attention"], "Date": "2015", "Abstract": "Parkinson's disease (PD) patients show signs of cognitive impairment, such as executive dysfunction, working memory problems and attentional disturbances, even in the early stages of the disease. Though motor symptoms of the disease are often successfully addressed by dopaminergic medication, it still remains unclear, how dopaminergic therapy affects cognitive function. The main objective of this study was to assess the effect of dopaminergic medication on visual and auditory attentional processing. 14 PD patients and 13 matched healthy controls performed a three-stimulus auditory and visual oddball task while their EEG was recorded. The patients performed the task twice, once on- and once off-medication. While the results showed no significant differences between PD patients and controls, they did reveal a significant increase in P3 amplitude on- vs. off-medication specific to processing of auditory distractors and no other stimuli. These results indicate significant effect of dopaminergic therapy on processing of distracting auditory stimuli. With a lack of between group differences the effect could reflect either 1) improved recruitment of attentional resources to auditory distractors; 2) reduced ability for cognitive inhibition of auditory distractors; 3) increased response to distractor stimuli resulting in impaired cognitive performance; or 4) hindered ability to discriminate between auditory distractors and targets. Further studies are needed to differentiate between these possibilities.", "Language": "en", "Citations": "8"},
{"Title": "Advanced framework for digital forensic technologies and procedures", "Authors": ["Trcek D.", "Abie H.", "Skomedal A.", "Starc I."], "Keywords": ["Admissible evidence", "Digital forensics", "Forensic procedures", "Forensic science", "Sensor networks", "Service-oriented architectures"], "Date": "2010", "Abstract": "Recent trends in global networks are leading toward service-oriented architectures and sensor networks. On one hand of the spectrum, this means deployment of services from numerous providers to form new service composites, and on the other hand this means emergence of Internet of things. Both these kinds belong to a plethora of realms and can be deployed in many ways, which will pose serious problems in cases of abuse. Consequently, both trends increase the need for new approaches to digital forensics that would furnish admissible evidence for litigation. Because technology alone is clearly not sufficient, it has to be adequately supported by appropriate investigative procedures, which have yet become a subject of an international consensus. This paper therefore provides appropriate a holistic framework to foster an internationally agreed upon approach in digital forensics along with necessary improvements. It is based on a top-down approach, starting with legal, continuing with organizational, and ending with technical issues. More precisely, the paper presents a new architectural technological solution that addresses the core forensic principles at its roots. It deploys so-called leveled message authentication codes and digital signatures to provide data integrity in a way that significantly eases forensic investigations into attacked systems in their operational state. Further, using a top-down approach a conceptual framework for forensics readiness is given, which provides levels of abstraction and procedural guides embellished with a process model that allow investigators perform routine investigations, without becoming overwhelmed by low-level details. As low-level details should not be left out, the framework is further evaluated to include these details to allow organizations to configure their systems for proactive collection and preservation of potential digital evidence in a structured manner. The main reason behind this approach is to stimulate efforts on an internationally agreed \"template legislation,\" similarly to model law in the area of electronic commerce, which would enable harmonized national implementations in the area of digital forensics. \u00a9 2010 American Academy of Forensic Sciences.", "Language": "en", "Citations": "11"},
{"Title": "Detecting semantic shifts in Slovene twitterese", "Authors": ["Fiser D.", "Ljubesic N."], "Keywords": ["Distributional semantics", "Semantic shift detection", "Tweets", "User-generated content", "Word embeddings"], "Date": "2016", "Abstract": "This paper presents first results of automatic semantic shift detection in Slovene tweets. We use word embeddings to compare the semantic behaviour of common words frequently occurring in a reference corpus of Slovene with their behaviour on Twitter. Words with the highest model distance between the corpora are considered as semantic shift candidates. They are manually analysed and classified in order to evaluate the proposed approach as well as to gain a better qualitative understanding of the nature of the problem. Apart from the noise due to preprocessing errors (45%), the approach yields a lot of valuable candidates, especially the novel senses occurring due to daily events and the ones produced in informal communication settings.", "Language": "en", "Citations": "1"},
{"Title": "Fuzzy cellular automata: From theory to applications", "Authors": ["Mraz M.", "Zimic N.", "Lapanja I.", "Bajec I."], "Keywords": ["Application software", "Automata", "Computer architecture", "Fires", "Fuzzy logic", "Fuzzy sets", "Information science", "Neck"], "Date": "2000", "Abstract": "In this paper we present a fuzzified cellular automata structure called fuzzy cellular automata. We begin our paper with a fuzzified entity called fuzzy automaton, then we present basics of cellular automata and finally we define fuzzy cellular automata. At the end we present some simulation results from the field of fire spread in homogeneous nature environment.", "Language": "en", "Citations": "14"},
{"Title": "Improving the reliability of medical software by predicting the dangerous software modules", "Authors": ["Podgorelec V.", "Hericko M.", "Juric M.B.", "Rozman I."], "Keywords": ["Medical software", "Prediction", "Software reliability"], "Date": "2005", "Abstract": "Software reliability analysis is inevitable for modern medical systems, since a large amount of medical system functionality is now dependent on software, and software does contribute to system failures. Most software reliability models are based on software failure data collected from the project. This creates a problem for the designers since, during the early stage, software failure data are not available. However, a valuable knowledge can be learned from the analysis of previous projects and applied to the new ones. This paper presents the approach that predicts the potentially dangerous software modules under development based on the analysis of the already finished modules using the machine-learning techniques. On the basis of the prediction given by our method software designers are able to devote more testing effort to the dangerous parts of the system, which results in a more reliable medical software system. \u00a9 2005 Springer Science+Business Media, Inc.", "Language": "en", "Citations": "1"},
{"Title": "System dynamics based risk management for distributed information systems", "Authors": ["Trcek D."], "Keywords": ["Distributed information systems", "Pervasive computing", "Risk management", "Simulation", "System dynamics"], "Date": "2009", "Abstract": "Networked information systems have not been restricted to closed organizations environments for more than a decade. They are now crucial in supporting operations of infrastructure, ranging from power plants to air-control systems. These networked information systems, essentially forming the current internet, are thus highly sensitive kinds of infrastructure where security plays a central role. However, assuring their security has to address certain specific aspects with regard to risk management. This paper presents a new approach to support decision making in this complex area. It is based on a generic risk management model for distributed information systems that deploys system dynamics. Such an approach provides many advantages, like suitability for interdisciplinary use, providing a graphical view on the system structure and components relationships, real-time support for \"what-if\" scenarios, and the possibility for inclusion in automated decision support systems. It is especially suitable for education and risk awareness programs in organizations. \u00a9 2009 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Web-Based Vascular Flow Simulation Visualization with Lossy Data Compression for Fast Transmission", "Authors": ["Oblak R.", "Bohak C.", "Marolt M."], "Keywords": ["Blood flow simulation", "Data visualization", "Visualization Toolkit"], "Date": "2018", "Abstract": "In this paper, we present a web-based system for visualization of flow simulation results in the vascular system for use with consumer-level hardware. The presented tool allows users to design, execute and visualize a flow simulation with a simple workflow on a desktop computer or a mobile device. The web interface allows users to select a vascular model, define the flow simulation parameters, execute the simulation, and interactively visualize the simulation results in real time using multiple visualization techniques. The server-side prepares the model for simulation and performs the simulation using SimVascular. To provide a more efficient transfer of the large amounts of simulation results to the web client, as well as reduce storage requirements on the server, we introduce a novel hybrid lossy compression method. The method uses an octree data subdivision approach combined with an iterative approach that regresses the data points to a B-Spline volume. The evaluation results show that our method achieves compression ratios of up to 5.7 for the tested examples at a given error rate, comparable to other approaches while specifically intended for visualization purposes.", "Language": "en", "Citations": "0"},
{"Title": "Analysis of medications change in Parkinson\u2019s disease progression data", "Authors": ["Valmarska A.", "Miljkovic D.", "Lavrac N.", "Robnik-Sikonja M."], "Keywords": ["Clustering", "Parkinson\u2019s disease", "Quality of life indicators", "Short time series", "Skip-grams"], "Date": "2018", "Abstract": "Parkinson\u2019s disease is a neurodegenerative disorder that affects people worldwide. Careful management of patient\u2019s condition is crucial to ensure the patient\u2019s independence and quality of life. This is achieved by personalized treatment based on individual patient\u2019s symptoms and medical history. The aim of this study is to determine patient groups with similar disease progression patterns coupled with patterns of medications change that lead to the improvement or decline of patients\u2019 quality of life symptoms. To this end, this paper proposes a new methodology for clustering of short time series of patients\u2019 symptoms and prescribed medications data, and time sequence data analysis using skip-grams to monitor disease progression. The results demonstrate that motor and autonomic symptoms are the most informative for evaluating the quality of life of Parkinson\u2019s disease patients. We show that Parkinson\u2019s disease patients can be divided into clusters ordered in accordance with the severity of their symptoms. By following the evolution of symptoms for each patient separately, we were able to determine patterns of medications change which can lead to the improvement or worsening of the patients\u2019 quality of life.", "Language": "en", "Citations": "2"},
{"Title": "Simulating predator attacks on schools: Evolving composite tactics", "Authors": ["Demsar J.", "Hemelrijk C.K.", "Hildenbrandt H.", "Bajec I.L."], "Keywords": ["Individual based model", "Predator attack tactics", "Predator tactic evolution", "Predator-prey interactions"], "Date": "2015", "Abstract": "One hypothesis about the origins and evolution of coordinated animal movements is that they may serve as a defensive mechanism against predation. Earlier studies of the possible evolution of coordinated movement in prey concentrated on predators with simple attack tactics. Numerous studies, however, suggest that to overcome the apparent defensive mechanisms which grouping and coordinated movement may provide to prey, predators in nature appear to use elaborate target selection and pursuit/hunting tactics. We here study predators that use composite tactics, (a) predators that in successive attacks based on probability choose one of several simple attack tactics, (b) predators that first disperse prey and then pick off isolated individuals. We develop an individual based model of a group of prey that is attacked by a solitary predator agent. By using genetic algorithms, we enable the predator agent to adapt (a) the probability that a specific tactic will be selected in the next attack, (b) the distance at which it stops dispersing the prey and the radius within which it searches for the most isolated prey. With a direct competition of the evolved predator agents we examine which is the better tactic against a group of prey moving in a polarized cohesive manner in three different settings. Our results suggest that, (a) a delayed response is an efficient advanced prey defence tactic, (b) predator confusion plays an important role in the evolution of composite tactics, and (c) when confusion is at play, the dispersing predator is a much better hunter, capable of at least partially diminishing the effectiveness of the prey's delayed response.", "Language": "en", "Citations": "11"},
{"Title": "Mesh-partitioning with the multiple ant-colony algorithm", "Authors": ["Korosec P.", "Silc J.", "Robic B."], "Keywords": [], "Date": "2004", "Abstract": "We present two heuristic mesh-partitioning methods, both of which build on the multiple ant-colony algorithm in order to improve the quality of the mesh partitions. The first method augments the multiple ant-colony algorithm with a multilevel paradigm, whereas the second uses the multiple ant-colony algorithm as a refinement to the initial partition obtained by vector quantization. The two methods are experimentally compared with the well-known mesh-partitioning programs. \u00a9 2004 Springer-Verlag.", "Language": "en", "Citations": "5"},
{"Title": "Exploring the Influences of the Use of Elements Comprising Information System Development Methodologies on Strategic Business Goals", "Authors": ["Hovelja T.", "Vasilecas O.", "Vavpotic D."], "Keywords": ["evaluation of information systems development methodologies", "information systems development methodologies", "strategic business goals", "strategic management"], "Date": "2015", "Abstract": "As the competitive pressure of the global market for information systems (IS) continues to increase, IS development enterprises should start to consider if and how the use of IS development methodologies (ISDM) influences their main strategic business goals. More precisely, they should start to consider two different dimensions of the actual use of ISDM: the number of times an opportunity for ISDM use arises and the number of times the ISDM is actually used. Otherwise, they run the risk of mismanaging their ISDM-related investments. The goal of this study is to develop a model that would enable academics and IS practitioners to better examine and understand how different dimensions of the use of ISDM influence strategic business goals of cost leadership, differentiation and cornering niche markets in IS development enterprises. Given the limited literature on the research topic, this study was considered exploratory and theory building in nature. The main result of the presented exploratory study is a clearly defined model for examining how different dimensions of ISDM influence strategic business goals. Exploratory results show that the actual use of ISDM has a significantly positive influence on strategic business goals of differentiation and cornering of niche markets, but not the cost leadership.", "Language": "en", "Citations": "1"},
{"Title": "A Comment on the Bias of Probabilities Derived From Betting Odds and Their Use in Measuring Outcome Uncertainty", "Authors": ["Strumbelj E."], "Keywords": ["entropy", "ordered logit", "outcome uncertainty", "probability forecasts", "Shin\u2019s model", "sports", "Theil index"], "Date": "2016", "Abstract": "Probabilities from bookmaker odds are often used in measures of short-run outcome uncertainty. We analyzed the most commonly used methods for deriving probability forecasts from odds and found that basic normalization (BN) produces biased probabilities. Furthermore, differences between probabilities produced with BN, regression models, or Shin probabilities are large enough to lead to contradictory conclusions when used to measure outcome uncertainty. We also provide evidence against the reported bias of bookmakers favoring better supported teams and show how past evidence of such a bias is possibly only due to a misinterpretation of the results.", "Language": "en", "Citations": "10"},
{"Title": "Deep Learning-Based Channel Prediction in Realistic Vehicular Communications", "Authors": ["Joo J.", "Park M.C.", "Han D.S.", "Pejovic V."], "Keywords": ["channel prediction", "Channel state information", "LSTM", "neural networks", "vehicular communications"], "Date": "2019", "Abstract": "Access to reliable estimates of the wireless channel, such as the channel state information (CSI) and the received signal strength would open opportunities for timely adaptation of transmission parameters and consequently increased throughput and transmission efficiency in vehicular communications. To design the adaptive transmission schemes, it is important to understand the realistic channel properties, especially in vehicular environments where the mobility of communication devices causes rapid channel variation. However, getting CSI estimates is challenging due to the lack of support for obtaining CSI from the chipset. In this paper, we present our efforts towards enabling reliable, up-to-date channel estimates in vehicular communications. We begin by designing and conducting a measurement campaign where we collect IQ (in-phase and quadrature) samples of the IEEE 802.11p transmission and implement CSI extraction algorithms to obtain and analyze wireless channel estimates from various real-world environments. We then propose a deep learning-based channel prediction for predicting future CSI and received signal levels. The trace-based evaluation demonstrates that our prediction approach improves the future power level estimate by 15% to 25% in terms of the root-mean-square-error compared to the latest known channel properties, thus, providing a sound basis for future efforts in anticipatory vehicular communication transmission adaptation.", "Language": "en", "Citations": "0"},
{"Title": "Robust stride segmentation of inertial signals based on local cyclicity estimation", "Authors": ["Sprager S.", "Juric M.B."], "Keywords": ["Biomedical signal processing", "Gait assessment", "Inertial sensors", "Inertial signals", "Stride segmentation"], "Date": "2018", "Abstract": "A novel approach for stride segmentation, gait sequence extraction, and gait event detection for inertial signals is presented. The approach operates by combining different local cyclicity estimators and sensor channels, and can additionally employ a priori knowledge on the fiducial points of gait events. The approach is universal as it can work on signals acquired by different inertial measurement unit (IMU) sensor types, is template-free, and operates unsupervised. A thorough evaluation was performed with two datasets: our own collected FRIgait dataset available for open use, containing long-term inertial measurements collected from 57 subjects using smartphones within the span of more than one year, and an FAU eGait dataset containing inertial data from shoe-mounted sensors collected from three cohorts of subjects: healthy, geriatric, and Parkinson\u2019s disease patients. The evaluation was performed in controlled and uncontrolled conditions. When compared to the ground truth of the labelled FRIgait and eGait datasets, the results of our evaluation revealed the high robustness, efficiency (F-measure of about 98%), and accuracy (mean absolute error MAE in about the range of one sample) of the proposed approach. Based on these results, we conclude that the proposed approach shows great potential for its applicability in procedures and algorithms for movement analysis.", "Language": "en", "Citations": "1"},
{"Title": "Lower bounds for topological complexity", "Authors": ["Franc A.", "Pavesic P."], "Keywords": ["Fibrewise Lusternik-Schnirelmann category", "Topological complexity"], "Date": "2013", "Abstract": "We introduce fibrewise Whitehead and Ganea definitions of monoidal topological complexity. We then define several lower bounds which improve on the standard lower bound in terms of nilpotency of the cohomology ring. Finally, the relationships between these lower bounds are studied. \u00a9 2013 Elsevier B.V.", "Language": "en", "Citations": "3"},
{"Title": "Wavelet based denoising integrated into multilayered perceptron", "Authors": ["Lotric U."], "Keywords": ["Denoising", "Gradient descent threshold adaptation", "Multilayered perceptron", "Time series prediction", "Wavelet multiresolution analysis"], "Date": "2004", "Abstract": "A denoising unit based on wavelet multiresolution analysis is added ahead of the multilayered perceptron. The cost function used in neural network learning is also applied as the denoising criterion and hence denoising itself is treated as a part of the integrated model. By introducing continuously derivable generalized soft thresholding function and infinite thresholds, a gradient based learning algorithm for simultaneous setting of all free parameters of the model is derived. The proposed model outmatches the classical multilayered perceptron and the multilayered perceptron with statistical denoising in noisy time series prediction problems. \u00a9 2004 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "21"},
{"Title": "Predictive power of fantasy sports data for soccer forecasting", "Authors": ["Strumbelj E.", "Robnik-Sikonja M."], "Keywords": ["Data analytics", "Fantasy sport game", "Forecasting", "Soccer"], "Date": "2015", "Abstract": "We analyse data from 5,000 competitors who participated in an online soccer managerial game which revolved around the English Premier League (EPL). We show that competitors incorporate into their decisions relevant information about the outcome of a soccer match. Furthermore, forecasts based on managerial game data are significantly better than random forecasts, forecasts based on relative frequency, and forecasts based on teams' attendance, but worse than bookmaker odds. Our work provides an evidence that crowds poses significant amount of information for the match outcome prediction.", "Language": "en", "Citations": "0"},
{"Title": "Reconstruction of the web application hypertext model using web logs", "Authors": ["Rozanc I.", "Pozenel M."], "Keywords": ["ATG", "Hypertext model", "Reverse engineering", "Web application", "Web log"], "Date": "2014", "Abstract": "An approach for automatic reconstruction of a part of a web application description, namely the hypertext model, using web logs is presented in the paper. The approach includes an advanced analysis of a large amount of the web logs data and the construction of the Application Transition Graph (ATG) as the formal description of hypertext model. Although the proposed reconstruction is limited on the navigational structure of the web application, the defined process does not depend on any particular technology and can be performed in an automatic way. The obtained results are (1) a complete model of those parts of a web application that are actually in use and (2) a very good starting point for obtaining a complete model including the unused parts as well. In either case, the approach significantly reduces the complexity and costs of reverse engineering. As a case study the approach is applied on the web student information system e \u030c Student from the University of Ljubljana, Slovenia.", "Language": "en", "Citations": "0"},
{"Title": "Initial state perturbations as a validation method for data-driven fuzzy models of cellular networks", "Authors": ["Magdevska L.", "Mraz M.", "Zimic N.", "Moskon M."], "Keywords": ["Circadian clock", "Data-driven modelling", "Dynamic modelling", "Fuzzy logic", "MAPK signalling pathway", "Model validation"], "Date": "2018", "Abstract": "Background: Data-driven methods that automatically learn relations between attributes from given data are a popular tool for building mathematical models in computational biology. Since measurements are prone to errors, approaches dealing with uncertain data are especially suitable for this task. Fuzzy models are one such approach, but they contain a large amount of parameters and are thus susceptible to over-fitting. Validation methods that help detect over-fitting are therefore needed to eliminate inaccurate models. Results: We propose a method to enlarge the validation datasets on which a fuzzy dynamic model of a cellular network can be tested. We apply our method to two data-driven dynamic models of the MAPK signalling pathway and two models of the mammalian circadian clock. We show that random initial state perturbations can drastically increase the mean error of predictions of an inaccurate computational model, while keeping errors of predictions of accurate models small. Conclusions: With the improvement of validation methods, fuzzy models are becoming more accurate and are thus likely to gain new applications. This field of research is promising not only because fuzzy models can cope with uncertainty, but also because their run time is short compared to conventional modelling methods that are nowadays used in systems biology.", "Language": "en", "Citations": "0"},
{"Title": "Commuting graphs and extremal centralizers", "Authors": ["Dolinar G.", "Guterman A.", "Kuzma B.", "Oblak P."], "Keywords": ["Centralizer", "Commuting graph", "Matrix ring"], "Date": "2014", "Abstract": "We determine the conditions for matrix centralizers which can guarantee the connectedness of the commuting graph for the full matrix algebra M ", "Language": "en", "Citations": "8"},
{"Title": "Two-stage flexible-choice problems under uncertainty", "Authors": ["Mihelic J.", "Mahjoub A.", "Rapine C.", "Robic B."], "Keywords": ["Combinatorial optimization", "Complexity theory", "Decision analysis", "Network flows", "Scenarios", "Uncertainty modeling"], "Date": "2010", "Abstract": "A significant input-data uncertainty is often present in practical situations. One approach to coping with this uncertainty is to describe the uncertainty with scenarios. A scenario represents a potential realization of the important parameters of the problem. In this paper we apply a recent approach, called flexibility, to solving two-stage flexible-choice problems. The first stage represents the present, where a decision maker must plan ahead to make a decision to hedge against uncertainty in the second stage, which represents the uncertain future, and is described as a set of scenarios. When one of the future scenarios is realized, a decision maker is willing to pay some recourse cost to augment the earlier solution to be more suitable for the realized scenario. Since all of the future scenarios are known, it is reasonable to presume that their desired solutions are also known. Thus, the aim of a decision maker is to find a solution in the present that is as easy as possible to adapt to solutions in the future. In this paper we study the problem where feasible solutions of the first stage are all p-element subsets of some finite set, and the solutions of the second stage are fixed p-element subsets. We present computational complexity results and algorithms for two versions of the two-stage flexible-choice problem. We formally define both problems, i.e., the sum-flexibility problem and the max-flexibility problem. For the sum-flexibility problem we describe an exact polynomial-time algorithm for the 3-scenario version, and we show non-approximability for the 4-scenario version. For the max-flexibility problem we show that the 3-scenario version is NP-hard, but approximable within a constant performance guarantee. Additionally, we prove non-approximability for the 4-scenario version of the problem. \u00a9 2009 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "0"},
{"Title": "Multi-agent strategic modeling in a robotic soccer domain", "Authors": ["Bezek A.", "Gams M.", "Bratko I."], "Keywords": ["Multi-agent strategic modeling", "RoboCup", "Strategy"], "Date": "2006", "Abstract": "This paper presents an algorithm for multi-agent strategic modeling (MASM) applied in a robotic soccer domain. It transforms a multi-agent action sequence into a set of strategic action descriptions in a graphical and symbolic form. By using hierarchically ordered domain knowledge, the algorithm is able to generate graphic and symbolic strategic action descriptions together with corresponding rules at different levels of abstraction. The method was evaluated on the RoboCup Soccer Server Internet League data. Copyright 2006 ACM.", "Language": "en", "Citations": "16"},
{"Title": "The RNA-binding proteomes from yeast to man harbour conserved enigmRBPs", "Authors": ["Beckmann B.M.", "Horos R.", "Fischer B.", "Castello A.", "Eichelbaum K.", "Alleaume A.-M.", "Schwarzl T.", "Curk T.", "Foehr S.", "Huber W.", "Krijgsveld J.", "Hentze M.W."], "Keywords": [], "Date": "2015", "Abstract": "RNA-binding proteins (RBPs) exert a broad range of biological functions. To explore the scope of RBPs across eukaryotic evolution, we determined the in vivo RBP repertoire of the yeast Saccharomyces cerevisiae and identified 678 RBPs from yeast and additionally 729 RBPs from human hepatocytic HuH-7 cells. Combined analyses of these and recently published data sets define the core RBP repertoire conserved from yeast to man. Conserved RBPs harbour defined repetitive motifs within disordered regions, which display striking evolutionary expansion. Only 60% of yeast and 73% of the human RBPs have functions assigned to RNA biology or structural motifs known to convey RNA binding, and many intensively studied proteins surprisingly emerge as RBPs (termed 'enigmRBPs'), including almost all glycolytic enzymes, pointing to emerging connections between gene regulation and metabolism. Analyses of the mitochondrial hydroxysteroid dehydrogenase (HSD17B10) uncover the RNA-binding specificity of an enigmRBP.", "Language": "en", "Citations": "131"},
{"Title": "FreeViz-An intelligent multivariate visualization approach to explorative analysis of biomedical data", "Authors": ["Demsar J.", "Leban G.", "Zupan B."], "Keywords": ["Explorative data analysis", "Intelligent visualisation", "Multivariate visualization", "Projection search for supervised data"], "Date": "2007", "Abstract": "Visualization can largely improve biomedical data analysis. It plays a crucial role in explorative data analysis and may support various data mining tasks. The paper presents FreeViz, an optimization method that finds linear projection and associated scatterplot that best separates instances of different class. In a single graph, the resulting FreeViz visualization can provide a global view of the classification problem being studied, reveal interesting relations between classes and features, uncover feature interactions, and provide information about intra-class similarities. The paper gives mathematical foundations of FreeViz, and presents its utility on various biomedical data sets. \u00a9 2007 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "27"},
{"Title": "Combining reconstructive and discriminative subspace methods for robust classification and regression by subsampling", "Authors": ["Fidler S.", "Skocaj D.", "Leonardis A."], "Keywords": ["CCA", "Discriminative methods", "High-breakdown point classification", "LDA", "Occlusion", "Outlier detection", "PCA", "Reconstructive methods", "Robust classification", "Robust regression", "Subsampling", "Subspace methods"], "Date": "2006", "Abstract": "Linear subspace methods that provide sufficient reconstruction of the data, such as PCA, offer an efficient way of dealing with missing pixels, outliers, and occlusions that often appear in the visual data. Discriminative methods, such as LDA, which, on the other hand, are better suited for classification tasks, are highly sensitive to corrupted data. We present a theoretical framework for achieving the best of both types of methods: An approach that combines the discrimination power of discriminative methods with the reconstruction property of reconstructive methods which enables one to work on subsets of pixels in images to efficiently detect and reject the outliers. The proposed approach is therefore capable of robust classification with a high-breakdown point. We also show that subspace methods, such as CCA, which are used for solving regression tasks, can be treated in a similar manner. The theoretical results are demonstrated on several computer vision tasks showing that the proposed approach significantly outperforms the standard discriminative methods in the case of missing pixels and images containing occlusions and outliers. \u00a9 2006 IEEE.", "Language": "en", "Citations": "134"},
{"Title": "Gene network inference by probabilistic scoring of relationships from a factorized model of interactions", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": [], "Date": "2014", "Abstract": "Motivation: Epistasis analysis is an essential tool of classical genetics for inferring the order of function of genes in a common pathway. Typically, it considers single and double mutant phenotypes and for a pair of genes observes whether a change in the first gene masks the effects of the mutation in the second gene. Despite the recent emergence of biotechnology techniques that can provide gene interaction data on a large, possibly genomic scale, few methods are available for quantitative epistasis analysis and epistasis-based network reconstruction. Results: We here propose a conceptually new probabilistic approach to gene network inference from quantitative interaction data. The approach is founded on epistasis analysis. Its features are joint treatment of the mutant phenotype data with a factorized model and probabilistic scoring of pairwise gene relationships that are inferred from the latent gene representation. The resulting gene network is assembled from scored pairwise relationships. In an experimental study, we show that the proposed approach can accurately reconstruct several known pathways and that it surpasses the accuracy of current approaches. \u00a9 2014 The Author. Published by Oxford University Press. All rights reserved.", "Language": "en", "Citations": "3"},
{"Title": "Qualitative data mining and its applications", "Authors": ["Bratko I.", "Suc D."], "Keywords": ["Data mining", "Machine learning", "Numerical regression", "Qualitative induction", "Qualitative reasoning"], "Date": "2003", "Abstract": "In machine learning from numerical data, usually the target concept is a numerical function that facilitates quantitative prediction. In contrastx to this, we consider qualitative data mining which aims at finding qualitative patterns, or qualitative relationships in numerical data. We present one approach to qualitative data mining, in which the target concepts are expressed as qualitative decision trees. We review some case studies in qualitative data mining, and discuss typical application scenarios that involve the learning of qualitative trees. \u00a9 2003 SRCE University Computing Centre.", "Language": "en", "Citations": "2"},
{"Title": "The development of the collaborative model of ICT learning systems for lifelong learning", "Authors": ["Istenic Starcic A.", "Brodnik A.", "Kljun M."], "Keywords": ["Collaborative model", "Computer network", "Computer supported collaborative learning", "Constructivism", "ICT learning system", "Interoperability", "Lifelong learning"], "Date": "2007", "Abstract": "The paper describes the development of the collaborative model of ICT learning system which accelerated introduction of e-learning in organizations with different social, cultural and technological background. It also introduced learning processes based on constructivist theory of teaching and learning. Information communication educational technologies were implemented in three forms: (1) multimedia learning material on a CD-ROM for personal computers without Internet connection, (2) computer supported learning environment for guided self-directed learning, and (3) computer supported collaborative learning environment. The aim was to foster quality of teaching and learning and raise digital literacy. Instructional design for adult learners with poor learning skills, collaboration skills and ICT knowledge was formed, and interoperability within different technological conditions, systems and delivery platforms was built.", "Language": "en", "Citations": "2"},
{"Title": "Introducing the vector C", "Authors": ["Bulic P.", "Gustin V."], "Keywords": [], "Date": "2003", "Abstract": "This paper presents the vector C (VC) language, which is designed for the multimedia extensions included in all modern microprocessors. The paper discusses the language syntax, the implementation of its compiler and its use in developing multimedia applications. The goal was to provide programmers with the most natural way of using multimedia processing facilities in the C language. The VC language has been used to develop some of the most frequently used multimedia kernels. The experiments on these scientific and multimedia applications have yielded good performance improvements. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "3"},
{"Title": "The use of prediction reliability estimates on imbalanced datasets: A case study of wall shear stress in the human carotid artery bifurcation", "Authors": ["Kosir D.", "Bosnic Z.", "Kononenko I."], "Keywords": [], "Date": "2012", "Abstract": "Data mining techniques are extensively used on medical data, which is typically composed of many normal examples and few interesting ones. When presented with highly imbalanced data, some standard classifiers tend to ignore the minority class which leads to poor performance. Various solutions have been proposed to counter this problem. Random undersampling, random oversampling, and SMOTE (Synthetic Minority Oversampling Technique) are the most well-known approaches. In recent years several approaches to evaluate the reliability of single predictions have been developed. Most recently a simple and efficient approach, based on the classifier's class probability estimates was shown to outperform the other reliability estimates. The authors propose to use this reliability estimate to improve the SMOTE algorithm. In this study, they demonstrate the positive effects of using the proposed algorithms on artificial datasets. The authors then apply the developed methodology on the problem of predicting the maximal wall shear stress (MWSS) in the human carotid artery bifurcation. The results indicate that it is feasible to improve the classifier's performance by balancing the data with their versions of the SMOTE algorithm. \u00a9 2012, IGI Global.", "Language": "en", "Citations": "0"},
{"Title": "GenePath: A system for automated construction of genetic networks from mutant data", "Authors": ["Zupan B.", "Demsar J.", "Bratko I.", "Juvan P.", "Halter J.A.", "Kuspa A.", "Shaulsky G."], "Keywords": [], "Date": "2003", "Abstract": "Motivation: Genetic networks are often used in the analysis of biological phenomena. In classical genetics, they are constructed manually from experimental data on mutants. The field lacks formalism to guide such analysis, and accounting for all the data becomes complicated when large amounts of data are considered. Results: We have developed GenePath, an intelligent assistant that automates the analysis of genetic data. GenePath employs expert-defined patterns to uncover gene relations from the data, and uses these relations as constraints in the search for a plausible genetic network. GenePath formalizes genetic data analysis, facilitates the consideration of all the available data in a consistent manner, and the examination of the large, number of possible consequences of planned experiments. It also provides an explanation, mechanism that traces every finding to the pertinent data.", "Language": "en", "Citations": "40"},
{"Title": "Continuously adaptive data fusion and model relearning for particle filter tracking with multiple features", "Authors": ["Xiao J.", "Stolkin R.", "Oussalah M.", "Leonardis A."], "Keywords": ["Color histogram", "Data fusion", "HOG feature", "Online model learning", "Particle filter", "Visual object tracking"], "Date": "2016", "Abstract": "This paper presents a new method for object tracking in a camera sensor with particle filters. The method enables multiple target and background models, arbitrarily spanning many features or imaging modalities, to be adaptively fused to provide optimal discriminating ability against changing backgrounds, which may present varying degrees of clutter and camouflage for different kinds of features at different times. Furthermore, we show how to continuously and robustly relearn all models for all feature modalities online during tracking and for targets whose appearance may be continually changing. Both the data fusion weightings and model relearning parameters are robustly adapted at each frame, by extracting contextual information to inform the saliency assessments of each part of each model. In addition, we propose a two-step estimation method for improving robustness, by preventing excessive drifting of particles during tracking past challenging, cluttered background scenes. We demonstrate the method by implementing a version of the tracker, which combines both shape and color models, and testing it on a publicly available benchmark data set. Results suggest that the proposed method outperforms a number of wellknown state-of-the-art trackers from the literature.", "Language": "en", "Citations": "12"},
{"Title": "Towards correct and informative evaluation methodology for texture classification under varying viewpoint and illumination", "Authors": ["Drbohlav O.", "Leonardis A."], "Keywords": ["Evaluation methodology", "Generalization ability", "Illumination invariance", "Texture classification", "Viewpoint invariance"], "Date": "2010", "Abstract": "3D texture classification under varying viewpoint and illumination has been a vivid research topic, and many methods have been developed. It is crucial that these methods be compared using an unbiased evaluation methodology. The most frequently employed methodologies use images from the Columbia-Utrecht Reflectance and Texture Database. These methodologies construct the training and test sets to be disjoint in the imaging parameters, but do not separate them spatially because they use images of the same surface patch for both. We perform a series of experiments which show that such practice leads to overestimation of classifier performance and distorts experimental findings. To correct that, we accurately register the images across all imaging conditions and split the surface patches to parts. The training and testing is then done on spatially disjoint parts. We show that such methodology gives a more realistic assessment of classifier performance. The sample annotations for all images are publicly available. \u00a9 2009 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "14"},
{"Title": "Survey of new research directions in microprocessors", "Authors": ["Silc J.", "Ungerer T.", "Robic B."], "Keywords": [], "Date": "2000", "Abstract": "Current microprocessors utilize the instruction-level parallelism by a deep processor pipeline and the superscalar instruction issue technique. VLSI technology offers several solutions for aggressive exploitation of the instruction-level parallelism in future generations of microprocessors. Technological advances will replace the gate delay by on-chip wire delay as the main obstacle to increase the chip complexity and cycle rate. The implication for the microarchitecture is that functionally partitioned designs with strict nearest neighbour connections must be developed. Among the major problems facing the microprocessor designers is the application of even higher degree of speculation in combination with functional partitioning of the processor, which prepares the way for exceeding the classical dataflow limit imposed by data dependences. In this paper we survey the current approaches to solving this problem, in particular we analyze several new research directions whose solutions are based on the complex uniprocessor architecture. A uniprocessor chip features a very aggressive superscalar design combined with a trace cache and superspeculative techniques. Superspeculative techniques exceed the classical dataflow limit where even with unlimited machine resources a program cannot execute any faster than the execution of the longest dependence chain introduced by the program's data dependences. Superspeculative processors also speculate about control dependences. The trace cache stores the dynamic instruction traces contiguously and fetches instructions from the trace cache rather than from the instruction cache. Since a dynamic trace of instructions may contain multiple taken branches, there is no need to fetch from multiple targets, as would be necessary when predicting multiple branches and fetching 16 or 32 instructions from the instruction cache. Multiscalar and trace processors define several processing cores that speculatively execute different parts of a sequential program in parallel. Multiscalar processors use a compiler to partition the program segments, whereas a trace processor uses a trace cache to generate dynamically trace segments for the processing cores. A datascalar processor runs the same sequential program redundantly on several processing elements where each processing element has different data set. This paper discusses and compares the performance potential of these complex uniprocessors.", "Language": "en", "Citations": "10"},
{"Title": "Grid computing", "Authors": ["Cibej U.", "Sulistio A.", "Buyya R."], "Keywords": [], "Date": "2009", "Abstract": "The vision of Grid computing is to develop a platform which gathers geographically distributed resources (such as computational power, data, and equipment) into one very powerful and easy to use system. In this chapter, we present the main motivations behind this technology. Furthermore, we outline the challenges that researchers need to face when constructing such a complex distributed system. To demonstrate the practical impact, we describe various tools and applications which are already been extensively used to solve real problems. Finally, we give some pointers to the future directions in which Grid computing will evolve. \u00a9 2009 Springer London.", "Language": "en", "Citations": "1"},
{"Title": "Finding repeating stanzas in folk songs", "Authors": ["Bohak C.", "Marolt M."], "Keywords": [], "Date": "2012", "Abstract": "Folk songs are typically composed of repeating parts - stanzas. To find such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of specific issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several methods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method consists of several steps. In the first step breathing (vocal) pauses are detected, which represent the candidate beginnings of individual stanzas. Next, a similarity measure is calculated between the first and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate beginnings represent the actual boundaries between stanzas, a scoring function is defined based on the calculated similarities between stanzas. A peak picking method is used in combination with global thresholding for the final selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive. \u00a9 2012 International Society for Music Information Retrieval.", "Language": "en", "Citations": "1"},
{"Title": "New heuristics for the vertex coloring problem based on semidefinite programming", "Authors": ["Govorcin J.", "Gvozdenovic N.", "Povh J."], "Keywords": ["Boundary point method", "Semidefinite programming", "Vertex coloring problem"], "Date": "2013", "Abstract": "The Lov\u00e1sz theta number Lov\u00e1sz (IEEE Trans Inf Theory 25:1-7, 1979) is a well-known lower bound on the chromatic number of a graph G, and \u03a8 ", "Language": "en", "Citations": "5"},
{"Title": "Frame\u2013based classification for cross-speed gait recognition", "Authors": ["Kovac J.", "Struc V.", "Peer P."], "Keywords": ["Biometric identification", "Computer vision", "Gait recognition", "Walking speed invariance"], "Date": "2019", "Abstract": "The use of human gait as the means of biometric identification has gained a lot of attention in the past few years, mostly due to its enormous potential. Such biometrics can be captured at public places from a distance without subjects collaboration, awareness and even consent. However, there are still numerous challenges caused by influence of covariate factors like changes of walking speed, view, clothing, footwear etc., that have negative impact on recognition performance. In this paper we tackle walking speed changes with a skeleton model-based gait recognition system focusing on improving algorithm robustness and improving the performance at higher walking speed changes. We achieve these by proposing frame based classification method, which overcomes the main shortcoming of distance based classification methods, which are very sensitive to gait cycle starting point detection. The proposed technique is starting point invariant with respect to gait cycle starts and as such ensures independence of classification from gait cycle start positions. Additionally, we propose wavelet transform based signal approximation, which enables the analysis of feature signals on different frequency space resolutions and diminishes the need for using feature transformation that require training. With the evaluation on OU-ISIR gait dataset we demonstrate state of the art performance of proposed methods.", "Language": "en", "Citations": "2"},
{"Title": "Prognostic value of C-reactive protein and other classical factors in patients with advanced non-small cell lung carcinoma treated in routine clinical practice", "Authors": ["Ovcaricek T.", "TriLLer N.", "Sadikov A.", "Cufer T."], "Keywords": ["Advanced disease", "C-reactive protein (CRP)", "HemogLobin (Hb)", "Non-smaLL ceLL Lung cancer (NSCLC)", "Prognostic factors"], "Date": "2010", "Abstract": "Background: Prognostic factors may help the clinician in treatment decision making. The significance of C-reactive protein (CRP) as a negative prognostic factor has been shown in patients with different malignancies. However, only few studies have analyzed CRP as a prognostic factor in patients with advanced non-small cell lung cancer (NCLCS). The aim of this study was to evaluate the prognostic value of CRP and other prognostic factors in a group of unselected population of patients with advanced NCLSC treated with platinum based chemotherapy. Methods: The retrospective study was conducted by reviewing 53 medical files of advanced NSCLC patients treated with platinum/gemcitabine at the University Clinic Golnik between May 2004 and November 2008. The median age of patients was 65 years, most of them were males (75 %), smokers or ex-smokers (81 %), with performance status 1 ( 64.2 %) and stage IV disease (83 %). The collected data included laboratory characteristics (Hb, platelet count, CRP, LDH) before chemotherapy, information on each individual patient's therapy and outcome. The median number of chemotherapy cycles received was 4 (range, 1-6).Results: The median progression free survival (PFS) for the entire group was 4.8 months (range 0-20 months). Patients with elevated CRP levels (\u2265 20 mg/l) had inferior PFS compared to those with low pretreatment CRP values (median PFS 8.4 vs 3.6 months, p=0.006). In Cox univariate regression analysis, CRP (p=0.016, HR=1.008, 95 % CI, 1.001-1.014), Hb (p=0.001, HR= 0.96, 95 %CI, 0.95-0.99), and comorbidity (p=0.051, HR=1.2, 95 %CI, 1.00-1.45) were found to be significant prognostic factors; age, LDH, and platelet count on the other hand were not found to be significant prognostic factors. In multivariate analysis only CRP (p=0.048, HR=0.50, 95 % CI 0.26-0.99) and Hb (p=0.005, HR=0.97, 95 %CI, 0.95-0.99) retained their independent prognostic value. Conclusion: The survival of patients with advanced NSCLC treated by chemotherapy is significantly influenced by the patient's pretreatment CRP and Hb levels, and comorbidity only borderline so. The major advantage of this study is that it was performed on an unselected population of patients, but still uniform with respect to diagnosis, stage and agents used in chemotherapy treatment schedule.", "Language": "en", "Citations": "2"},
{"Title": "Macro extension for SIMD processing", "Authors": ["Bulic P.", "Gustin V."], "Keywords": [], "Date": "2001", "Abstract": "The need for multimedia applications has prompted the addition of a SIMD instruction set On the one hand we have modern multimedia execution hardware and on the other we have the software and the general compilers which are not able to automatically exploit the multimedia instruction set. Our solution to these problems is to find statement candidates in the program written in the language C/C++ (as we mainly use this language), and to employ the SIMD instruction set in the easiest possible way. We proposed the algorithm for identifying candidates for parallel processing (ICPP) which is based on the syntax and semantic cheching of statements. We define the macro library MacroVect. c as the substitution for the discovered statement candidates.", "Language": "en", "Citations": "5"},
{"Title": "Procedural generation of a tropic island and coral reef Proceduralno generiranje tropskega otoka in koralnega grebena", "Authors": ["Korosec O.", "Bajec I.L."], "Keywords": ["Coral reef", "GPU", "Procedural generation", "Simulation", "Terrain generation", "Thermal and hydraulic erosion"], "Date": "2017", "Abstract": "In computer graphics there is a frequent need for displaying large vistas of a naturally looking terrain. Designing such terrain by hand is typically time consuming. With procedural generation, on the other hand, larger areas of a naturally looking terrain can be generated with or with no minimal intervention in a relatively short time. In this work we present a process of procedural generation of a tropical island with an associated corral reef. We start by generating a heightmap for the base terrain. The heightmap is then transformed by simulating processes of hydraulic and thermal erosion to achieve a more natural look of the terrain. As coral reefs often grow around tropical islands, we also simulate their growth as part of the last step. Real-time visualization is enabled during simulation, so that one can observe evolution of the terrain. Here we dynamically apply textures to the terrain based on its local characteristics. The result is a naturally looking model of a textured tropical island and corral reef.", "Language": "en", "Citations": "0"},
{"Title": "Characterization of spatiotemporal changes for the classification of dynamic contrast-enhanced magnetic-resonance breast lesions", "Authors": ["Milenkovic J.", "Hertl K.", "Kosir A.", "Zibert J.", "Tasic J.F."], "Keywords": ["Computer-aided diagnosis", "Dynamic contrast-enhanced magnetic-resonance imaging", "Mass breast lesions", "Spatial variation of temporal enhancements"], "Date": "2013", "Abstract": "Objective: The early detection of breast cancer is one of the most important predictors in determining the prognosis for women with malignant tumours. Dynamic contrast-enhanced magnetic-resonance imaging (DCE-MRI) is an important imaging modality for detecting and interpreting the different breast lesions from a time sequence of images and has proved to be a very sensitive modality for breast-cancer diagnosis. However, DCE-MRI exhibits only a moderate specificity, thus leading to a high rate of false positives, resulting in unnecessary biopsies that are stressful and physically painful for the patient and lead to an increase in the cost of treatment. There is a strong medical need for a DCE-MRI computer-aided diagnosis tool that would offer a reliable support to the physician's decision providing a high level of sensitivity and specificity. Methods: In our study we investigated the possibility of increasing differentiation between the malignant and the benign lesions with respect to the spatial variation of the temporal enhancements of three parametric maps, i.e., the initial enhancement (IE) map, the post-initial enhancement (PIE) map and the signal enhancement ratio (SER) map, by introducing additional methods along with the grey-level co-occurrence matrix, i.e., a second-order statistical method already applied for quantifying the spatiotemporal variations. We introduced the grey-level run-length matrix and the grey-level difference matrix, representing two additional, second-order statistical methods, and the circular Gabor as a frequency-domain-based method. Each of the additional methods is for the first time applied to the DCE-MRI data to differentiate between the malignant and the benign breast lesions. We applied the least-square minimum-distance classifier (LSMD), logistic regression and least-squares support vector machine (LS-SVM) classifiers on a total of 115 (78 malignant and 37 benign) breast DCE-MRI cases. The performances were evaluated using ten experiments of a ten-fold cross-validation. Results: Our experimental analysis revealed the PIE map, together with the feature subset in which the discriminating ability of the co-occurrence features was increased by adding the newly introduced features, to be the most significant for differentiation between the malignant and the benign lesions. That diagnostic test - the aforementioned combination of parametric map and the feature subset achieved the sensitivity of 0.9193 which is statistically significantly higher compared to other diagnostic tests after ten-experiments of a ten-fold cross-validation and gave a statistically significantly higher specificity of 0.7819 for the fixed 95% sensitivity after the receiver operating characteristic (ROC) curve analysis. Combining the information from all the three parametric maps significantly increased the area under the ROC curve (AUC) of the aforementioned diagnostic test for the LSMD and logistic regression; however, not for the LS-SVM. The LSMD classifier yielded the highest area under the ROC curve when using the combined information, increasing the AUC from 0.9651 to 0.9755. Conclusion: Introducing new features to those of the grey-level co-occurrence matrix significantly increased the differentiation between the malignant and the benign breast lesions, thus resulting in a high sensitivity and improved specificity. \u00a9 2013 Elsevier B.V.", "Language": "en", "Citations": "10"},
{"Title": "Goal-oriented conceptualization of procedural knowledge", "Authors": ["Mozina M.", "Guid M.", "Sadikov A.", "Groznik V.", "Bratko I."], "Keywords": ["argument-based machine learning", "chess", "domain conceptualization", "goal-oriented rule learning", "procedural knowledge"], "Date": "2012", "Abstract": "Conceptualizing procedural knowledge is one of the most challenging tasks of building systems for intelligent tutoring. We present an algorithm that enables teachers to accomplish this task semi automatically. We used the algorithm on a difficult king, bishop, and knight versus the lone king (KBNK) chess endgame, and obtained concepts that could serve as textbook instructions. A pilot experiment with students and a separate evaluation of the instructions by experienced chess trainers were deemed very positive. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "2"},
{"Title": "Attacking end users' applications by run time modifications", "Authors": ["Trampus M.", "Ciglaric M.", "Pancur M.", "Vidmar T."], "Keywords": ["Applications", "Attack", "Digital signature", "Run time modification", "Security"], "Date": "2003", "Abstract": "The paper focuses on the attacks on system and application infrastructure. Main idea of our approach is to take advantage of existing applications and attack them while they are executing. We analyze the steps that need to be taken in such attacks and point out the properties of the applications and execution environments that can be exploited. To demonstrate the findings, we present two case studies of such attacks. The first exploits a web browser which uses SSL (Secure Sockets Layer) and the second an e-mail client which uses digital signatures. In both cases we are able to successfully perform the attack which escapes the end user's notice. In the conclusion we present possible defence against such attacks.", "Language": "en", "Citations": "0"},
{"Title": "Discovering language of the stocks", "Authors": ["Pozenel M.", "Lavbic D."], "Keywords": ["Japanese candlesticks", "NLP", "Stock price prediction", "Trading strategy", "Word2Vec"], "Date": "2019", "Abstract": "Stock prediction has always been attractive area for researchers and investors since the financial gains can be substantial. However, stock prediction can be a challenging task since stocks are influenced by a multitude of factors whose influence vary rapidly through time. This paper proposes a novel approach (Word2Vec) for stock trend prediction combining NLP and Japanese candlesticks. First, we create a simple language of Japanese candlesticks from the source OHLC data. Then, sentences of words are used to train the NLP Word2Vec model where training data classification also takes into account trading commissions. Finally, the model is used to predict trading actions. The proposed approach was compared to three trading models Buy & Hold, MA and MACD according to the yield achieved. We first evaluated Word2Vec on three shares of Apple, Microsoft and Coca-Cola where it outperformed the comparative models. Next we evaluated Word2Vec on stocks from Russell Top 50 Index where our Word2Vec method was also very successful in test phase and only fall behind the Buy & Hold method in validation phase. Word2Vec achieved positive results in all scenarios while the average yields of MA and MACD were still lower compared to Word2Vec.", "Language": "en", "Citations": "0"},
{"Title": "Widespread binding of FUS along nascent RNA regulates alternative splicing in the brain", "Authors": ["Rogelj B.", "Easton L.E.", "Bogu G.K.", "Stanton L.W.", "Rot G.", "Curk T.", "Zupan B.", "Sugimoto Y.", "Modic M.", "Haberman N.", "Tollervey J.", "Fujii R.", "Takumi T.", "Shaw C.E.", "Ule J."], "Keywords": [], "Date": "2012", "Abstract": "Fused in sarcoma (FUS) and TAR DNA-binding protein 43 (TDP-43) are RNA-binding proteins pathogenetically linked to amyotrophic lateral sclerosis (ALS) and frontotemporal lobar degeneration (FTLD), but it is not known if they regulate the same transcripts. We addressed this question using crosslinking and immunoprecipitation (iCLIP) in mouse brain, which showed that FUS binds along the whole length of the nascent RNA with limited sequence specificity to GGU and related motifs. A saw-tooth binding pattern in long genes demonstrated that FUS remains bound to pre-mRNAs until splicing is completed. Analysis of FUS \u011d\u0302'/\u011d\u0302' brain demonstrated a role for FUS in alternative splicing, with increased crosslinking of FUS in introns around the repressed exons. We did not observe a significant overlap in the RNA binding sites or the exons regulated by FUS and TDP-43. Nevertheless, we found that both proteins regulate genes that function in neuronal development.", "Language": "en", "Citations": "137"},
{"Title": "General context-aware data matching and merging framework", "Authors": ["Zitnik S.", "Subelj L.", "Lavbic D.", "Vasilecas O.", "Bajec M."], "Keywords": ["entity resolution", "ontologies", "redundancy elimination", "semantic elevation", "trust"], "Date": "2013", "Abstract": "Due to numerous public information sources and services, many methods to combine heterogeneous data were proposed recently. However, general end-to-end solutions are still rare, especially systems taking into account different context dimensions. Therefore, the techniques often prove insufficient or are limited to a certain domain. In this paper we briefly review and rigorously evaluate a general framework for data matching and merging. The framework employs collective entity resolution and redundancy elimination using three dimensions of context types. In order to achieve domain independent results, data is enriched with semantics and trust. However, the main contribution of the paper is evaluation on five public domain-incompatible datasets. Furthermore, we introduce additional attribute, relationship, semantic and trust metrics, which allow complete framework management. Besides overall results improvement within the framework, metrics could be of independent interest. \u00a9 2013 Vilnius University.", "Language": "en", "Citations": "1"},
{"Title": "A decade of euroleague basketball: An analysis of trends and recent rule change effects", "Authors": ["Strumbelj E.", "Vracar P.", "Robnik-Sikonja M.", "Dezman B.", "Erculj F."], "Keywords": ["Basketball", "FIBA", "Shot-clock", "Statistics", "Three-point arc"], "Date": "2013", "Abstract": "The International Basketball Federation (FIBA) recently introduced major rule changes that came into effect with the 2010/11 season. Most notably, moving the three-point arc and changing the shot-clock. The purpose of this study was to investigate and quantify how these changes affect the game performance of top-level European basketball players. In order to better understand these changes, we also investigated past seasons and showed the presence of several trends, even in the absence of significant rule changes. A large set of game statistics for 10 seasons and 2198 Euroleague basketball games in which top European clubs competed was analyzed. Results show that the effects of the rule changes are contrary to trends in recent years. \u00a9 Editorial Committee of Journal of Human Kinetics.", "Language": "en", "Citations": "10"},
{"Title": "Prosody evaluation for embedded slovene speech-synthesis systems", "Authors": ["Mihelic F.", "Vesnicer B.", "Zibert J.", "Noth E."], "Keywords": ["Embedded systems", "HMM acoustic modeling", "Prosodic tags recognition", "Prosody modeling", "RGB kernel", "Speech synthesis", "Speech synthesis evaluation", "Support vector machines"], "Date": "2007", "Abstract": "This paper describes an evaluation of the prosody modeling in an HMM-based Slovene speech-synthesis system that is suitable for embedded systems due to its relatively small memory footprint. The objective-evaluation procedure is based on the results of the automatic recognition of syntactic-prosodic boundary positions and accented words In the synthetic speech, We have shown that the recognition results represent a close match with the prosodic notations, labeled by the human expert on the natural-speech counterpart produced by the speaker whose speech was used to train the speech-synthesis system, Therefore, the recognition rate of the prosodic events is proposed as an objective evaluation measure for the quality of the prosodic modeling in the speech-synthesis system. The results of the proposed evaluation method are also in accordance with previous subjective-listening evaluation tests, where high scores for the naturalness for such a type of speech synthesis were observed.", "Language": "en", "Citations": "3"},
{"Title": "Analysis of alternative splicing associated with aging and neurodegeneration in the human brain", "Authors": ["Tollervey J.R.", "Wang Z.", "Hortobagyi T.", "Witten J.T.", "Zarnack K.", "Kayikci M.", "Clark T.A.", "Schweitzer A.C.", "Rot G.", "Curk T.", "Zupan B.", "Rogelj B.", "Shaw C.E.", "Ule J."], "Keywords": [], "Date": "2011", "Abstract": "Age is the most important risk factor for neurodegeneration; however, the effects of aging and neurodegeneration on gene expression in the human brain have most often been studied separately. Here, we analyzed changes in transcript levels and alternative splicing in the temporal cortex of individuals of different ages who were cognitively normal, affected by frontotemporal lobar degeneration (FTLD), or affected by Alzheimer's disease (AD). We identified age-related splicing changes in cognitively normal individuals and found that these were present also in 95% of individuals with FTLD or AD, independent of their age. These changes were consistent with increased polypyrimidine tract binding protein (PTB)-dependent splicing activity. We also identified disease-specific splicing changes that were present in individuals with FTLD or AD, but not in cognitively normal individuals. These changes were consistent with the decreased neuro-oncological ventral antigen (NOVA)-dependent splicing regulation, and the decreased nuclear abundance of NOVA proteins. As expected, a dramatic down-regulation of neuronal genes was associated with disease, whereas a modest down-regulation of glial and neuronal genes was associated with aging. Whereas our data indicated that the age-related splicing changes are regulated independently of transcript-level changes, these two regulatory mechanisms affected expression of genes with similar functions, including metabolism and DNA repair. In conclusion, the alternative splicing changes identified in this study provide a new link between aging and neurodegeneration. \u00a9 2011 by Cold Spring Harbor Laboratory Press.", "Language": "en", "Citations": "109"},
{"Title": "Controlling an ant colony optimization based search in distributed datasets", "Authors": ["Slivnik B.", "Jovanovic U."], "Keywords": ["Ant colony optimization", "Distributed search"], "Date": "2007", "Abstract": "An ant colony optimization method for searching in (possibly dynamic and/or unstructured) distributed datasets, as introduced by Jovanovi\u010d et. al [1], is considered. This paper provides two new results. Firstly, it describes how this method can easily be controlled by using different kinds of ants for aggregation of data found: \"classic\" pheromone aggregation ants should be used if network load caused by a distributed search should be strictly kept within given limits, while one-time aggregation ants should be used if the search process should react quickly due to changes in a dynamic distributed dataset. Secondly, it demonstrates that one-time aggregation ants are more effective than pheromone aggregation ants.", "Language": "en", "Citations": "0"},
{"Title": "Hadwiger's conjecture for circular colorings of edge-weighted graphs", "Authors": ["Fijavz G."], "Keywords": ["Circular coloring", "Edge-weighted graph", "Edge-weighted minor", "Hadwiger's conjecture"], "Date": "2007", "Abstract": "Let G", "Language": "fr", "Citations": "1"},
{"Title": "Search versus knowledge: An empirical study of minimax on KRK", "Authors": ["Sadikov A.", "Bratko I.", "Kononenko I."], "Keywords": ["Bias", "Evaluation-function quality", "KRK chess endgame", "Minimax principle"], "Date": "2004", "Abstract": "This article presents the results of an empirical experiment designed to gain insight into what is the effect of the minimax algorithm on the evaluation function. The experiment' s simulations were performed upon the KRK chess endgame. Our results show that dependencies between evaluations of sibling nodes in a game tree and an abundance of possibilities to commit blunders present in the KRK endgame are not sufficient to explain the success of the minimax principle in practicat game-playing as was previously believed. The article shows that minimax in combination with a noisy evaluation function introduces a bias into the backed-up evaluations and argues that this bias is what masked the effectiveness of the minimax in previous studies. \u00a9 2004 by Springer Science+Business Media New York.", "Language": "en", "Citations": "0"},
{"Title": "Influence of security mechanisms on web services interoperability Vpliv varnostnih mehanizmov na povezljivost spletnih storitev", "Authors": ["Kocbek S.", "Juric M.B."], "Keywords": [".NET", "Integration", "Java", "Web services", "WS-security"], "Date": "2007", "Abstract": "By integration of information systems using web services, special focus has to be put on security. This article studies security mechanisms for web services. We describe use of cryptography, such as digital signature and encryption in WSS (Web Services Security) specification. We study support for secure web services development on Java platform and .NET framework, analyze interoperability, and implement a secure web service in Microsoft .NET and its client in Java. We identify and analyze the problems related to interoperability. When we talk about security in web services, we have to think about many things. Because web services connect many nodes in a network, we have to secure every single node with limited access, identity management, password administration, authorization, etc. But special care must also be taken of securing messages that are sent between web services. They are called SOAP messages and they are constructed from XML (extensible Markup Language). WSS standard dictates how to use security mechanisms in a SOAP message (figure 1). It does not say which security mechanisms to include; it just says how to include them. Because of that, the safety of a message depends on application's security requests and the proper use of WSS standard. There are several main security mechanisms that can be included in a SOAP message: security tokens, encrypted data, digital signatures and time stamps. Security tokens are used for authentication and authorization. There are many security tokens defined and also the introduction of new ones is allowed in WSS. Three main types of security tokens are known: username tokens, binary tokens and XML security tokens. Username security tokens are identified with \u3008UsemameToken\u3009 tag. They are designed for including username and password information into a SOAP message header. Password can be visible or hidden. Username tokens are popular mainly because of their simplicity. Binary security tokens are identified with \u3008BinarySecurityToken\u3009. They can contain binary data such as digital certificates used for authentication. XML tokens do not have common identification tag as binary and username tokens do. Instead of that every XML token has its own tag. Encryption in SOAP messages is based on XML encryption standard. It is possible to encrypt all of the content in the message or just parts of the message. Digital signature in WSS is XML digital signature. It has two important roles. First one is to verify the validity of security tokens. For instance certificates in binary security tokens can be used for user's authentication. Because just owning a certificate and include it in a message is not enough proof of sender's identity, he must sign the certificate with his personal key. The second role of digital signature in WSS is to check and proof the integrity of the message. With time stamps we can import time information in messages. They tell us when a message was created, when it was received and when it expires. We must take in consideration that time stamp refers to the time on the system the message was created on. Our thesis was that all security mechanisms mentioned above should not influence over interoperability of web services. Having that in mind, we designed an application composed of two components: a web service in .NET and its client in Java. With the application, bank account holders can use the functionality that the bank offers to them (figure 2). All the messages, sent between the service and the client, are secured with digital signature, encryption, authentication information and a time stamp. As we discovered, WSS standard has some influence on interoperability, but all the problems were successfully solved. The main problems were default algorithms used for encryption and key generation on both platforms. Especially on Java platform, the problem was JCE (Java Cryptography Extension) package which is an algorithm framework used for encryption. The package is a part of Java SDK (Software Development Kit) and has limited support for some algorithms that are illegal in some countries (table 1). In spite of all that WSS standard itself does not interfere with web service interoperability.", "Language": "en", "Citations": "0"},
{"Title": "Automatic classification of long-term ambulatory ECG records according to type of ischemic heart disease", "Authors": ["Smrdel A.", "Jager F."], "Keywords": [], "Date": "2011", "Abstract": "Background: Elevated transient ischemic ST segment episodes in the ambulatory electrocardiographic (AECG) records appear generally in patients with transmural ischemia (e. g. Prinzmetal's angina) while depressed ischemic episodes appear in patients with subendocardial ischemia (e. g. unstable or stable angina). Huge amount of AECG data necessitates automatic methods for analysis. We present an algorithm which determines type of transient ischemic episodes in the leads of records (elevations/depressions) and classifies AECG records according to type of ischemic heart disease (Prinzmetal's angina; coronary artery diseases excluding patients with Prinzmetal's angina; other heart diseases).Methods: The algorithm was developed using 24-hour AECG records of the Long Term ST Database (LTST DB). The algorithm robustly generates ST segment level function in each AECG lead of the records, and tracks time varying non-ischemic ST segment changes such as slow drifts and axis shifts to construct the ST segment reference function. The ST segment reference function is then subtracted from the ST segment level function to obtain the ST segment deviation function. Using the third statistical moment of the histogram of the ST segment deviation function, the algorithm determines deflections of leads according to type of ischemic episodes present (elevations, depressions), and then classifies records according to type of ischemic heart disease.Results: Using 74 records of the LTST DB (containing elevated or depressed ischemic episodes, mixed ischemic episodes, or no episodes), the algorithm correctly determined deflections of the majority of the leads of the records and correctly classified majority of the records with Prinzmetal's angina into the Prinzmetal's angina category (7 out of 8); majority of the records with other coronary artery diseases into the coronary artery diseases excluding patients with Prinzmetal's angina category (47 out of 55); and correctly classified one out of 11 records with other heart diseases into the other heart diseases category.Conclusions: The developed algorithm is suitable for processing long AECG data, efficient, and correctly classified the majority of records of the LTST DB according to type of transient ischemic heart disease. \u00a9 2011 Smrdel and Jager; licensee BioMed Central Ltd.", "Language": "en", "Citations": "3"},
{"Title": "Biofouling of stainless steel surfaces by four common pathogens: the effects of glucose concentration, temperature and surface roughness", "Authors": ["Bezek K.", "Nipic D.", "Torkar K.G.", "Oder M.", "Drazic G.", "Abram A.", "Zibert J.", "Raspor P.", "Bohinc K."], "Keywords": ["Biofouling", "glucose concentration", "stainless steel surface", "temperature"], "Date": "2019", "Abstract": "There is a wide range of factors affecting bacterial adhesion and biofilm formation. However, in both food processing and medical settings, it is very hard to obtain suitably controlled conditions so that the factors that reduce surface colonisation and biofouling can be studied. The aim of this study was to evaluate the effect of glucose concentration, temperature and stainless steel (SS) surface roughness on biofouling by four common pathogens (Escherichia coli, Staphylococcus aureus, Pseudomonas aeruginosa and L. monocytogenes). Among the tested variables, the untreated SS surface (3C) was shown to be fouled more than 3D polished, brushed or electropolished SS surfaces. Although an array of parameters influenced biofouling, the most promising control measure was the influence of low temperature (4 \u00b0C) that reduced biofouling even in the case of the psychrophilic Listeria monocytogenes. The study findings could significantly contribute to the prevention of SS surface contamination and consequential biofouling by food and healthcare associated pathogens.", "Language": "en", "Citations": "0"},
{"Title": "The Q-polynomial idempotents of a distance-regular graph", "Authors": ["Jurisic A.", "Terwilliger P.", "Zitnik A."], "Keywords": ["Characterization", "Distance-regular graphs", "Q-polynomial", "Tail"], "Date": "2010", "Abstract": "We obtain the following characterization of Q-polynomial distance-regular graphs. Let \u0393 denote a distance-regular graph with diameter d\u22653. Let E denote a minimal idempotent of \u0393 which is not the trivial idempotent E0. Let {\u03b8i*}i=0d denote the dual eigenvalue sequence for E. We show that E is Q-polynomial if and only if (i) the entry-wise product E\u00b0E is a linear combination of E0, E, and at most one other minimal idempotent of \u0393; (ii) there exists a complex scalar \u03b2 such that \u03b8i*1-\u03b2\u0393i*+\u03b8i+1* is independent of i for 1\u2264i\u2264d-1; (iii) \u03b8i*\u2260\u03b80* for 1\u2264i\u2264d. \u00a9 2010 Elsevier Inc.", "Language": "en", "Citations": "7"},
{"Title": "Contour based superquadric tracking", "Authors": ["Krivic J.", "Solina F."], "Keywords": ["Contours", "Object tracking", "Superquadrics"], "Date": "2003", "Abstract": "This paper proposes a technique for tracking a superquadric-modelled object over a monocular video sequences. The object is currently modelled with a single superquadric. Object's position and orientation in the first frame of the sequence are assumed known. A frame in a sequence is first processed to find object's contour. Contour is determined by extracting edges on the frame in the vicinity of model's contour from the previous frame. The model's relative translation and rotation parameters are then calculated by fitting model's contour to the frame's contour. This fitting is achieved by minimizing the cost function, which is based on model to image mapping.", "Language": "en", "Citations": "2"},
{"Title": "Automatic selection of reliability estimates for individual regression predictions", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": [], "Date": "2010", "Abstract": "In machine learning and its risk-sensitive applications (e.g. medicine, engineering, business), the reliability estimates for individual predictions provide more information about the individual prediction error (the difference between the true label and regression prediction) than the average accuracy of predictive model (e.g. relative mean squared error). Furthermore, they enable the users to distinguish between more and less reliable predictions. The empirical evaluations of the existing individual reliability estimates revealed that the successful estimates performance depends on the used regression model and on the particular problem domain. In the current paper, we focus on that problem as such and propose and empirically evaluate two approaches for automatic selection of the most appropriate estimate for a given domain and regression model: the internal cross-validation approach and the meta-learning approach. The testing results of both approaches demonstrated an advantage in the performance of dynamically chosen reliability estimates to the performance of the individual reliability estimates. The best results were achieved using the internal cross-validation procedure, where reliability estimates significantly positively correlated with the prediction error in 73% of experiments. In addition, the preliminary testing of the proposed methodology on a medical domain demonstrated the potential for its usage in practice. Copyright \u00a9 2010 Cambridge University Press.", "Language": "en", "Citations": "7"},
{"Title": "Grohar: Automated Visualization of Genome-Scale Metabolic Models and Their Pathways", "Authors": ["Moskon M.", "Zimic N.", "Mraz M."], "Keywords": ["flux balance analysis", "genome-scale metabolic models", "pathway alignment", "systems biology", "visualization of metabolic networks."], "Date": "2018", "Abstract": "Genome-scale metabolic models (GEMs) have become a powerful tool for the investigation of the entire metabolism of the organism in silico. These models are, however, often extremely hard to reconstruct and also difficult to apply to the selected problem. Visualization of the GEM allows us to easier comprehend the model, to perform its graphical analysis, to find and correct the faulty relations, to identify the parts of the system with a designated function, etc. Even though several approaches for the automatic visualization of GEMs have been proposed, metabolic maps are still manually drawn or at least require large amount of manual curation. We present Grohar, a computational tool for automatic identification and visualization of GEM (sub)networks and their metabolic fluxes. These (sub)networks can be specified directly by listing the metabolites of interest or indirectly by providing reference metabolic pathways from different sources, such as KEGG, SBML, or Matlab file. These pathways are identified within the GEM using three different pathway alignment algorithms. Grohar also supports the visualization of the model adjustments (e.g., activation or inhibition of metabolic reactions) after perturbations are induced.", "Language": "en", "Citations": "0"},
{"Title": "Image segmentation and parameterization for automatic diagnostics of whole-body scintigrams: Basic concepts", "Authors": ["Sajn L.", "Kononenko I."], "Keywords": [], "Date": "2009", "Abstract": "Bone scintigraphy or whole-body bone scan is one of the most common diagnostic procedures in nuclear medicine. Since expert physicians evaluate images manually, some automated procedure for pathology detection is desired. Scintigraphy segmentation into the main skeletal regions is briefly presented. The algorithm is simultaneously applied on anterior and posterior whole-body bone scintigrams. The expert\u2019s knowledge is represented as a set of parameterized rules, used to support image processing algorithms. The segmented bone regions are parameterized with algorithms for classifying patterns so the pathologies can be classified with machine learning algorithms. This approach enables automatic scintigraphy evaluation of pathological changes; thus, in addition to detection of pointlike high-uptake lesions, other types can be discovered. We extend the parameterization of the bone regions with a multiresolutional approach and present an algorithm for image parameterization using the association rules.", "Language": "en", "Citations": "5"},
{"Title": "Comprehensible evaluation of prognostic factors and prediction of wound healing", "Authors": ["Robnik-Sikonja M.", "Cukjati D.", "Kononenko I."], "Keywords": ["Comprehensibility of attribute evaluation", "Electric stimulation", "Machine learning", "Relief algorithms", "Wound healing prediction"], "Date": "2003", "Abstract": "We analyzed the data of a controlled clinical study of the chronic wound healing acceleration as a result of electrical stimulation. The study involved a conventional conservative treatment, sham treatment, biphasic pulsed current, and direct current electrical stimulation. Data was collected over 10 years and suffices for an analysis with machine learning methods. So far, only a limited number of studies have investigated the wound and patient attributes which affect the chronic wound healing. There is none to our knowledge to include treatment attributes. The aims of our study are to determine effects of the wound, patient and treatment attributes on the wound healing process and to propose a system for prediction of the wound healing rate. First we analyzed which wound and patient attributes play a predominant role in the wound healing process and investigated a possibility to predict the wound healing rate at the beginning of the treatment based on the initial wound, patient and treatment attributes. Later we tried to enhance the wound healing rate prediction accuracy by predicting it after a few weeks of the wound healing follow-up. Using the attribute estimation algorithms ReliefF and RReliefF we obtained a ranking of the prognostic factors which was comprehensible to experts. We used regression and classification trees to build models for prediction of the wound healing rate. The obtained results are encouraging and may form a basis for an expert system for the chronic wound healing rate prediction. If the wound healing rate is known, then the provided information can help to formulate the appropriate treatment decisions and orient resources towards individuals with poor prognosis. \u00a9 2003 Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "13"},
{"Title": "U-Sphere: Strengthening scalable flat-name routing for decentralized networks", "Authors": ["Kos J.", "Aiash M.", "Loo J.", "Trcek D."], "Keywords": ["Compact routing", "Decentralized networks", "Privacy", "Security"], "Date": "2015", "Abstract": "Supporting decentralized peer-to-peer communication between users is crucial for maintaining privacy and control over personal data. State-of-the-art protocols mostly rely on distributed hash tables (DHTs) in order to enable user-to-user communication. They are thus unable to provide transport address privacy and guaranteed low path stretch while ensuring sub-linear routing state together with tolerance of insider adversaries. In this paper we present U-Sphere, a novel location-independent routing protocol that is tolerant to Sybil adversaries and achieves low O(1) path stretch while maintaining \u00d5(\u221an) per-node state. Departing from DHT designs, we use a landmark-based construction with node color groupings to aid flat name resolution while maintaining the stretch and state bounds. We completely remove the need for landmark-based location directories and build a name-record dissemination overlay that is able to better tolerate adversarial attacks under the assumption of social trust links established between nodes. We use large-scale emulation on both synthetic and actual network topologies to show that the protocol successfully achieves the scalability goals in addition to mitigating the impact of adversarial attacks.", "Language": "en", "Citations": "3"},
{"Title": "Crosslinking-immunoprecipitation (iCLIP) analysis reveals global regulatory roles of hnRNP L", "Authors": ["Rossbach O.", "Hung L.-H.", "Khrameeva E.", "Schreiner S.", "Konig J.", "Curk T.", "Zupan B.", "Ule J.", "Gelfand M.S.", "Bindereif A."], "Keywords": ["CLIP", "hnRNP L", "microRNA", "Splicing regulation"], "Date": "2014", "Abstract": "Heterogeneous nuclear ribonucleoprotein L (hnRNP L) is a multifunctional RNA-binding protein that is involved in many different processes, such as regulation of transcription, translation, and RNA stability. We have previously characterized hnRNP L as a global regulator of alternative splicing, binding to CA-repeat, and CA-rich RNA elements. Interestingly, hnRNP L can both activate and repress splicing of alternative exons, but the precise mechanism of hnRNP L-mediated splicing regulation remained unclear. To analyze activities of hnRNP L on a genome-wide level, we performed individual-nucleotide resolution crosslinking-immunoprecipitation in combination with deep-sequencing (iCLIP-Seq). Sequence analysis of the iCLIP crosslink sites showed significant enrichment of C/A motifs, which perfectly agrees with the in vitro binding consensus obtained earlier by a SELEX approach, indicating that in vivo hnRNP L binding targets are mainly determined by the RNA-binding activity of the protein. Genome-wide mapping of hnRNP L binding revealed that the protein preferably binds to introns and 3\u2032 UTR. Additionally, position-dependent splicing regulation by hnRNP L was demonstrated: The protein represses splicing when bound to intronic regions upstream of alternative exons, and in contrast, activates splicing when bound to the downstream intron. These findings shed light on the longstanding question of differential hnRNP L-mediated splicing regulation. Finally, regarding 3\u2032 UTR binding, hnRNP L binding preferentially overlaps with predicted microRNA target sites, indicating global competition between hnRNP L and microRNA binding. Translational regulation by hnRNP L was validated for a subset of predicted target 3\u2032UTRs. \u00a9 2014 Landes Bioscience.", "Language": "en", "Citations": "48"},
{"Title": "Deduplication in unstructured-data storage systems", "Authors": ["Tolic A.", "Brodnik A."], "Keywords": ["Bloom filter", "Deduplication", "Distributed systems", "Redundancy elimination", "Storage systems"], "Date": "2015", "Abstract": "The paper addresses the issue of deduplication, a process of identifying and eliminating redundancy in large data sets of unstructured data. Storage systems for the unstructured data handle an ever increasing amount of information, a large portion of which may be redundant. While the well-known methods, such as entropy encoding, solve the issue to a certain extent, they fail to detect and eliminate the redundant data more than a few gigabytes apart. The basics of deduplication are explained and a detailed description is given of the steps involved. The state-of-the-art deduplication techniques are described.", "Language": "en", "Citations": "4"},
{"Title": "Extending applications using an advanced approach to DLL injection and API hooking", "Authors": ["Berdajs J.", "Bosnic Z."], "Keywords": ["API hooking", "Application extensions", "Detours", "DLL injection", "Programming"], "Date": "2010", "Abstract": "When programmers need to modify third-party applications, they frequently do not have access to their source code. In such cases, DLL injection and API hooking are techniques that can be used to modify applications without intervening into their source code. The commonly used varieties of injection and hooking approaches have many practical limitations: they are inconvenient for a programmer to implement, do not work reliably in conjunction with all applications and with certain low-level machine instructions. In this paper we present two novel approaches to DLL injection and API hooking, which we call Debuggeraided DLL injection and Single Instruction Hooking. Our approaches overcome the limitations of the state-of-the art approaches. Despite incurring greater execution times, our approach allows extending of the applications in situations where the comparable approaches fail. As such, it has a notable practical value for beneficial practical applications of injection and hooking approaches, which are present in malware detection programs and computer security tools. Copyright \u00a9 2010 John Wiley & Sons, Ltd. Copyright \u00a9 2010 John Wiley & Sons, Ltd.", "Language": "en", "Citations": "16"},
{"Title": "Finding optimal solutions of vaguely defined assignment problems", "Authors": ["Moskon M."], "Keywords": ["Assignment problems", "Fuzzy hungarian algorithm", "Fuzzy logic", "Hungarian algorithm", "Optimal resource assignment"], "Date": "2011", "Abstract": "The article presents an extension of the Hungarian algorithm (also known as Kuhn-Munkres algorithm) which is used for solving the assignment problems in polynomial time. The fact that the original version of the algorithm is only able to solve the assignment problems with precisely defined inputs (i.e. demands and resources) presents a major problem in many real-life scenarios while the nature of these problems is such that inputs are commonly defined only vaguely (i.e. fuzzily). In order to solve them, their precise formalization is needed which is normally far from being a straightforward procedure and can present large costs in the meaning of time and money. Fuzzy logic on the other hand successfully copes with the processing of imprecise data. The Hungarian algorithm was extended with the introduction of fuzzy logic methods in order to be able to effi- ciently solve vaguely defined assignment problems. The extended version of the algorithm (i.e. fuzzy Hungarian algorithm) is thus able to cope with vaguely defined assignment problems, can be used more efficiently (i.e. with no further formalization of vaguely defined terms) and in a wider scope of assignment problems than the basic approach. Here we describe the basic version of the Hungarian algorithm which was firstly presented by Harold Kuhn. Its extension with the introduction of fuzzy methods is also described. Its usage is justified by the comparison of the results between its crisp (i.e. basic) and fuzzy (i.e. extended) version on the same problem.", "Language": "en", "Citations": "0"},
{"Title": "Fuzzy cellular automata as a concept of synthesis approach of modelling Mehki celularni avtomati kot koncept za sintezni pristop k modeliranju", "Authors": ["Mraz M."], "Keywords": ["Cellular automata", "Conway's game of life", "Fuzzy cellular automata", "Fuzzy logic"], "Date": "2001", "Abstract": "In our paper we present a structure of fuzzy cellular automata (FCA). To have FCA defined, we expand the current definition of fuzzy automaton. The fuzzy automaton is used as an abstract model of the main FCA entity which is called a cell. Introducing the fuzzy logic into the cellular automata field enables the study of fuziness of the defining cell's neighbourhood and fuziness of the cell's (automaton's) state. At the end of our paper we also present the FCA structure usage through an application case of the well known Conway's game of life. We introduce the uncertain knowledge about the evolution of cells into fuzzy rules.", "Language": "en", "Citations": "0"},
{"Title": "Hard and soft security provisioning for computationally weak pervasive computing systems in E-health", "Authors": ["Trcek D.", "Brodnik A."], "Keywords": [], "Date": "2013", "Abstract": "We are witnessing increasing penetration of pervasive computing into business and personal environments. Its advances, among other things, enable new opportunities for better health services but, as a side effect, it introduces new threats to privacy. If anywhere, it is in the health care sector that privacy is of utmost importance. Knowing further that internet of things devices typically lack computing and energy resources, the need for providing appropriate privacy is a hard issue. This paper therefore addresses privacy for internet of things technologies by focusing on the most \u00bf primitive\u00bf members, bare sensors and RFIDs. Based on lessons learned in this domain, a strategy of incrementally adjusting existing protocols is adopted for deployment in the area of wireless medical sensors body area networks. By doing so, new contributions that are quantifiably lightweight and that enable privacy, together with confidential exchange of captured measured quantities, are provided. In addition to such hard security solutions, the paper addresses trust management methods as a complementary, soft mean for security provisioning. This latter contribution also paves the way for further development and for applications of pervasive computing in general. \u00a9 2013 IEEE.", "Language": "en", "Citations": "9"},
{"Title": "Sieve-based relation extraction of gene regulatory networks from biological literature", "Authors": ["Zitnik S.", "Zitnik M.", "Zupan B.", "Bajec M."], "Keywords": ["Conditional random fields", "Gene regulatory networks", "Literature mining", "Relation extraction"], "Date": "2015", "Abstract": "Background: Relation extraction is an essential procedure in literature mining. It focuses on extracting semantic relations between parts of text, called mentions. Biomedical literature includes an enormous amount of textual descriptions of biological entities, their interactions and results of related experiments. To extract them in an explicit, computer readable format, these relations were at first extracted manually from databases. Manual curation was later replaced with automatic or semi-automatic tools with natural language processing capabilities. The current challenge is the development of information extraction procedures that can directly infer more complex relational structures, such as gene regulatory networks. Results: We develop a computational approach for extraction of gene regulatory networks from textual data. Our method is designed as a sieve-based system and uses linear-chain conditional random fields and rules for relation extraction. With this method we successfully extracted the sporulation gene regulation network in the bacterium Bacillus subtilis for the information extraction challenge at the BioNLP 2013 conference. To enable extraction of distant relations using first-order models, we transform the data into skip-mention sequences. We infer multiple models, each of which is able to extract different relationship types. Following the shared task, we conducted additional analysis using different system settings that resulted in reducing the reconstruction error of bacterial sporulation network from 0.73 to 0.68, measured as the slot error rate between the predicted and the reference network. We observe that all relation extraction sieves contribute to the predictive performance of the proposed approach. Also, features constructed by considering mention words and their prefixes and suffixes are the most important features for higher accuracy of extraction. Analysis of distances between different mention types in the text shows that our choice of transforming data into skip-mention sequences is appropriate for detecting relations between distant mentions. Conclusions: Linear-chain conditional random fields, along with appropriate data transformations, can be efficiently used to extract relations. The sieve-based architecture simplifies the system as new sieves can be easily added or removed and each sieve can utilize the results of previous ones. Furthermore, sieves with conditional random fields can be trained on arbitrary text data and hence are applicable to broad range of relation extraction tasks and data domains.", "Language": "en", "Citations": "5"},
{"Title": "A practical approach to the 2D incremental nearest-point problem suitable for different point distributions", "Authors": ["Zadravec M.", "Brodnik A.", "Mannila M.", "Wanne M.", "Zalik B."], "Keywords": ["(a, b)-tree", "Incremental nearest-point problem", "Point distribution", "Skip list"], "Date": "2008", "Abstract": "In this paper we present a new practical approach to solve the incremental nearest-point problem in the plane. We used the proposed approach in industrial applications with a superior behaviour to the theoretically better solutions. The method efficiently avoids the requirement of initial randomization of the input points by splitting the plane in strips using a heuristic. Points in strips are stored either in (a, b)-skip lists or in (a, b)-trees. Testing of the algorithms at different point distributions shows that our algorithm, using proposed heuristic, is almost insensible to distributions of input points, what makes the algorithm very attractive for various engineering applications. \u00a9 2007 Pattern Recognition Society.", "Language": "en", "Citations": "4"},
{"Title": "Robust network community detection using balanced propagation", "Authors": ["Subelj L.", "Bajec M."], "Keywords": [], "Date": "2011", "Abstract": "Label propagation has proven to be an extremely fast method for detecting communities in large complex networks. Furthermore, due to its simplicity, it is also currently one of the most commonly adopted algorithms in the literature. Despite various subsequent advances, an important issue of the algorithm has not yet been properly addressed. Random (node) update orders within the algorithm severely hamper its robustness, and consequently also the stability of the identified community structure. We note that an update order can be seen as increasing propagation preferences from certain nodes, and propose a balanced propagation that counteracts for the introduced randomness by utilizing node balancers. We have evaluated the proposed approach on synthetic networks with planted partition, and on several real-world networks with community structure. The results confirm that balanced propagation is significantly more robust than label propagation, when the performance of community detection is even improved. Thus, balanced propagation retains high scalability and algorithmic simplicity of label propagation, but improves on its stability and performance. \u00a9 2011 EDP Sciences, SIF, Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "83"},
{"Title": "PhD forum: Hierarchical feature scheme for object recognition in visual sensor networks", "Authors": ["Sulic V.", "Pers J.", "Kristan M.", "Kovacic S."], "Keywords": ["Computer vision", "Object recognition", "Visual sensor network"], "Date": "2009", "Abstract": "We propose a framework for hierarchical feature distribution scheme for object recognition in a network of visual sensors. The key of our approach is the principle that individual nodes in the network hold only a small amount of information about objects seen by the network, however, this small amount is sufficient to efficiently route queries through the network. Preliminary results suggest that amount of data transmitted through the network can be reduced in comparison to simpler feature distribution schemes. \u00a9 2009 IEEE.", "Language": "en", "Citations": "1"},
{"Title": "Automated feedback generation for argument-based intelligent tutoring systems", "Authors": ["Guid M.", "Pavlic M.", "Mozina M."], "Keywords": ["ABML Knowledge Refinement Loop", "Argument-based Machine Learning (ABML)", "Feedback Generation", "Financial Statements", "Intelligent Tutoring Systems", "Learning by Arguing"], "Date": "2019", "Abstract": "Argument-based machine learning provides the ability to develop interactive learning environments that are able to automatically select relevant examples and counter-examples to be explained by the students. However, in order to build successful argument-based intelligent tutoring systems, it is essential to provide useful feedback on students\u2019 arguments and explanations. To this end, we propose three types of feedback for this purpose: (1) a set of relevant counter-examples, (2) a numerical evaluation of the quality of the argument, and (3) the generation of hints on how to refine the arguments. We have tested our approach in an application that allows students to learn by arguing with the aim of improving their understanding of financial statements.", "Language": "en", "Citations": "0"},
{"Title": "Ladar data generation fused with virtual targets and visualization for small drone detection system", "Authors": ["Kim B.H.", "Khan D.", "Bohak C.", "Kim J.K.", "Choi W.", "Lee H.J.", "Kim M.Y."], "Keywords": ["classification", "data fusion", "Laser Radar", "target detection", "visualization"], "Date": "2018", "Abstract": "For detection of a small target using electro-optical systems, multi-band 2D image sensors are used such as visible, NIR, MWIR, and LWIR. However, 2D imaging systems are not capable to detect a very small target and they are also not capable of calculating target 3D position coordinates to develop the strategic counter method. 3D sensors (e.g. Lidar, RGBD and stereo camera) are utilized to control unmanned vehicles for detecting threats and response for specific situations. Conventional Lidar systems are unable to detect small drone threat at distances higher than their maximum detecting range of 100 ?1/4 120 meters. To overcome this limitation, laser radar (LADAR) systems are being developed, which allow the detection at distances up to 2 kilometers. In the development of LADAR, it is difficult to acquire datasets that contain cases of long distant targets. In this study, a fusion data generation with virtual targets technique based on minimum real LADAR initial map dataset is proposed, and precise small target detection method using voxel-based clustering and classification are studied. We present the process of data fusion generation and the experimental results for a small target detection. The presented approach also includes effective visualization of high-resolution 3D data and the results of small target detection in real time. This study is expected to contribute to the optimization of a drone threat detection system for various environments and characteristics.", "Language": "en", "Citations": "2"},
{"Title": "Convex skeletons of complex networks", "Authors": ["Subelj L."], "Keywords": ["Complex networks", "Convex skeletons", "Network backbones", "Network convexity"], "Date": "2018", "Abstract": "A convex network can be defined as a network such that every connected induced subgraph includes all the shortest paths between its nodes. A fully convex network would therefore be a collection of cliques stitched together in a tree. In this paper, we study the largest high-convexity part of empirical networks obtained by removing the least number of edges, which we call a convex skeleton. A convex skeleton is a generalization of a network spanning tree in which each edge can be replaced by a clique of arbitrary size. We present different approaches for extracting convex skeletons and apply them to social collaboration and protein interactions networks, autonomous systems graphs and food webs. We show that the extracted convex skeletons retain the degree distribution, clustering, connectivity, distances, node position and also community structure, while making the shortest paths between the nodes largely unique. Moreover, in the Slovenian computer scientists coauthorship network, a convex skeleton retains the strongest ties between the authors, differently from a spanning tree or high-betweenness backbone and high-salience skeleton. A convex skeleton thus represents a simple definition of a network backbone with applications in coauthorship and other social collaboration networks.", "Language": "en", "Citations": "1"},
{"Title": "Evaluating multi-class learning strategies in a hierarchical framework for object detection", "Authors": ["Fidler S.", "Boben M.", "Leonardis A."], "Keywords": [], "Date": "2009", "Abstract": "Multi-class object learning and detection is a challenging problem due to the large number of object classes and their high visual variability. Specialized detectors usually excel in performance, while joint representations optimize sharing and reduce inference time - but are complex to train. Conveniently, sequential class learning cuts down training time by transferring existing knowledge to novel classes, but cannot fully exploit the shareability of features among object classes and might depend on ordering of classes during learning. In hierarchical frameworks these issues have been little explored. In this paper, we provide a rigorous experimental analysis of various multiple object class learning strategies within a generative hierarchical framework. Specifically, we propose, evaluate and compare three important types of multi-class learning: 1.) independent training of individual categories, 2.) joint training of classes, and 3.) sequential learning of classes. We explore and compare their computational behavior (space and time) and detection performance as a function of the number of learned object classes on several recognition datasets. We show that sequential training achieves the best trade-off between inference and training times at a comparable detection performance and could thus be used to learn the classes on a larger scale.", "Language": "en", "Citations": "16"},
{"Title": "CELF4 Regulates Translation and Local Abundance of a Vast Set of mRNAs, Including Genes Associated with Regulation of Synaptic Function", "Authors": ["Wagnon J.L.", "Briese M.", "Sun W.", "Mahaffey C.L.", "Curk T.", "Rot G.", "Ule J.", "Frankel W.N."], "Keywords": [], "Date": "2012", "Abstract": "RNA-binding proteins have emerged as causal agents of complex neurological diseases. Mice deficient for neuronal RNA-binding protein CELF4 have a complex neurological disorder with epilepsy as a prominent feature. Human CELF4 has recently been associated with clinical features similar to those seen in mutant mice. CELF4 is expressed primarily in excitatory neurons, including large pyramidal cells of the cerebral cortex and hippocampus, and it regulates excitatory but not inhibitory neurotransmission. We examined mechanisms underlying neuronal hyperexcitability in Celf4 mutants by identifying CELF4 target mRNAs and assessing their fate in the absence of CELF4 in view of their known functions. CELF4 binds to at least 15%-20% of the transcriptome, with striking specificity for the mRNA 3\u2032 untranslated region. CELF4 mRNA targets encode a variety of proteins, many of which are well established in neuron development and function. While the overall abundance of these mRNA targets is often dysregulated in Celf4 deficient mice, the actual expression changes are modest at the steady-state level. In contrast, by examining the transcriptome of polysome fractions and the mRNA distribution along the neuronal cell body-neuropil axis, we found that CELF4 is critical for maintaining mRNA stability and availability for translation. Among biological processes associated with CELF4 targets that accumulate in neuropil of mutants, regulation of synaptic plasticity and transmission are the most prominent. Together with a related study of the impact of CELF4 loss on sodium channel Na", "Language": "en", "Citations": "43"},
{"Title": "Linter - A tool for finding bugs and potential problems in Scala code", "Authors": ["Potocnik M.", "Cibej U.", "Slivnik B."], "Keywords": ["Abstract interpretation", "Scala", "Static analysis"], "Date": "2014", "Abstract": "Linter is a static analysis tool for Scala. To check for possible bugs, inefficient code, and coding style problems it combines simple pattern matching used in many similar static analysis tools for other programming languages with abstract interpretation on some builtin types like integers and strings. Taking advantage of the Scala compiler plugin interface it relies on the Scala compiler (a) to parse the source code and (b) to provide the abstract syntax tree with all needed information. This paper provides an overview of Linter and its implementation. Using a case study the performance of Linter is evaluated in terms of time consumption and code issues detected. Copyright 2014 ACM.", "Language": "en", "Citations": "1"},
{},
{"Title": "Distributed text classification with an ensemble kernel-based learning approach", "Authors": ["Silva C.", "Lotric U.", "Ribeiro B.", "Dobnikar A."], "Keywords": ["Distributed learning", "Ensembles", "Kernel-based machines", "Text classification"], "Date": "2010", "Abstract": "Constructing a single text classifier that excels in any given application is a rather inviable goal. As a result, ensemble systems are becoming an important resource, since they permit the use of simpler classifiers and the integration of different knowledge in the learning process. However, many text-classification ensemble approaches have an extremely high computational burden, which poses limitations in applications in real environments. Moreover, state-of-the-art kernel-based classifiers, such as support vector machines and relevance vector machines, demand large resources when applied to large databases. Therefore, we propose the use of a new systematic distributed ensemble framework to tackle these challenges, based on a generic deployment strategy in a cluster distributed environment. We employ a combination of both task and data decomposition of the text-classification system, based on partitioning, communication, agglomeration, and mapping to define and optimize a graph of dependent tasks. Additionally, the framework includes an ensemble system where we exploit diverse patterns of errors and gain from the synergies between the ensemble classifiers. The ensemble data partitioning strategy used is shown to improve the performance of baseline state-of-the-art kernel-based machines. The experimental results show that the performance of the proposed framework outperforms standard methods both in speed and classification. \u00a9 2006 IEEE.", "Language": "en", "Citations": "22"},
{"Title": "A topological approach to delineation and arrhythmic beats detection in unprocessed long-term ECG signals", "Authors": ["Faganeli Pucer J.", "Kukar M."], "Keywords": ["Arrhythmia", "Discrete morse theory", "ECG", "Machine learning"], "Date": "2018", "Abstract": "Background and objective: Arrhythmias are one of the most common symptoms of cardiac failure. They are usually diagnosed using ECG recordings, particularly long ambulatory recordings (AECG). These recordings are tedious to interpret by humans due to their extent (up to 48 h) and the relative scarcity of arrhythmia events. This makes automated systems for detecting various AECG anomalies indispensable. In this work we present a novel procedure based on topological principles (Morse theory) for detecting arrhythmic beats in AECG. It works in nearly real-time (delayed by a 14 s window), and can be applied to raw (unprocessed) ECG signals. Methods: The procedure is based on a subject-specific adaptation of the one-dimensional discrete Morse theory (ADMT), which represents the signal as a sequence of its most important extrema. The ADMT algorithm is applied twice; for low-amplitude, high-frequency noise removal, and for detection of the characteristic waves of individual ECG beats. The waves are annotated using the ADMT algorithm and template matching. The annotated beats are then compared to the adjacent beats with two measures of similarity: the distance between two beats, and the difference in shape between them. The two measures of similarity are used as inputs to a decision tree algorithm that classifies the beats as normal or abnormal. The classification performance is evaluated with the leave-one-record-out cross-validation method. Results: Our approach was tested on the MIT-BIH database, where it exhibited a classification accuracy of 92.73%, a sensitivity of 73.35%, a specificity of 96.70%, a positive predictive value of 88.01%, and a negative predictive value of 95.73%. Conclusions: Compared to related studies, our algorithm requires less preprocessing while retaining the capability to detect and classify beats in almost real-time. The algorithm exhibits a high degree of accuracy in beats detection and classification that are at least comparable to state-of-the-art methods.", "Language": "en", "Citations": "0"},
{"Title": "A Compact Convolutional Neural Network for Textured Surface Anomaly Detection", "Authors": ["Racki D.", "Tomazevic D.", "Skocaj D."], "Keywords": [], "Date": "2018", "Abstract": "Convolutional neural methods have proven to outperform other approaches in various computer vision tasks. In this paper we apply the deep learning technique to the domain of automated visual surface inspection. We design a unified CNN-based framework for segmentation and detection of surface anomalies. We investigate whether a compact CNN architecture, which exhibit fewer parameters that need to be learned, can be used, while retaining high classification accuracy. We propose and evaluate a compact CNN architecture on a dataset consisting of diverse textured surfaces with variously-shaped weakly-labeled anomalies. The proposed approach achieves state-of-the-art results in terms of anomaly segmentation as well as classification.", "Language": "en", "Citations": "5"},
{"Title": "Rapid ontology development model based on rule management approach in business applications", "Authors": ["Lavbic D."], "Keywords": ["Business rules", "Methodologies for ontology development", "Ontologies", "Ontology evaluation", "Rapid ontology development", "ROD", "Semantic web"], "Date": "2012", "Abstract": "In this paper rapid ontology development with emphasis on facilitating development with constant evaluation of steps in the process of ontology development is presented. The review of related work pointed out that existing methodologies for ontology development are complex and high level of technical knowledge is required. Business users and developers usually don't pose such knowledge therefore ontology completeness indicator was introduced in this approach. The role of this indicator is to guide developer throughout the development process and constantly aid user with recommendations to progress to next step and improve the quality of ontology. While evaluating the ontology, several aspects are considered; from description, partition, consistency, redundancy and to anomaly. The approach was verified on Financial Instruments and Trading Strategies (FITS) ontology and compared to other approaches.", "Language": "en", "Citations": "1"},
{"Title": "Gene discovery by chemical mutagenesis and wholegenome sequencing in Dictyostelium", "Authors": ["Li C.-L.F.", "Santhanam B.", "Webb A.N.", "Zupan B.", "Shaulsky G."], "Keywords": [], "Date": "2016", "Abstract": "Whole-genome sequencing is a useful approach for identification of chemical-induced lesions, but previous applications involved tedious genetic mapping to pinpoint the causative mutations. We propose that saturation mutagenesis under low mutagenic loads, followed by whole-genome sequencing, should allow direct implication of genes by identifying multiple independent alleles of each relevant gene. We tested the hypothesis by performing three genetic screens with chemical mutagenesis in the social soil amoeba Dictyostelium discoideum. Through genome sequencing, we successfully identified mutant genes with multiple alleles in near-saturation screens, including resistance to intense illumination and strong suppressors of defects in an allorecognition pathway. We tested the causality of the mutations by comparison to published data and by direct complementation tests, finding both dominant and recessive causative mutations. Therefore, our strategy provides a cost- And time-efficient approach to gene discovery by integrating chemical mutagenesis and whole-genome sequencing. The method should be applicable to many microbial systems, and it is expected to revolutionize the field of functional genomics in Dictyostelium by greatly expanding the mutation spectrum relative to other common mutagenesis methods.", "Language": "en", "Citations": "10"},
{"Title": "Tumour necrosis factor-alpha inhibitor-induced hepatic injury in patients with rheumatoid arthritis: Two case reports and an analysis of the laboratory data from the Slovenian national biologicals registry", "Authors": ["Perdan-Pirkmajer K.", "Hocevar A.", "Rotar Z.", "Zibert J.", "Marolt V.F.", "Gucev F.", "Tomsic M."], "Keywords": ["Alanine aminotransferase", "Aspartate aminotransferase", "Liver injury", "Rheumatoid arthritis", "TNF-alpha inhibitors"], "Date": "2013", "Abstract": "Tumour necrosis factor-alpha (TNF-\u03b1) inhibitors are widely used in the management of patients with rheumatoid arthritis (RA) and spondylarthritides. However, TNF-\u03b1 inhibition may lead to adverse events, including liver injury. The RA patients are frequently treated with several potentially hepatotoxic drugs concomitantly; hence, a causative link between TNF-\u03b1 inhibitors and liver injury is usually difficult to establish. We report two cases of RA patients who developed histologically manifest liver injury shortly after the introduction of treatment with two different TNF-\u03b1 inhibitors. Furthermore, we present the analysis of the laboratory data from the BioRx.si registry (the Slovenian national registry of rheumatologic patients treated with biologicals) and provide evidence that elevated levels of serum aminotransferase can be observed in patients treated with TNF-\u03b1 inhibitors. Additionally, our analysis suggests no significant differences between the impact of adalimumab and etanercept on aminotransferase levels. Although the use of TNF-alpha inhibitors is safe and efficient, we suggest that continuous careful monitoring of aminotransferase levels in patients treated with these agents is probably warranted. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "3"},
{"Title": "Improving vehicle aeroacoustics using machine learning", "Authors": ["Kuznar D.", "Mozina M.", "Giordanino M.", "Bratko I."], "Keywords": ["Aeroacoustics", "Automobile industry", "Machine learning", "Noise frequency spectrum analysis", "Process automation"], "Date": "2012", "Abstract": "This paper presents a new approach to improving the overall aeroacoustic comfort of a vehicle, an important feature of vehicle design. The traditional improvement process is extended to benefit extensively from machine learning, information retrieval and information extraction technologies to assist the wind tunnel engineers with difficult tasks. The paper first describes the general approach and then focuses on providing a detailed description of the most important task of assessing the degree of discomfort for a human caused by wind noise in a vehicle, when the noise spectrum is known. For this purpose a novel approach of learning linear regression models that are consistent with experts domain knowledge is presented. The results of the end user evaluation of the entire system are also presented to reflect the strengths of this approach. \u00a9 2011 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "3"},
{"Title": "Efficacy of tyrosine kinase inhibitors in routine clinical practice: Epidermal growth factor mutations and their implications", "Authors": ["Ovcaricek T.", "Cufer T.", "Kern I.", "Sodja E.", "Sadikov A."], "Keywords": ["Epidermal growth factor mutations", "Non-small cell lung cancer", "Routine clinical practice", "Tyrosine kinase inhibitors"], "Date": "2013", "Abstract": "Background: Activating mutations in the epidermal growth factor (EGFR) gene confer sensitivity to the tyrosine kinase inhibitors (TKIs) in patients with advanced non-small cell lung cancer (NSCLC). TKI treatment efficacy and EGFR mutation implications were evaluated in clinically selected advanced NSCLC patients treated with TKIs in routine clinical practice. Materials and Methods: A retrospective chart review for clinicopathological characteristics and mutation status (EGFR, KRAS) analysis of 40 consecutive patients treated with TKIs between 2005 and 2010 was performed. Statistical Analysis Used: PFS and OS were estimated by the Kaplan-Meier method, the log-rank test was used to test for differences. The strength of the associations between the EGFR mutation status and clinicopathological characteristics were tested with the Mann-Whitney U-test or the Kruskal-Wallis H-test. Results: The prevalence of EGFR mutations was 45% with a predominance of deletion mutations in exon 19 (55.5%). Significant correlations between gender, histology, and EGFR mutations were observed. Median progression-free survival (mPFS) for the entire group of patients was 8.7 months and median overall survival (mOS) was not yet reached. Patients with EGFR mutant tumors derived significantly higher benefit from TKI therapy compared to patients with mutation-negative disease; with mPFS of 22.0 vs. 3.2 months (HR: 3.9, 95% CI 1.56-9.89) and with a trend towards better OS (probability of survival at 12 months 82.0 vs. 63.0%, P = 0.080). Conclusion: We demonstrated that screening for EGFR mutations is reliable in a routine clinical setting and might allow for a better selection of NSCLC patients for anti-EGFR TKI therapy.", "Language": "en", "Citations": "4"},
{"Title": "Trust management - From pervasive computing environments to mathematical economy and sociology", "Authors": ["Trcek D."], "Keywords": ["Mathematical economics and sociology", "Pervasive computing", "Qualitative assessment dynamics", "Trust management", "Web technologies"], "Date": "2011", "Abstract": "Trust management in pervasive computing environments is one of top research areas for quite some years. Although first efforts started in the mid-nineties, the real momentum came some twelve years ago. From that time such methodologies started to appear that were addressing the core of trust phenomenon. Among those nae Bayesian statistics based methodologies should be mentioned first, then Dempster-Shaffer Theory of evidence based ones, and game theoretic ones. A different perspective, based on linguistic grounds as to operators and operands, has been taken by Qualitative Assessment Dynamics, or QAD, which is presented in this paper. QAD effectively complements other existing methodologies by addressing uncovered issues, like assumptions of rational agents, transitivity of trust, and so on. QAD enables rigorous formal treatment, is implementable in computing environments, and provides multi-disciplinary results that exceed computational environments. Its results can be used as a basis for simulations to address important questions like how to manage societies to increase possibilities that they will become more prospective in terms of pre-assured trusted relationships.", "Language": "en", "Citations": "0"},
{"Title": "An algorithm to estimate the ST segment level in 24-hour ambulatory ECG records", "Authors": ["Smrdel A.", "Jager F."], "Keywords": [], "Date": "2008", "Abstract": "We present an algorithm to estimate the ST segment level, and to construct the ST segment level function. The algorithm was developed and tested using the Long-Term ST Database (LTST DB). The algorithm determines the positions of the isoelectric level and the J point in average heart beats constructed from 16-second windows of normal and non-noisy heart beats. Then the samples of the ST segment level function are derived for each ECG lead. The aggregate average error between the amplitudes of the samples of the ST segment level functions, for 190 ECG leads of the LTST DB, constructed automatically and those constructed on the basis of manually set positions of the isoelectric level and the J point by the human expert annotators of the database was 0.69 \u03bcV (std. 8.89 \u03bcV).", "Language": "en", "Citations": "3"},
{"Title": "Refinement and selection heuristics in subgroup discovery and classification rule learning", "Authors": ["Valmarska A.", "Lavrac N.", "Furnkranz J.", "Robnik-Sikonja M."], "Keywords": ["Inverted heuristics", "Rule learning", "Subgroup discovery"], "Date": "2017", "Abstract": "Classification rules and rules describing interesting subgroups are important components of descriptive machine learning. Rule learning algorithms typically proceed in two phases: rule refinement selects conditions for specializing the rule, and rule selection selects the final rule among several rule candidates. While most conventional algorithms use the same heuristic for guiding both phases, recent research indicates that the use of two separate heuristics is conceptually better justified, improves the coverage of positive examples, and may result in better classification accuracy. The paper presents and evaluates two new beam search rule learning algorithms: DoubleBeam-SD for subgroup discovery and DoubleBeam-RL for classification rule learning. The algorithms use two separate beams and can combine various heuristics for rule refinement and rule selection, which widens the search space and allows for finding rules with improved quality. In the classification rule learning setting, the experimental results confirm previously shown benefits of using two separate heuristics for rule refinement and rule selection. In subgroup discovery, DoubleBeam-SD algorithm variants outperform several state-of-the-art related algorithms.", "Language": "en", "Citations": "7"},
{"Title": "An extended ANSI C for multimedia processing", "Authors": ["Bulic P.", "Gustin V.", "Pipan L."], "Keywords": [], "Date": "2003", "Abstract": "This paper presents the Multimedia C language, which is appropriate for the multimedia extensions included in all modern microprocessors. The paper discusses the language syntax, the implementation of its compiler and its use in developing multimedia applications. The goal was to provide programmers with the most natural way of using multimedia processing facilities in the C language. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "1"},
{"Title": "An efficient explanation of individual classifications using game theory", "Authors": ["Strumbelj E.", "Kononenko I."], "Keywords": ["Classification", "Data postprocessing", "Explanation", "Visualization"], "Date": "2010", "Abstract": "We present a general method for explaining individual predictions of classification models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method's initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efficient and that the explanations are intuitive and useful. \u00a9 2010 Erik \u0160trumbelj and Igor Kononenko.", "Language": "en", "Citations": "52"},
{"Title": "The maximum of the minimal multiplicity of eigenvalues of symmetric matrices whose pattern is constrained by a graph", "Authors": ["Oblak P.", "Smigoc H."], "Keywords": ["Graph", "Minimal rank", "Multiplicity of an eigenvalue", "Symmetric matrix"], "Date": "2017", "Abstract": "In this paper we introduce a parameter Mm(G), defined as the maximum over the minimal multiplicities of eigenvalues among all symmetric matrices corresponding to a graph G. We develop basic properties of Mm(G) and compute it for several families of graphs.", "Language": "en", "Citations": "0"},
{"Title": "Invertible and nilpotent matrices over antirings", "Authors": ["Dolzan D.", "Oblak P."], "Keywords": ["Antiring", "Invertible matrix", "Nilpotent matrix"], "Date": "2009", "Abstract": "In this paper, we characterize invertible matrices over an arbitrary commutative antiring S with 1 and find the structure of GL", "Language": "en", "Citations": "19"},
{"Title": "SEMIA: Semi-automatic interactive graphic editing tool to annotate ambulatory ECG records", "Authors": ["Dorn R.", "Jager F."], "Keywords": ["Ambulatory ECG time-series representation", "Annotated ECG database", "Dynamic interface controls", "Graphic user interface", "Long-Term ST database", "Semi-automatic interactive graphic editing tool"], "Date": "2004", "Abstract": "We designed and developed a special purpose interactive graphic editing tool semi-automatic (SEMIA) to annotate transient ischaemic ST segment episodes and other non-ischaemic ST segment events in 24h ambulatory electrocardiogram (ECG) records. The tool allows representation and viewing of the data, interaction with the data globally and locally at different resolutions, examining data at any point, manual adjustment of heart-beat fiducial points, and manual and automatic editing of annotations. Efficient and fast display of ambulatory ECG signal waveforms, display of diagnostic and morphology feature-vector time-series, dynamic interface controls, and automated procedures to help annotate, made the tool efficient, user friendly and usable. Human expert annotators used the SEMIA tool to successfully annotate the Long-Term ST database (LTST DB), a result of a multinational effort. The tool supported paperless editing of annotations at dislocated geographical sites. We present design, characteristic \"look and feel\", functionality, and development of SEMIA annotating tool. \u00a9 2004 Elsevier Ireland Ltd. All rights reserved.", "Language": "en", "Citations": "4"},
{"Title": "Tracking multiple players in sport games using the visual information Sledenje ve\u010d igralcev v \u0161portnih igrah na podlagi vizualne informacije", "Authors": ["Kristan M.", "Perse M.", "Kovacic S.", "Pers J."], "Keywords": ["Kalman filter", "Monte Carlo", "Multiple target tracking", "Particle filters", "Sports"], "Date": "2007", "Abstract": "In this paper we deal with the problem of tracking multiple players in indoor sports using the visual information obtained by the camera overlooking the court. In particular, we are interested in tracking multiple visually-identical targets as they interact. We propose a tracker based on a recursive estimator (Eqs. 1,2) of the posterior over the target's state, i.e., the player's position, which is implemented in the form of the well known particle fiter [1], namely, the CONDENSATION algorithm [2]. Our tracker uses the probabilistic model of the player's appearance as well as the player's dynamical model (section 3). The player's appearance is modelled by an elliptical region, within which a color histogram, describing the texture, is sampled. A mask function is employed to make the appearance model robust to the clutter and an autoregressive scheme is used to update the model to the local changes in the appearance. The player's dynamical model is based on the assumption that the player cannot abruptly change his/hers velocity due to the effects of inertia. Section 4 deals with the problem of tracking multiple players through collisions and interactions. In particular, the collisions among the visually-similar players pose a threat of losing the correct identities of the colliding players. We propose a novel method (Algorithm 1) for solving the problem of the identity ambiguities, where at each time-step, the players are visually separated by constructing a Voronoi diagram [3] among their estimated positions (Fig. 3), such that each cell of the diagram contains only one player. This results in a simplification of tracking, since each player can then be tracked efficiently by an independent single-player tracker. Experiments were conducted on a demanding data-set to evaluate the performance of the proposed multiple-player tracking scheme. The results (Tab. 1) showed that the proposed scheme increased the performance of the tracker by a few orders in terms of a lower failure rate.", "Language": "en", "Citations": "1"},
{"Title": "Validation of the Slovenian version of patient assessment of chronic illness Care (PACIC) in patients with coronary heart disease Vrednovanje slovenske ina\u010dice modela patient assessment of chronic illness care (PACIC) u pacijenata s koronarnom bolesti srca", "Authors": ["Tusek-Bunc K.", "Petek-Ster M.", "Ster B.", "Petek D.", "Kersnik J."], "Keywords": ["Chronic diseases", "PACIC", "Patient perspective", "Slovenia", "Validation"], "Date": "2014", "Abstract": "The Chronic Care Model (CCM) is a conceptual framework that supports the evidence-based proactive and planned care of chronic diseases. Our aim was to validate a Slovenian translation of Patient Assessment of Chronic Illness Care (PACIC) \u2013 a self-reported instrument designed to measure the extent to which patients with chronic illnesses receive care congruent with CCM \u2013 on a sample of patients with coronary heart disease. Secondary analysis of patients\u2019 evaluation of general practice care (EPA Cardio study) was done in patients with coronary heart disease in Slovenia. Patients completed a written questionnaire, which included the instrument for assessing chronic illness care (PACIC), the EUROPEP questionnaire and demographical data. Internal consistency was expressed in terms of Cronbach\u2019s \u00e1. Reliability was expressed as the intra class correlation coefficient (ICC). Correlation between PACIC and EUROPEP was considered as a measure of construct validity. Factor analysis was done to identify number and types of domains in the instrument. Questionnaires of 843 patients were analysed. The mean age was 68.2 (SD 11.1) years, 34.6% of participants were female. 32.7% of PACIC questionnaires were not completely fulfilled. The internal consistency of the entire questionnaire assessed by Cronbach\u2019s \u00e1 was 0.953 and reliability was 0.937. Construct validity was confirmed with important and significant correlation between PACIC and EUROPEP questionnaire (Spearman\u2019s correlation coefficient =0.60, p<0.001). Principal component factor analysis identifies two major factors which we labeled according to the PACIC domains as \u00bbPatient activation, decision support and problem solving\u00ab and \u00bbGoal settings and coordination\u00ab. A translated and validated Slovenian version of PACIC questionnaire is now available. Further research on its validity in other groups of chronically ill patients and the use of instrument for monitoring changes of chronic care over time is recommended.", "Language": "en", "Citations": "3"},
{"Title": "Quantum-dot cellular automata serial comparator", "Authors": ["Lampreht B.", "Stepancic L.", "Vizec I.", "Zankar B.", "Mraz M.", "Lebar Bajec I.", "Pecar P."], "Keywords": [], "Date": "2008", "Abstract": "The Quantum-dot Cellular Automata (QCA) are one of the few alternative computing platforms that meet most of the criteria desired for computing platforms of the future. One of the basic concepts that has popularized the QCA platform to computer designers is adiabatic pipelining, which implicitly assures the correct data flow and in this view simplifies digital design. When designing QCA devices this should be taken advantage of instead of copying digital device designs that were optimized for the CMOS technology. One of the basic digital devices and probably a vital part of many modern computing platforms is the comparator. We here present a novel approach to comparator design, which takes advantage of adiabatic pipelining. Our analysis was based on the QCADesigner tool. \u00a9 2008 IEEE.", "Language": "en", "Citations": "10"},
{"Title": "Extracting SIMD parallelism from 'for' loops", "Authors": ["Gustin V.", "Bulic P."], "Keywords": ["Algorithm design and analysis", "Application software", "Assembly", "Data mining", "Hardware", "Information science", "Libraries", "Microprocessors", "Parallel processing", "Program processors"], "Date": "2001", "Abstract": "The need for multimedia applications has prompted the addition of a multimedia instruction set (MMX) to most existing general-purpose microprocessors. The introduction of short single-instruction multiple data (SIMD) i.e. \"vectorized\" instructions to the microprocessor \"scalar\" instruction set is supported by special hardware which enables the execution of one instruction on multiple data sets. Such a vectorized instruction set is primarily used in multimedia applications, and it seems likely that it will grow rapidly over the next few years. Thus on the one hand we have modern multimedia execution hardware and on the other we have the software and the general compilers which are not able to automatically exploit the multimedia instruction set. In addition, the compiler is not able to locate SIMD parallelism within a basic block. Our solution to these problems is to find statement candidates in the program written in the language C/C++ (as we mainly use this language), and to employ the SIMD instruction set in the easiest possible way. As we know that the compiler cannot be user-changed or modified, we can only extend the functionality of the program (compiler) by the use of specialised library routines or by macros. We prefer the latter. Why? We believe that the use of the macro library is faster than function calls, and we expect it to be simpler and more friendly for the user. The algorithm for identifying candidates for parallel processing (ICPP) is based on the fact that the program does not need any \"correction\" or \"adoption\" prior to being analysed andfinally to being translated into the SIMD instruction set. We define the macro library MacroVect.c as the substitution for the discovered statement candidates.", "Language": "en", "Citations": "8"},
{"Title": "Computational folkloristics: A semantic analysis and visualization of topic distribution of song types Ra\u010dunalni\u0161ka folkloristika. Semanti\u010dna analiza in vizualizacija tematske porazdelitve pesemskih tipov", "Authors": ["Strle G.", "Marolt M."], "Keywords": ["Computational semantics", "Folk song", "Folkloristics", "LDA (latent dirichlet allocation)", "Natural language processing (NLP)", "Typology"], "Date": "2014", "Abstract": "The article presents research into the latent semantic structure of Slovenian folk songs using natural language processing (NLP) methods. The aim is to determine the appropriateness of NLP for discovering general patterns and relationships on the level of song types and genres, and to specify the basic procedure for the computational analysis of folkloristic materials. The results of the analysis show that the appropriate computational method can generate multidimensional semantic space on the basis of the distribution of topics and similarity measures, and therefore enable a more nuanced typological analysis of folkloristic materials.", "Language": "en", "Citations": "0"},
{"Title": "Modern parameterization and explanation techniques in diagnostic decision support system: A case study in diagnostics of coronary artery disease", "Authors": ["Kukar M.", "Kononenko I.", "Groselj C."], "Keywords": ["Association rules", "Coronary artery disease diagnostics", "Machine learning", "Multi-resolution image parameterization", "Principal component analysis"], "Date": "2011", "Abstract": "Objective: Coronary artery disease has been described as one of the curses of the western world, as it is one of its most important causes of mortality. Therefore, clinicians seek to improve diagnostic procedures, especially those that allow them to reach reliable early diagnoses. In the clinical setting, coronary artery disease diagnostics are typically performed in a sequential manner. The four diagnostic levels consist of evaluation of (1) signs and symptoms of the disease and electrocardiogram at rest, (2) sequential electrocardiogram testing during the controlled exercise, (3) myocardial perfusion scintigraphy, and (4) finally coronary angiography, that is considered as the \" gold standard\" reference method. Our study focuses on improving diagnostic performance of the third, virtually non-invasive, diagnostic level. Methods and materials: Myocardial scintigraphy results in a series of medical images that are obtained by relatively inexpensive means. In clinical practice, these images are manually described (parameterized) by expert physicians. In the paper we present an innovative alternative to manual image evaluation-an automatic image parameterization on multiple resolutions, based on texture description with specialized association rules. Extracted image parameters are combined into more informative composite parameters by means of principal component analysis, and finally used to build automatic classifiers with machine learning methods. Results: Our experiments with synthetic datasets show that association-rule-based multi-resolution image parameterization works very well for scintigraphic images of the heart. In coronary artery disease diagnostics we confirm these results as our approach significantly improves on clinical results in terms of diagnostic performance. We improve diagnostic accuracy by 17%, specificity by 12% and sensitivity by 22%. We also significantly improve the number of reliably diagnosed patients by 19% for positive diagnoses, and 16% for negative diagnoses, so that no costly further tests are necessary for them. Conclusions: Multi-resolution image parameterization equals or even betters that of the physicians in terms of the diagnostic quality of image parameters. By using these parameters for building machine learning classifiers, we can significantly improve diagnostic performance with respect to the results of clinical practice, affect process rationalization, as well as possibly provide novel insights into the diagnostic problems, features and/or processes. \u00a9 2011 Elsevier B.V.", "Language": "en", "Citations": "9"},
{"Title": "Visualization and concept drift detection using explanations of incremental models", "Authors": ["Demsar J.", "Bosnic Z.", "Kononenko I."], "Keywords": ["Concept drift detection", "Data stream mining", "Visual perception"], "Date": "2014", "Abstract": "The temporal dimension that is ever more prevalent in data makes data stream mining (incremental learning) an important field of machine learning. In addition to accurate predictions, explanations of the models and examples are a crucial component as they provide insight into model's decision and lessen its black box nature, thus increasing the user's trust. Proper visual representation of data is also very relevant to user's understanding - visualization is often utilised in machine learning since it shifts the balance between perception and cognition to take fuller advantage of the brain's abilities. In this paper we review visualisation in incremental setting and devise an improved version of an existing visualisation of explanations of incremental models. Additionally, we discuss the detection of concept drift in data streams and experiment with a novel detection method that uses the stream of model's explanations to determine the places of change in the data domain.", "Language": "en", "Citations": "3"},
{"Title": "Sample size for assessment of new feature\u2019s relevance in a given problem", "Authors": ["Bohanec M.", "Borstnar M.K.", "Robnik-Sikonja M."], "Keywords": ["Feature evaluation", "Feature ranking", "Machine learning"], "Date": "2017", "Abstract": "In practical use of machine learning models users may add ad hoc new features to an existing classification model, reflecting their (changed) empirical understanding of a field. New features potentially increase classification accuracy of the model or improve its interpretability. We introduce a guideline for determination of sample size needed to reliably estimate the impact of a new feature. Our approach is based on the feature evaluation measure ReliefF and bootstrap-based estimation of confidence intervals for feature ranks. The results show that new features with high or low rank can be detected with a relatively small number of instances, but features ranked near the border of useful features need larger samples to determine their impact. We test our approach on qualitative business-to-business sales forecasting data.", "Language": "en", "Citations": "0"},
{"Title": "Pricing and QoS", "Authors": ["Stiller B.", "Barlet-Ros P.", "Cushnie J.", "Domingo-Pascual J.", "Hutchison D.", "Lopes R.", "Mauthe A.", "Popa M.", "Roberts J.", "Sole-Pareta J.", "Trcek D.", "Veciana C."], "Keywords": [], "Date": "2003", "Abstract": "In this chapter the state of the art of pricing for Internet services and its relation to Quality-of-Service (QoS) is addressed. Essential economic and technology basics, covering terms, accounting, and security are followed by a user-centered view, a content-based scheme, and a cost sharing approach. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "2"},
{"Title": "Motivational modulation of self-initiated and externally triggered movement speed induced by threat of shock: Experimental evidence for paradoxical kinesis in Parkinson's disease", "Authors": ["McDonald L.M.", "Griffin H.J.", "Angeli A.", "Torkamani M.", "Georgiev D.", "Jahanshahi M."], "Keywords": [], "Date": "2015", "Abstract": "Background Paradoxical kinesis has been observed in bradykinetic people with Parkinson's disease. Paradoxical kinesis occurs in situations where an individual is strongly motivated or influenced by relevant external cues. Our aim was to induce paradoxical kinesis in the laboratory. We tested whether the motivation of avoiding a mild electric shock was sufficient to induce paradoxical kinesis in externally-triggered and self-initiated conditions in people with Parkinson's disease tested on medication and in age-matched controls. Methods Participants completed a shock avoidance behavioural paradigm in which half of the trials could result in a mild electric shock if the participant did not move fast enough. Half of the trials of each type were self-initiated and half were externally-triggered. The criterion for avoiding shock was a maximum movement time, adjusted according to each participant's performance on previous trials using a staircase tracking procedure. Results On trials with threat of shock, both patients with Parkinson's disease and controls had faster movement times compared to no potential shock trials, in both self-initiated and externallytriggered conditions. The magnitude of improvement of movement time from no potential shock to potential shock trials was positively correlated with anxiety ratings. Conclusions When motivated to avoid mild electric shock, patients with Parkinson's disease, similar to healthy controls, showed significant speeding ofmovement execution. This was observed in both self-initiated and externally-triggered versions of the task. Nevertheless, in the ET condition the improvement of reaction times induced by motivation to avoid shocks was greater for the PD patients than controls, highlighting the value of external cues for movement initiation in PD patients. Themagnitude of improvement from the no potential shock to the potential shock trials was associated with the threat-induced anxiety. This demonstration of paradoxical kinesis in the laboratory under both self-initiated and externally-triggered conditions has implications for motivational and attentional enhancement of movement speed in Parkinson's disease. Copyright:", "Language": "en", "Citations": "5"},
{"Title": "Service-oriented framework for human task support and automation", "Authors": ["Sasa A.", "Juric M.B.", "Krisper M."], "Keywords": ["Business process automation", "Multi-agent system", "Ontology", "Service-oriented architecture"], "Date": "2008", "Abstract": "Due to increasingly demanding requirements for business flexibility and agility, automation of end-to-end industrial processes has become an important topic. Business process execution needs to support automated tasks execution as well as human tasks. In this paper we show that for certain types of human tasks it is relevant to consider their further automation. We propose a service-oriented architectural framework for human task execution, which improves their execution by automating and semi-automating decision making based on ontologies and agent technology. The approach is generic and can be used for any type of industrial or industrial support business process. As a proof-of-concept we have developed a system providing the above-described support for human task intensive business processes in an electric power transmission company, which has shown considerable improvements in the efficiency of human tasks. \u00a9 2006 IEEE.", "Language": "en", "Citations": "36"},
{"Title": "The prognostic value of Stathmin-1, S100A2, and SYK proteins in ER-positive primary breast cancer patients treated with adjuvant tamoxifen monotherapy: An immunohistochemical study", "Authors": ["Golouh R.", "Cufer T.", "Sadikov A.", "Nussdorfer P.", "Usher P.A.", "Brunner N.", "Schmitt M.", "Lesche R.", "Maier S.", "Timmermans M.", "Foekens J.A.", "Martens J.W.M."], "Keywords": ["ER", "Immunohistochemistry", "Primary breast carcinoma", "S100A2", "Stathmin-1", "SYK", "Tamoxifen"], "Date": "2008", "Abstract": "Introduction: We recently found that DNA methylation of S100A2, spleen tyrosine kinase (SYK), and Stathmin-1 (STMN1) correlates with response to tamoxifen therapy in metastatic breast cancer. In this retrospective study, we investigated immunohistochemically whether these three markers are predictors of relapse in early breast cancer (EBC) patients treated with adjuvant tamoxifen alone. Methods: Immunohistochemical staining was performed for S100A2, SYK and STMN1 on a tissue microarray containing ER-positive invasive breast carcinomas from a study cohort of 215 operable breast cancer patients, who underwent radical local therapy and who were treated with adjuvant tamoxifen monotherapy. Cox regression was used to correlate staining intensity of the three markers with main endpoints in our study; disease-free survival (DFS), and disease-specific survival (DSS). Results: In univariate analysis, only STMN1 staining intensity strongly correlated with DFS (P = 0.014) and DSS (P = 0.002). In the groups of low and high STMN1 intensity, DFS was 84% and 63%, and DSS was 89% and 70%. STMN1 retained its prognostic value for DFS (P = 0.002) and DSS (<0.001) in the multivariate model together with lymph node status. We found also a trend to better DFS in patients with low STMN1 intensity in both lymph node-positive (P = 0.001) and -negative patients (P = 0.065). As the tumour cells did not express S100A2 (except in one case) the potential prognostic value of this marker was not evaluated. Conclusions: Staining intensity of STMN1, but not SYK, predicted outcome in our collective of ER- positive tamoxifen treated EBC patients. \u00a9 2007 Springer Science+Business Media, LLC.", "Language": "en", "Citations": "44"},
{"Title": "Emulation of the iskra delta partner computer Emulacija ra\u010dunalnika Iskra Delta Partner", "Authors": ["Horvat M.", "Mihelic J."], "Keywords": [], "Date": "2018", "Abstract": "In the 1980s, the computer industry in Slovenia was at its peak of development, and nowadays its achievements are practicaly nowhere to be found. Preservation of the Slovene computer heritage is of great importance from several perspectives, from educational and research to preservation of the national identity. In this paper we present various activities that we carried out for the conservation of one of the most important Slovenian computers, the Iskra Delta Partner. In doing so, we focus primarily on creating a computer emulator, which involves emulation of the processor and several related devices. We created the emulator in the C programming language, thus enabling its efficiency and portability, while with the help of appropriate tools, it can also be executed within a Web browser environment. In the first part of the paper we discuss the basic concepts of emulation and briefly examine the emulated computer. In the main part we present the emulator, its construction, and give a detailed description of the emulated devices.", "Language": "en", "Citations": "0"},
{"Title": "A bilingual HMM-based speech synthesis system for closely related languages", "Authors": ["Justin T.", "Pobar M.", "Ipsic I.", "Mihelic F.", "Zibert J."], "Keywords": ["bilingual speech synthesis", "Croatian synthesis", "HMM speech synthesis", "Slovenian synthesis"], "Date": "2012", "Abstract": "In this paper we investigate a bilingual HMM-based speech synthesis developed for Slovenian and Croatian languages. The primary goals of this research are to investigate the performance of an HMM-based synthesis build from two similar languages and to perform a comparison of such synthesis system with standard monolingual speaker-dependent HMM-based synthesis. The bilingual HMM synthesis is built by joining all the speech material from both languages by defining proper mapping of Slovenian and Croatian phonemes and by adapting acoustic models of Slovenian and Croatian speakers. Adapted acoustic models are then served as basic building blocks for speech synthesis in both languages. In such a way we are able to obtain synthesized speech of both languages, but with the same speaker voice. We made the quantitative comparison of such kind of synthesis with monolingual counterparts and study the performance of the synthesis in a relation to the amount of data, which is used for building the synthesis system. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "6"},
{"Title": "An edit-distance model for the approximate matching of timed strings", "Authors": ["Dobrisek S.", "Zibert J.", "Pavesic N.", "Mihelic F."], "Keywords": ["Classifier evaluation", "Edit distance", "Pattern matching", "Similarity measures", "Speech recognition"], "Date": "2009", "Abstract": "An edit-distance model that can be used for the approximate matching of contiguous and non-contiguous timed strings is presented. The model extends the concept of the weighted string-edit distance by introducing timed edit operations and by making the edit costs time dependent. Special attention is paid to the timed null symbols that are associated with the timed insertions and deletions. The usefulness of the presented model is demonstrated on the classification of phone-recognition errors using the TIMIT speech database. \u00a9 2009 IEEE.", "Language": "en", "Citations": "7"},
{"Title": "Evaluation and analysis of ear recognition models: performance, complexity and resource requirements", "Authors": ["Emersic Z.", "Meden B.", "Peer P.", "Struc V."], "Keywords": ["Convolutional neural networks", "Covariate analysis", "Ear recognition", "Feature extraction"], "Date": "2018", "Abstract": "Ear recognition technology has long been dominated by (local) descriptor-based techniques due to their formidable recognition performance and robustness to various sources of image variability. While deep-learning-based techniques have started to appear in this field only recently, they have already shown potential for further boosting the performance of ear recognition technology and dethroning descriptor-based methods as the current state of the art. However, while recognition performance is often the key factor when selecting recognition models for biometric technology, it is equally important that the behavior of the models is understood and their sensitivity to different covariates is known and well explored. Other factors, such as the train- and test-time complexity or resource requirements, are also paramount and need to be consider when designing recognition systems. To explore these issues, we present in this paper a comprehensive analysis of several descriptor- and deep-learning-based techniques for ear recognition. Our goal is to discover weak points of contemporary techniques, study the characteristics of the existing technology and identify open problems worth exploring in the future. We conduct our analysis through identification experiments on the challenging Annotated Web Ears (AWE) dataset and report our findings. The results of our analysis show that the presence of accessories and high degrees of head movement significantly impacts the identification performance of all types of recognition models, whereas mild degrees of the listed factors and other covariates such as gender and ethnicity impact the identification performance only to a limited extent. From a test-time-complexity point of view, the results suggest that lightweight deep models can be equally fast as descriptor-based methods given appropriate computing hardware, but require significantly more resources during training, where descriptor-based methods have a clear advantage. As an additional contribution, we also introduce a novel dataset of ear images, called AWE Extended (AWEx), which we collected from the web for the training of the deep models used in our experiments. AWEx contains 4104 images of 346 subjects and represents one of the largest and most challenging (publicly available) datasets of unconstrained ear images at the disposal of the research community.", "Language": "en", "Citations": "2"},
{"Title": "I-graphs and the corresponding configurations", "Authors": ["Boben M.", "Pisanski T.", "Zitnik A."], "Keywords": ["Configurations", "Graphs", "Incidence structures"], "Date": "2005", "Abstract": "We consider the class of I-graphs I(n; j; k), which is a generalization over the class of the generalized Petersen graphs. We study different properties of I-graphs, such as connectedness, girth, and whether they are bipartite or vertex-transitive. We give an efficient test for isomorphism of I-graphs and characterize the automorphism groups of I-graphs. Regular bipartite graphs with girth at least 6 can be considered as Levi graphs of some symmetric combinatorial configurations. We consider configurations that arise from bipartite I-graphs. Some of them can be realized in the plane as cyclic astral configurations, i.e., as geometric configurations with maximal isometric symmetry. \u00a9 2005 Wiley Periodicals, Inc.", "Language": "en", "Citations": "28"},
{"Title": "Minor-minimal 6-regular graphs in the Klein bottle", "Authors": ["Fijavz G."], "Keywords": [], "Date": "2004", "Abstract": "Let K", "Language": "fr", "Citations": "4"},
{"Title": "Transductive reliability estimation for individual classifications in machine learning and data mining", "Authors": ["Kukar M.Z.."], "Keywords": [], "Date": "2012", "Abstract": "Machine learning and data mining approaches are nowadays being used in many fields as valuable data analysis tools. However, their serious practical use is affected by the fact, that more often than not, they cannot produce reliable and unbiased assessments of their predictions, quality. In last years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. In the chapter we describe typicalness and transductive reliability estimation frameworks and propose a joint approach that compensates their weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into a joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense (e.g., a confidence level of 95% means that in 95% the predicted class is also a true class), as well as provides us with a general principle that is independent of to the particular underlying classifier", "Language": "en", "Citations": "2"},
{"Title": "Diameters of connected components of commuting graphs", "Authors": ["Dolzan D.", "Konvalinka M.", "Oblak P."], "Keywords": ["Commuting graph", "Diameter", "Semiring", "Symmetric group"], "Date": "2013", "Abstract": "In this paper, diameters of connected components of commuting graphs of GL", "Language": "en", "Citations": "0"},
{"Title": "Accurate indoor sound level measurement on a low-power and low-cost wireless sensor node", "Authors": ["Risojevic V.", "Rozman R.", "Pilipovic R.", "Cesnovar R.", "Bulic P."], "Keywords": ["A-weighting", "Environmental noise monitoring", "Hardware platform", "Noise sensing", "Wireless sensor network"], "Date": "2018", "Abstract": "Wireless sensor networks can provide a cheap and flexible infrastructure to support the measurement of noise pollution. However, the processing of the gathered data is challenging to implement on resource-constrained nodes, because each node has its own limited power supply, low-performance and low-power micro-controller unit and other limited processing resources, as well as limited amount of memory. We propose a sensor node for monitoring of indoor ambient noise. The sensor node is based on a hardware platform with limited computational resources and utilizes several simplifications to approximate more complex and costly signal processing stage. Furthermore, to reduce the communication between the sensor node and a sink node, as well as the power consumed by the IEEE 802.15.4 (ZigBee) transceiver, we perform digital A-weighting filtering and non-calibrated calculation of the sound pressure level on the node. According to experimental results, the proposed sound level meter can accurately measure the noise levels of up to 100 dB, with the mean difference of less than 2 dB compared to Class 1 sound level meter. The proposed device can continuously monitor indoor noise for several days. Despite the limitations of the used hardware platform, the presented node is a promising low-cost and low-power solution for indoor ambient noise monitoring.", "Language": "en", "Citations": "2"},
{"Title": "Volumetric models from 3D point clouds: The case study of sarcophagi cargo from a 2nd/3rd century AD Roman shipwreck near Sutivan on island Bra\u010d, Croatia", "Authors": ["Jaklic A.", "Eric M.", "Mihajlovic I.", "Stopinsek Z.", "Solina F."], "Keywords": ["3D models", "Marble blocks", "Multi-image photogrammetry", "Segmentation", "Superquadrics", "Under-water archeology"], "Date": "2015", "Abstract": "Multi-image photogrammetry can in favorable conditions even under water generate large clouds of 3D points which can be used for visualization of sunken heritage. For analysis of under-water archeological sites and comparison of artifacts, more compact shape models must be reconstructed from 3D points, where each object or a part of it is modeled individually. Volumetric models and superquadric models in particular are good candidates for such modeling since automated methods for their reconstruction and segmentation from 3D points exist. For the study case we use an underwater wreck site of a Roman ship from 2nd/3rd century AD located near Sutivan on island Bra\u010d in Croatia. We demonstrate how superquadric models of sarcophagi and other stone blocks can be reconstructed from an unsegmented cloud of 3D points obtained by multi-image photogrammetry. We compare the dimensions of stone objects measured directly on the corresponding 3D point cloud with dimensions of the reconstructed superquadric models and discuss other advantages of these volumetric models. The average difference between point-to-point measurements of stone blocks and the dimensions of the corresponding superquadric model is on the order of few centimeters.", "Language": "en", "Citations": "11"},
{"Title": "Node mixing and group structure of complex software networks", "Authors": ["Subelj L.", "Zitnik S.", "Blagus N.", "Bajec M."], "Keywords": ["node groups", "node mixing", "software engineering", "Software networks"], "Date": "2014", "Abstract": "Large software projects are among most sophisticated human-made systems consisting of a network of interdependent parts. Past studies of software systems from the perspective of complex networks have already led to notable discoveries with different applications. Nevertheless, our comprehension of the structure of software networks remains to be only partial. Here we investigate correlations or mixing between linked nodes and show that software networks reveal dichotomous node degree mixing similar to that recently observed in biological networks. We further show that software networks also reveal characteristic clustering profiles and mixing. Hence, node mixing in software networks significantly differs from that in, e.g., the Internet or social networks. We explain the observed mixing through the presence of groups of nodes with common linking pattern. More precisely, besides densely linked groups known as communities, software networks also consist of disconnected groups denoted modules, core/periphery structures and other. Moreover, groups coincide with the intrinsic properties of the underlying software projects, which promotes practical applications in software engineering.", "Language": "en", "Citations": "10"},
{"Title": "Real-time Large Scale Traffic Sign Detection", "Authors": ["Avramovic A.", "Tabernik D.", "Skocaj D."], "Keywords": ["Deep learning", "real-time", "traffic sign detection", "traffic sign recognition", "YOLO"], "Date": "2018", "Abstract": "Automatic traffic sign detection and recognition has achieved good results using convolutional neural networks. Novel architectures are still being proposed in order to improve accuracy of detection and segmentation of traffic sings. In this paper, we are examining the possibility for traffic sign detection and recognition in real-time. For that purpose, we employed a novel YOLOv3 architecture, which has been proven to be fast and accurate method for object detection. It was shown that real-time detection can be achieved, even on HD images, with mAP above 88%.", "Language": "en", "Citations": "0"},
{"Title": "Development and evaluation of an intelligent traceability system for frozen tilapia fillet processing", "Authors": ["Xiao X.", "Fu Z.", "Qi L.", "Mira T.", "Zhang X."], "Keywords": ["Fault tree analysis (FTA)", "Intelligent traceability system", "Statistical process control (SPC)", "Tilapia fillet"], "Date": "2015", "Abstract": "BACKGROUND: The main export varieties in China are brand-name, high-quality bred aquatic products. Among them, tilapia has become the most important and fast-growing species since extensive consumer markets in North America and Europe have evolved as a result of commodity prices, year-round availability and quality of fresh and frozen products. As the largest tilapia farming country, China has over one-third of its tilapia production devoted to further processing and meeting foreign market demand. RESULTS: Using by tilapia fillet processing, this paper introduces the efforts for developing and evaluating ITS-TF: an intelligent traceability system integrated with statistical process control (SPC) and fault tree analysis (FTA). Observations, literature review and expert questionnaires were used for system requirement and knowledge acquisition; scenario simulation was applied to evaluate and validate ITS-TF performance. CONCLUSION: The results show that traceability requirement is evolved from a firefighting model to a proactive model for enhancing process management capacity for food safety; ITS-TF transforms itself as an intelligent system to provide functions on early warnings and process management by integrated SPC and FTA. The valuable suggestion that automatic data acquisition and communication technology should be integrated into ITS-TF was achieved for further system optimization, perfection and performance improvement.", "Language": "en", "Citations": "7"},
{"Title": "Is science driven by principal investigators?", "Authors": ["Kastrin A.", "Klisara J.", "Luzar B.", "Povh J."], "Keywords": ["Bibliographic network", "Career performance", "Principal investigator", "Research evaluation", "Research performance"], "Date": "2018", "Abstract": "In this paper we consider the question what is the scientific and career performance of principal investigators (PI\u2019s) of publicly funded research projects compared to scientific performance of all researchers. Our study is based on high quality data about (1) research projects awarded in Slovenia in the period 1994\u20132016 (7508 projects with 2725 PI\u2019s in total) and (2) about scientific productivity of all researchers in Slovenia that were active in the period 1970\u20132016\u2014there are 19,598 such researchers in total, including the PI\u2019s. We compare average productivity, collaboration, internationality and interdisciplinarity of PI\u2019s and of all active researchers. Our analysis shows that for all four indicators the average performance of PI\u2019s is much higher compared to average performance of all active researchers. Additionally, we analyze careers of both groups of researchers. The results show that the PI\u2019s have on average longer and more fruitful career compared to all active researchers, with regards to all career indicators. The PI\u2019s that have received a postdoc grant have at the beginning outstanding scientific performance, but later deviate towards average. On long run, the PI\u2019s leading the research programs (the most prestigious grants) on average demonstrate the best scientific performance. In the last part of the paper we study 23 co-authorship networks, spanned by all active researchers in the periods 1970\u20131994,\u00a0..,\u00a01970\u20132016. We find out that they are well connected and that PI\u2019s are well distributed across these networks forming their backbones. Even more, PI\u2019s generate new PI\u2019s, since more than 90% of new PI\u2019s are connected (have at least one joint scientific publication) with existing PI\u2019s. We believe that our study sheds new light to the relations between the public funding of the science and the scientific output and can be considered as an affirmative answer to the question posed in the title.", "Language": "en", "Citations": "2"},
{"Title": "A local-motion-based probabilistic model for visual tracking", "Authors": ["Kristan M.", "Pers J.", "Kovacic S.", "Leonardis A."], "Keywords": ["Local motion", "Occlusion", "Probabilistic visual models", "Visual tracking"], "Date": "2009", "Abstract": "Color-based tracking is prone to failure in situations where visually similar targets are moving in a close proximity or occlude each other. To deal with the ambiguities in the visual information, we propose an additional color-independent visual model based on the target's local motion. This model is calculated from the optical flow induced by the target in consecutive images. By modifying a color-based particle filter to account for the target's local motion, the combined color/local-motion-based tracker is constructed. We compare the combined tracker to a purely color-based tracker on a challenging dataset from hand tracking, surveillance and sports. The experiments show that the proposed local-motion model largely resolves situations when the target is occluded by, or moves in front of, a visually similar object. \u00a9 2009 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "32"},
{"Title": "Transcription of polyphonic piano music with neural networks", "Authors": ["Marolt Matija"], "Keywords": [], "Date": "2000", "Abstract": "This paper presents our experiences in building a system for transcription of polyphonic piano music. By transcription we mean the conversion of an audio recording of a polyphonic piano performance to a series of notes and their starting times. Our final goal is to build a transcription system that would transcribe polyphonic piano music over the entire piano range and with large polyphony. The system consists of three main stages. We first use a cochlear model based on the gammatone filterbank to transform an audio signal of a piano performance into time-frequency space. In the second stage we use a network of coupled adaptive oscillators to extract partial tracks from the output of the cochlear model and in the third stage we employ artificial neural networks acting as pattern recognizers to extract notes from the output of the oscillator network. The system uses several networks each trained to recognize the occurrence of a specific not in the input signal.", "Language": "en", "Citations": "7"},
{"Title": "Development of a bilingual spoken dialog system for weather information retrieval", "Authors": ["Zibert J.", "Martincic-Ipsic S.", "Hajdinjak M.", "Ipsic I.", "Mihelic F."], "Keywords": [], "Date": "2003", "Abstract": "In this paper we present a strategy, current activities and results of a joint project in designing a spoken dialog system for Slovenian and Croatian weather information retrieval. We give a brief description of the system design, of the procedures we have performed in order to obtain domain specific speech databases, monolingual and bilingual speech recognition experiments and WOZ simulation experiments. Recognition results for Croatian and Slovenian speech are presented, as well as bilingual speech recognition results when using common acoustic models. We propose two different approaches to the language identification problem and show recognition results for the both acoustically similar languages. Results of dialog simulations, performed in order to gain user behaviors when accessing a spoken dialog system, are also presented.", "Language": "en", "Citations": "15"},
{"Title": "Multi-resolution parameterization for texture classification and its use in the scintigraphic image analysis", "Authors": ["Sajn L."], "Keywords": ["Association rules", "Medical image analysis", "Multi-resolution texture parameterization", "Scintigraphy analysis", "Texture analysis", "Texture classification", "Whole-body bone scintigraphy segmentation"], "Date": "2007", "Abstract": "The role of Multi-resolution Parameterization in Texture Classification and its use in the Scintigraphic Image Analysis is discussed. An original algorithm ARes for finding more informative resolutions in the sense of classification accuracy is also mentioned. The main theme of multi-resolution approach is based on the algorithm SHIFT, a computer vision algorithm for extracting distinctive features from images that can be used in algorithms for matching different views of an object or scene and object recognition. Two medical cases have been used for multi-resolution parameterization application including, sequential diagnostics of coronary artery disease (CAD) and diagnostics of whole-body bone scintigraphy. This computer-aided system for bone scintigraphy is a base for automating the routine medical procedures and can be used as an additional tool for radiologists. The significant contribution of CAD study is the betterment of the predictive power of the sequential diagnostic process.", "Language": "en", "Citations": "0"},
{"Title": "Computer networks and routing models Ra\u010dunalni\u0161ka omre\u017eja in usmerjevalni modeli", "Authors": ["Dobravec T.", "Robic B.", "Vilfan B."], "Keywords": ["Computer networks", "Parallel computing", "Routing models"], "Date": "2001", "Abstract": "In the paper we discuss fundamental issues on communication in multiprocessor systems including the topology of the underlying network, its regularity, and scalability. After pointing out the basic limitations of interconnection networks, we define and discuss general properties of computer networks, such as network topology, regularity, vertex neighborhood, vertex degree, distance between vertices, network diameter, scalability, and fault tolerance. Next, networks with special topologies are analyzed, such as linear network, ring, fully connected network, hypercube, tree, star, mesh, torus and circulant network. This static view of computer networks is followed by issues related to dynamic properties of the networks, in particular routing models. We discuss circuit switching, routing along network of buses, as well as packet routing. Four different approaches to solving packet collisions are analyzed: buffering, blocking, dropping, and misrouting. We also discuss wormhole and store-and-forward routing. Finally, the SIMD and MIMD routing models are described.", "Language": "en", "Citations": "0"},
{"Title": "Assessment of classification models with small amounts of data", "Authors": ["Brumen B.", "Juric M.B.", "Welzer T.", "Rozman I.", "Jaakkola H.", "Papadopoulos A."], "Keywords": ["Accuracy", "Assessment", "Classification", "Learning curve", "Sampling"], "Date": "2007", "Abstract": "One of the tasks of data mining is classification, which provides a mapping from attributes (observations) to pre-specified classes. Classification models are built by using underlying data. In principle, the models built with more data yield better results. However, the relationship between the available data and the performance is not well understood, except that the accuracy of a classification model has diminishing improvements as a function of data size. In this paper, we present an approach for an early assessment of the extracted knowledge (classification models) in the terms of performance (accuracy), based on the amount of data used. The assessment is based on the observation of the performance on smaller sample sizes. The solution is formally defined and used in an experiment. In experiments we show the correctness and utility of the approach.", "Language": "en", "Citations": "12"},
{"Title": "Worst case constant time priority queue", "Authors": ["Brodnik A.", "Carlsson S.", "Karlsson J.", "Munro J.I."], "Keywords": ["Algorithms", "Measurement", "Performance", "Theory", "Verification"], "Date": "2001", "Abstract": "We present a new data structure of size 3M + &Ogr;(M) bits for solving the \"discrete priority queue\" problem. When this data structure is used in combination with a new memory topology it provides an O(1) worst case time solution. In doing so we demonstrate how an unconventional, but practically implementable, memory architecture can be employed to sidestep a lower bound (of lg lg M) and achieve constant time performance. Copyright \u00a9 2009 ACM, Inc.", "Language": "en", "Citations": "11"},
{"Title": "The complexity of static data replication in data grids", "Authors": ["Cibej U.", "Slivnik B.", "Robic B."], "Keywords": ["Data replication", "Grid computing", "Non-approximable problem", "NP-hard problem", "Optimization problem"], "Date": "2005", "Abstract": "Data replication is a well-known technique used in distributed computing to improve access to data and/or system fault-tolerance. Recently, studies of its applications to grid computing have also been initiated. In this article we describe data replication on data grids as a static optimization problem. We show that this problem is NP-hard and non-approximable. We discuss two approaches to solving it, i.e. integer programming and simplifications. \u00a9 2005 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "50"},
{"Title": "Measuring the complexity of domain-specific languages developed using MDD", "Authors": ["Slivnik B."], "Keywords": ["Domain-specific languages", "Metamodel quality", "Model-driven development", "Quality metrics"], "Date": "2016", "Abstract": "The standard ISO/IEC 25010 (SQuaRE) defines appropriateness as one of the three components of functional suitability, the other two components being completeness and correctness. As users of domain-specific language (DSL) are quite often domain experts with limited programming skills, a DSL might be considered appropriate if the resulting domain-specific programs do not contain an excessive amount of nondomain-related programming elements. This paper describes a metric for measuring the appropriateness of DSLs that are developed using model-driven development (MDD), its evaluation and use. The metric measures the depth of the deepest domain-specific command within abstract syntax trees generated by a DSL. It is aimed at being used during the development of a new DSL and for comparing different DSLs defined over the same domain. It is assumed that during MDD, the metamodel describes the domain-independent part of the DSL, while the model supplies the domain-specific part. This resembles the implementation of DSLs using existing metaprogramming tools that provide off-the-shelf implementations of programming constructs but require manual implementation of the domain-specific language elements.", "Language": "en", "Citations": "0"},
{"Title": "Linear chromatic adaptation transform based on Delaunay triangulation", "Authors": ["Kreslin R.", "Calvo P.M.", "Corzo L.G.", "Peer P."], "Keywords": [], "Date": "2014", "Abstract": "Computer vision algorithms that use color information require color constant images to operate correctly. Color constancy of the images is usually achieved in two steps: first the illuminant is detected and then image is transformed with the chromatic adaptation transform (CAT). Existing CAT methods use a single transformation matrix for all the colors of the input image. The method proposed in this paper requires multiple corresponding color pairs between source and target illuminants given by patches of the Macbeth color checker. It uses Delaunay triangulation to divide the color gamut of the input image into small triangles. Each color of the input image is associated with the triangle containing the color point and transformed with a full linear model associated with the triangle. Full linear model is used because diagonal models are known to be inaccurate if channel color matching functions do not have narrow peaks. Objective evaluation showed that the proposed method outperforms existing CAT methods by more than 21%; that is, it performs statistically significantly better than other existing methods. \u00a9 2014 Rok Kreslin et al.", "Language": "en", "Citations": "3"},
{"Title": "Predicting the level of text standardness in user-generated content", "Authors": ["Ljubesic N.", "Fiser D.", "Erjavec T.", "Cibej J.", "Marko D.", "Pollak S.", "Skrjanec I."], "Keywords": [], "Date": "2015", "Abstract": "Non-standard language as it appears in user-generated content has recently attracted much attention. This paper proposes that non-standardness comes in two basic varieties, technical and linguistic, and develops a machine-learning method to discriminate between standard and nonstandard texts in these two dimensions. We describe the manual annotation of a dataset of Slovene user-generated content and the features used to build our regression models. We evaluate and discuss the results, where the mean absolute error of the best performing method on a three-point scale is 0.38 for technical and 0.42 for linguistic standardness prediction. Even when using no language-dependent information sources, our predictor still outperforms an OOVratio baseline by a wide margin. In addition, we show that very little manually annotated training data is required to perform good prediction. Predicting standardness can help decide when to attempt to normalise the data to achieve better annotation results with standard tools, and provide linguists who are interested in nonstandard language with a simple way of selecting only such texts for their research.", "Language": "en", "Citations": "4"},
{"Title": "A connectionist approach to automatic transcription of polyphonic piano music", "Authors": ["Marolt M."], "Keywords": ["Adaptive oscillators", "Music transcription", "Neural networks"], "Date": "2004", "Abstract": "In this paper, we present a connectionist approach to automatic transcription of polyphonic piano music. We first compare the performance of several neural network models on the task of recognizing tones from time-frequency representation of a musical signal. We then propose a new partial tracking technique, based on a combination of an auditory model and adaptive oscillator networks. We show how synchronization of adaptive oscillators can be exploited to track partials in a musical signal. We also present an extension of our technique for tracking individual partials to a method for tracking groups of partials by joining adaptive oscillators into networks. We show that oscillator networks improve the accuracy of transcription with neural networks. We also provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing transcription systems.", "Language": "en", "Citations": "97"},
{"Title": "Scalable non-negative matrix tri-factorization", "Authors": ["Copar A.", "Zitnik M.", "Zupan B."], "Keywords": ["Block-wise multiplication", "Graphics-processing unit", "Large scale latent factor analysis", "Matrix factorization", "Non-negative block value decomposition", "Non-negative matrix tri-factorization"], "Date": "2017", "Abstract": "Background: Matrix factorization is a well established pattern discovery tool that has seen numerous applications in biomedical data analytics, such as gene expression co-clustering, patient stratification, and gene-disease association mining. Matrix factorization learns a latent data model that takes a data matrix and transforms it into a latent feature space enabling generalization, noise removal and feature discovery. However, factorization algorithms are numerically intensive, and hence there is a pressing challenge to scale current algorithms to work with large datasets. Our focus in this paper is matrix tri-factorization, a popular method that is not limited by the assumption of standard matrix factorization about data residing in one latent space. Matrix tri-factorization solves this by inferring a separate latent space for each dimension in a data matrix, and a latent mapping of interactions between the inferred spaces, making the approach particularly suitable for biomedical data mining. Results: We developed a block-wise approach for latent factor learning in matrix tri-factorization. The approach partitions a data matrix into disjoint submatrices that are treated independently and fed into a parallel factorization system. An appealing property of the proposed approach is its mathematical equivalence with serial matrix tri-factorization. In a study on large biomedical datasets we show that our approach scales well on multi-processor and multi-GPU architectures. On a four-GPU system we demonstrate that our approach can be more than 100-times faster than its single-processor counterpart. Conclusions: A general approach for scaling non-negative matrix tri-factorization is proposed. The approach is especially useful parallel matrix factorization implemented in a multi-GPU environment. We expect the new approach will be useful in emerging procedures for latent factor analysis, notably for data integration, where many large data matrices need to be collectively factorized.", "Language": "en", "Citations": "2"},
{"Title": "Simultaneous EEG and EMG biofeedback for peak performance in musicians.", "Authors": ["Markovska-Simoska S.", "Pop-Jordanova N.", "Georgiev D."], "Keywords": [], "Date": "2008", "Abstract": "The aim of this study was to determine the effects of alpha neurofeedback and EMG biofeedback protocols for improvement of musical performance in violinists. The sample consisted of 12 music students (10 violinists and 2 viola players) from the Faculty of Music, Skopje (3 males, mean age of 20 +/- 0 and 9 females, mean age = 20.89 +/- 2.98). Six of them had a low alpha peak frequency (APF) (< 10 Hz), and six a high APF (> 10 Hz). The sample was randomized in two groups. The students from the experimental group participated in 20 sessions of biofeedback (alpha/EMG), combined with music practice, while the students from the control group did only music practice. Average absolute power, interhemispheric coherence in the alpha band, alpha peak frequency (APF), individual alpha band width (IABW), amount of alpha suppression (AAS) and surface forehead integrated EMG power (IEMG), as well as a score on musical performance and inventories measuring anxiety, were assessed. Alpha-EEG/EMG-biofeedback was associated with a significant increase in average alpha power, APF and IABW in all the participants and with decreases in IEMG only in high-APF musicians. The biofeedback training success was positively correlated with the alpha power, IcoH, APF, IABW and baseline level of APF and IABW. Alpha-EEG/EMG biofeedback is capable of increasing voluntary self-regulation and the quality of musical performance. The efficiency of biofeedback training depends on the baseline EEG alpha activity status, in particular the APF.", "Language": "en", "Citations": "10"},
{"Title": "The differential expression of alternatively polyadenylated transcripts is a common stress-induced response mechanism that modulates mammalian mRNA expression in a quantitative and qualitative fashion", "Authors": ["Hollerer I.", "Curk T.", "Haase B.", "Benes V.", "Hauer C.", "Neu-Yilik G.", "Bhuvanagiri M.", "Hentze M.W.", "Kulozik A.E."], "Keywords": ["3' end processing", "Alternative polyadenylation", "MRNA-seq", "Polyadenylation site mapping", "Stress"], "Date": "2016", "Abstract": "Stress adaptation plays a pivotal role in biological processes and requires tight regulation of gene expression. In this study, we explored the effect of cellular stress on mRNA polyadenylation and investigated the implications of regulated polyadenylation site usage on mammalian gene expression. High-confidence polyadenylation site mapping combined with global pre-mRNA and mRNA expression profiling revealed that stress induces an accumulation of genes with differentially expressed polyadenylated mRNA isoforms in human cells. Specifically, stress provokes a global trend in polyadenylation site usage toward decreased utilization of promoter-proximal poly(A) sites in introns or ORFs and increased utilization of promoter-distal polyadenylation sites in intergenic regions. This extensively affects gene expression beyond regulating mRNA abundance by changing mRNA length and by altering the configuration of open reading frames. Our study highlights the impact of posttranscriptional mechanisms on stress-dependent gene regulation and reveals the differential expression of alternatively polyadenylated transcripts as a common stress-induced mechanism in mammalian cells.", "Language": "en", "Citations": "9"},
{"Title": "A methodology and tool support for managing business rules in organisations", "Authors": ["Bajec M.", "Krisper M."], "Keywords": ["Business rule", "Business rule management", "Enterprise modeling"], "Date": "2005", "Abstract": "Business rules are evidently important for organisations as they describe how they are doing business. Their value has also been recognised within the information system (IS) domain, mostly because of their ability to make applications flexible and amenable to change. In this paper, we propose a methodology that helps business people and developers to keep business rules at the business level inline with the rules that are implemented at the system level. In contrast to several existing approaches that primarily focus on business rules in the scope of an application, our methodology addresses the entire IS of an organisation. The paper also describes requirements for a tool support that would be appropriate to support the methodology. \u00a9 2004 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "86"},
{"Title": "How trustworthy is CRAFTY's analysis of world chess champions?", "Authors": ["Guid M.", "Perez A.", "Bratko I."], "Keywords": [], "Date": "2008", "Abstract": "In 2006, Guid and Bratko carried out a computer analysis of games played by World Chess Champions in an attempt to assess as objective as possible one aspect of the playing strength of chess players of different times. The chess program CRAFTY was used in the analysis. Given that CRAFTY's official chess rating is lower than the rating of many of the players analysed, the question arises to what degree that analysis could be trusted. In this paper, we investigate this question and other aspects of the trustworthiness of those results. Our study shows that, at least for pairs of the players whose scores differ significantly, it is not very likely that their relative rankings would change if (1) a stronger chess program was used, or (2) if the program would search more deeply, or (3) larger sets of positions were available for the analysis. Experimental results and theoretical explanations are provided to show that, in order to obtain a sensible ranking of the players according to the criterion considered, it is not necessary to use a computer that is stronger than the players themselves.", "Language": "en", "Citations": "12"},
{"Title": "Histograms of optical flow for efficient representation of body motion", "Authors": ["Pers J.", "Sulic V.", "Kristan M.", "Perse M.", "Polanec K.", "Kovacic S."], "Keywords": ["Human motion", "Image sequences", "Levenshtein distance", "Optical flow"], "Date": "2010", "Abstract": "A novel method for efficient encoding of human body motion, extracted from image sequences is presented. Optical flow field is calculated from sequential images, and the part of the flow field containing a person is subdivided into six segments. For each of the segments, a two dimensional, eight-bin histogram of optical flow is calculated. A symbol is generated, corresponding to the bin with the maximum sample count. Since the optical flow sequences before and after the temporal reference point are processed separately, twelve symbol sequences are obtained from the whole image sequence. Symbol sequences are purged of all symbol repetitions. To establish the similarity between two motion sequences, two sets of symbol sequences are compared. In our case, this is done by the means of normalized Levenshtein distance. Due to use of symbol sequences, the method is extremely storage efficient. It is also performance efficient, as it could be performed in near-realtime using the motion vectors from MPEG4 encoded video sequences. The approach has been tested on video sequences of persons entering restricted area using keycard and fingerprint reader. We show that it could be applied both to verification of person identities due to minuscule differences in their motion, and to detection of unusual behavior, such as tailgating. \u00a9 2010 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "45"},
{"Title": "InterruptMe: Designing intelligent prompting mechanisms for pervasive applications", "Authors": ["Pejovic V.", "Musolesi M."], "Keywords": ["Context-aware computing", "Interruptibility", "Machine learning", "Mobile sensing"], "Date": "2014", "Abstract": "The mobile phone represents a unique platform for interactive applications that can harness the opportunity of an immediate contact with a user in order to increase the impact of the delivered information. However, this accessibility does not necessarily translate to reachability, as recipients might refuse an initiated contact or disfavor a message that comes in an inappropriate moment./// In this paper we seek to answer whether, and how, suitable moments for interruption can be identified and utilized in a mobile system. We gather and analyze a real-world smartphone data trace and show that users' broader context, including their activity, location, time of day, emotions and engagement, determine different aspects of interruptibility. We then design and implement InterruptMe, an interruption management library for Android smartphones. An extensive experiment shows that, compared to a context-unaware approach, interruptions elicited through our library result in increased user satisfaction and shorter response times.", "Language": "en", "Citations": "114"},
{"Title": "Basketball shot types and shot success in different levels of competitive basketball", "Authors": ["Erculj F.", "Strumbelj E."], "Keywords": [], "Date": "2015", "Abstract": "The purpose of our research was to investigate the relative frequencies of different types of basketball shots (above head, hook shot, layup, dunk, tip-in), some details about their technical execution (one-legged, two-legged, drive, cut,...), and shot success in different levels of basketball competitions. We analysed video footage and categorized 5024 basketball shots from 40 basketball games and 5 different levels of competitive basketball (National Basketball Association (NBA), Euroleague, Slovenian 1<sup>st</sup> Division, and two Youth basketball competitions). Statistical analysis with hierarchical multinomial logistic regression models reveals that there are substantial differences between competitions. However, most differences decrease or disappear entirely after we adjust for differences in situations that arise in different competitions (shot location, player type, and attacks in transition). Differences after adjustment are mostly between the Senior and Youth competitions: more shots executed jumping or standing on one leg, more uncategorised shot types, and more dribbling or cutting to the basket in the Youth competitions, which can all be attributed to lesser technical and physical ability of developing basketball players. The two discernible differences within the Senior competitions are that, in the NBA, dunks are more frequent and hook shots are less frequent compared to European basketball, which can be attributed to better athleticism of NBA players. The effect situational variables have on shot types and shot success are found to be very similar for all competitions.", "Language": "en", "Citations": "22"},
{"Title": "The role of social connections in plagiarism detection", "Authors": ["Zrnec A.", "Lavbic D."], "Keywords": ["Integration", "Plagiarism detection process", "Plagiarism visualisation", "Social network anaylsis"], "Date": "2015", "Abstract": "Plagiarism is considered as an unethical act. Over the past few years its rate has increased considerably due to a widespread access to electronic documents on the Web. Existing tools for plagiarism detection are not efficient enough and if we want to successfully prevent these kind of acts we must improve today\u2019s plagiarism detection approaches. The paper proposes a framework for improved detection of plagiarism, where we focus on integration of information from social networks, information from the Web and semantically enriched visualization of information about authors and plagiates. Visualization enables exploring data and seeking of advanced patterns of plagiarism. We also developed a special tool to support the proposed framework. The results of evaluation confirmed our hypothesis that employment of social network analysis and advanced visualization techniques improves plagiarism detection process.", "Language": "en", "Citations": "0"},
{"Title": "Argument based machine learning from examples and text", "Authors": ["Mozina M.", "Giuliano C.", "Bratko I."], "Keywords": [], "Date": "2009", "Abstract": "We introduce a novel approach to cross-media learning based on argument based machine learning (ABML). ABML is a recent method that combines argumentation and machine learning from examples, and its main idea is to use arguments for some of the learning examples. Arguments are usually provided by a domain expert. In this paper, we present an alternative approach, where arguments used in ABML are automatically extracted from text with a technique for relation extraction. We demonstrate and evaluate the approach through a case study of learning to classify animals by using arguments automatically extracted from Wikipedia. \u00a9 2009 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "A bistable genetic switch based on designable DNA-binding domains", "Authors": ["Lebar T.", "Bezeljak U.", "Golob A.", "Jerala M.", "Kadunc L.", "Pirs B.", "Strazar M.", "Vucko D.", "Zupancic U.", "Bencina M.", "Forstneric V.", "Gaber R.", "Lonzaric J.", "Majerle A.", "Oblak A.", "Smole A.", "Jerala R."], "Keywords": [], "Date": "2014", "Abstract": "Bistable switches are fundamental regulatory elements of complex systems, ranging from electronics to living cells. Designed genetic toggle switches have been constructed from pairs of natural transcriptional repressors wired to inhibit one another. The complexity of the engineered regulatory circuits can be increased using orthogonal transcriptional regulators based on designed DNA-binding domains. However, a mutual repressor-based toggle switch comprising DNA-binding domains of transcription-activator-like effectors (TALEs) did not support bistability in mammalian cells. Here, the challenge of engineering a bistable switch based on monomeric DNA-binding domains is solved via the introduction of a positive feedback loop composed of activators based on the same TALE domains as their opposing repressors and competition for the same DNA operator site. This design introduces nonlinearity and results in epigenetic bistability. This principle could be used to employ other monomeric DNA-binding domains such as CRISPR for applications ranging from reprogramming cells to building digital biological memory.", "Language": "en", "Citations": "28"},
{"Title": "BzpF is a CREB-like transcription factor that regulates spore maturation and stability in Dictyostelium", "Authors": ["Huang E.", "Talukder S.", "Hughes T.R.", "Curk T.", "Zupan B.", "Shaulsky G.", "Katoh-Kurasawa M."], "Keywords": ["CREB-like bZIP transcription factor BzpF", "Dictyostelium", "Late-developmental target genes", "PKA pathway", "Spore maturation", "Spore stability"], "Date": "2011", "Abstract": "The cAMP response element-binding protein (CREB) is a highly conserved transcription factor that integrates signaling through the cAMP-dependent protein kinase A (PKA) in many eukaryotes. PKA plays a critical role in Dictyostelium development but no CREB homologue has been identified in this system. Here we show that Dictyostelium utilizes a CREB-like protein, BzpF, to integrate PKA signaling during late development. bzpF", "Language": "en", "Citations": "13"},
{"Title": "Abductive inference of genetic networks", "Authors": ["Zupan B.", "Bratko I.", "Demsar J.", "Robert Beck J.", "Kuspa A.", "Shaulsky G."], "Keywords": [], "Date": "2001", "Abstract": "GenePath is an automated system for reasoning on genetic networks, wherein a set of genes have various influences on one another and on a biological outcome. It acts on a set of experiments in which genes are knocked out or overexpressed, and the outcome of interest is evaluated. Implemented in Prolog, GenePath uses abductive inference to elucidate network constraints based on prior knowledge and experimental results. Two uses of the system are demonstrated: synthesis of a consistent network from abduced constraints, and qualitative reasoning-based approach that generates a set of networks consistent with the data. In practice, illustrated by an example using Dictyostelium aggregation, a combination of constraint satisfaction and qualitative reasoning produces a small set of plausible networks.", "Language": "en", "Citations": "5"},
{"Title": "Algorithms for drawing polyhedra from 3-connected planar graphs", "Authors": ["Orbanic A.", "Boben M.", "Jaklic G.", "Pisanski T."], "Keywords": ["Graph drawing", "Polyhedral representations", "Representation of a graph", "Tutte's drawing method"], "Date": "2004", "Abstract": "Two algorithms for producing polyhedral representations for 3-connected planar graphs are discussed in the paper. One of them uses Tutte's drawing algorithm [11] to produce a 2D drawing. Then the drawing is lifted into 3D space obtaining a polyhedral embedding. The other is a simple algorithm by G. Hart [4] for drawing canonical polyhedral representations. Some alternative aspects (physical model, Markov chain model) in algorithms for obtaining Tutte's drawings are presented and proved.", "Language": "en", "Citations": "0"},
{"Title": "ICLIP reveals the function of hnRNP particles in splicing at individual nucleotide resolution", "Authors": ["Konig J.", "Zarnack K.", "Rot G.", "Curk T.", "Kayikci M.", "Zupan B.", "Turner D.J.", "Luscombe N.M.", "Ule J."], "Keywords": [], "Date": "2010", "Abstract": "In the nucleus of eukaryotic cells, nascent transcripts are associated with heterogeneous nuclear ribonucleoprotein (hnRNP) particles that are nucleated by hnRNP C. Despite their abundance, however, it remained unclear whether these particles control pre-mRNA processing. Here, we developed individual-nucleotide resolution UV cross-linking and immunoprecipitation (iCLIP) to study the role of hnRNP C in splicing regulation. iCLIP data show that hnRNP C recognizes uridine tracts with a defined long-range spacing consistent with hnRNP particle organization. hnRNP particles assemble on both introns and exons but remain generally excluded from splice sites. Integration of transcriptome-wide iCLIP data and alternative splicing profiles into an 'RNA map' indicates how the positioning of hnRNP particles determines their effect on the inclusion of alternative exons. The ability of high-resolution iCLIP data to provide insights into the mechanism of this regulation holds promise for studies of other higher-order ribonucleoprotein complexes. \u00a9 2010 Nature America, Inc. All rights reserved.", "Language": "en", "Citations": "537"},
{"Title": "Self-similar scaling of density in complex real-world networks", "Authors": ["Blagus N.", "Subelj L.", "Bajec M."], "Keywords": ["Community structure", "Complex networks", "Network density", "Self-similarity"], "Date": "2012", "Abstract": "Despite their diverse origin, networks of large real-world systems reveal a number of common properties including small-world phenomena, scale-free degree distributions and modularity. Recently, network self-similarity as a natural outcome of the evolution of real-world systems has also attracted much attention within the physics literature. Here we investigate the scaling of density in complex networks under two classical box-covering renormalizationsnetwork coarse-grainingand also different community-based renormalizations. The analysis on over 50 real-world networks reveals a power-law scaling of network density and size under adequate renormalization technique, yet irrespective of network type and origin. The results thus advance a recent discovery of a universal scaling of density among different real-world networks [P.J. Laurienti, K.E. Joyce, Q.K. Telesford, J.H. Burdette, S. Hayasaka, Universal fractal scaling of self-organized networks, Physica A 390 (20) (2011) 36083613] and imply an existence of a scale-free density also withinamong different self-similar scales ofcomplex real-world networks. The latter further improves the comprehension of self-similar structure in large real-world networks with several possible applications. \u00a9 2011 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "44"},
{"Title": "Genome sequence of a lethal strain of xylem-invading Verticillium nonalfalfae", "Authors": ["Jakse J.", "Jelen V.", "Radisek S.", "de Jonge R.", "Mandelc S.", "Majer A.", "Curk T.", "Zupan B.", "Thomma B.P.H.J.", "Javornik B."], "Keywords": [], "Date": "2018", "Abstract": "Verticillium nonalfalfae, a soilborne vascular phytopathogenic fungus, causes wilt disease in several crop species. Of great concern are outbreaks of highly aggressive V. nonalfalfae strains, which cause a devastating wilt disease in European hops. We report here the genome sequence and annotation of V. nonalfalfae strain T2, providing genomic information that will allow better understanding of the molecular mechanisms underlying the development of highly aggressive strains.", "Language": "en", "Citations": "6"},
{"Title": "Energy efficient communication in next generation rural-area wireless networks", "Authors": ["Pejovic V.", "Belding E."], "Keywords": ["Algorithms", "Design", "Experimentation", "Performance", "Theory"], "Date": "2010", "Abstract": "White space frequencies are highly attractive for long-distance communication due to greater signal propagation. The lack of standards and licensing issues with increased flexibility provided by the cognitive radio allow for sophisticated customized solutions for white spaces. Rural-area networks are seen as the main beneficiaries and white spaces communication is expected to outperform current wireless solutions in this domain. However, rural networks often have to rely on a constrained energy budget and highly benefit from energy-efficient operation. We investigate the efficiency of flexible wireless transmission over longdistance white space links. We theoretically and experimentally examine the impact of channel width, modulation and coding and transmission amplitude on energy consumption. From our findings we derive the physical layer parameter settings that achieve energy optimality and develop Power- Rate, a protocol that dynamically adjusts transmission parameters according to channel state. We implement PowerRate in GNUradio and evaluate its energy-saving potential in various fading environments.", "Language": "en", "Citations": "7"},
{"Title": "Multiprocess time queue", "Authors": ["Brodnik A.", "Karlsson J."], "Keywords": [], "Date": "2001", "Abstract": "We show how to implement a bounded time queue for two dierent processes. The time queue is a variant of a priority queue with elements from a discrete universe. The bounded time queue has elements from a discrete bounded universe. One process has time constraints and may only spend constant worst case time on each operation while the other process may spend more time. The time constrained process only has to be able to perform some of the time queue operations while the other process has to be able to perform all operations.We show how to do a deamortization of the deleteMin cost and to provide mutual exclusion for the parts of the data structure that both processes maintain. \u00a9 2001 Springer Berlin Heidelberg.", "Language": "en", "Citations": "0"},
{"Title": "A simple pipelined logarithmic multiplier", "Authors": ["Bulic P.", "Babic Z.", "Avramovic A."], "Keywords": [], "Date": "2010", "Abstract": "Digital signal processing algorithms often rely heavily on a large number of multiplications, which is both time and power consuming. However, there are many practical solutions to simplify multiplication, like truncated and logarithmic multipliers. These methods consume less time and power but introduce errors. Nevertheless, they can be used in situations where a shorter time delay is more important than accuracy. In digital signal processing, these conditions are often met, especially in video compression and tracking, where integer arithmetic gives satisfactory results. This paper presents and compare different multipliers in a logarithmic number system. For the hardware implementation assessment, the multipliers are implemented on the Spartan 3 FPGA chip and are compared against speed, resources required for implementation, power consumption and error rate. We also propose a simple and efficient logarithmic multiplier with the possibility to achieve an arbitrary accuracy through an iterative procedure. In such a way, the error correction can be done almost in parallel (actually this is achieved through pipelining) with the basic multiplication. The hardware solution involves adders and shifters, so it is not gate and power consuming. The error of proposed multiplier for operands ranging from 8 bits to 16 bits indicates a very low relative error percentage. \u00a9 2010 IEEE.", "Language": "en", "Citations": "9"},
{"Title": "Catadioptric image-based rendering for mobile robot localization", "Authors": ["Bakstein H.", "Leonardis A."], "Keywords": [], "Date": "2007", "Abstract": "We present an approach to view-based mobile robot localization using a X-slits image based rendering (IBR) method for creating novel views from a set of input images. The input images are acquired by a non-central catadioptric sensor mounted on a robot moving on a straight line. We propose to use the IBR for column ordering only, where occlusions in the horizontal direction are modeled and the sensor can be non-central. For the column matching between a query view at an unknown position and virtual views created by IBR, we use correlation of columns. \u00a92007 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "Using GMB methodology on a large crisis model", "Authors": ["Hernantes J.", "Torres J.M.", "Lauge A.", "Sarriegi J.M.", "Starc I.", "Zupancic E.", "Trcek D."], "Keywords": ["Collaborative methods", "Collaborative modeling process", "Crisis management", "Group Model Building (GMB)"], "Date": "2010", "Abstract": "Mitigating, detecting, evaluating, responding and recovering from crises are highly complex tasks that involve many decision makers (agents). As a consequence using collaborative methods that allow the cooperation among these agents during the crisis management strategy and procedures design is of significant importance. Group Model Building (GMB) is a robust collaborative methodology that has been successfully used for modelling several complex socio-technical problems, where different agents may have diverse perspectives or interests in the problem under analysis. Through the development of a series of exercises, GMB allows the integration of these initially fragmented perspectives. Modellers translate the knowledge elicited from experts during GMB workshops into simulation models that reproduce the behaviour of the problem. This paper presents the use and adaptation of the GMB methodology in a research project about large pan European crises due to outages in the electricity sector.", "Language": "en", "Citations": "2"},
{"Title": "Comprehensive Identification of RNA-Binding Domains in Human Cells", "Authors": ["Castello A.", "Fischer B.", "Frese C.K.", "Horos R.", "Alleaume A.-M.", "Foehr S.", "Curk T.", "Krijgsveld J.", "Hentze M.W."], "Keywords": [], "Date": "2016", "Abstract": "Mammalian cells harbor more than a thousand RNA-binding proteins (RBPs), with half of these employing unknown modes of RNA binding. We developed RBDmap to determine the RNA-binding sites of native RBPs on a proteome-wide scale. We identified 1,174 binding sites within 529 HeLa cell RBPs, discovering numerous RNA-binding domains (RBDs). Catalytic centers or protein-protein interaction domains are in close relationship with RNA-binding sites, invoking possible effector roles of RNA in the control of protein function. Nearly half of the RNA-binding sites map to intrinsically disordered regions, uncovering unstructured domains as prevalent partners in protein-RNA interactions. RNA-binding sites represent hot spots for defined posttranslational modifications such as lysine acetylation and tyrosine phosphorylation, suggesting metabolic and signal-dependent regulation of RBP function. RBDs display a high degree of evolutionary conservation and incidence of Mendelian mutations, suggestive of important functional roles. RBDmap thus yields profound insights into native protein-RNA interactions in living cells.", "Language": "en", "Citations": "114"},
{"Title": "Fast segmentation, conversion and rendering of volumetric data using GPU", "Authors": ["Bohak C.", "Sodja A.", "Marolt M.", "Mitrovic U.", "Pernus F."], "Keywords": ["3D visualisation", "GPU computation", "medical visualisation", "volume data segmentation"], "Date": "2014", "Abstract": "In this paper we present a proof-of-concept implementation of fast volumetric data segmentation, conversion to polygonal mesh geometry and rendering. All parts of the method are implemented on the graphical processing unit, which allows high degree of parallelization. Implementations of presented algorithms are done in the OpenCL framework and are integrated in blood vessel visualisation software Neck Veins. This paper presents where and to what degree parts of method can be parallelized. In results we also show to what degree we can speed-up the implementation by using parallel computing power of the graphical processing units. \u00a9 2014 University of Zagreb.", "Language": "en", "Citations": "1"},
{"Title": "Sensory trick efficacy in cervical dystonia is linked to processing of neck proprioception", "Authors": ["Brugger F.", "Peters A.", "Georgiev D.", "Kagi G.", "Balint B.", "Bhatia K.P.", "Day B.L."], "Keywords": ["Cervical dystonia", "Kinematic analysis", "Neck vibration", "Posturography", "Sensory trick"], "Date": "2019", "Abstract": "Background: Muscle vibration activates muscle spindles and when applied over posterior neck muscles during stance modulates global body orientation. This is characterised by a tonic forward sway response that is reportedly diminished or absent in patients with idiopathic cervical dystonia. Objective: To investigating the impact of the sensory trick on vibration-induced postural responses. Methods: 20 patients with idiopathic cervical dystonia and a sensory trick, 15 patients without a trick, and 16 healthy controls were recruited. Neck muscle vibration was applied bilaterally over the upper trapezius under three different conditions: 1) Quiet standing; 2) standing while performing the trick (or trick-like movement in non-responders); 3) standing while elevating the flexed arm without touching any part of the body. Centre of pressure position and whole-body orientation in the sagittal plane were analysed. Results: Patients with a sensory trick responded similarly to healthy controls: neck muscle vibration led to an initial forward sway of the body that slowly increased during the prolonged vibration for all three conditions. This response was mainly mediated by ankle flexion. In patients without a trick, the initial sagittal sway was significantly reduced in all three conditions and the later slow increase was absent. Performance of the trick did not have an effect on any aspect of the response in either cervical dystonia group. Conclusions: The whole-body response to neck vibration in cervical dystonia differs depending on the effectiveness of the sensory trick to alleviate the dystonic neck posture. Variable pathophysiology of proprioceptive processing may be the common factor.", "Language": "en", "Citations": "1"},
{"Title": "Estimation of a fluorescent lamp spectral distribution for color image in machine vision", "Authors": ["Corzo L.G.", "Penaranda J.A.", "Peer P."], "Keywords": ["Constrained least square", "Illumination spectral distribution", "Machine vision", "Quadratic programming"], "Date": "2005", "Abstract": "We present a technique to quickly estimate the Illumination Spectral Distribution (ISD) in an image illuminated by a fluorescent lamp. It is assumed that the object colors are a set of colors for which spectral reflectances are available (in our experiments we use spectral measurements of 12 colors checker chart), the sensitivities of the camera sensors are known and the camera response is linear. Thus, the ISD can be approximated by a finite linear combinations of a small number of basis functions.", "Language": "en", "Citations": "9"},
{"Title": "Diurnal changes of heart rate and sympatho-vagal activity for temporal patterns of transient ischemia", "Authors": ["Smrdel A.", "Jager F."], "Keywords": [], "Date": "2005", "Abstract": "Using all 86 records of the Long-Term ST Database we studied diurnal variations of ischemia and heart rate among patients exhibiting different temporal patterns of ischemia: salvo, periodic and sporadic pattern. The results show, that the incidence of ischemia increases during the morning interval. The decrease of sympathetic and vagal activity during ischemia is the most prominent for the sporadic group, while for the salvo group only minor changes were observed. The results support our hypothesis, that there are at least two distinct populations which differ according to mechanisms and temporal patterns of ischemia. \u00a9 2005 IEEE.", "Language": "en", "Citations": "1"},
{"Title": "Automatic Cell Counter for cell viability estimation", "Authors": ["Lojk J.", "Sajn L.", "Cibej U.", "Pavlin M."], "Keywords": [], "Date": "2014", "Abstract": "Despite several methods that exist in different fields of life sciences, certain biotechnological applications still require microscopic analysis of the samples and in many instances, counting of cells. Some of those are drug delivery, transfection or analysis of mechanism fluorescent probes are used to detect cell viability, efficiency of a specific drug delivery or some other effect. For analysis and quantification of these results it is necessary to either manually or automatically count and analyze microscope images. However, in everyday use many researchers still count cells manually since existing solutions require either some specific knowledge of computer vision and/or manual fine tuning of various parameters. Here we present a new software solution (named CellCounter) for automatic and semi-automatic cell counting of fluorescent microscopic images. This application is specifically designed for counting fluorescently stained cells. The program enables counting of cell nuclei or cell cytoplasm stained with different fluorescent stained. This simplifies image analysis for several biotechnological applications where fluorescent microscopy is used. We present results and validate the presented automatic cell counting program for cell viability application. We give empirical results showing the efficiency of the proposed solution by comparing manual counts with the results returned by automated counting. We also show how the results can be further improved by combining manual and automated counts. \u00a9 2014 MIPRO.", "Language": "en", "Citations": "4"},
{"Title": "An approach for concurrent evaluation of technical and social aspects of software development methodologies", "Authors": ["Vavpotic D.", "Bajec M."], "Keywords": ["Method engineering", "Methodology evaluation", "Software development methodology", "Software process improvement"], "Date": "2009", "Abstract": "The paper presents an approach for evaluation of software development methodologies (SDM) that considers the aspects of a SDM's social adoption and technical efficiency. It builds on existing evaluation models used in the field of SDM. Case study approach was used to validate the model in four software development organisations. In all four cases the management confirmed that the model provided valuable new insights into adoption and efficiency of the companies' SDM. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "32"},
{"Title": "Elicitation of neurological knowledge with ABML", "Authors": ["Groznik V.", "Guid M.", "Sadikov A.", "Mozina M.", "Georgiev D.", "Kragelj V.", "Ribaric S.", "Pirtosek Z.", "Bratko I."], "Keywords": [], "Date": "2011", "Abstract": "The paper describes the process of knowledge elicitation for a neurological decision support system. To alleviate the difficult problem of knowledge elicitation from data and domain experts, we used a recently developed technique called ABML (Argument Based Machine Learning). The paper demonstrates ABML's advantage in combining machine learning and expert knowledge. ABML guides the expert to explain critical special cases which cannot be handled automatically by machine learning. This very efficiently reduces the expert's workload, and combines it with automatically learned knowledge. We developed a decision support system to help the neurologists differentiate between three types of tremors: Parkinsonian, essential, and mixed tremor (co-morbidity). The system is intended to act as a second opinion for the neurologists, and most importantly to help them reduce the number of patients in the \"gray area\" that require a very costly further examination (DaTSCAN). \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "2"},
{"Title": "Generating inter-dependent data streams for recommender systems", "Authors": ["Jakomin M.", "Curk T.", "Bosnic Z."], "Keywords": ["Data fusion", "Multiple data streams", "Recommender systems", "Relational data", "Synthetic data generator"], "Date": "2018", "Abstract": "Recommender systems are essential tools in modern e-commerce, streaming services, search engines, social networks and many other areas including the scientific community. However, lack of publicly available data hinders the development and evaluation of recommender algorithms. To address this problem, we propose a Generator of Inter-dependent Data Streams (GIDS), capable of generating multiple temporal and inter-dependent synthetic datasets of relational data. The generator is able to simulate a collection of time-changing data streams, helping to effectively evaluate a variety of recommender systems, data fusion algorithms and incremental algorithms. The evaluation using recommender and data fusion algorithms showed that our generator can successfully mimic real datasets in terms of statistical data properties, and achieved performance of recommender systems.", "Language": "en", "Citations": "1"},
{"Title": "Concurrent software architectures for exploratory data analysis", "Authors": ["Staric A.", "Demsar J.", "Zupan B."], "Keywords": [], "Date": "2015", "Abstract": "Decades ago, increased volume of data made manual analysis obsolete and prompted the use of computational tools with interactive user interfaces and rich palette of data visualizations. Yet their classic, desktop-based architectures can no longer cope with the ever-growing size and complexity of data. Next-generation systems for explorative data analysis will be developed on client-server architectures, which already run concurrent software for data analytics but are not tailored to for an engaged, interactive analysis of data and models. In explorative data analysis, the key is the responsiveness of the system and prompt construction of interactive visualizations that can guide the users to uncover interesting data patterns. In this study, we review the current software architectures for distributed data analysis and propose a list of features to be included in the next generation frameworks for exploratory data analysis. The new generation of tools for explorative data analysis will need to address integrated data storage and processing, fast prototyping of data analysis pipelines supported by machine-proposed analysis workflows, pre-emptive analysis of data, interactivity, and user interfaces for intelligent data visualizations. The systems will rely on a mixture of concurrent software architectures to meet the challenge of seamless integration of explorative data interfaces at client site with management of concurrent data mining procedures on the servers.", "Language": "en", "Citations": "0"},
{"Title": "Genetic variability of inflammation and oxidative stress genes does not play a major role in the occurrence of adverse events of dopaminergic treatment in Parkinson's disease", "Authors": ["Redensek S.", "Flisar D.", "Kojovic M.", "Kramberger M.G.", "Georgiev D.", "Pirtosek Z.", "Trost M.", "Dolzan V."], "Keywords": ["Adverse events", "Inflammation", "Oxidative stress", "Parkinson's disease", "Polymorphism", "Susceptibility"], "Date": "2019", "Abstract": "Background: Inflammation and oxidative stress are recognized as important contributors to Parkinson's disease pathogenesis. As such, genetic variability in these pathways could have a role in susceptibility for the disease as well as in the treatment outcome. Dopaminergic treatment is effective in management of motor symptoms, but poses a risk for motor and non-motor adverse events. Our aim was to evaluate the impact of selected single-nucleotide polymorphisms in genes involved in inflammation and oxidative stress on Parkinson's disease susceptibility and the occurrence of adverse events of dopaminergic treatment. Methods: In total, 224 patients were enrolled, and their demographic and clinical data on the disease course were collected. Furthermore, a control group of 146 healthy Slovenian blood donors were included for Parkinson's disease' risk evaluation. Peripheral blood was obtained for DNA isolation. Genotyping was performed for NLRP3 rs35829419, CARD8 rs2043211, IL1\u03b2 rs16944, IL1\u03b2 rs1143623, IL6 rs1800795, CAT rs1001179, CAT rs10836235, SOD2 rs4880, NOS1 rs2293054, NOS1 rs2682826, TNF-\u03b1 rs1800629, and GPX1 rs1050450. Logistic regression was used for analysis of possible associations. Results: We observed a nominally significant association of the IL1\u03b2 rs1143623 C allele with the risk for Parkinson's disease (OR = 0.59; 95%CI = 0.38-0.92, p = 0.021). CAT rs1001179 A allele was significantly associated with peripheral edema (OR = 0.32; 95%CI = 0.15-0.68; p = 0.003). Other associations observed were only nominally significant after adjustments: NOS1 rs2682826 A allele and excessive daytime sleepiness and sleep attacks (OR = 1.75; 95%CI = 1.00-3.06, p = 0.048), SOD2 rs4880 T allele and nausea/vomiting (OR = 0.49, 95%CI = 0.25-0.94; p = 0.031), IL1\u03b2 rs1143623 C allele and orthostatic hypotension (OR = 0.57, 95%CI = 0.32-1.00, p = 0.050), and NOS1 rs2682826 A allele and impulse control disorders (OR = 2.59; 95%CI = 1.09-6.19; p = 0.032). We did not find any associations between selected polymorphisms and motor adverse events. Conclusions: Apart from some nominally significant associations, one significant association between CAT genetic variability and peripheral edema was observed as well. Therefore, the results of our study suggest some links between genetic variability in inflammation- and oxidative stress-related pathways and non-motor adverse events of dopaminergic treatment. However, the investigated polymorphisms do not play a major role in the occurrence of the disease and the adverse events of dopaminergic treatment.", "Language": "en", "Citations": "0"},
{"Title": "Supporting smart construction with dependable edge computing infrastructures and applications", "Authors": ["Kochovski P.", "Stankovski V."], "Keywords": ["Container-based systems", "Dependability", "Edge computing", "Internet of Things", "Smart construction"], "Date": "2018", "Abstract": "The Internet of Things (IoT) such as the use of robots, sensors, actuators, electronic signalization and a variety of other Internet enabled physical devices may provide for new advanced smart applications to be used in construction in the very near future. Such applications require real-time responses and are therefore time-critical. Therefore, in order to support collaboration, control, monitoring, supply management, safety and other construction processes, they have to meet dependability requirements, including requirements for high Quality of Service (QoS). Dependability and high QoS can be achieved by using adequate number and quality of computing resources, such as processing, memory and networking elements, geographically close to the smart environments. The goal of this study is to develop a practical edge computing architecture and design, which can be used to support smart construction environments with high QoS. This study gives particular attention to the solution design, which relies on latest cloud and software engineering approaches and technologies, and provides elasticity, interoperability and adaptation to companies\u2019 specific needs. Two edge computing applications supporting video communications and construction process documentation are developed and demonstrate a viable edge computing design for smart construction.", "Language": "en", "Citations": "7"},
{"Title": "Estimating confidence values of individual predictions by their typicalness and reliability", "Authors": ["Kukar M."], "Keywords": [], "Date": "2004", "Abstract": "Although machine learning algorithms have been successfully used in many problems, and are emerging as valuable data analysis tools, their serious practical use is affected by the fact that often they cannot produce reliable and unbiased assessments of their predictions' quality. There exist several approaches for estimating reliability or confidence for individual classifications, and many of them build upon the algorithmic theory of randomness, such as transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: Either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper we propose a joint approach that compensates the mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into a joint confidence machine.", "Language": "en", "Citations": "0"},
{"Title": "Comparison of cloud vs. tape backup performance and costs with oracle database", "Authors": ["Zrnec A.", "Lavbic D."], "Keywords": ["Bandwidth", "Cloud", "Computer center", "Cost", "Performance", "Remote location", "Service"], "Date": "2011", "Abstract": "Current practice of backing up data is based on using backup tapes and remote locations for storing data. Nowadays, with the advent of cloud computing a new concept of database backup emerges. The paper presents the possibility of making backup copies of data in the cloud. We are mainly focused on performance and economic issues of making backups in the cloud in comparison to traditional backups. We tested the performance and overall costs of making backup copies of data in Oracle database using Amazon S3 and EC2 cloud services. The costs estimation was performed on the basis of the prices published on Amazon S3 and Amazon EC2 sites.", "Language": "en", "Citations": "2"},
{"Title": "Design considerations of an electromechanical dice gambling machine", "Authors": ["Bergant Urban", "Zimic Niko", "Lapanja Iztok"], "Keywords": [], "Date": "2000", "Abstract": "The most important module of electronic dice machine is detection of dice location and throw. A detection method based on contactless electronic identification (ID) keys which are positioned on the facets of the dice is presented. The method provides high precision and reliability electronic gambling machines, which uses mechanical random number generators.", "Language": "en", "Citations": "4"},
{"Title": "Publication boost in web of science journals and its effect on citation distributions", "Authors": ["Subelj L.", "Fiala D."], "Keywords": [], "Date": "2017", "Abstract": "In this article, we show that the dramatic increase in the number of research articles indexed in the Web of Science database impacts the commonly observed distributions of citations within these articles. First, we document that the growing number of physics articles in recent years is attributed to existing journals publishing more and more articles rather than more new journals coming into being as it happens in computer science. Second, even though the references from the more recent articles generally cover a longer time span, the newer articles are cited more frequently than the older ones if the uneven article growth is not corrected for. Nevertheless, despite this change in the distribution of citations, the citation behavior of scientists does not seem to have changed.", "Language": "en", "Citations": "5"},
{"Title": "Development of a program for playing progressive chess", "Authors": ["Janko V.", "Guid M."], "Keywords": [], "Date": "2015", "Abstract": "We present the design of a computer program for playing Progressive Chess. In this game, players play progressively longer series of moves rather than just making one move per turn. Our program follows the generally recommended strategy for this game, which consists of three phases: looking for possibilities to checkmate the opponent, playing generally good moves when no checkmate can be found, and preventing checkmates from the opponent. In this paper, we focus on efficiently searching for checkmates, putting to test various heuristics for guiding the search. We also present the findings of self-play experiments between different versions of the program.", "Language": "en", "Citations": "1"},
{"Title": "Microarray data mining with visual programming", "Authors": ["Curk T.", "Demsar J.", "Xu Q.", "Leban G.", "Petrovic U.", "Bratko I.", "Shaulsky G.", "Zupan B."], "Keywords": [], "Date": "2005", "Abstract": "Summary: Visual programming offers an intuitive means of combining known analysis and visualization methods into powerful applications. The system presented here enables users who are not programmers to manage microarray and genomic data flow and to customize their analyses by combining common data analysis tools to fit their needs. \u00a9 Oxford University Press 2004; all rights reserved.", "Language": "en", "Citations": "105"},
{"Title": "QADE: a novel trust and reputation model for handling false trust values in e\u2013commerce environments with subjectivity consideration", "Authors": ["Zupancic E.", "Trcek D."], "Keywords": ["e-commerce", "false trust values", "subjectivity", "trust and reputation management systems"], "Date": "2017", "Abstract": "Trust is essential to economic efficiency. Trading partners choose each other and make decisions based on how much they trust one another. The way to assess trust in e-commerce is different from those in brick and mortar businesses, as there are limited indicators available in online environments. One way is to deploy trust and reputation management systems that are based on collecting feedbacks about partners\u2019 transactions. One of the problems within such systems is the presence of unfair ratings. In this paper, an innovative QADE trust model is presented, which assumes the existence of unfairly reported trust assessments. Subjective nature of trust is considered, where differently reported trust values do not necessarily mean false trust values but can also imply differences in dispositions to trust. The method to identify and filter out the presumably false values is defined. In our method, a trust evaluator finds other agents in society that are similar to him, taking into account pairwise similarity of trust values and similarity of agents\u2019 general mindsets. In order to reduce the effect of unfair ratings, the values reported by the non-similar agents are excluded from the trust computation. Simulations have been used to compare the effectiveness of algorithms to decrease the effect of unfair ratings. The simulations have been carried out in environments with varying number of attackers and targeted agents, as well as with different kinds of attackers. The results showed significant improvements of our proposed method. On average 6% to 13% more unfair trust ratings have been detected by our method. Unfair rating effects on trust assessment were reduced with average improvements from 26% to 57% compared to the other most effective filtering methods by Whitby and Teacy.", "Language": "en", "Citations": "4"},
{"Title": "Convolutional encoder-decoder networks for pixel-wise ear detection and segmentation", "Authors": ["Emersic Z.", "Gabriel L.L.", "Struc V.", "Peer P."], "Keywords": [], "Date": "2018", "Abstract": "Object detection and segmentation represents the basis for many tasks in computer and machine vision. In biometric recognition systems the detection of the region-of-interest (ROI) is one of the most crucial steps in the processing pipeline, significantly impacting the performance of the entire recognition system. Existing approaches to ear detection, are commonly susceptible to the presence of severe occlusions, ear accessories or variable illumination conditions and often deteriorate in their performance if applied on ear images captured in unconstrained settings. To address these shortcomings, we present a novel ear detection technique based on convolutional encoder-decoder networks (CEDs). We formulate the problem of ear detection as a two-class segmentation problem and design and train a CED-network architecture to distinguish between imagepixels belonging to the ear and the non-ear class. Unlike competing techniques, our approach does not simply return a bounding box around the detected ear, but provides detailed, pixel-wise information about the location of the ears in the image. Experiments on a dataset gathered from the web (a.k.a. in the wild) show that the proposed technique ensures good detection results in the presence of various covariate factors and significantly outperforms competing methods from the literature.", "Language": "en", "Citations": "8"},
{"Title": "Dynamic neural network architecture inspired by the immune algorithm to predict preterm deliveries in pregnant women", "Authors": ["Hussain A.J.", "Fergus P.", "Al-Askar H.", "Al-Jumeily D.", "Jager F."], "Keywords": ["Classification", "Electrohysterography", "Machine learning", "Neural networks", "Preterm delivery", "Term delivery"], "Date": "2015", "Abstract": "There has been some improvement in the treatment of preterm infants, which has helped to increase their chance of survival. However, the rate of premature births is still globally increasing. As a result, this group of infants is most at risk of developing severe medical conditions that can affect the respiratory, gastrointestinal, immune, central nervous, auditory and visual systems. There is a strong body of evidence emerging that suggests the analysis of uterine electrical signals, from the abdominal surface (Electrohysterography - EHG), could provide a viable way of diagnosing true labour and even predict preterm deliveries. This paper explores this idea further and presents a new dynamic self-organized network immune algorithm that classifies term and preterm records, using an open dataset containing 300 records (38 preterm and 262 term). Using the dataset, oversampling and cross validation techniques are evaluated against other similar studies. The proposed approach shows an improvement on existing studies with 89% sensitivity, 91% specificity, 90% positive predicted value, 90% negative predicted value, and an overall accuracy of 90%.", "Language": "en", "Citations": "25"},
{},
{"Title": "Hardware Implementation of FAST Algorithm for Mobile Applications", "Authors": ["Soberl D.", "Zimic N.", "Leonardis A.", "Krivic J.", "Moskon M."], "Keywords": ["Corner detection", "FAST-9", "FPGA", "Image feature", "Image recognition"], "Date": "2015", "Abstract": "Simple inexpensive cameras are often built in small devices such as mobile phones or mp3 players. Besides the usual image recording, other ways of their use have been proposed which usually involve intensive image processing. In such processing, corner detection is often found as a preliminary operation. Many corner detection algorithms have been introduced, but due to their computational complexity very few are suitable for real-time applications. One of novel approaches to corner detection is the so called FAST algorithm which is specially optimized for speed. However, on simple and slow devices even this algorithm can be too slow and energy consuming when executed on the in-built processor. In this paper we present hardware implementation of FAST algorithm, capable of processing images at constant speed of one pixel per clock. The results showed that nearly forty times faster corner detection could be achieved on mobile object detection and localization application, if the existing software detector is replaced by our hardware module.", "Language": "en", "Citations": "1"},
{"Title": "Movement-related potentials in Parkinson's disease", "Authors": ["Georgiev D.", "Lange F.", "Seer C.", "Kopp B.", "Jahanshahi M."], "Keywords": ["Bereitschaftspotential", "Contingent Negative Variation", "Lateralized readiness potential", "Movement-related potentials", "Parkinson's disease"], "Date": "2016", "Abstract": "To date, many different approaches have been used to study the impairment of motor function in Parkinson's disease (PD). Event-related potentials (ERPs) are averaged amplitude fluctuations of the ongoing EEG activity that are time locked to specific sensory, motor or cognitive events, and as such can be used to study different brain processes with an excellent temporal resolution. Movement-related potentials (MRPs) are ERPs associated with processes of voluntary movement preparation and execution in different paradigms. In this review we concentrate on MRPs in PD. We review studies recording the Bereitschaftspotential, the Contingent Negative Variation, and the lateralized readiness potential in PD to highlight the contributions they have made to further understanding motor deficits in PD. Possible directions for future research are also discussed.", "Language": "en", "Citations": "10"},
{"Title": "Multiple interacting targets tracking with application to team sports", "Authors": ["Kristan M.", "Pers J.", "Perse M.", "Kovacic S.", "Bon M."], "Keywords": [], "Date": "2005", "Abstract": "The interest in the field of computer aided analysis of sport events is ever growing and the ability of tracking objects during a sport event has become an elementary task for nearly every sport analysis system. We present in this paper a color based probabilistic tracker that is suitable for tracking players on the playground during a sport game. Since the players are being tracked in their natural environment, and this environment is subjected to certain rules of the game, we use the concept of closed worlds, to model the scene context and thus improve the reliability of tracking.", "Language": "en", "Citations": "14"},
{"Title": "Towards a model independent method for explaining classification for individual instances", "Authors": ["Strumbelj E.", "Kononenko I."], "Keywords": [], "Date": "2008", "Abstract": "Recently, a method for explaining the model's decision for an instance was introduced by Robnik-\u0160ikonja and Kononenko. It is a rare example of a model-independent explanation method. In this paper we make a step towards formalization of the model-independent explanation methods by defining the criteria and a testing environment for such methods. We extensively test the aforementioned method and its variations. The results confirm some of the qualities of the original method as well as expose several of its shortcomings. We propose a new method, based on attribute interactions, that overcomes the shortcomings of the original method and serves as a theoretical framework for further work. \u00a9 2008 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "3"},
{"Title": "Automatic natural and man-made scene differentiation using perspective geometrical properties of the scenes", "Authors": ["Kovac J.", "Peer P.", "Solina F."], "Keywords": ["Automathic scene differentiation", "Vanishing lines", "Vanishing points"], "Date": "2008", "Abstract": "In this paper we are trying to establish a framework for natural and man-made scene differentiation based on perspective geometrical properties of the scenes. Although, we have not jet achieved expected results with built classifier, evidence of distinguishing attributes exists and some of them are introduced in this paper.", "Language": "en", "Citations": "1"},
{"Title": "Tia1 dependent regulation of mRNA subcellular location and translation controls p53 expression in B cells", "Authors": ["Diaz-Munoz M.D.", "Kiselev V.Y.", "Novere N.L.", "Curk T.", "Ule J.", "Turner M."], "Keywords": [], "Date": "2017", "Abstract": "Post-transcriptional regulation of cellular mRNA is essential for protein synthesis. Here we describe the importance of mRNA translational repression and mRNA subcellular location for protein expression during B lymphocyte activation and the DNA damage response. Cytoplasmic RNA granules are formed upon cell activation with mitogens, including stress granules that contain the RNA binding protein Tia1. Tia1 binds to a subset of transcripts involved in cell stress, including p53 mRNA, and controls translational silencing and RNA granule localization. DNA damage promotes mRNA relocation and translation in part due to dissociation of Tia1 from its mRNA targets. Upon DNA damage, p53 mRNA is released from stress granules and associates with polyribosomes to increase protein synthesis in a CAP-independent manner. Global analysis of cellular mRNA abundance and translation indicates that this is an extended ATM-dependent mechanism to increase protein expression of key modulators of the DNA damage response.", "Language": "en", "Citations": "7"},
{"Title": "Transcriptional profiling of Dictyostelium with RNA sequencing", "Authors": ["Miranda E.R.", "Rot G.", "Toplak M.", "Santhanam B.", "Curk T.", "Shaulsky G.", "Zupan B."], "Keywords": ["Data mining", "dictyExpress", "Dictyostelium", "Differential expression", "Multiplexing", "Orange", "PIPA", "RNA sequencing", "Visual programming", "Web-based applications"], "Date": "2013", "Abstract": "Transcriptional profiling methods have been utilized in the analysis of various biological processes in Dictyostelium. Recent advances in high-throughput sequencing have increased the resolution and the dynamic range of transcriptional profiling. Here we describe the utility of RNA sequencing with the Illumina technology for production of transcriptional profiles. We also describe methods for data mapping and storage as well as common and specialized tools for data analysis, both online and offline. \u00a9 Springer Science+Business Media, LLC 2013.", "Language": "en", "Citations": "10"},
{"Title": "Towards a unified taxonomy and architecture of cloud frameworks", "Authors": ["Dukaric R.", "Juric M.B."], "Keywords": ["Architectural framework", "Cloud computing", "Infrastructure as a service", "Taxonomy"], "Date": "2012", "Abstract": "Infrastructure as a Service (IaaS) is one of the most important layers of Cloud Computing. However, there is an evident deficiency of mechanisms for analysis, comparison and evaluation of IaaS cloud implementations, since no unified taxonomy or reference architecture is available. In this article, we propose a unified taxonomy and an IaaS architectural framework. The taxonomy is structured around seven layers: core service layer, support layer, value-added services, control layer, management layer, security layer and resource abstraction.Wesurvey various IaaS systems and map them onto our taxonomy to evaluate the classification.Wethen introduce an IaaS architectural framework that relies on the unified taxonomy. We provide a detailed description of each layer and define dependencies between the layers and components. Finally, we evaluate the proposed IaaS architectural framework on several real-world projects, while performing a comprehensive analysis of the most important commercial and open-source IaaS products. The evaluation results show notable distinction of feature support and capabilities between commercial and open-source IaaS platforms, significant deficiency of important architectural components in terms of fulfilling true promise of infrastructure clouds, and real-world usability of the proposed taxonomy and architectural framework. \u00a9 2012 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "31"},
{"Title": "Learning faster by discovering and exploiting object similarities", "Authors": ["Janez T.", "Zabkar J.", "Mozina M.", "Bratko I."], "Keywords": ["Autonomous learning agents", "Domain complexity", "Learning by experimentation", "Learning speed", "Machine learning"], "Date": "2013", "Abstract": "In this paper we explore the question: \"Is it possible to speed up the learning process of an autonomous agent by performing experiments in a more complex environment (i.e., an environment with a greater number of different objects)?\" To this end, we use a simple robotic domain, where the robot has to learn a qualitative model predicting the change in the robot's distance to an object. To quantify the environment's complexity, we defined cardinal complexity as the number of objects in the robot's world, and behavioural complexity as the number of objects' distinct behaviours. We propose Error reduction merging (ERM), a new learning method that automatically discovers similarities in the structure of the agent's environment. ERM identifies different types of objects solely from the data measured and merges the observations of objects that behave in the same or similar way in order to speed up the agent's learning. We performed a series of experiments in worlds of increasing complexity. The results in our simple domain indicate that ERM was capable of discovering structural similarities in the data which indeed made the learning faster, clearly superior to conventional learning. This observed trend occurred with various machine learning algorithms used inside the ERM method. \u00a9 2013 Jane\u017e et al.; licensee InTech.", "Language": "en", "Citations": "0"},
{"Title": "Introductory programming course: Motivating students with prior knowledge", "Authors": ["Furst L.", "Mahnic V."], "Keywords": [], "Date": "2013", "Abstract": "In this article, the authors present an approach to motivating the students of an introductory programming course at the University of Ljubljana, Ljubljana, Slovenia. They focus on the students whose background knowledge of programming is at least equivalent to the level taught in the course and who, consequently, tend to feel disappointed with the course. To make the course more attractive to experienced programmers, the authors introduced two annual events: a two-player board game programming competition and a contest for project proposals on the subject of inheritance in object-oriented programming. Both events regularly attract considerable interest from all students attending the course, not only from those with sufficient prior knowledge. The contest for project proposals benefits not only the proponents themselves but also the teaching staff, since the best proposals may be re-used as homework projects in the future years. \u00a9 2013 WIETE.", "Language": "en", "Citations": "3"},
{"Title": "Orange: From experimental machine learning to interactive data mining", "Authors": ["Demsar J.", "Zupan B.", "Leban G.", "Curk T."], "Keywords": [], "Date": "2004", "Abstract": "Orange (www.ailab.si/orange) is a suite for machine learning and data mining. For researchers in machine learning, Orange offers scripting to easily prototype new algorithms and experimental procedures. For explorative data analysis, it provides a visual programming framework with emphasis on interactions and creative combinations of visual components. \u00a9 Springer-Verlag Berlin Heidelberg 2004.", "Language": "en", "Citations": "98"},
{"Title": "Ontology-based multi-label classification of economic articles", "Authors": ["Vogrincic S.", "Bosnic Z."], "Keywords": ["Document classification", "Economics", "Machine learning", "Multi-label classification", "Ontology", "Text categorization"], "Date": "2011", "Abstract": "The paper presents an approach to the task of automatic document categorization in the field of economics. Since the documents can be annotated with multiple keywords (labels), we approach this task by applying and evaluating multi-label classification methods of supervised machine learning. We describe forming a test corpus of 1015 economic documents that we automatically classify using a tool which integrates ontology construction with text mining methods. In our experimental work, we evaluate three groups of multi-label classification approaches: transformation to single-class problems, specialized multi- label models, and hierarchical/ranking models. The classification accuracies of all tested classification models indicate that there is a potential for using all of the evaluated methods to solve this task. The results show the benefits of using complex groups of approaches which benefit from exploiting dependence between the labels. A good alternative to these approaches is also single-class naive Bayes classifiers coupled with the binary relevance transformation approach.", "Language": "en", "Citations": "28"},
{"Title": "A cooperative development system for an interactive introductory programming course", "Authors": ["Furst L.", "Mahnic V."], "Keywords": [], "Date": "2012", "Abstract": "In this article, the authors present a system for the cooperative development of computer programs that was created for the laboratory sessions of an introductory programming course at the University of Ljubljana in Slovenia. The system has relieved students of the tedious task of retyping programs developed by the teaching assistant, and has enabled them to cooperate with the teaching assistant in solving programming problems. The authors have, thus, made the laboratory sessions more efficient and interactive, and have brought them closer to the spirit of active learning approaches. \u00a9 2012 WIETE.", "Language": "en", "Citations": "4"},
{"Title": "The Moodo dataset: Integrating user context with emotional and color perception of music for affective music information retrieval", "Authors": ["Pesek M.", "Strle G.", "Kavcic A.", "Marolt M."], "Keywords": ["affective computing", "music datasets", "music emotion recognition", "music information retrieval", "user context"], "Date": "2017", "Abstract": "This paper presents a new multimodal dataset Moodo that can aid the development of affective music information retrieval systems. Moodo\u2019s main novelties are a multimodal approach that links emotional and color perception to music and the inclusion of user context. Analysis of the dataset reveals notable differences in emotion-color associations and their valence-arousal ratings in non-music and music context. We also show differences in ratings of perceived and induced emotions, especially for those with perceived negative connotation, as well as the influence of genre and user context on perception of emotions. By applying an intermediate data fusion model, we demonstrate the importance of user profiles for predictive modeling in affective music information retrieval scenarios.", "Language": "en", "Citations": "2"},
{"Title": "Prediction of music pairwise preferences from facial expressions", "Authors": ["Tkalcic M.", "Elahi M.", "Maleki N.", "Ricci F.", "Pesek M.", "Marolt M."], "Keywords": ["Emotions", "Facial expressions", "Implicit preference elicitation", "Pairwise scores"], "Date": "2019", "Abstract": "Users of a recommender system may be requested to express their preferences about items either with evaluations of items (e.g. a rating) or with comparisons of item pairs. In this work we focus on the acquisition of pairwise preferences in the music domain. Asking the user to explicitly compare music, i.e., which, among two listened tracks, is preferred, requires some user effort. We have therefore developed a novel approach for automatically extracting these preferences from the analysis of the facial expressions of the users while listening to the compared tracks. We have trained a predictor that infers user's pairwise preferences by using features extracted from these data. We show that the predictor performs better than a commonly used baseline, which leverages the user's listening duration of the tracks to infer pairwise preferences. Furthermore, we show that there are differences in the accuracy of the proposed method between users with different personalities and we have therefore adapted the trained model accordingly. Our work shows that by introducing a low user effort preference elicitation approach, which, however, requires to access information that may raise potential privacy issues (face expression), one can obtain good prediction accuracy of pairwise music preferences.", "Language": "en", "Citations": "1"},
{"Title": "Unsupervised learning of a hierarchy of topological maps using omnidirectional images", "Authors": ["Stimec A.", "Jogan M.", "Leonardis A."], "Keywords": ["Appearance-based recognition", "Clustering", "Hierarchical methods", "Mobile robot", "Omnidirectional vision", "Robot localization", "Topological mapping", "Unsupervised learning", "Visual learning"], "Date": "2008", "Abstract": "This paper presents a novel appearance-based method for path-based map learning by a mobile robot equipped with an omnidirectional camera. In particular, we focus on an unsupervised construction of topological maps, which provide an abstraction of the environment in terms of visual aspects. An unsupervised clustering algorithm is used to represent the images in multiple subspaces, forming thus a sensory grounded representation of the environment's appearance. By introducing transitional fields between clusters we are able to obtain a partitioning of the image set into distinctive visual aspects. By abstracting the low-level sensory data we are able to efficiently reconstruct the overall topological layout of the covered path. After the high level topology is estimated, we repeat the procedure on the level of visual aspects to obtain local topological maps. We demonstrate how the resulting representation can be used for modeling indoor and outdoor environments, how it successfully detects previously visited locations and how it can be used for the estimation of the current visual aspect and the retrieval of the relative position within the current visual aspect. \u00a9 2008 World Scientific Publishing Company.", "Language": "en", "Citations": "12"},
{"Title": "Distractor-supported single target tracking in extremely cluttered scenes", "Authors": ["Xiao J.", "Qiao L.", "Stolkin R.", "Leonardis A."], "Keywords": [], "Date": "2016", "Abstract": "This paper presents a novel method for single target tracking in RGB images under conditions of extreme clutter and camouflage, including frequent occlusions by objects with similar appearance as the target. In contrast to conventional single target trackers, which onlymaintain the estimated target status, we propose a multi-level clustering-based robust estimation for online detection and learning of multiple targetlike regions, called distractors, when they appear near to the true target. To distinguish the target from these distractors, we exploit a global dynamic constraint (derived from the target and the distractors) in a feedback loop to improve single target tracking performance in situations where the target is camouflaged in highly cluttered scenes. Our proposed method successfully prevents the estimated target location from erroneously jumping to a distractor during occlusion or extreme camouflage interactions. To gain an insightful understanding of the evaluated trackers, we have augmented publicly available benchmark videos, by proposing a new set of clutter and camouflage sub-attributes, and annotating these sub-attributes for all frames in all sequences. Using this dataset, we first evaluate the effect of each key component of the tracker on the overall performance. Then, the proposed tracker is compared to other highly ranked single target tracking algorithms in the literature. The experimental results show that applying the proposed global dynamic constraint in a feedback loop can improve single target tracker performance, and demonstrate that the overall algorithm significantly outperforms other state-ofthe- art single target trackers in highly cluttered scenes.", "Language": "en", "Citations": "11"},
{"Title": "Search-based estimation of problem difficulty for humans", "Authors": ["Guid M.", "Bratko I."], "Keywords": ["Heuristic search", "Human problem solving", "Problem difficulty"], "Date": "2013", "Abstract": "The research question addressed in this paper is: Given a problem, can we automatically predict how difficult the problem will be to solve by humans? We focus our investigation on problems in which the difficulty arises from the combinatorial complexity of problems. We propose a measure of difficulty that is based on modeling the problem solving effort as search among alternatives and the relations among alternative solutions. In experiments in the chess domain, using data obtained from very strong human players, this measure was shown at a high level of statistical significance to be adequate as a genuine measure of difficulty for humans. \u00a9 2013 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "2"},
{"Title": "On using planning poker for estimating user stories", "Authors": ["Mahnic V.", "Hovelja T."], "Keywords": ["Agile software development", "Effort estimation", "Expert judgment", "Planning poker", "Scrum", "User story"], "Date": "2012", "Abstract": "While most studies in psychology and forecasting stress the possible hazards of group processes when predicting effort and schedule, agile software development methods recommend the use of a group estimation technique called planning poker for estimating the size of user stories and developing release and iteration plans. It is assumed that the group discussion through planning poker helps in identifying activities that individual estimators could overlook, thus providing more accurate estimates and reducing the over-optimism that is typical for expert judgment-based methods. In spite of the widespread use of agile methods, there is little empirical evidence regarding the accuracy of planning poker estimates. In order to fill this gap a study was conducted requiring 13 student teams to develop a Web-based student records information system. All teams were given the same set of user stories which had to be implemented in three Sprints. Each team estimated the stories using planning poker and the estimates provided by each team member during the first round were averaged to obtain the statistical combination for further comparison. In the same way the stories were estimated by a group of experts. The study revealed that students' estimates were over-optimistic and that planning poker additionally increased the over-optimism. On the other hand, the experts' estimates obtained through planning poker were much closer to actual effort spent and tended to be more accurate than the statistical combination of their individual estimates. The results indicate that the optimism bias caused by group discussion diminishes or even disappears as the expertise of the people involved in the group estimation process increases. \u00a9 2012 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "67"},
{"Title": "Framework for web application domain knowledge extraction", "Authors": ["Rozanc I."], "Keywords": [], "Date": "2013", "Abstract": "A decade ago a web application e-\u0160tudent was built with aim to provide electronic support for student enrolment and examination/alumni records management at the University of Ljubljana. Due to issues emerging from the Bologna reform a new e-\u0160tudent is to be build using a modern technology in the near future. The old e-\u0160tudent encapsulates a huge amount of domain knowledge. Unfortunately, it was developed using agile approach resulting in poor technical documentation, thus an alternative approach for the domain knowledge extraction has to be defined. In the paper a framework for an effective web application domain knowledge extraction is defined. It has five elements. The main principles (1) of extraction are defined to perform effective reengineering of different application views at a defined abstract level. A proper knowledge representation using diverse models (2) has to be determined next, and the Model Driven Architecture using UML models is considered a suitable choice. The procedure (3) for extraction has to be defined using appropriate (usually custom made) tools (4) and performed by skilled staff (5), possibly members of the old development team. The use of framework is demonstrated on the web application e-\u0160tudent outlining several custom made tools, the results and the most valuable lessons learnt. \u00a9 2013 MIPRO.", "Language": "en", "Citations": "2"},
{"Title": "Utilizing betting odds for rating basketball teams across different competitions", "Authors": ["Strumbelj E."], "Keywords": ["ELO Rating", "Pairwise Comparisons", "Probability Forecasting", "Sport"], "Date": "2014", "Abstract": "In this paper we investigate rating sports teams within the same competition and across different competitions. In particular, we investigate the problem of only small number of games played between teams from different competitions, which results in slow convergence of ratings. We deal with this issue by fitting a latent strength model on probabilities from betting odds instead of fitting on observed outcomes. We base our approach on the ELO rating, which we extend to account for home court advantage. Instead of the usual incremental updating, we implement batch fitting, which facilitates propagation of information backwards in time. Evaluation on European club basketball confirms that the proposed approach is a substantial improvement both in terms of higher predictive accuracy and fewer games needed to converge. We include the first systematic comparison of European basketball club competitions. The proposed approach easily generalizes to other sports.", "Language": "en", "Citations": "0"},
{"Title": "Towards an integrated robot with multiple cognitive functions", "Authors": ["Hawes N.", "Sloman A.", "Wyatt J.", "Zillich M.", "Jacobsson H.", "Kruijff G.-J.M.", "Brenner M.", "Berginc G.", "Skocaj D."], "Keywords": [], "Date": "2007", "Abstract": "We present integration mechanisms for combining heterogeneous components in a situated information processing system, illustrated by a cognitive robot able to collaborate with a human and display some understanding of its surroundings. These mechanisms include an architectural schema that encourages parallel and incremental information processing, and a method for binding information from distinct representations that when faced with rapid change in the world can maintain a coherent, though distributed, view of it. Provisional results are demonstrated in a robot combining vision, manipulation, language, planning and reasoning capabilities interacting with a human and manipulable objects. Copyright \u00a9 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.", "Language": "en", "Citations": "34"},
{"Title": "Notes on fuzzy cellular automata", "Authors": ["Mraz M.", "Lapanja I.", "Zimic N.", "Virant J."], "Keywords": ["Cellular Automata", "Fuzzy Automaton", "Fuzzy Cellular Automata", "Fuzzy Logic"], "Date": "2000", "Abstract": "In our paper we present some characteristics of cellular automata fuzzification. More precisely, we would like to incorporate fuzzy logic theory to cellular automata. We also present the use of such a generalized approach by applying fuzzy cellular automata in a simple application examples. \u00a9 2000 Taylor & Francis Group, LLC.", "Language": "en", "Citations": "6"},
{"Title": "Unit-time predecessor queries on massive data sets", "Authors": ["Brodnik A.", "Iacono J."], "Keywords": [], "Date": "2010", "Abstract": "New data structures are presented for very fast predecessor queries on integer data sets stored on multiple disks. A structure is presented that supports predecessor queries in one disk seek performed in parallel over multiple disks, no matter how large the data set. For truly massive data sets, the space requirement of this structure approaches twice the space needed to simply store the data on disk. A second structure is presented that supports predecessor queries in the time it takes to perform two disk seeks, but has more moderate space requirements. Its space usage approaches the space needed to store the data on disk, and has manageable space requirements for smaller massive data sets. \u00a9 2010 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "A Bayes-spectral-entropy-based measure of camera focus using a discrete cosine transform", "Authors": ["Kristan M.", "Pers J.", "Perse M.", "Kovacic S."], "Keywords": ["Bayes entropy", "Camera focusing", "DCT", "Spectral methods"], "Date": "2006", "Abstract": "In this paper we present a novel measure of camera focus based on the Bayes spectral entropy of an image spectrum. In order to estimate the degree of focus, the image is divided into non-overlapping sub-images of 8 \u00d7 8 pixels. Next, sharpness values are calculated separately for each sub-image and their mean is taken as a measure of the overall focus. The sub-image spectra are obtained by an 8 \u00d7 8 discrete cosine transform (DCT). Comparisons were made against four well-known measures that were chosen as reference, on images captured with a standard visible-light camera and a thermal camera. The proposed measure outperformed the reference measures by exhibiting a wider working range and a smaller failure rate. To assess its robustness to noise, additional tests were conducted with noisy images. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "86"},
{"Title": "Spoken language resources at LUKS of the University of Ljubljana", "Authors": ["Mihelic F.", "Gros J.", "Dobrisek S.", "Zibert J.", "Pavesic N."], "Keywords": ["Read speech", "Slovene language", "Speech corpora", "Spontaneous speech"], "Date": "2003", "Abstract": "An overview is given of the Slovene-language spoken resources acquired at the Laboratory of Artificial Perception, Systems and Cybernetics (LUKS) at the Faculty of Electrical Engineering, University of Ljubljana over the past ten years. All the resources are accompanied by relevant text transcriptions, lexicons and various segmentation labels.", "Language": "en", "Citations": "28"},
{"Title": "Use of MATLAB neural networks toolbox in a character recognition problem", "Authors": ["Trebar M."], "Keywords": ["Character recognition", "Data encoding", "Input-output mapping", "Neural networks"], "Date": "2005", "Abstract": "We present the use of the MATLAB Neural Network Toolbox (NN Toolbox) in simulations of neural networks. We suggest ways for undergraduate students to solve a character recognition problem with feed-forward neural networks. The software provides the user with a very simple way to define several neural network architectures with different parameters. The solution of the character recognition problem is described from the beginning: collecting the data, data encoding, defining the input-output mapping architecture to the training, and testing the neural networks with the NN Toolbox. \u00a9 2005 Wiley Periodicals, Inc.", "Language": "en", "Citations": "6"},
{"Title": "Multivariate online kernel density estimation with Gaussian kernels", "Authors": ["Kristan M.", "Leonardis A.", "Skocaj D."], "Keywords": ["Gaussian mixture models", "Kernel density estimation", "Online models", "Probability density estimation"], "Date": "2011", "Abstract": "We propose a novel approach to online estimation of probability density functions, which is based on kernel density estimation (KDE). The method maintains and updates a non-parametric model of the observed data, from which the KDE can be calculated. We propose an online bandwidth estimation approach and a compression/revitalization scheme which maintains the KDE's complexity low. We compare the proposed online KDE to the state-of-the-art approaches on examples of estimating stationary and non-stationary distributions, and on examples of classification. The results show that the online KDE outperforms or achieves a comparable performance to the state-of-the-art and produces models with a significantly lower complexity while allowing online adaptation. \u00a9 2011 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "103"},
{"Title": "Hardware implementation of a modified delay-coordinate mapping-based QRS complex detection algorithm", "Authors": ["Cvikl M.", "Jager F.", "Zemva A."], "Keywords": [], "Date": "2007", "Abstract": "We present a modified delay-coordinate mapping-based QRS complex detection algorithm, suitable for hardware implementation. In the original algorithm, the phase-space portrait of an electrocardiogram signal is reconstructed in a two-dimensional plane using the method of delays. Geometrical properties of the obtained phase-space portrait are exploited for QRS complex detection. In our solution, a bandpass filter is used for ECG signal prefiltering and an improved method for detection threshold-level calculation is utilized. We developed the algorithm on the MIT-BIH Arrhythmia Database (sensitivity of 99.82% and positive predictivity of 99.82%) and tested it on the long-term ST database (sensitivity of 99.72% and positive predictivity of 99.37%). Our algorithm outperforms several well-known QRS complex detection algorithms, including the original algorithm.", "Language": "en", "Citations": "17"},
{"Title": "Hierarchical appearance models in visual tracking Hierarhicni modeli videza v vizualnem sledenju", "Authors": ["Zajc L.C.", "Leonardis A.", "Kristan M."], "Keywords": ["Appearance model", "Computer vision", "Hierarchy", "Visual tracking"], "Date": "2016", "Abstract": "The paper addresses the problem of short-term visual tracking in the scope of which we present a concept of hierarchical models to describe the appearance of an object. The key property of these models is that they structure the appearance description into multiple layers. The lowers layers contain the most specific information that can change quickly, while the higher layers contain the appearance information in a more general and lasting form. The structure is also reflected in the update process where the higher layers are guiding the update process of the lower layers, while the lower layers provide a reliable information for updating the higher layers. The benefits of this hierarchical organization are presented with a summary of two shuch models in two visual trackers that are primarily designed for tracking articulated and non-rigid objects, which present a difficulty for many tracking approaches. The first implementation is composed of two layers, while the second one adds another layer to address several shortcomings of the first implementation. The presented experimental analysis on several established benchmarks shows that the described trackers are comparable to the state-of-the-art and excel in tracking non-rigid objects.", "Language": "en", "Citations": "0"},
{"Title": "Adaptation and Evaluation of the Simplex Algorithm for a Data-Flow Architecture", "Authors": ["Cibej U.", "Mihelic J."], "Keywords": ["Algorithm engineering", "Data-flow", "Linear programming", "Simplex"], "Date": "2017", "Abstract": "The main goal of this chapter is to present a novel adaptation of the classical simplex algorithm for a data-flow architecture. Due to the recent reemergence of the data-flow paradigm, most of the algorithms need to be reengineered in order to use the features of the new platform. By exploring various possibilities of implementations and by extensive empirical testing we manage to show the suitability of the data-flow paradigm for the simplex algorithm, as well as pinpoint the strengths and some of the weaknesses of the used architecture.", "Language": "en", "Citations": "3"},
{"Title": "Inference of the molecular mechanism of action from genetic interaction and gene expression data", "Authors": ["Mattiazzi M.", "Curk T.", "Krizaj I.", "Zupan B.", "Petrovic U."], "Keywords": [], "Date": "2010", "Abstract": "Inference of new and useful hypotheses from heterogeneous sources of genome-scale experimental data requires new computational methods that can integrate different types of data. Gene expression and genetic interaction data are two most informative data types, each allowing the identification of genes at different levels of cellular regulatory network hierarchy. We present an integrative data analysis approach, which, rather than correlating the findings from the two data sets, uses each type of data independently to identify the components of molecular pathways and combines them into a single directed network. Our computational genomics approach is based on a set of inference rules traditionally used for reasoning on genetic experiments, which we have formalized and implemented in a software tool. The approach uses chemogenetic interaction and expression data to infer the type of relation between the chemical substance (perturber) and a transcription factor by using previous knowledge on the set of genes whose expression the transcription factor in question regulates. We have used the proposed approach to successfully infer the models for the action of the drug rapamycin and of a DNA damaging agent on their molecular targets and pathways in yeast cells. The developed method is available as a web-based tool at http://www.ailab.si/perturbagen. \u00a9 2010, Mary Ann Liebert, Inc.", "Language": "en", "Citations": "2"},
{"Title": "Wireless sensors grouping proofs for medical care and ambient assisted-living deployment", "Authors": ["Trcek D."], "Keywords": ["Ambient assisted living", "Health care", "Internet of things", "Lightweight protocols", "PPDR networks", "RFID", "Security", "Wireless networks", "Yoking proofs"], "Date": "2016", "Abstract": "Internet of Things (IoT) devices are rapidly penetrating e-health and assisted living domains, and an increasing proportion among them goes on the account of computationally-weak devices, where security and privacy provisioning alone are demanding tasks, not to mention grouping proofs. This paper, therefore, gives an extensive analysis of such proofs and states lessons learnt to avoid possible pitfalls in future designs. It sticks with prudent engineering techniques in this field and deploys in a novel way the so called non-deterministic principle to provide not only grouping proofs, but (among other) also privacy. The developed solution is analyzed by means of a tangible metric and it is shown to be lightweight, and formally for security", "Language": "en", "Citations": "4"},
{"Title": "LiverSex computational model: Sexual aspects in hepatic metabolism and abnormalities", "Authors": ["Tomas T.C.", "Urlep Z.", "Moskon M.", "Mraz M.", "Rozman D."], "Keywords": ["Hepatic metabolism", "Large-scale metabolic model", "Liver", "NAFLD", "Sexual dimorphism", "Systems medicine"], "Date": "2018", "Abstract": "The liver is to date the best example of a sexually dimorphic non-reproductive organ. Over 1,000 genes are differentially expressed between sexes indicating that female and male livers are two metabolically distinct organs. The spectrum of liver diseases is broad and is usually prevalent in one or the other sex, with different contributing genetic and environmental factors. It is thus difficult to predict individual's disease outcomes and treatment options. Systems approaches including mathematical modeling can aid importantly in understanding the multifactorial liver disease etiology leading toward tailored diagnostics, prognostics and therapy. The currently established computational models of hepatic metabolism that have proven to be essential for understanding of non-alcoholic fatty liver disease (NAFLD) and hepatocellular carcinoma (HCC) are limited to the description of gender-independent response or reflect solely the response of the males. Herein we present LiverSex, the first sex-based multi-tissue and multi-level liver metabolic computational model. The model was constructed based on in silico liver model SteatoNet and the object-oriented modeling. The crucial factor in adaptation of liver metabolism to the sex is the inclusion of estrogen and androgen receptor responses to respective hormones and the link to sex-differences in growth hormone release. The model was extensively validated on literature data and experimental data obtained from wild type C57BL/6 mice fed with regular chow and western diet. These experimental results show extensive sex-dependent changes and could not be reproduced in silico with the uniform model SteatoNet. LiverSex represents the first large-scale liver metabolic model, which allows a detailed insight into the sex-dependent complex liver pathologies, and how the genetic and environmental factors interact with the sex in disease appearance and progression. We used the model to identify the most important sex-dependent metabolic pathways, which are involved in accumulation of triglycerides representing initial steps of NAFLD. We identified PGC1A, PPARa, FXR, and LXR as regulatory factors that could become important in sex-dependent personalized treatment of NAFLD.", "Language": "en", "Citations": "2"},
{"Title": "WhiteRate: A context-aware approachto wireless rate adaptation", "Authors": ["Pejovic V.", "Belding E.M."], "Keywords": ["context awareness", "energy efficiency", "network protocols", "software-defined radio", "Wireless communication"], "Date": "2014", "Abstract": "The increased demand for wireless connectivity emphasizes the necessity of efficient wireless communication as resources such as the available spectrum and energy reserves become limiting factors for network proliferation. Recent advancements in software-defined radio enable high flexibility of the physical layer allowing fine grained transmission adjustments. Although communication efficiency can greatly benefit from physical layer flexibility, modern wireless protocols can neither handle these new opportunities nor allocate resources according to the overlying application needs. In this work we develop WhiteRate, a method for physical layer parameter adaptation that efficiently utilizes available energy and spectrum resources, while maintaining the desired quality of communication. Our solution adjusts the modulation and coding scheme, and channel width to achieve a communication profile that matches application requirements. We implement WhiteRate in GNUradio and evaluate it in both indoor and outdoor environments. We demonstrate improvements on two important fronts: spectrum utilization and energy efficiency. Moreover, we show that by using WhiteRate, both benefits can be achieved simultaneously. \u00a9 2002-2012 IEEE.", "Language": "en", "Citations": "6"},
{"Title": "Fuzzy logic used for modelling and simulation of massive processes: A useful tool for nanotechnologies?", "Authors": ["Mraz M.", "Bajec I.L.", "Zimic N."], "Keywords": ["Fuzzy animats", "Fuzzy cellular automata", "Fuzzy logic", "Nanocomputing", "Nanotechnology"], "Date": "2004", "Abstract": "In our paper we are presenting two concepts of modelling massive processes by using fuzzy cellular automata and fuzzy animate. The first structure enables us to establish the relations between the entities that are regularly distributed in space and the second one enables us to establish the relations between those that are unregularly distributed. Both concepts of modelling could be applied to the fields of nanotechnologies due to the safety reasons, due to their low cost and less time consuming characteristics.", "Language": "en", "Citations": "2"},
{"Title": "An optimal message routing algorithm for circulant networks", "Authors": ["Dobravec T.", "Zerovnik J.", "Robic B."], "Keywords": ["Algorithm design", "Circulant networks", "Restricted shortest paths", "Two-terminal routing"], "Date": "2006", "Abstract": "A k-circulant network G(n; h", "Language": "en", "Citations": "12"},
{"Title": "Kanban in software engineering education: An outline of the literature", "Authors": ["Mahnic V."], "Keywords": [], "Date": "2019", "Abstract": "Kanban, an agile software development methodology, is growing increasingly popular in the software industry. In order to meet industry needs, universities must find a way to integrate Kanban into their software engineering courses. In this article is provided an outline of existing literature on this topic. The search of studies in the Scopus database revealed 13 papers that were identified as primary studies relevant to this research. These studies are classified by their main topic into four groups, viz. 1) teaching Kanban through practical work on student projects; 2)teaching Kanban through educational games; 3) reporting students' perceptions of Kanban and/or Scrumban(a related methodology) and 4) position papers. A short description of each study and its main results are provided. It was found that the literature on the use of Kanban in software engineering education is sparse and of limited quality. Only a few studies contain discussions on teaching the most important Kanban concepts (i.e. the structure of the Kanban board, setting of work-in-progress limits, maximising workflow and measuring lead time) in sufficient detail.", "Language": "en", "Citations": "0"},
{"Title": "Graph minors and minimum degree", "Authors": ["Fijavz G.", "Wood D.R."], "Keywords": [], "Date": "2010", "Abstract": "Let D", "Language": "fr", "Citations": "2"},
{"Title": "Extracting qualitative relations from categorical data", "Authors": ["Zabkar J.", "Bratko I.", "Demsar J."], "Keywords": ["Ceteris paribus", "Machine learning", "Qualitative modeling"], "Date": "2016", "Abstract": "Qualitative modeling is traditionally concerned with the abstraction of numerical data. In numerical domains, partial derivatives describe the relation between the independent and dependent variable; qualitatively, they tell us the trend of the dependent variable. In this paper, we address the problem of extracting qualitative relations in categorical domains. We generalize the notion of partial derivative by defining the probabilistic discrete qualitative partial derivative (PDQ PD). PDQ PD is a qualitative relation between the target class c and the discrete attribute; the derivative corresponds to ordering the attribute's values, a", "Language": "en", "Citations": "3"},
{"Title": "Automatic adaptation of filter sequences for cell counting", "Authors": ["Cibej U.", "Lojk J.", "Pavlin M.", "Sajn L."], "Keywords": [], "Date": "2015", "Abstract": "Manual cell counting in microscopic images is usually tedious, time consuming and prone to human error. Several programs for automatic cell counting have been developed so far, but most of them demand some specific knowledge of image analysis and/or manual fine tuning of various parameters. Even if a set of filters is found and fine tuned to the specific application, small changes to the image attributes might make the automatic counter very unreliable. The goal of this article is to present a new application that overcomes this problem by learning the set of parameters for each application, thus making it more robust to changes in the input images. The users must provide only a small representative subset of images and their manual count, and the program offers a set of automatic counters learned from the given input. The user can check the counters and choose the most suitable one. The resulting application (which we call Learn123) is specifically tailored to the practitioners, i.e. even though the typical workflow is more complex, the application is easy to use for non-technical experts.", "Language": "en", "Citations": "1"},
{"Title": "Online reliability estimates for individual predictions in data streams", "Authors": ["Rodrigues P.P.", "Gama J.", "Bosnic Z."], "Keywords": [], "Date": "2008", "Abstract": "Several predictive systems are nowadays vital for operations and decision support. The quality of these systems is most of the time defined by their average accuracy which has low or no information at all about the estimated error of each individual prediction. In many sensitive applications, users should be allowed to associate a measure of reliability to each prediction. In the case of batch systems, reliability measures have already been defined, mostly empirical measures as the estimation using the local sensitivity analysis. However, with the advent of data streams, these reliability estimates should also be computed online, based only on available data and current model's state. In this paper we define empirical measures to perform online estimation of reliability of individual predictions when made in the context of online learning systems. We present preliminary results and evaluate the estimators in two different problems. \u00a9 2008 IEEE.", "Language": "en", "Citations": "9"},
{"Title": "Organizational Learning Supported by Machine Learning Models Coupled with General Explanation Methods: A Case of B2B Sales Forecasting", "Authors": ["Bohanec M.", "Robnik-Sikonja M.", "Kljajic Borstnar M."], "Keywords": ["B2B sales forecasting", "decision support", "explanations", "machine learning", "organizational learning"], "Date": "2017", "Abstract": "Background and Purpose: The process of business to business (B2B) sales forecasting is a complex decision-making process. There are many approaches to support this process, but mainly it is still based on the subjective judgment of a decision-maker. The problem of B2B sales forecasting can be modeled as a classification problem. However, top performing machine learning (ML) models are black boxes and do not support transparent reasoning. The purpose of this research is to develop an organizational model using ML model coupled with general explanation methods. The goal is to support the decision-maker in the process of B2B sales forecasting. Design/Methodology/Approach: Participatory approach of action design research was used to promote acceptance of the model among users. ML model was built following CRISP-DM methodology and utilizes R software environment. Results: ML model was developed in several design cycles involving users. It was evaluated in the company for several months. Results suggest that based on the explanations of the ML model predictions the users' forecasts improved. Furthermore, when the users embrace the proposed ML model and its explanations, they change their initial beliefs, make more accurate B2B sales predictions and detect other features of the process, not included in the ML model. Conclusions: The proposed model promotes understanding, foster debate and validation of existing beliefs, and thus contributes to single and double-loop learning. Active participation of the users in the process of development, validation, and implementation has shown to be beneficial in creating trust and promotes acceptance in practice.", "Language": "en", "Citations": "2"},
{"Title": "Yeast Saccharomyces cerevisiae adiponectin receptor homolog Izh2 is involved in the regulation of zinc, phospholipid and pH homeostasis", "Authors": ["Mattiazzi Usaj M.", "Prelec M.", "Brloznik M.", "Primo C.", "Curk T.", "Scancar J.", "Yenush L.", "Petrovic U."], "Keywords": [], "Date": "2015", "Abstract": "The functional link between zinc homeostasis and membrane-related processes, including lipid metabolism regulation, extends from yeast to humans, and has a likely role in the pathogenesis of diabetes. The yeast Izh2 protein has been previously implicated in zinc ion homeostasis and in the regulation of lipid and phosphate metabolism, but its precise molecular function is not known. We performed a chemogenomics experiment to determine the genes conferring resistance or sensitivity to different environmental zinc concentrations. We then determined at normal, depleted and excess zinc concentrations, the genetic interactions of IZH2 at the genome-wide level and measured changes in the transcriptome caused by deletion of IZH2. We found evidence for an important cellular function of the Rim101 pathway in zinc homeostasis in neutral or acidic environments, and observed that phosphatidylinositol is a source of inositol when zinc availability is limited. Comparison of our experimental profiles with published gene expression and genetic interaction profiles revealed pleiotropic functions for Izh2. We propose that Izh2 acts as an integrator of intra- and extracellular signals in providing adequate cellular responses to maintain homeostasis under different external conditions, including-but not limited to-alterations in zinc concentrations.", "Language": "en", "Citations": "1"},
{"Title": "Controlling mozart's dice music using acceleration sensors", "Authors": ["Bohak C.", "Niitsuma M.", "Marolt M."], "Keywords": [], "Date": "2012", "Abstract": "This paper presents how the modem game controller Wi-imote can be used to control Mozart's dice music. The proposed method allows users to explore and expand their experience with endless music within space where note measures are arranged in a 2-layer structure that can be treated as a playlist. Experimental results show that the proposed organization of measures gives higher melodic similarity between consequent parts according to melodic distance measures such as distribution of pitch classes, distribution of intervals and distribution of note durations. The result is a system that allows user to create different music than in Mozart's original Dice game and allows exploration of endless music. We also compare the use of different metrics for organizing individual layers in order to achieve melodically more meaningful music.", "Language": "en", "Citations": "0"},
{"Title": "A system for interactive learning in dialogue with a tutor", "Authors": ["Skocaj D.", "Kristan M.", "Vrecko A.", "Mahnic M.", "Janicek M.", "Kruijff G.-J.M.", "Hanheide M.", "Hawes N.", "Keller T.", "Zillich M.", "Zhou K."], "Keywords": [], "Date": "2011", "Abstract": "In this paper we present representations and mechanisms that facilitate continuous learning of visual concepts in dialogue with a tutor and show the implemented robot system. We present how beliefs about the world are created by processing visual and linguistic information and show how they are used for planning system behaviour with the aim at satisfying its internal drive - to extend its knowledge. The system facilitates different kinds of learning initiated by the human tutor or by the system itself. We demonstrate these principles in the case of learning about object colours and basic shapes. \u00a9 2011 IEEE.", "Language": "en", "Citations": "18"},
{"Title": "Long-term ST database: A reference for the development and evaluation of automated ischaemia detectors and for the study of the dynamics of myocardial ischaemia", "Authors": ["Jager F.", "Taddei A.", "Moody G.B.", "Emdin M.", "Antolic G.", "Dorn R.", "Smrdel A.", "Marchesi C.", "Mark R.G."], "Keywords": ["Annotated ECG database performance evaluation of instrumentation", "Mechanics of transient myocardial ischaemia", "Myocardial ischaemia", "Non-ischaemic ST segment changes", "ST-segment change analysis"], "Date": "2003", "Abstract": "The long-term ST database is the result of a multinational research effort. The goal was to develop a challenging and realistic research resource for development and evaluation of automated systems to detect transient ST segment changes in electrocardiograms and for supporting basic research into the mechanisms and dynamics of transient myocardial ischaemia. Twenty-four hour ambulatory ECG records were selected from routine clinical practice settings in the USA and Europe, between 1994 and 2000, on the basis of occurrence of ischaemic and non-ischaemic ST segment changes. Human expert annotators used newly developed annotation protocols and a specially developed interactive graphic editor tool (SEMIA) that supported paperless editing of annotations and facilitated international co-operation via the Internet. The database contains 86 two- and three-channel 24 h annotated ambulatory records from 80 patients and is stored on DVD-ROMs. The database annotation files contain ST segment annotations of transient ischaemic (1155) and heart-rate related ST episodes and annotations of non-ischaemic ST segment events related to postural changes and conduction abnormalities. The database is intended to complement the European Society of Cardiology ST-T database and the MIT-BIH and AHA arrhythmia databases. It provides a comprehensive representation of 'real-world' data, with numerous examples of transient ischaemic and non-ischaemic ST segment changes, arrhythmias, conduction abnormalities, axis shifts, noise and artifacts.", "Language": "en", "Citations": "104"},
{"Title": "Student projects as a means of cooperation between academia and industry: Some experiences in the area of software engineering education", "Authors": ["Mahnic V."], "Keywords": [], "Date": "2017", "Abstract": "Cooperation between academia and industry provides an opportunity for software engineering students to experience real projects as a part of their programmes of study. In this way, they improve both professional and transferable skills. The article describes two kinds of cooperation with industry that have recently been used at the University of Ljubljana's Faculty of Computer and Information Science. The first is based on collaboration with big software vendors requiring students to use their development tools in order to develop a project of their choice and evaluate its business implication, audience, benefits and critical success factors. The second requires students to solve a real-life problem on the basis of a specification provided by a partner from industry. The focus is on providing a working solution by strictly following the prescribed development process and meeting user requirements. Advantages and disadvantages of both approaches are analysed and compared to each other. Regardless of their differences, both approaches have proved to be successful and were accepted favourably by all parties involved.", "Language": "en", "Citations": "2"},
{"Title": "Electronic environments for integrated care management: Case of depression treatment", "Authors": ["Meglic M.", "Brodnik A."], "Keywords": [], "Date": "2010", "Abstract": "This chapter provides a basic overview of care process management and active patient engagement principles. It builds upon these principles to describe in more detail the way information and communication technology can provide support for them. It later discusses their impact on quality and cost-efficiency of care. The authors specify care models suitable for ICT support, specific process support characteristics related to health care, standards and communication devices that are being used. The chapter also provides a description of development and implementation of such an environment to support treatment of patients with depression. \u00a9 2010, IGI Global.", "Language": "en", "Citations": "4"},
{"Title": "Nomograms for visualization of naive Bayesian classifier", "Authors": ["Mozina M.", "Demsar J.", "Kattan M.", "Zupan B."], "Keywords": [], "Date": "2004", "Abstract": "Besides good predictive performance, the naive Bayesian classifier can also offer a valuable insight into the structure of the training data and effects of the attributes on the class probabilities. This structure may be effectively revealed through visualization of the classifier. We propose a new way to visualize the naive Bayesian model in the form of a nomogram. The advantages of the proposed method are simplicity of presentation, clear display of the effects of individual attribute values, and visualization of confidence intervals. Nomograms are intuitive and when used for decision support can provide a visual explanation of predicted probabilities. And finally, with a nomogram, a naive Bayesian model can be printed out and used for probability prediction without the use of computer or calculator. \u00a9 Springer-Verlag Berlin Heidelberg 2004.", "Language": "en", "Citations": "66"},
{"Title": "The Visual Object Tracking VOT2015 Challenge Results", "Authors": ["Kristan M.", "Matas J.", "Leonardis A.", "Felsberg M.", "Cehovin L.", "Fernandez G.", "Vojir T.", "Hager G.", "Nebehay G.", "Pflugfelder R.", "Gupta A.", "Bibi A.", "Lukezic A.", "Garcia-Martin A.", "Saffari A.", "Petrosino A.", "Montero A.S.", "Varfolomieiev A.", "Baskurt A.", "Zhao B.", "Ghanem B.", "Martinez B.", "Lee B.", "Han B.", "Wang C.", "Garcia C.", "Zhang C.", "Schmid C.", "Tao D.", "Kim D.", "Huang D.", "Prokhorov D.", "Du D.", "Yeung D.-Y.", "Ribeiro E.", "Khan F.S.", "Porikli F.", "Bunyak F.", "Zhu G.", "Seetharaman G.", "Kieritz H.", "Yau H.T.", "Li H.", "Qi H.", "Bischof H.", "Possegger H.", "Lee H.", "Nam H.", "Bogun I.", "Jeong J.-C.", "Cho J.-I.", "Lee J.-Y.", "Zhu J.", "Shi J.", "Li J.", "Jia J.", "Feng J.", "Gao J.", "Choi J.Y.", "Kim J.-W.", "Lang J.", "Martinez J.M.", "Choi J.", "Xing J.", "Xue K.", "Palaniappan K.", "Lebeda K.", "Alahari K.", "Gao K.", "Yun K.", "Wong K.H.", "Luo L.", "Ma L.", "Ke L.", "Wen L.", "Bertinetto L.", "Pootschi M.", "Maresca M.", "Danelljan M.", "Wen M.", "Zhang M.", "Arens M.", "Valstar M.", "Tang M.", "Chang M.-C.", "Khan M.H.", "Fan N.", "Wang N.", "Miksik O.", "Torr P.H.S.", "Wang Q.", "Martin-Nieto R.", "Pelapur R.", "Bowden R.", "Laganiere R.", "Moujtahid S.", "Hare S.", "Hadfield S.", "Lyu S.", "Li S.", "Zhu S.-C.", "Becker S.", "Duffner S.", "Hicks S.L.", "Golodetz S.", "Choi S.", "Wu T.", "Mauthner T.", "Pridmore T.", "Hu W.", "Hubner W.", "Wang X.", "Li X.", "Shi X.", "Zhao X.", "Mei X.", "Shizeng Y.", "Hua Y.", "Li Y.", "Lu Y.", "Li Y.", "Chen Z.", "Huang Z.", "Chen Z.", "Zhang Z.", "He Z.", "Hong Z."], "Keywords": ["Area measurement", "Australia", "Benchmark testing", "Object tracking", "Performance evaluation", "Target tracking", "Visualization"], "Date": "2015", "Abstract": "The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 predecessor are: (i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website.", "Language": "en", "Citations": "283"},
{"Title": "Automatic extraction of AST patterns for debugging student programs", "Authors": ["Lazar T.", "Mozina M.", "Bratko I."], "Keywords": ["Abstract syntax tree", "Error diagnosis", "Hint generation", "Programming tutors", "Syntactic features"], "Date": "2017", "Abstract": "When implementing a programming tutor it is often difficult to manually consider all possible errors encountered by students. An alternative is to automatically learn a bug library of erroneous patterns from students\u2019 programs. We propose abstract-syntax-tree (AST) patterns as features for learning rules to distinguish between correct and incorrect programs. We use these rules to debug student programs: rules for incorrect programs (buggy rules) indicate mistakes, whereas rules for correct programs group programs with the same solution strategy. To generate hints, we first check buggy rules and point out incorrect patterns. If no buggy rule matches, we use rules for correct programs to recognize the student\u2019s intent and suggest missing patterns. We evaluate our approach on past student programming data for a number of Prolog problems. For 31 out of 44 problems, the induced rules correctly classify over 85% of programs based only on their structural features. For approximately 73% of incorrect submissions, we are able to generate hints that were implemented by the student in some subsequent submission.", "Language": "en", "Citations": "6"},
{"Title": "Student modelling based on fuzzy inference mechanisms", "Authors": ["Kavcic A.", "Pedraza-Jimenez R.", "Molina-Bulla H.", "Valverde-Albacete F.J.", "Cid-Sueiro J.", "Navia-Vazquez A."], "Keywords": ["Educational systems", "Fuzzy logic", "Personalized navigation", "User modelling"], "Date": "2003", "Abstract": "The paper presents a competence-based instructional design system and a way to provide a personalization of navigation in the course content. The navigation aid tool builds on the competence graph and the student model, which includes the elements of uncertainty in the assessment of students. An individualized navigation graph is constructed for each student, suggesting the competences the student is more prepared to study. We use fuzzy set theory for dealing with uncertainty. The marks of the assessment tests are transformed into linguistic terms and used for assigning values to linguistic variables. For each competence, the level of difficulty and the level of knowing its prerequisites are calculated based on the assessment marks. Using these linguistic variables and approximate reasoning (fuzzy IF-THEN rules), a crisp category is assigned to each competence regarding its level of recommendation.", "Language": "en", "Citations": "20"},
{"Title": "Ranking and 1-dimensional projection of cell development transcription profiles", "Authors": ["Zagar L.", "Mulas F.", "Bellazzi R.", "Zupan B."], "Keywords": ["cell development", "projection", "ranking", "regression", "staging", "temporal ordering"], "Date": "2011", "Abstract": "Genome-scale transcription profile is known to be a good reporter of the state of the cell. Much of the early predictive modelling and cell-type clustering relied on this relation and has experimentally confirmed it. We have examined if this also holds for prediction of cell's staging, and focused on the inference of stage prediction models for stem cell development. We show that the problem relates to rank learning and, from the user's point of view, to projection of transcription profile data to a single dimension. Our comparison of several state-of-the-art algorithms on 10 data sets from Gene Expression Omnibus shows that rank-learning can be successfully applied to developmental cell staging, and that relatively simple techniques can perform surprisingly well. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "Algebraic integrability of the confluent Neumann system", "Authors": ["Vuk M."], "Keywords": [], "Date": "2008", "Abstract": "In this paper we study the Neumann system, which describes the harmonic oscillator (of arbitrary dimension) constrained to the sphere. In particular we will consider the confluent case where two eigenvalues of the potential coincide, which implies that the system has S", "Language": "en", "Citations": "2"},
{"Title": "A comparative evaluation of e-learning and traditional pedagogical process elements", "Authors": ["Vavpotic D.", "Zvanut B.", "Trobec I."], "Keywords": ["E-learning", "Evaluation", "Model", "Pedagogical process", "Teaching methods", "Tools"], "Date": "2013", "Abstract": "In modern pedagogical processes various teaching methods and approaches (elements of the pedagogical process - EPPs) are used ranging from traditional ones (e.g., lectures, books) to more recent ones (e.g., e-discussion boards, e-quizzes). Different models for evaluation of the appropriateness of EPPs have been proposed in the past. However, the literature shows that these models typically focus only on the appropriateness of a single EPP and do not provide information about its relative appropriateness in relation to other EPPs. Unfortunately, this considerably limits the use of such evaluation models for the needs of the educational institutions' management. In order to decide which EPPs to promote or modify, management requires a comparative overview of the appropriateness of all EPPs that are part of the pedagogical process under consideration. Therefore the goal of our study was to design a model which would facilitate a comparative evaluation of many e-learning and traditional EPPs by simultaneously considering perspectives of students' and teachers' who participate in a certain pedagogical process. We applied the proposed model to three real-life pedagogical processes that are presented in this paper. Three groups of students, their teachers, and the college's management participated in the study. The management confirmed that the evaluation model provided them with valuable information in order to plan actions for improvement of the pedagogical processes. \u00a9 International Forum of Educational Technology & Society (IFETS).", "Language": "en", "Citations": "7"},
{"Title": "Combined application of theoretical modeling and neural networks in vulcametry", "Authors": ["Ster B.", "Lotric U.", "Susteric Z."], "Keywords": ["Computer modeling", "Mechanical properties", "Rubber", "Vulcametry"], "Date": "2009", "Abstract": "Results of vulcametry are related to mechanical properties of vulcanized rubber. Despite standing on firm theoretical grounds, these relations often prove to be inaccurate in practice because they are influenced by various other factors, hardly manageable by the theory. That makes predictions less reliable, thus demanding ample additional laboratory testing to be performed. The same goes for possibilities of theoretical extrapolations or simulations of extensive tests that would facilitate laboratory work. It is shown in this work how a sensible combination of both the theoretical models and neural networks can boost the performance in prediction of mechanical properties. Thus it can effectively be used to rationalize laboratory work in the field of rubber vulcametry and property predictions, without losing essentials, but rather gaining them.", "Language": "en", "Citations": "0"},
{"Title": "Geometric constructions on cycles", "Authors": ["Zlobec B.J.", "Kosta N.M."], "Keywords": [], "Date": "2004", "Abstract": "A point, plane or sphere in R", "Language": "en", "Citations": "0"},
{"Title": "Graphlet counting", "Authors": ["Hocevar T."], "Keywords": ["Bioinformatics", "Counting", "Graphlets", "Network"], "Date": "2018", "Abstract": "Graphlet analysis describes a graph's topology by observing frequencies of 4-or 5-node induced subgraphs (graphlets) in the graph. We developed a graphlet counting approach based on a system of linear equations that relate the graphlet counts. For sparse large networks, such as those appearing in bioinformatics, the resulting algorithm is an order of magnitude faster than the existing approaches.", "Language": "en", "Citations": "1"},
{"Title": "Distributed message routing in unstructured P2P network overlays", "Authors": ["Ciglaric M.", "Pancur M.", "Trampus M.", "Vidmar T."], "Keywords": ["Distributed search", "Message routing", "Peer-to-peer network", "Unstructured overlay"], "Date": "2003", "Abstract": "The paper presents an alternative message routing approach in unstructured peer-to-peer overlay networks. Unstructured peer-to-peer systems are loosely coupled, highly autonomous systems without hierarchy. Usually, flooding-based routing mechanisms are used for sending messages through the application layer overlay. Due to the limited scalability and high network load experiences, the paper suggests an improvement to reduce the traffic and overall performance: a peer remembers recently forwarded answers, so that he may route the next query messages with the same contents only to the relevant neighbor, not to all of them. Preliminary network simulations have shown promising results.", "Language": "en", "Citations": "1"},
{"Title": "Context aware exception handling in business process execution language", "Authors": ["Laznik J.", "Juric M.B."], "Keywords": ["BPEL fault handling", "Business process exception handling", "Semantic Web", "SOA", "Workflow exception handling patterns"], "Date": "2013", "Abstract": "Context Fault handling represents a very important aspect of business process functioning. However, fault handling has thus far been solved statically, requiring the definition of fault handlers and handling logic to be defined at design time, which requires a great deal of effort, is error-prone and relatively difficult to maintain and extend. It is sometimes even impossible to define all fault handlers at design time. Objective To address this issue, we describe a novel context-aware architecture for fault handling in executable business process, which enables dynamic fault handling during business process execution. Method We performed analysis of existing fault handling disadvantages of WS-BPEL. We designed the artifact which complements existing statically defined fault handling in such a way that faults can be defined dynamically during business process run-time. We evaluated the artifact with analysis of system performance and performed a comparison against a set of well known workflow exception handling patterns. Results We designed an artifact, that comprises an Observer component, Exception Handler Bus, Exception Knowledge Base and Solution Repository. A system performance analysis shows a significantly decreased repair time with the use of context aware activities. We proved that the designed artifact extends the range of supported workflow exception handling patterns. Conclusion The artifact presented in this research considerably improves static fault handling, as it enables the dynamic fault resolution of semantically similar faults with continuous enhancement of fault handling in run-time. It also results in broader support of workflow exception handling patterns. \u00a9 2013 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "7"},
{"Title": "Empirical comparison of network sampling: How to choose the most appropriate method?", "Authors": ["Blagus N.", "Subelj L.", "Bajec M."], "Keywords": ["Comparison of sampling techniques", "Complex networks", "Network sampling", "Sampling accuracy", "Sampling selection scheme", "Subgraph induction"], "Date": "2017", "Abstract": "In the past few years, the storage and the analysis of large-scale and fast evolving networks presents a great challenge. Therefore, a number of different techniques have been proposed for sampling large networks. Studies on network sampling primarily analyze the changes of network properties under the sampling. In general, network exploration techniques approximate the original networks more accurate than random node and link selection. Yet, link selection with additional subgraph induction step outperforms most other techniques. In this paper, we apply subgraph induction also to random walk and forest-fire sampling and evaluate the effects of subgraph induction on the sampling accuracy. We analyze different real-world networks and the changes of their properties introduced by sampling. The results reveal that the techniques with subgraph induction improve the performance of techniques without induction and create denser sample networks with larger average degree. Furthermore, the accuracy of sampling decrease consistently across various sampling techniques, when the sampled networks are smaller. Based on the results of the comparison, we introduce the scheme for selecting the most appropriate technique for network sampling. Overall, the breadth-first exploration sampling proves as the best performing technique.", "Language": "en", "Citations": "2"},
{"Title": "Testing the significance of attribute interactions", "Authors": ["Jakulin A.", "Bratko I."], "Keywords": [], "Date": "2004", "Abstract": "Attribute interactions are the irreducible dependencies between attributes. Interactions underlie feature relevance and selection, the structure of joint probability and classification models: if and only if the attributes interact, they should be connected. While the issue of 2-way interactions, especially of those between an attribute and the label, has already been addressed, we introduce an operational definition of a generalized n-way interaction by highlighting two models: the reductionistic part-to-whole approximation, where the model of the whole is reconstructed from models of the parts, and the holistic reference model, where the whole is modelled directly. An interaction is deemed significant if these two models are significantly different. In this paper, we propose the Kirkwood superposition approximation for constructing part-to-whole approximations. To model data, we do not assume a particular structure of interactions, but instead construct the model by testing for the presence of interactions. The resulting map of significant interactions is a graphical model learned from the data. We confirm that the P-values computed with the assumption of the asymptotic x", "Language": "en", "Citations": "71"},
{"Title": "A statistical approach to texture description of medical images: A preliminary study", "Authors": ["Bevk M.", "Kononenko I."], "Keywords": [], "Date": "2002", "Abstract": "The article deals with the problem of texture description. It presents a statistical approach. Specifically it introduces the use of first- and second-order statistics on texture color spaces. At the end we also give estimations of computational time complexities of calculations of parameters presented in this article and describe our experience on one application domain. This study is a preliminary preparation for application of these methods on medical images.", "Language": "en", "Citations": "40"},
{"Title": "Range image acquisition of objects with non-uniform albedo using structured light range sensor", "Authors": ["Skocaj D.", "Leonardis A."], "Keywords": [], "Date": "2000", "Abstract": "We present a novel approach to acquisition of range images of objects with non-uniform albedo using a structured light sensor. The main idea is to systematically vary the intensity of the light projector and to form high dynamic scale radiance maps. The range images are then formed from these radiance maps. We tested the method on the objects which have surfaces with very different reflectance properties. We demonstrate that the range images obtained from the high dynamic scale radiance maps are of much better quality than those obtained directly from the original images of a limited dynamic scale. \u00a9 2000 IEEE.", "Language": "en", "Citations": "26"},
{"Title": "Emotion Recognition on Twitter: Comparative Study and Training a Unison Model", "Authors": ["Colneric N.", "Demsar J."], "Keywords": ["Convolutional neural networks", "Convolutional Neural Networks", "Emotion recognition", "Emotion Recognition", "Machine learning", "Mood", "Recurrent Neural Networks", "Tagging", "Text Mining", "Training", "Twitter", "Twitter"], "Date": "2018", "Abstract": "Despite recent successes of deep learning in many fields of natural language processing, previous studies of emotion recognition on Twitter mainly focused on the use of lexicons and simple classifiers on bag-of-words models. The central question of our study is whether we can improve their performance using deep learning. To this end, we exploit hashtags to create three large emotion-labeled data sets corresponding to different classifications of emotions. We then compare the performance of several word- and character-based recurrent and convolutional neural networks with the performance on bag-of-words and latent semantic indexing models. We also investigate the transferability of the final hidden state representations between different classifications of emotions, and whether it is possible to build a unison model for predicting all of them using a shared representation. We show that recurrent neural networks, especially character-based ones, can improve over bag-of-words and latent semantic indexing models. Although the transfer capabilities of these models are poor, the newly proposed training heuristic produces a unison model with performance comparable to that of the three single models.", "Language": "en", "Citations": "1"},
{"Title": "Prediction of aircraft performances based on data collected by air traffic control centers", "Authors": ["Hrastovec M.", "Solina F."], "Keywords": ["Aircraft performances", "Machine learning", "Prediction", "Trajectory calculation"], "Date": "2016", "Abstract": "Accurate prediction of aircraft position is becoming more and more important for the future of air traffic. Currently, the lack of information about flights prevents us to fulfill future demands for the needed accuracy in 4D trajectory prediction. Until we get the necessary information from aircraft and until new more accurate methods are implemented and used, we propose an alternative method for predicting aircraft performances using machine learning from historical data about past flights collected in a multidimensional database. In that way, we can improve existing applications by providing them better inputs for their trajectory calculations. Our method uses flight plan data to predict performance values, which are suited individually for each flight. The results show that based on recorded past aircraft performances and related flight data we can effectively predict performances for future flights based on how similar flights behaved in the past.", "Language": "en", "Citations": "9"},
{"Title": "Community structure of complex software systems: Analysis and applications", "Authors": ["Subelj L.", "Bajec M."], "Keywords": ["Community structure", "Complex networks", "Software systems"], "Date": "2011", "Abstract": "Due to notable discoveries in the fast evolving field of complex networks, recent research in software engineering has also focused on representing software systems with networks. Previous work has observed that these networks follow scale-free degree distributions and reveal small-world phenomena, while we here explore another property commonly found in different complex networks, i.e. community structure. We adopt class dependency networks, where nodes represent software classes and edges represent dependencies among them, and show that these networks reveal a significant community structure, characterized by similar properties as observed in other complex networks. However, although intuitive and anticipated by different phenomena, identified communities do not exactly correspond to software packages. We empirically confirm our observations on several networks constructed from Java and various third party libraries, and propose different applications of community detection to software engineering. \u00a9 2011 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "55"},
{"Title": "Evaluation of angiogram visualization methods for fast and reliable aneurysm diagnosis", "Authors": ["Lesar Z.", "Bohak C.", "Marolt M."], "Keywords": ["angiogram visualization", "evaluation", "image perception", "ray casting", "Volume rendering"], "Date": "2015", "Abstract": "In this paper we present the results of an evaluation of different visualization methods for angiogram volumetric data-ray casting, marching cubes, and multi-level partition of unity implicits. There are several options available with ray-casting: isosurface extraction, maximum intensity projection and alpha compositing, each producing fundamentally different results. Different visualization methods are suitable for different needs, so this choice is crucial in diagnosis and decision making processes. We also evaluate visual effects such as ambient occlusion, screen space ambient occlusion, and depth of field. Some visualization methods include transparency, so we address the question of relevancy of this additional visual information. We employ transfer functions to map data values to color and transparency, allowing us to view or hide particular tissues. All the methods presented in this paper were developed using OpenCL, striving for real-time rendering and quality interaction. An evaluation has been conducted to assess the suitability of the visualization methods. Results show superiority of isosurface extraction with ambient occlusion effects. Visual effects may positively or negatively affect perception of depth, motion, and relative positions in space.", "Language": "en", "Citations": "1"},
{"Title": "Asymmetric clustering using the alpha-beta divergence", "Authors": ["Olszewski D.", "Ster B."], "Keywords": ["Alpha-Beta divergence", "Asymmetry", "Clustering", "Dissimilarity"], "Date": "2014", "Abstract": "We propose the use of an asymmetric dissimilarity measure in centroid-based clustering. The dissimilarity employed is the Alpha-Beta divergence (AB-divergence), which can be asymmetrized using its parameters. We compute the degree of asymmetry of the AB-divergence on the basis of the within-cluster variances. In this way, the proposed approach is able to flexibly model even clusters with significantly different variances. Consequently, this method overcomes one of the major drawbacks of the standard symmetric centroid-based clustering. \u00a9 2013 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "11"},
{"Title": "Puzzle game", "Authors": ["Kosmac M.", "Krota A.", "Batagelj B."], "Keywords": ["Camera", "Canvas", "Infra-red (IR)", "Interaction"], "Date": "2009", "Abstract": "The idea of the project was build at the University of Ljubljana, Faculty of Computer and Information Science, in scope of the course Communication Methods, in collaboration with Academy of Fine Arts and Design. It's main goal was to build an interactive system between a user and a canvas, using only a projector, infra-red (IR) transmitter and IR camera. In other words, user will be able to play some game with IR transmitter, which will be recognized by IR camera, with the main window of the application displayed on the canvas. The game implemented in the project is a puzzle and it's just for the purpose of showing, how the system can interact with it's surroundings. Projector projects the image on to the canvas and the player standing in front of the canvas is choosing the particles of puzzle he wants to exchange. Game has more different puzzle images to be solved, each composed of different number of particles. Number of puzzle particles grows, when the puzzle is solved. The most significant part of the project was to develop a method that recognizes the position of the IR transmitter in the image acquired from the IR camera. We will describe used methods and the difficulties we came across. We will also describe all elements needed, from the software used to hardware. The aplication can be transformed into more complex interactive system.", "Language": "en", "Citations": "0"},
{"Title": "Explaining prediction models and individual predictions with feature contributions", "Authors": ["Strumbelj E.", "Kononenko I."], "Keywords": ["Data mining", "Decision support", "Interpretability", "Knowledge discovery", "Visualization"], "Date": "2014", "Abstract": "We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method\u2019s usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method\u2019s explanations improved the participants\u2019 understanding of the model.", "Language": "en", "Citations": "24"},
{"Title": "Layout design of manufacturable quantum-dot cellular automata", "Authors": ["Janez M.", "Pecar P.", "Mraz M."], "Keywords": ["Design rules", "Emergent technology", "Layout design", "Quantum-dot cellular automata"], "Date": "2012", "Abstract": "Quantum-dot cellular automaton (QCA) is an emergent technology that is not hindered by quantum effects that limit the scaling of CMOS technology, but instead employs them to perform computation. However, this brings its own impediments, such as the influence of the thermodynamic effects. Beside that, QCA has to be coupled with CMOS circuitry of different size features to enable clocking. We discussed all these facts and devised a floorplan which would facilitate manufacturability. Based on it we developed the process of QCA layout design and defined the design rules that must be considered in order to ensure correct operation. These instructions enable the automatization of designing a QCA circuit layout. \u00a9 2012 Elsevier Ltd.", "Language": "en", "Citations": "37"},
{"Title": "The total zero-divisor graph of a commutative ring", "Authors": ["Duric A.", "Jevcrossed d signenic S.", "Oblak P.", "Stopar N."], "Keywords": ["Commutative ring", "total graph", "zero-divisor graph"], "Date": "2018", "Abstract": "In this paper, we initiate the study of the total zero-divisor graph over a commutative ring with unity. This graph is constructed by both relations that arise from the zero-divisor graph and from the total graph of a ring and give a joint insight of the structure of zero-divisors in a ring. We characterize Artinian rings with the connected total zero-divisor graphs and give their diameters. Moreover, we compute major characteristics of the total zero-divisor graph of the ring Zm of integers modulo m and prove that the total zero-divisor graphs of Zm and Zn are isomorphic if and only if m = n.", "Language": "en", "Citations": "0"},
{"Title": "GenePath: From mutations to genetic networks and back", "Authors": ["Juvan P.", "Demsar J.", "Shaulsky G.", "Zupan B."], "Keywords": [], "Date": "2005", "Abstract": "GenePath is a web-based application for the analysis of mutant-based experiments and synthesis of genetic networks. Here, we introduce GenePath and describe a number of new approaches, including conflict resolution, handling cyclic pathways, confidence level assignment, what-if analysis and new experiment proposal. We illustrate the key concepts using data from a study of adhesion genes in Dictyostelium discoideum and show that GenePath discovered genetic interactions that were ignored in the original publication. GenePath is available at http://www.genepath.org/genepath2. \u00a9 2005 Oxford University Press.", "Language": "en", "Citations": "12"},
{"Title": "My phone and me: Understanding people's receptivity to mobile notifications", "Authors": ["Mehrotra A.", "Pejovic V.", "Vermeulen J.", "Hendley R.", "Musolesi M."], "Keywords": ["Mobile sensing", "Notifications, interruptibility, context-aware computing"], "Date": "2016", "Abstract": "Notifications are extremely beneficial to users, but they often demand their attention at inappropriate moments. In this paper we present an in-situ study of mobile interruptibility focusing on the effect of cognitive and physical factors on the response time and the disruption perceived from a notification. Through a mixed method of automated smartphone logging and experience sampling we collected 10372 in-thewild notifications and 474 questionnaire responses on notification perception from 20 users. We found that the response time and the perceived disruption from a notification can be influenced by its presentation, alert type, sender-recipient relationship as well as the type, completion level and complexity of the task in which the user is engaged. We found that even a notification that contains important or useful content can cause disruption. Finally, we observe the substantial role of the psychological traits of the individuals on the response time and the disruption perceived from a notification.", "Language": "en", "Citations": "51"},
{"Title": "Process models of interrelated speech intentions from online health-related conversations", "Authors": ["Epure E.V.", "Compagno D.", "Salinesi C.", "Deneckere R.", "Bajec M.", "Zitnik S."], "Keywords": ["Conversation analysis", "Conversational processes", "Intention mining", "Machine learning", "Natural language processing", "Process mining", "Speech acts", "Speech intentions", "Text mining"], "Date": "2018", "Abstract": "Being related to the adoption of new beliefs, attitudes and, ultimately, behaviors, analyzing online communication is of utmost importance for medicine. Multiple health care, academic communities, such as information seeking and dissemination and persuasive technologies, acknowledge this need. However, in order to obtain understanding, a relevant way to model online communication for the study of behavior is required. In this paper, we propose an automatic method to reveal process models of interrelated speech intentions from conversations. Specifically, a domain-independent taxonomy of speech intentions is adopted, an annotated corpus of Reddit conversations is released, supervised classifiers for speech intention prediction from utterances are trained and assessed using 10-fold cross validation (multi-class, one-versus-all and multi-label setups) and an approach to transform conversations into well-defined, representative logs of verbal behavior, needed by process mining techniques, is designed. The experimental results show that: (1) the automatic classification of intentions is feasible (with Kappa scores varying between 0.52 and 1); (2) predicting pairs of intentions, also known as adjacency pairs, or including more utterances from even other heterogeneous corpora can improve the predictions of some classes; and (3) the classifiers in the current state are robust to be used on other corpora, although the results are poorer and suggest that the input corpus may not sufficiently capture varied ways of expressing certain speech intentions. The extracted process models of interrelated speech intentions open new views on grasping the formation of beliefs and behavioral intentions in and from speech, but in-depth evaluation of these conversational models is further required.", "Language": "en", "Citations": "1"},
{"Title": "Predicting mechanical properties of rubber compounds with neural networks and support vector machines", "Authors": ["Trebar M.", "Lotric U."], "Keywords": [], "Date": "2007", "Abstract": "The quality of rubber compounds is assessed by rheological and mechanical tests. Since mechanical tests are very time consuming, the main idea of this work is to quest for strong nonlinear relationships between rheological and mechanical tests in order to reduce the latter. The multilayered perceptron and support vector machine combined with data preprocessing were applied to model hardness and density of the vulcanizates from the rheological parameters of the raw compounds. The results outline the advantage of proper data preprocessing. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "0"},
{"Title": "Text mining approaches for automated literature knowledge extraction and representation", "Authors": ["Nuzzo A.", "Mulas F.", "Gabetta M.", "Arbustini E.", "Zupan B.", "Larizza C.", "Bellazzi R."], "Keywords": ["Annotation networks", "Candidate gene study", "Gene ranking", "Text Mining"], "Date": "2010", "Abstract": "Due to the overwhelming volume of published scientific papers, information tools for automated literature analysis are essential to support current biomedical research. We have developed a knowledge extraction tool to help researcher in discovering useful information which can support their reasoning process. The tool is composed of a search engine based on Text Mining and Natural Language Processing techniques, and an analysis module which process the search results in order to build annotation similarity networks. We tested our approach on the available knowledge about the genetic mechanism of cardiac diseases, where the target is to find both known and possible hypothetical relations between specific candidate genes and the trait of interest. We show that the system i) is able to effectively retrieve medical concepts and genes and ii) plays a relevant role assisting researchers in the formulation and evaluation of novel literature-based hypotheses. \u00a9 2010 IMIA and SAHIA. All rights reserved.", "Language": "en", "Citations": "7"},
{"Title": "A framework for visual-context-aware object detection in still images", "Authors": ["Perko R.", "Leonardis A."], "Keywords": ["Context integration", "Object detection", "Visual context"], "Date": "2010", "Abstract": "Visual context provides cues about an object's presence, position and size within the observed scene, which should be used to increase the performance of object detection techniques. However, in computer vision, object detectors typically ignore this information. We therefore present a framework for visualcontext- aware object detection. Methods for extracting visual contextual information from still images are proposed, which are then used to calculate a prior for object detection. The concept is based on a sparse coding of contextual features, which are based on geometry and texture. In addition, bottom-up saliency and object co-occurrences are exploited, to define auxiliary visual context. To integrate the individual contextual cues with a local appearance-based object detector, a fully probabilistic framework is established. In contrast to other methods, our integration is based on modeling the underlying conditional probabilities between the different cues, which is done via kernel density estimation. This integration is a crucial part of the framework which is demonstrated within the detailed evaluation. Our method is evaluated using a novel demanding image data set and compared to a state-of-the-art method for context- aware object detection. An in-depth analysis is given discussing the contributions of the individual contextual cues and the limitations of visual context for object detection. \u00a9 2010 Elsevier Inc.", "Language": "en", "Citations": "20"},
{"Title": "Anticipatory mobile computing for behaviour change interventions", "Authors": ["Pejovic V.", "Musolesi M."], "Keywords": ["Anticipatory computing", "Behaviour change interventions", "Mobile computing", "Mobile sensing"], "Date": "2014", "Abstract": "Behavioural change interventions represent a powerful means for tackling a number of health and well-being issues, from obesity to stress and addiction. In the current medical practice, the change is induced through tailored coaching, support and information delivery. However, with the advent of smartphones, innovative ways of delivering interventions are emerging. Indeed, mobile phones, equipped with an array of sensors, and carried by their users at all times, enable therapists to both learn about the user behaviour, and impact the behaviour through the delivery of more relevant and personalised information. In this work we propose harnessing pervasive computing to not only learn from users' past behaviour, but also predict future actions and emotional states, deliver interventions proactively, evaluate their impact at run-time, and over time learn a personal intervention-effect model of a participant.", "Language": "en", "Citations": "25"},
{"Title": "Robust recognition and pose determination of 3-D objects using range images in eigenspace approach", "Authors": ["Skocaj D.", "Leonardis A."], "Keywords": ["Clouds", "Computer vision", "Feature extraction", "Image coding", "Image recognition", "Information science", "Noise robustness", "Pixel", "Principal component analysis", "Shape"], "Date": "2001", "Abstract": "In this paper we propose a robust method for recognition and pose determination of 3-D objects using range images in the eigenspace approach. Instead of computing the coefficients by a projection of the data onto the eigenimages, we determine the coefficients by solving a set of linear equations in a robust manner. The method efficiently overcomes the problem of missing pixels, noise and occlusions in range images. The results show that the proposed method outperforms the standard one in recognition and pose determination.", "Language": "en", "Citations": "11"},
{"Title": "Distance-regular graphs with complete multipartite \u03bc-graphs and AT4 family", "Authors": ["Jurisic A.", "Koolen J."], "Keywords": ["\u03bc-graphs", "Antipodal", "AT4 family", "Distance-regular graphs", "Locally strongly regular", "Tight"], "Date": "2007", "Abstract": "Let \u0393 be an antipodal distance-regular graph of diameter 4, with eigenvalues \u03b8", "Language": "en", "Citations": "3"},
{"Title": "Lightning may pose a danger to patients receiving deep brain stimulation: Case report", "Authors": ["Prezelj N.", "Trost M.", "Georgiev D.", "Flisar D."], "Keywords": ["Deep brain stimulation", "Electromagnetic interference", "Functional neurosurgery", "Lightning"], "Date": "2019", "Abstract": "Deep brain stimulation (DBS) is an established treatment option for advanced stages of Parkinson\u2019s disease and other movement disorders. It is known that DBS is susceptible to strong electromagnetic fields (EMFs) that can be generated by various electrical devices at work, home, and in medical environments. EMFs can interfere with the proper functioning of implantable pulse generators (IPGs). Very strong EMFs can generate induction currents in implanted electrodes and even damage the brain. Manufacturers of DBS devices have issued a list of warnings on how to avoid this danger. Strong EMFs can result from natural forces as well. The authors present the case of a 66-year-old woman who was being treated with a rechargeable DBS system for neck dystonia when her apartment was struck by lightning. Domestic electronic devices that were operating during the event were burned and destroyed. The woman\u2019s IPG switched off but remained undamaged, and she suffered no neurological consequences.", "Language": "en", "Citations": "0"},
{"Title": "Othello syndrome in patients with parkinson's disease", "Authors": ["Georgiev D.", "Danieli A.", "Ocepek L.", "Novak D.", "Zupancic-Kriznar N.", "Trost M.", "Pirtosek Z."], "Keywords": ["Atypical neuroleptics", "Delusional jealousy", "Dopamine agonists", "Othello syndrome", "Parkinson's disease"], "Date": "2010", "Abstract": "Background: Othello syndrome (OS) is an organic delusional disorder with prevailing jealousy symptoms presumably appearing as side effect of antiparkinsonian therapy. The clinical spectrum of psychiatric symptoms in Parkinson's disease (PD) is very wide, including symptoms of depresion and anxiety, hallucinations, delusions, with prevalent paranoid symptoms, agitation, delirium and sleep disorders. At our knowledge, just a few cases of patients with PD and OS were reported till now. Methods: three neurologists working in a tertiary referral centre were asked to report cases of pathological jealousy as defined by the DSM IV criteria (Kaplan et al. 1994). The following data were collected retrospectively: sex, age at PD onset, age at OS onset, duration of PD, duration of PD treatment, duration of treatment with dopamine agonists (DAs), treatment of OS, past history of alcoholism, premorbid personality disorder, family history of psychiatric disorders and data about general cognitive condition. Results: Five PD patients (three males) with OS were investigated. The mean age of the patients at the PD onset was 46.80\u00b18.87 (SD), the mean age at the OS onset was 56.40\u00b18.76 (SD). Before the onset of OS, all of them were taking dopamine agonists. The first patient was treated with pramipexole, apomorphine infusion and levodopa/carbidopa, the second with apomorphine infusion plus levodopa/carbidopa/entacapone, the third with pramipexole, the fourth and fifth with ropinirole. Decrease of dopamine agonist led to clinical improvement in three patients (complete reduction of the symptoms in two, reduction of symptoms in one patients). In two patients, the symptoms remained the same. In three patients atypical neuroleptics had to be added: clozapine in two and quetiapine in one patient. Conclusions: We believe that OS is a more common psychiatric side effect in PD patients on treatment with dopamine agonists than usually believed, particulary in those with early disease onset. It is a very disturbing symptom for patients and their partners, often underestimated by them, and should therefore be actively searched for. \u00a9 Medicinska naklada.", "Language": "en", "Citations": "35"},
{"Title": "Parallel implementation of Viterbi algorithm Vzporedna izvedba viterbijevega algoritma", "Authors": ["Suligoj D.", "Trobec R.", "Robic B."], "Keywords": ["Convolutional decoding", "Multiprocessor system", "Parallel Viterbi algorithm", "VLSI"], "Date": "2002", "Abstract": "Viterbi algorithm is an optimal convolutional decoding algorithm with superpolynomial time complexity. Basic principles of Viterbi algorithm are shown in Figures 1, 2, and 3. In order to improve the algorithm throughput, one has to apply parallelism. This can be done at different levels, e.g., bit, word, or algorithm level. The paper discusses various approaches to the parallelisation of the decoding algorithm, some implemented in VLSI processing elements, and the other implemented by multiprocessor systems with general purpose processors. A Viterbi decoder basically consists of three main functional blocks shown in Figure 4. Branch Metrics block BM calculates in each time step all branch weights. Add-Compare-Select ACS unit calculates sums of weights and selects optimal survivor paths. Survivor Memory SM analyses partial results from BM and ACS and outputs decoded data within a time delay D. Note that data dependent loop is present in the ACS unit that limits the speed of the decoding procedure because actual branch weight has to be added to the accumulated weights of the survivor path at each time step. Performances of the Viterbi decoder can be improved on bit level by breaking the data dependant loop using carry-save addition and pipelining (see Figure 5). Further, several ACS units can be used in parallel on the word level. Finally, more independent decoders may work on different blocks of input data. After decoding procedure the final result can be obtained by the multiplexing of decoded segments. The mentioned principles can be implemented either in VLSI components connected into a ring topology or by several independent general purpose or DSP processors (see Figures 6 and 7). Theoretical speedup attainable by parallel processing is estimated to be S = pN M / E, where pN represents the number of processors, E the length of the decoded block and M \u2264 E the length of the uniquely decoded data in a block. Considering the performance of contemporary processing elements, the achievable decoding speed using 16 processors will be in the range of 10 Mbits/sec.", "Language": "en", "Citations": "0"},
{"Title": "Separating interleaved HTTP sessions using a stochastic model", "Authors": ["Pozenel M.", "Mahnic V.", "Kukar M."], "Keywords": ["Clickstream analysis", "Markov model", "Web user behavior"], "Date": "2010", "Abstract": "We describe a novel method for interleaved HTTP session reconstruction based on first order Markov model. Interleaved session is generated by a user who is concurrently browsing a web site in two or more web sessions (browser windows). In order to assure data quality for subsequent phases in analyzing user's browsing behavior, such sessions need to be separated in advance. We propose a separating process based on trained first order Markov chains. We develop a testing method based on various measures of reconstructed sessions similarity to original ones. We evaluate the developed method on two real world clickstream data sources: A web shop and a university student records information system. Preliminary results show that the proposed method performs well.", "Language": "en", "Citations": "1"},
{"Title": "Boosting audio chord estimation using multiple classifiers", "Authors": ["Pesek M.", "Leonardis A.", "Marolt M."], "Keywords": ["audio chord estimation", "compositional hierarchical model", "deep learning", "stacking generalization"], "Date": "2014", "Abstract": "The paper addresses the task of automatic audio chord estimation using stacked generalization of multiple classifiers over Hidden Markov model (HMM) estimators. We evaluated two feature types for chord estimation: a new compositional hierarchical model and standard chroma feature vectors. The compositional hierarchical model is presented as an alternative deep learning approach. Both feature types are further modelled with two separate Hidden Markov models (HMMs) in order to estimate chords in music recordings. Further, a binary decision tree and support vector machine are proposed binding the HMM estimations into a new feature vector. The additional stacking of the classifiers provides a classification boost by 17.55% with a binary decision tree and and 21.96% using the support vector machine. \u00a9 2014 University of Zagreb.", "Language": "en", "Citations": "0"},
{"Title": "Selective recurrent neural network", "Authors": ["Ster B."], "Keywords": ["Finite state automata", "Latch", "Long-term dependencies", "Multiplexer", "Recurrent neural networks", "Temporal processing"], "Date": "2013", "Abstract": "It is known that recurrent neural networks may have difficulties remembering data over long time lags. To overcome this problem, we propose an extended architecture of recurrent neural networks, which is able to deal with long time lags between relevant input signals. A register of latches at the input layer of the network is applied to bypass irrelevant input information and to propagate relevant inputs. The latches are implemented with differentiable multiplexers, thus enabling the derivatives to be propagated through the network. The relevance of input vectors is learned concurrently with the weights of the network using a gradient-based algorithm. \u00a9 2012 Springer Science+Business Media New York.", "Language": "en", "Citations": "6"},
{"Title": "Computational modelling of liver metabolism and its applications in research and the clinics", "Authors": ["Tomas T.C.", "Moskon M.", "Mraz M.", "Rozman D."], "Keywords": ["Hepatic metabolism", "Large-scale metabolic models", "Liver", "Modelling", "NAFLD", "Simulation", "Systems medicine"], "Date": "2018", "Abstract": "Computational models of liver metabolism are gaining an increasing importance within the research community. Moreover, their first clinical applications have been reported in recent years in the context of personalised and systems medicine. Herein, we survey selected experimental models together with the computational modelling approaches that are used to describe the metabolic processes of the liver in silico. We also review the recent developments in the large-scale hepatic computational models where we focus on object-oriented models as a part of our research. The object-oriented modelling approach is beneficial in efforts to describe the interactions between the tissues, such as how metabolism of the liver interacts with metabolism of other tissues via blood. Importantly, this modelling approach can account as well for transcriptional and post-translational regulation of metabolic reactions which is a difficult task to achieve. The current and potential clinical applications of large-scale hepatic models are also discussed. We conclude with the future perspectives within the systems and translational medicine research community.", "Language": "en", "Citations": "2"},
{"Title": "An approach to create project- specific software development process Pristop za prilagajanje procesa razvoja programske opreme", "Authors": ["Zrnec A.", "Vavpotic D."], "Keywords": ["Base process", "Decision rule", "Method engineering", "Process adaptation", "Project characteristics"], "Date": "2008", "Abstract": "In the paper we discuss an approach to software process adaptation to project specific needs. As the proposed approach is much simpler than any known approach suggested by the situational method engineering, it can be easily applied in practice as a special software tool used by method engineers. The ability of the software process to adapt to project-specific needs is one of the most important factors for assuring software development methodology suitability and acceptance by software engineers in particular organization. The idea of software development methodology adaptation is not new. Inability of traditional methodologies to adapt to particular situational needs or particular needs of the project, their rigidness and technical unsuitability has led to development of a special scientific field, called situational method engineering (SME). The field of SME is very complex, because it focuses mainly on theoretical aspects of methodology construction and adaptation, and uses rather complex mathematical formalisms for the method construction description and method fragments inquiry from method fragment repository. This is also the main reason why SME has never been widely acknowledged or practiced by software engineers. Also SME also doesn't deal with social suitability and acceptance of the methodology by its users. That has been another reason why the field hasn't been accepted by the engineers. The key to successful introduction of software development methodologies into organizations, their later use and acceptance, is the ability of its prescribed software process to be adaptable to project-specific needs. The methodology has to define a flexible software process, capable of adapting to specific situations in which new projects emerge. The aim of this paper is to combine theoretical knowledge from SME and practical knowledge from organizations for software development. On this basis we formally define a new approach for software development process adaptation to projects specific needs. The approach servers as a basis for definition of the decision model which supports software process adaptation. The proposed approach to process adaptation presented in this paper is based upon selecting the most suitable path through the given process used in a particular organization. We define the software process used in the observed organization as a base process. The approach extends the idea of the SME field theory, of choosing one of several possible paths through a given base process. Our upgrading of the basic idea involves cancellation of all the formerly predefined paths through the process. In our approach, the construction of the adapted instance of the base process is directed by several decision rules. So, for each process element of the base process, the decision rules give an answer about its inclusion in the adapted instance of the base software process. We use several project characteristics to describe different properties of the projects. Besides technological characteristics, we also define sociological characteristics and customer requirements to be able to describe projects more precisely. Multiple types of characteristics also ensure that in the end the adapted instance of the base process is more precisely attuned to the project at hand.", "Language": "en", "Citations": "3"},
{"Title": "Influence of dexmedetomidine and lidocaine on perioperative opioid consumption in laparoscopic intestine resection: a randomized controlled clinical trial", "Authors": ["Andjelkovic L.", "Novak-Jankovic V.", "Pozar-Lukanovic N.", "Bosnic Z.", "Spindler-Vesel A."], "Keywords": ["Analgesia", "cognitive function", "dexmedetomidine", "laparoscopy", "lidocaine", "neuralgia"], "Date": "2018", "Abstract": "Objective: The consumption of opioid analgesics could be reduced by the use of analgesics with different mechanisms of action. We investigated whether additional treatment with dexmedetomidine or lidocaine could reduce opioid consumption. Methods: We randomized 59 study participants into three groups and examined: (i) fentanyl consumption, (ii) consumption of piritramide, and (iii) cognitive function and neuropathic pain. The control group received continuous propofol infusion and fentanyl boluses. Continuous intravenous infusion of dexmedetomidine (0.5 \u00b5g/kg/h) was administered to the dexmedetomidine group and lidocaine (1.5 mg/kg/h) was administered to the lidocaine group. Results: No reduction in fentanyl consumption was observed among the groups. However, we noted a significantly lower consumption of piritramide on the first and second postoperative day in the lidocaine group. Total consumption of piritramide was significantly lower in the lidocaine group compared with the control group. Conclusions: Lidocaine and dexmedetomidine reduced intraoperative propofol consumption, while lidocaine reduced postoperative piritramide consumption. Clinical trial registration: NCT02616523.", "Language": "en", "Citations": "1"},
{"Title": "Improving information quality of Wikipedia articles with cooperative principle", "Authors": ["Fidler M.", "Lavbic D."], "Keywords": ["Conversation maxims", "Cooperative principle", "Improving information quality", "Information quality assessment and analysis", "Interrater reliability", "Presenting information in relevant way"], "Date": "2017", "Abstract": "Purpose - The purpose of this paper is to investigate the impact of cooperative principle on the information quality (IQ) bymaking objects more relevant for consumer needs, in particular caseWikipedia articles for students. Design/methodology/approach - The authors performed a quantitative study with participants being invited to complete an online survey. Each rater evaluated three selected and re-written articles from Wikipedia by four IQ dimensions (accuracy, completeness, objectivity, and representation). Grice's maxims and submaxims were used to re-write articles and make them more relevant for student cognitive needs. The results were analyzed with statistical methods of mean, standard deviation, Cronbach's \u03b1, and ICC (two-way random model of single measure). Findings - The study demonstrates that Wikipedia articles can be made more relevant for student needs by using cooperative principle with increase in IQ and also achieving higher consistency of students' scores as recent research. In particular, students in the research perceived the abstract, constructed with cooperative principle, more objective and complete as reported in recent research. Practical implications - The work can benefit encyclopedia editors to improve IQ of existing articles as well as consumers that would obtain more relevant information in less reading time. Originality/value - This is one of the first attempts to empirically investigate the application of cooperate principle to make objects more relevant for consumer needs and impact of this on IQ. IQ improvement evidence is provided and impacts on IQ dimensions such as objectivity, completeness, accuracy, and representation for research community to validate and compare results.", "Language": "en", "Citations": "1"},
{"Title": "Computerized segmentation and diagnostics of whole-body bone scintigrams", "Authors": ["Sajn L.", "Kononenko I.", "Milcinski M."], "Keywords": ["Automatic segmentation", "Image processing", "Machine learning", "Whole-body bone scan"], "Date": "2007", "Abstract": "Bone scintigraphy or whole-body bone scan is one of the most common diagnostic procedures in nuclear medicine. Since expert physicians evaluate images manually some automated procedure for pathology detection is desired. A robust knowledge based methodology for segmenting body scans into the main skeletal regions is presented. The algorithm is simultaneously applied on anterior and posterior whole-body bone scintigrams. Expert knowledge is represented as a set of parameterized rules, used to support standard image processing algorithms. The segmented bone regions are parameterized with algorithms for classifying patterns so the pathologies can be classified with machine learning algorithms. This approach enables automatic scintigraphy evaluation of pathological changes, thus in addition to detection of point-like high-uptake lesions also other types can be discovered. Our study includes 467 consecutive, non-selected scintigrams. Automatic analysis of whole-body bone scans using our segmentation algorithm gives more accurate and reliable results than previous studies. Preliminary experiments show that our expert system based on machine learning closely mimics the results of expert physicians. \u00a9 2007 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "3"},
{"Title": "FPGA-based SIC/XE processor and supporting toolchain", "Authors": ["Kloboves K.", "Mihelic J.", "Bulic P.", "Dobravec T."], "Keywords": ["FPGA implementation", "Hardware design", "SIC/XE educational computer", "Software toolchain", "System software", "Teaching concepts"], "Date": "2017", "Abstract": "SIC/XE is a well-known educational computer architecture designed and widely used for teaching system-software concepts and programming. In this paper, we present a design and implementation of SIC/XE computer, which we implemented using a field-programmable gate-array development board. The system consists of a processor, device controllers, and other auxiliary components. Additionally, we developed a suite of system software utilities (a simple toolchain) for use with our system. The suite includes assembler, linker, and simulator utilities, used to develop and run SIC/XE programs. Programs can be transferred from a personal computer to the board over a serial connection using a provided software tool. The presented hardware and software components have proven to be a convenient tool for teaching both, System-Software and Hardware design course on the undergraduate level.", "Language": "en", "Citations": "1"},
{"Title": "NutIE - A modern open source natural language processing toolkit", "Authors": ["Zitnik S.", "Draskovic D.", "Nikolic B.", "Bajec M."], "Keywords": ["Libraries", "Natural Language Processing", "Scala", "Toolkits", "Web application"], "Date": "2017", "Abstract": "Nowadays, there are many tools and applications available that use novel machine learning algorithms, but only few tools exist that bridge the gap between the computer scientists and general public. The latter especially holds for the field of natural language processing. Therefore in this paper we present a new information extraction toolkit, called nutIE, that offers a frontend for end-to-end text analysis using a modern javascript-based web application. The core of the toolkit includes all the high-level information extraction techniques with supporting preprocessing and evaluation methods. All these can be accessed by a Scala programmatic API or an arbitrary third-party application via REST interface. It has already been shown that the integrated toolkit's algorithms achieve state-of-the art results. The toolkit is currently applied at international text processing projects and courses.", "Language": "en", "Citations": "0"},
{"Title": "Dana36: A multi-camera image dataset for object identification in surveillance scenarios", "Authors": ["Pers J.", "Kenk V.S.", "Mandeljc R.", "Kristan M.", "Kovacic S."], "Keywords": [], "Date": "2012", "Abstract": "We present a novel dataset for evaluation of object matching and recognition methods in surveillance scenarios. Dataset consists of more than 23,000 images, depicting 15 persons and nine vehicles. A ground truth data - the identity of each person or vehicle - is provided, along with the coordinates of the bounding box in the full camera image. The dataset was acquired from 36 stationary camera views using a variety of surveillance cameras with resolutions ranging from standard VGA to three megapixel. 27 cameras observed the persons and vehicles in an outdoor environment, while the remaining nine observed the same persons indoors. The activity of persons was planned in advance; they drive the cars to the parking lot, exit the cars and walk around the building, through the main entrance, and up the stairs, towards the first floor of the building. The intended use of the dataset is performance evaluation of computer vision methods that aim to (re)identify people and objects from many different viewpoints in different environments and under variable conditions. Due to variety of camera locations, vantage points and resolutions, the dataset provides means to adjust the difficulty of the identification task in a controlled and documented manner. An interface for easy use of dataset within Matlab is provided as well, and the data is complemented by baseline results using a basic color histogram-based descriptor. While the cropped images of persons and vehicles represent the primary data in our dataset, we also provide full-frame images and a set of tracklets for each object as a courtesy to the dataset users. \u00a9 2012 IEEE.", "Language": "en", "Citations": "9"},
{"Title": "Explaining instance classifications with interactions of subsets of feature values", "Authors": ["Strumbelj E.", "Kononenko I.", "Robnik Sikonja M."], "Keywords": ["Classification", "Data mining", "Explanation", "Knowledge discovery", "Machine learning", "Visualization"], "Date": "2009", "Abstract": "In this paper, we present a novel method for explaining the decisions of an arbitrary classifier, independent of the type of classifier. The method works at the instance level, decomposing the model's prediction for an instance into the contributions of the attributes' values. We use several artificial data sets and several different types of models to show that the generated explanations reflect the decision-making properties of the explained model and approach the concepts behind the data set as the prediction quality of the model increases. The usefulness of the method is justified by a successful application on a real-world breast cancer recurrence prediction problem. \u00a9 2009 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "24"},
{"Title": "Prognostic factors in the prediction of chronic wound healing by electrical stimulation", "Authors": ["Cukjati D.", "Robnik-Sikonja M.", "Rebersek S.", "Kononenko I.", "Miklavcic D."], "Keywords": ["Electric stimulation", "Inductive learning", "Predictors of wound healing"], "Date": "2001", "Abstract": "The aim of the study is to determine the effects of wound, patient and treatment attributes on the wound healing rate and to propose a system for wound healing rate prediction. Predicting the wound healing rate from the initial wound, patient and treatment data collected in a database of 300 chronic wounds is not possible. After considering weekly follow-ups, it was determined that the best prognostic factors are weekly follow-ups of the wound healing process, which alone were found to predict accurately the wound healing rate after a minimum follow-up period of four weeks (at least five measurements of wound area). After combining the follow-ups with wound, patient and treatment attributes, the minimum follow-up period was reduced to two weeks (at least three measurements of wound area). After a follow-up period of two weeks, it was possible to predict the wound healing rate of an independent test set of chronic wounds with a relative squared error of 0.347, and after three weeks, with a relative squared error of 0.181 (using regression trees with linear equations in its leaves). Regression trees with a relative squared error close to 0 produce better prediction than with an error closer to 1. Results show that the type of treatment is just one of many prognostic factors. Arranged in order of decreasing prediction capability, prognostic factors are: wound size, patient's age, elapsed time from wound appearance to the beginning of the treatment, width-to-length ratio, location and type of treatment. The data collected support former findings that the biphasic- and direct-current stimulation contributes to faster healing of chronic wounds. The model of wound healing dynamics aids the prediction of chronic wound healing rate, and hence helps with the formulation of appropriate treatment decisions.", "Language": "en", "Citations": "23"},
{"Title": "Topological complexity of the telescope", "Authors": ["Franc A."], "Keywords": ["Fibrewise Lusternik-Schnirelmann category", "Mapping telescope", "Topological complexity"], "Date": "2012", "Abstract": "We use an alternative definition of topological complexity to show that the topological complexity of the mapping telescope of a sequence X1\u2192f1X2\u2192f2X3\u2192f3... is bounded above by 2max{TC(X ", "Language": "en", "Citations": "0"},
{"Title": "Analysis of elective courses selection in post-Bologna programmes", "Authors": ["Rozanc I.", "Slivnik B."], "Keywords": [], "Date": "2014", "Abstract": "One of the most important results of the Bologna reform at the Faculty of Computer and Information Science at the University of Ljubljana is a much larger number of elective courses. The elective courses of the academic study programme were grouped into modules while the professional study programme permits selection of individual elective courses under minor restrictions. Both approaches fulfilled the reform requirements. However, the academic study programme is more rigid and thus easier to carry out than the professional one but students prefer the flexibility of the latter. In this paper we analyze the actual data of elective course selection by students of both programs. First, we examine the link between different module selections to optimize the distribution of elective courses into modules. This examination is possible as a number of students, i.e., the most successful ones, were allowed to select courses regardless of modules. Second, we investigate whether some commonly selected groups of elective courses of the professional study programme could be identified in order to simplify the programme and its implementation. The results show some important improvements could be introduced. \u00a9 2014 MIPRO.", "Language": "en", "Citations": "1"},
{"Title": "Food object recognition using a mobile device: State of the art", "Authors": ["Knez S.", "Sajn L."], "Keywords": [], "Date": "2015", "Abstract": "In this paper nine mobile food recognition systems are described based on their system architecture and their core properties (the core properties and experimental results are shown on the last page). While the mobile hardware increased its power through the years (2009 - 2013) and the food detection algorithms got optimized, still there was no uniform approach to the question of food detection. Also, some system used additional information for better detection, like voice data, OCR and bounding boxes. Three systems included a volume estimation feature. First five systems were implemented on a client-server architecture, while the last three took advantage of the available hardware in later years and proposed a client only based architecture.", "Language": "en", "Citations": "1"},
{"Title": "Selection of reference images for image-based scene representations", "Authors": ["Werner T.", "Pajdla T.", "Hlavac V.", "Leonardis A.", "Matousek M."], "Keywords": ["Image based rendering", "Reference images", "Three-dimensional computer vision"], "Date": "2002", "Abstract": "In computer vision, an attention has been devoted to image-based scene representations. They allow to construct an arbitrary view of a rigid 3-D scene using transfer from real 2-D reference images, rather than by rendering an explicit 3-D model. We focus on selecting the optimal set of reference images from a large set of images representing the scene. The selected set minimizes a weighted sum of the number of reference views and the total fit error. We propose two different algorithms for solving this optimization problem. The experimental results on synthetic and real data indicate the feasibility of the approach for one-parameter camera motion. The possibility is shown to extend one of the algorithms for more general cases.", "Language": "en", "Citations": "2"},
{"Title": "Collocations dictionary of modern Slovene", "Authors": ["Kosem I.", "Krek S.", "Gantar P.", "Holdt S.A.", "Cibej J.", "Laskowski C."], "Keywords": ["Collocations", "Database", "Dictionary", "Interface"], "Date": "2018", "Abstract": "The paper presents the compilation of the Collocations Dictionary of Modern Slovene, a new resource targeting the language production needs of Slovene speakers. An important aspect of the compilation of the dictionary is the immediate publication of all the entries, from automatic, postprocessed, finalized by lexicographers and so on, and indicating to the users their status, i.e. the stage in the compilation process. Furthermore, we discuss the introduction of crowdsourcing into the lexicographic workflow. The paper also focuses the development and presentation of the interface, which introduces new approaches to collocation presentation. The aim was to develop a collocation-driven interface that would allow different types of users a great deal of flexibility and customizability in exploring collocational information about words. In this way, the interface represents a hybrid between a more corpus-based presentation of collocations (e.g. in tools such as Word Sketch) and a traditional sense-driven presentation of collocations as found in existing collocations dictionaries.", "Language": "en", "Citations": "2"},
{"Title": "Designable DNA-binding domains enable construction of logic circuits in mammalian cells", "Authors": ["Gaber R.", "Lebar T.", "Majerle A.", "Ster B.", "Dobnikar A.", "Bencina M.", "Jerala R."], "Keywords": [], "Date": "2014", "Abstract": "Electronic computer circuits consisting of a large number of connected logic gates of the same type, such as NOR, can be easily fabricated and can implement any logic function. In contrast, designed genetic circuits must employ orthogonal information mediators owing to free diffusion within the cell. Combinatorial diversity and orthogonality can be provided by designable DNA- binding domains. Here, we employed the transcription activator-like repressors to optimize the construction of orthogonal functionally complete NOR gates to construct logic circuits. We used transient transfection to implement all 16 two-input logic functions from combinations of the same type of NOR gates within mammalian cells. Additionally, we present a genetic logic circuit where one input is used to select between an AND and OR function to process the data input using the same circuit. This demonstrates the potential of designable modular transcription factors for the construction of complex biological information-processing devices.", "Language": "en", "Citations": "60"},
{"Title": "Learning computer architecture concepts with the FPGA-based \"move\" microprocessor", "Authors": ["Gustin V.", "Bulic P."], "Keywords": ["Configuration of the CPU", "FPGA", "Microprocessor design", "Programmable logic devices", "RISC", "VHDL"], "Date": "2006", "Abstract": "In this article we introduce the use of a programmable logic device (PLD) in an application-oriented study as an example of designing a microprocessor based on reduced instruction set computer (RISC) architecture. Since the concept of an in-system configurable logic circuit is becoming increasingly popular, we now use it for the purpose of logic design. We suggest that students use PLDs when constructing a central processing unit (CPU) with their own configured functions that are directly implemented in the logic. Such an approach could greatly increase the understanding of the architectural concept of the CPU. \u00a9 2006 Wiley Periodicals, Inc.", "Language": "en", "Citations": "8"},
{"Title": "Twitter sentiment around the earnings announcement events", "Authors": ["Peter G.", "Darko A.", "Igor M.", "Miha G."], "Keywords": [], "Date": "2017", "Abstract": "We investigate the relationship between social media, Twitter in particular, and stock market. We provide an in-depth analysis of the Twitter volume and sentiment about the 30 companies in the Dow Jones Industrial Average index, over a period of three years. We focus on Earnings Announcements and show that there is a considerable difference with respect to when the announcements are made: before the market opens or after the market closes. The two different timings of the Earnings Announcements were already investigated in the financial literature, but not yet in the social media. We analyze the differences in terms of the Twitter volumes, cumulative abnormal returns, trade returns, and earnings surprises. We report mixed results. On the one hand, we show that the Twitter sentiment (the collective opinion of the users) on the day of the announcement very well reflects the stock moves on the same day. We demonstrate this by applying the event study methodology, where the polarity of the Earnings Announcements is computed from the Twitter sentiment. Cumulative abnormal returns are high (2-4%) and statistically significant. On the other hand, we find only weak predictive power of the Twitter sentiment one day in advance. It turns out that it is important how to account for the announcements made after the market closes. These after-hours announcements draw high Twitter activity immediately, but volume and price changes in trading are observed only on the next day. On the day before the announcements, the Twitter volume is low, and the sentiment has very weak predictive power. A useful lesson learned is the importance of the proper alignment between the announcements, trading and Twitter data.", "Language": "en", "Citations": "8"},
{"Title": "The General Explanation Method with NMR Spectroscopy Enables the Identification of Metabolite Profiles Specific for Normal and Tumor Cell Lines", "Authors": ["Pecnik K.", "Todorovic V.", "Bosnjak M.", "Cemazar M.", "Kononenko I.", "Sersa G.", "Plavec J."], "Keywords": ["cancer", "general explanation method", "machine learning", "metabolomics", "NMR spectroscopy"], "Date": "2018", "Abstract": "Machine learning models in metabolomics, despite their great prediction accuracy, are still not widely adopted owing to the lack of an efficient explanation for their predictions. In this study, we propose the use of the general explanation method to explain the predictions of a machine learning model to gain detailed insight into metabolic differences between biological systems. The method was tested on a dataset of ", "Language": "en", "Citations": "0"},
{"Title": "Supporting Regenerative Medicine by Integrative Dimensionality Reduction", "Authors": ["Mulas F.", "Zagar L.", "Zupan B.", "Bellazzi R."], "Keywords": ["Gene subset selection", "Predictive modelling", "Principal component analysis", "Regenerative medicine", "Stem cells"], "Date": "2012", "Abstract": "Objective: The assessment of the developmental potential of stem cells is a crucial step towards their clinical application in regenerative medicine. It has been demonstrated that genome-wide expression profiles can predict the cellular differentiation stage by means of dimensionality reduction methods. Here we show that these techniques can be further strengthened to support decision making with i) a novel strategy for gene selection; ii) methods for combining the evidence from multiple data sets. Methods: We propose to exploit dimensionality reduction methods for the selection of genes specifically activated in different stages of differentiation. To obtain an integrated predictive model, the expression val - ues of the selected genes from multiple data sets are combined. We investigated distinct approaches that either aggregate data sets or use learning ensembles. Results: We analyzed the performance of the proposed methods on six publicly available data sets. The selection procedure identified a reduced subset of genes whose expression values gave rise to an accurate stage prediction. The assessment of predictive accuracy demonstrated a high quality of predictions for most of the data integration methods pre - sented. Conclusion: The experimental results highlighted the main potentials of proposed approaches. These include the ability to predict the true staging by combining multiple training data sets when this could not be inferred from a single data source, and to focus the analysis on a reduced list of genes of similar predictive performance. \u00a9 Schattauer 2012.", "Language": "en", "Citations": "4"},
{"Title": "Towards a reusable fault handling in WS-BPEL", "Authors": ["Kocbek A.", "Juric M.B."], "Keywords": ["Fault handling", "PDFHF for BPEL", "policy driven mechanism", "Service Oriented Architecture", "WS-BPEL"], "Date": "2014", "Abstract": "Service Oriented Architecture (SOA) is an evolution of distributed computing and it is based on the concepts of interoperable services. To enable reliable and robust service oriented information systems, it is important to establish an effective fault handling. WS-BPEL 2.0 specification does not provide sophisticated and reusable support for handling faults and challenges process designers with many obstacles in the process implementation. We introduce a novel policy driven fault handling framework for BPEL by extending the WS-BPEL 2.0 specification. We propose to separate business process and fault handling logic with the aim to decrease code duplication, process complexity and overall process size. The proposed framework consists of a fault policy which includes the definition of BPEL fault handling logic. The fault policy defines fault handlers and fault handling recovery actions that can be used to design handling BPEL process faults. As a proof-of-concept, we have developed a prototype implementation of the proposed policy driven fault handling framework for BPEL and tested it on 117 real world BPEL scenarios. We have confirmed that the proposed solution decreases the code duplication, the process complexity and overall the process size. Even more, we successfully improved the reliability and readability of BPEL processes. \u00a9 2014 World Scientific Publishing Company.", "Language": "en", "Citations": "1"},
{"Title": "The visual object tracking VOT2013 challenge results", "Authors": ["Kristan M.", "Pflugfelder R.", "Leonardis A.", "Matas J.", "Porikli F.", "Cehovin L.", "Nebehay G.", "Fernandez G.", "Vojir T.", "Gatt A.", "Khajenezhad A.", "Salahledin A.", "Soltani-Farani A.", "Zarezade A.", "Petrosino A.", "Milton A.", "Bozorgtabar B.", "Li B.", "Chan C.S.", "Heng C.", "Ward D.", "Kearney D.", "Monekosso D.", "Karaimer H.C.", "Rabiee H.R.", "Zhu J.", "Gao J.", "Xiao J.", "Zhang J.", "Xing J.", "Huang K.", "Lebeda K.", "Cao L.", "Maresca M.E.", "Lim M.K.", "ELHelw M.", "Felsberg M.", "Remagnino P.", "Bowden R.", "Goecke R.", "Stolkin R.", "Lim S.Y.Y.", "Maher S.", "Poullot S.", "Wong S.", "Satoh S.", "Chen W.", "Hu W.", "Zhang X.", "Li Y.", "Niu Z."], "Keywords": ["Visual object tracking challenge", "VOT2013"], "Date": "2013", "Abstract": "Visual tracking has attracted a significant attention in the last few decades. The recent surge in the number of publications on tracking-related problems have made it almost impossible to follow the developments in the field. One of the reasons is that there is a lack of commonly accepted annotated data-sets and standardized evaluation protocols that would allow objective comparison of different tracking methods. To address this issue, the Visual Object Tracking (VOT) workshop was organized in conjunction with ICCV2013. Researchers from academia as well as industry were invited to participate in the first VOT2013 challenge which aimed at single-object visual trackers that do not apply pre-learned models of object appearance (model-free). Presented here is the VOT2013 benchmark dataset for evaluation of single-object visual trackers as well as the results obtained by the trackers competing in the challenge. In contrast to related attempts in tracker benchmarking, the dataset is labeled per-frame by visual attributes that indicate occlusion, illumination change, motion change, size change and camera motion, offering a more systematic comparison of the trackers. Furthermore, we have designed an automated system for performing and evaluating the experiments. We present the evaluation protocol of the VOT2013 challenge and the results of a comparison of 27 trackers on the benchmark dataset. The dataset, the evaluation tools and the tracker rankings are publicly available from the challenge website (http://votchallenge. net). \u00a9 2013 IEEE.", "Language": "en", "Citations": "126"},
{"Title": "Automatic distinguishing between ischemic and heart-rate related transient ST segment episodes in ambulatory ECG records", "Authors": ["Faganeli J.", "Jager F."], "Keywords": [], "Date": "2008", "Abstract": "In ambulatory ECG records, ischemia is manifested by transient ST segment episodes which may or may not be accompanied by increase in heart rate. There are also transient heart-rate related non-ischemic ST segment episodes present which are caused by change in heart rate. The goal of this work was to classify between these two types of ST episodes. The selected features to classify the ST episodes were changes of heart rate, changes of time domain morphologic parameters of the ST segment and changes of the Legendre orthonormal polynomial coefficients of the ST segment, all obtained on 20-second intervals at the beginning and at the extrema of each ST episode. The obtained sensitivity in classifying ischemic versus heart-rate related ST episodes using the LTST DB was 77.9%, while specificity was 73.9%.", "Language": "en", "Citations": "8"},
{"Title": "Machine learning for medical diagnosis: History, state of the art and perspective", "Authors": ["Kononenko I."], "Keywords": ["Kirlian camera", "Machine learning", "Medical diagnosis", "Reliability of prediction"], "Date": "2001", "Abstract": "The paper provides an overview of the development of intelligent data analysis in medicine from a machine learning perspective: a historical view, a state-of-the-art view, and a view on some future trends in this subfield of applied artificial intelligence. The paper is not intended to provide a comprehensive overview but rather describes some subareas and directions which from my personal point of view seem to be important for applying machine learning in medical diagnosis. In the historical overview, I emphasize the naive Bayesian classifier, neural networks and decision trees. I present a comparison of some state-of-the-art systems, representatives from each branch of machine learning, when applied to several medical diagnostic tasks. The future trends are illustrated by two case studies. The first describes a recently developed method for dealing with reliability of decisions of classifiers, which seems to be promising for intelligent data analysis in medicine. The second describes an approach to using machine learning in order to verify some unexplained phenomena from complementary medicine, which is not (yet) approved by the orthodox medical community but could in the future play an important role in overall medical diagnosis and treatment. Copyright \u00a9 2001 Elsevier Science B.V.", "Language": "en", "Citations": "472"},
{"Title": "ABML knowledge refinement loop: A case study", "Authors": ["Guid M.", "Mozina M.", "Groznik V.", "Georgiev D.", "Sadikov A.", "Pirtosek Z.", "Bratko I."], "Keywords": [], "Date": "2012", "Abstract": "Argument Based Machine Learning (ABML) was recently demonstrated to offer significant benefits for knowledge elicitation. In knowledge acquisition, ABML is used by a domain expert in the so-called ABML knowledge refinement loop. This draws the expert's attention to the most critical parts of the current knowledge base, and helps the expert to argue about critical concrete cases in terms of the expert's own understanding of such cases. Knowledge elicited through ABML refinement loop is therefore more consistent with expert's knowledge and thus leads to more comprehensible models in comparison with other ways of knowledge acquisition with machine learning from examples. Whereas the ABML learning method has been described elsewhere, in this paper we concentrate on detailed mechanisms of the ABML knowledge refinement loop. We illustrate these mechanisms with examples from a case study in the acquisition of neurological knowledge, and provide quantitative results that demonstrate how the model evolving through the ABML loop becomes increasingly more consistent with the expert's knowledge during the process. \u00a9 2012 Springer-Verlag.", "Language": "en", "Citations": "5"},
{"Title": "Smartphones for large-scale behavior change interventions", "Authors": ["Lathia N.", "Pejovic V.", "Rachuri K.K.", "Mascolo C.", "Musolesi M.", "Rentfrow P.J."], "Keywords": ["Behavior change interventions", "Distributed systems", "Pervasive computing", "Smartphone sensing", "Social and behavioral sciences", "Social psychology", "User/machine systems"], "Date": "2013", "Abstract": "Equipped with cutting-edge sensing technology and high-end processors, smartphones can unobtrusively sense human behavior and deliver feedback and behavioral therapy. The authors discuss two applications for behavioral monitoring and change and present UBhave, the first holistic platform for large-scale digital behavior change intervention. \u00a9 2002-2012 IEEE.", "Language": "en", "Citations": "108"},
{"Title": "Relationship between particulate matter pollution and acute coronary syndrome incidence", "Authors": ["Ravljen M.", "Hovelja T.", "Vavpotic D."], "Keywords": ["Air pollution", "Lag effect", "Morbidity", "Myocardial infarction", "PM10"], "Date": "2019", "Abstract": "(1) Background: In recent decades, studies have reported on the increased cardiovascular risk associated with increased levels of air pollutants, especially particulate matters (PM). It remains unclear whether the specific subgroups share the same involvement and whether the effect is delayed. (2) Methods: Data for acute coronary syndrome (ACS) incidences from 2008 to 2011 were gathered in two major medical centres in Slovenia. A time series analysis was conducted in which daily ACS incidence data were linked with daily concentrations of PM", "Language": "en", "Citations": "0"},
{"Title": "An OpenCL library for parallel random number generators", "Authors": ["Ciglaric T.", "Cesnovar R.", "Strumbelj E."], "Keywords": ["GPU", "OpenCL", "Parallelization", "PractRand", "Pseudo-random number generation", "TestU01"], "Date": "2019", "Abstract": "We present a library of 22 pseudo-random number generators on the GPU. The library is implemented in OpenCL and all generators are tested using the TestU01 and PractRand libraries. We evaluated the efficiency of all generators on five different computing devices. Among the generators that pass all tests, Tyche-i was the fastest on most devices and on average. Tyche-i and several other generators from our library can be used to generate random numbers several times faster than generators from existing libraries.", "Language": "en", "Citations": "0"},
{"Title": "The ternary quantum-dot cell and ternary logic", "Authors": ["Lebar Bajec I.", "Zimic N.", "Mraz M."], "Keywords": [], "Date": "2006", "Abstract": "Quantum-dot cellular automata (QCAs) are increasingly becoming one of the most promising candidates for the alternative processing platform of the future. Since their advent in the early 1990s the required technological processes, as well as the QCA structures that implement the basic and functionally complete set of binary logic functions, have been developed. This paper, however, presents an extension of the (standard) binary quantum-dot cell that is focused on the enrichment of the cell's processing capabilities. It is shown that the newly introduced ternary quantum-dot cell can be used to represent three logic values and that only minor modifications of the corresponding binary QCA structures are required to implement the functionally complete set of \u0141ukasiewicz ternary logic functions. \u00a9 2006 IOP Publishing Ltd.", "Language": "en", "Citations": "27"},
{"Title": "High-dimensional feature matching: Employing the concept of meaningful nearest neighbors", "Authors": ["Omercevic D.", "Drbohlav O.", "Leonardis A."], "Keywords": [], "Date": "2007", "Abstract": "Matching of high-dimensional features using nearest neighbors search is an important part of image matching methods which are based on local invariant features. In this work we highlight effects pertinent to high-dimensional spaces that are significant for matching, yet have not been explicitly accounted for in previous work. In our approach, we require every nearest neighbor to be meaningful, that is, sufficiently close to a query feature such that it is an outlier to a background feature distribution. We estimate the background feature distribution from the extended neighborhood of a query feature given by its k nearest neighbors. Based on the concept of meaningful nearest neighbors, we develop a novel high-dimensional feature matching method and evaluate its performance by conducting image matching on two challenging image data sets. A superior performance in terms of accuracy is shown in comparison to several state-of-the-art approaches. Additionally, to make search for k nearest neighbors more efficient, we develop a novel approximate nearest neighbors search method based on sparse coding with an overcomplete basis set that provides a ten-fold speed-up over an exhaustive search even for high dimensional spaces and retains excellent approximation to an exact nearest neighbors search. \u00a92007 IEEE.", "Language": "en", "Citations": "33"},
{"Title": "Dataflow processing of matrices and vectors: Experimental analysis", "Authors": ["Mihelic J.", "Cibej U."], "Keywords": ["algorithm", "choreography", "dataflow", "evaluation", "experiment", "matrix"], "Date": "2017", "Abstract": "In this paper we focus on algorithms that extend classical control-flow computation with a dataflow computing paradigm. In particular, our goal is to experimentally explore various dataflow techniques and features, which enable the acceleration of algorithms. One of the most important challenges in designing a dataflow algorithm is to determine the data choreography resulting the running-time performance as good as possible. In the paper, our subject of interest are the algorithms that use matrices and vectors as an underlaying data structure. We discuss several variants of data choreographies for streaming of matrices. We also present the results of experimental evaluation of using these choreographies on several matrix-related problems.", "Language": "en", "Citations": "0"},
{"Title": "An approach to improve the information system development process by using a heuristics for business process improvement Ogrodje za izbolj\u0161anje procesov razvoja informacijskih sistemov z uporabo hevristik za izbolj\u0161ave splo\u0161nih poslovnih procesov", "Authors": ["Kojic A.", "Hovelja T.", "Vavpoti D."], "Keywords": ["Case study", "Evaluation models", "Heuristics", "Information systems development"], "Date": "2016", "Abstract": "The paper presents an approach to improve the information system development process by applying a heuristics for the general business process improvement on the information system development process. The developed comprehensive approach helps enterprises in their selecting an appropriate heuristics to improve their information system development process. The approach is tested in a case study performed in an enterprise developing information systems.", "Language": "en", "Citations": "3"},
{"Title": "Exercise-induced effects on a gym atmosphere", "Authors": ["Zitnik M.", "Bucar K.", "Hiti B.", "Barba Z.", "Rupnik Z.", "Zaloznik A.", "Zitnik E.", "Rodriguez L.", "Mihevc I.", "Zibert J."], "Keywords": ["Indoor air quality", "Particulate matter", "Perspiration", "Physical exercises", "PM 10", "Temporal resolution"], "Date": "2016", "Abstract": "We report results of analysis of a month-long measurement of indoor air and environment quality parameters in one gym during sporting activities such as football, basketball, volleyball, badminton, boxing, and fitness. We have determined an average single person's contribution to the increase of temperature, humidity, and dust concentration in the gym air volume of 12500 m", "Language": "en", "Citations": "8"},
{"Title": "Applying practices of extreme programming", "Authors": ["Hericko M.", "Juric M.B.", "Rostaher M.", "Repinc P."], "Keywords": ["Agile processes", "Extreme programming", "Software methodologies", "XP practices"], "Date": "2003", "Abstract": "This paper presents the results of research that was conducted to investigate how well individual XP practices are recognized and applied in real projects. Refactoring was recognized as an XP practice that is the most difficult to implement. An empirical experiment was carried out to quantify shares of programming activities such as testing, refactoring and adding new functionalities. A simple tool was developed to automate the activity tracking needed to gather quantitative data.", "Language": "en", "Citations": "0"},
{"Title": "Gravitational clustering of the self-organizing map", "Authors": ["Ilc N.", "Dobnikar A."], "Keywords": ["clustering", "data analysis", "gravitational clustering", "self-organizing map", "two-level approach"], "Date": "2011", "Abstract": "Data clustering is the fundamental data analysis method, widely used for solving problems in the field of machine learning. Numerous clustering algorithms exist, based on various theories and approaches, one of them being the well-known Kohonen's self-organizing map (SOM). Unfortunately, after training the SOM there is no explicitly obtained information about clusters in the underlying data, so another technique for grouping SOM units has to be applied afterwards. In this paper, a contribution towards clustering of the SOM is presented, employing principles of Gravitational Law. On the first level of the proposed algorithm, SOM is trained on the input data and prototypes are extracted. On the second level, each prototype acts as a unit-mass point in a feature space, in which presence of gravitational force is simulated, exploiting information about connectivity gained on the first level. The proposed approach is capable of discovering complex cluster shapes, not only limited to the spherical ones, and is able to automatically determine the number of clusters. Experiments with synthetic and real data are conducted to show performance of the presented method in comparison with other clustering techniques. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "4"},
{"Title": "Standardizing tweets with character-level machine translation", "Authors": ["Ljubesic N.", "Erjavec T.", "Fiser D."], "Keywords": ["character-level machine translation", "standardization", "twitterese"], "Date": "2014", "Abstract": "This paper presents the results of the standardization procedure of Slovene tweets that are full of colloquial, dialectal and foreign-language elements. With the aim of minimizing the human input required we produced a manually normalized lexicon of the most salient out-of-vocabulary (OOV) tokens and used it to train a character-level statistical machine translation system (CSMT). Best results were obtained by combining the manually constructed lexicon and CSMT as fallback with an overall improvement of 9.9% increase on all tokens and 31.3% on OOV tokens. Manual preparation of data in a lexicon manner has proven to be more efficient than normalizing running text for the task at hand. Finally we performed an extrinsic evaluation where we automatically lemmatized the test corpus taking as input either original or automatically standardized wordforms, and achieved 75.1% per-token accuracy with the former and 83.6% with the latter, thus demonstrating that standardization has significant benefits for upstream processing. \u00a9 2014 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "12"},
{"Title": "Efficient feature distribution for object matching in visual-sensor networks", "Authors": ["Sulic V.", "Pers J.", "Kristan M.", "Kovacic S."], "Keywords": ["Computer vision", "distributed systems", "object matching", "visual-sensor networks"], "Date": "2011", "Abstract": "In this paper, we propose a framework of hierarchical feature distribution for object matching in a network of visual sensors. In our approach, we hierarchically distribute the information in such a way that each individual node maintains only a small amount of information about the objects seen by the network. Nevertheless, this amount is sufficient to efficiently route queries through the network without any degradation of the matching performance. A set of requirements that have to be fulfilled by the object-matching method to be used in such a framework is defined. We provide examples of mapping four well-known, object-matching methods to a hierarchical feature-distribution scheme. The proposed approach was tested on a standard COIL-100 image database and in a basic surveillance scenario using our own distributed network simulator. The results show that the amount of data transmitted through the network can be significantly reduced in comparison to naive feature-distribution schemes such as flooding. \u00a9 2011 IEEE.", "Language": "en", "Citations": "10"},
{"Title": "Acceleration of information-theoretic data analysis with graphics processing units Przyspieszenie analizy danych opartej o teori{ogonek}e informacji przy pomocy programowalnych kart graficznych)", "Authors": ["Sluga D.", "Curk T.", "Zupan B.", "Lotric U."], "Keywords": ["CUDA", "Data mining", "Information-theoretic measures", "Parallelization"], "Date": "2012", "Abstract": "Abstract. Information-theoretic measures are frequently employed to assess the degree of feature interactions when mining attribute-value data sets. For large data sets, obtaining these measures quickly poses an unmanageable computational burden. In this work we examine the applicability of consumer graphics processing units supporting CUDA architecture to speed-up the computation of information-theoretic measures. Our implementation was tested on a variety of data sets, and compared with the performance of sequential algorithms running on the central processing unit.", "Language": "en", "Citations": "0"},
{"Title": "Prosodic and phonetic features for speaker clustering in speaker diarization systems", "Authors": ["Zibert J.", "Mihelic F."], "Keywords": ["Prosodic features", "Speaker clustering", "Speaker diarization"], "Date": "2011", "Abstract": "This work is focused on speaker clustering methods that are used in speaker diarization systems. The purpose of speaker clustering is to associate together segments that belong to the same speaker and is usually applied in the last stage of the speaker-diarization process. We concentrate on developing proper representations of speaker segments for clustering. We realize two different speaker clustering systems. The first is a standard approach using a bottom-up agglomerative clustering principle with the Bayesian Information Criterion as a merging criterion. In the second system we developed a fusionbased speaker-clustering, where speaker segments are modeled by acoustic and prosodic representations. In this way we additionally model the speaker prosodic and phonetic characteristics and combine them with the basic acoustic information of speakers. This leads to improved clustering of the segments in the case of similar speaker acoustic properties and poor acoustic conditions. Copyright \u00a9 2011 ISCA.", "Language": "en", "Citations": "2"},
{"Title": "On maximal distances in a commuting graph", "Authors": ["Dolinar G.", "Kuzma B.", "Oblak P."], "Keywords": ["Algebraically closed field", "Centralizer", "Commuting graph", "Distance in graphs", "Matrix algebra"], "Date": "2012", "Abstract": "It is shown that matrices over algebraically closed fields that are farthest apart in the commuting graph must be non-derogatory. Rank-one matrices and diagonalizable matrices are also characterized in terms of the commuting graph.", "Language": "en", "Citations": "10"},
{"Title": "A computerized support tool for conducting a scrum-based software engineering capstone course", "Authors": ["Mahnic V.", "Casar A."], "Keywords": ["Agile methods", "Capstone course", "Effort estimation", "Project management", "Scrum", "Software engineering education"], "Date": "2016", "Abstract": "A software engineering capstone course is often used for the introduction of agile methods like Scrum. Apart from exposing students to state-of-the-art topics, the capstone course also enables teachers to use modern ways of teaching through practical problem solving and gives researchers opportunities to conduct empirical studies with students as subjects. In order to satisfy the needs of all parties involved, a good computerized support tool is needed. The students need such a tool to manage their projects, the teachers require instruments for maintaining project requirements and monitoring student progress, while the researchers are interested in data for evidence-driven assessment of the development process. In this paper, an example of such a tool that was developed to support a Scrum-based software engineering capstone course is described. The course design, which requires students to develop a quasi-real project, is described first. Following this, a step-by-step description of the course execution is provided and the tool support of each step is illustrated. Finally, the opinions of 57 students obtained through an anonymous survey after using the tool for the first time are analyzed. The students found the tool intuitive and easy to use, providing good visualization of the project progress and making the execution of their projects simpler and more efficient. The tool gives directions on how their collaboration should proceed and prevents them from exploring their projects blindly. By visualizing the development process, it helps all parties involved to know what each team member is doing, thus preventing procrastination and \"free-rider\" syndrome.", "Language": "en", "Citations": "8"},
{"Title": "Group detection in complex networks: An algorithm and comparison of the state of the art", "Authors": ["Subelj L.", "Bajec M."], "Keywords": ["Clustering", "Complex networks", "Group detection", "Hierarchy discovery", "Label propagation"], "Date": "2014", "Abstract": "Complex real-world networks commonly reveal characteristic groups of nodes like communities and modules. These are of value in various applications, especially in the case of large social and information networks. However, while numerous community detection techniques have been presented in the literature, approaches for other groups of nodes are relatively rare and often limited in some way. We present a simple propagation-based algorithm for general group detection that requires no a priori knowledge and has near ideal complexity. The main novelty here is that different types of groups are revealed through an adequate hierarchical group refinement procedure. The proposed algorithm is validated on various synthetic and real-world networks, and rigorously compared against twelve other state-of-the-art approaches on group detection, hierarchy discovery and link prediction tasks. The algorithm is comparable to the state of the art in community detection, while superior in general group detection and link prediction. Based on the comparison, we also discuss some prominent directions for future work on group detection in complex networks. \u00a9 2013 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "17"},
{"Title": "Visual Object Tracking Performance Measures Revisited", "Authors": ["Cehovin L.", "Leonardis A.", "Kristan M."], "Keywords": ["experimental evaluation", "performance evaluation", "performance measures", "visual object tracking"], "Date": "2016", "Abstract": "The problem of visual tracking evaluation is sporting a large variety of performance measures, and largely suffers from lack of consensus about which measures should be used in experiments. This makes the cross-paper tracker comparison difficult. Furthermore, as some measures may be less effective than others, the tracking results may be skewed or biased toward particular tracking aspects. In this paper, we revisit the popular performance measures and tracker performance visualizations and analyze them theoretically and experimentally. We show that several measures are equivalent from the point of information they provide for tracker comparison and, crucially, that some are more brittle than the others. Based on our analysis, we narrow down the set of potential measures to only two complementary ones, describing accuracy and robustness, thus pushing toward homogenization of the tracker evaluation methodology. These two measures can be intuitively interpreted and visualized and have been employed by the recent visual object tracking challenges as the foundation for the evaluation methodology.", "Language": "en", "Citations": "62"},
{"Title": "Zhao-Atlas-Marks representation of speech signals Zhao-Atlas-Marksova predstavitev govornih signalov", "Authors": ["Zibert J.", "Mihelic F."], "Keywords": ["Acoustic speech features", "Speech recognition", "Speech signal processing", "Time-frequency representations", "Zhao-Atlas-Marks distribution"], "Date": "2002", "Abstract": "Joint time-frequency signal representations characterize signals over lime-frequency plane. They combine time and frequency domain analyzes to yield a potentially more revealing picture of the temporal localization of the signal spectral components. The paper reviews two basic time-frequency representations, spectrogram and Zhao-Adas-Marks distribution (cone-shaped kernel distribution). They are members of the Cohen's class of quadratic time-frequency representations. In this paper the spectrogram and Zhao-Alias-Marks distribution are applied to speech signals. We study, analyze and compare behavior, properties and effects of the two quadratic representations on speech signals. The Zhao-Atlas-Marks distribution having been found to possess several useful properties for the speech signal analysis, it was successfully applied in our speech recognition system for speech features extraction. We propose a new modified method for speech features extracting based on met-frequency cepstral coefficients with the use of different time-frequency distributions. We also study dynamic features that we modelled from basic features obtained with the new method. We evaluate several estimates of the time derivatives obtained with our basic measurements. The first and second time derivatives are approximated with regression coefficients, coefficients of the arc-Lan function, and functions sine and cosine. Analyzes and tests are made for different sets of speech features obtained from the spectrogram and Zhao-Atlas-Marks distribution using a speech recognition system built with hidden Markov models. Results of our investigation m speech recognition show a high efficiency level of speech features derived from the Zhao-Atlas-Marks distribution with addition of the lime derivatives modelled with coefficients of functions sine and cosine.", "Language": "en", "Citations": "4"},
{"Title": "Deep ear recognition pipeline", "Authors": ["Emersic Z.", "Krizaj J.", "Struc V.", "Peer P."], "Keywords": [], "Date": "2019", "Abstract": "Ear recognition has seen multiple improvements in recent years and still remains very active today. However, it has been approached from recognition and detection perspective separately. Furthermore, deep-learning-based approaches that are popular in other domains have seen limited use in ear recognition and even more so in ear detection. Moreover, to obtain a usable recognition system a unified pipeline is needed. The input in such system should be plain images of subjects and the output identities based only on ear biometrics. We conduct separate analysis through detection and identification experiments on the challenging dataset and, using the best approaches, present a novel, unified pipeline. The pipeline is based on convolutional neural networks (CNN) and presents, to the best of our knowledge, the first CNN-based ear recognition pipeline. The pipeline incorporates both, the detection of ears on arbitrary images of people, as well as recognition on these segmented ear regions. The experiments show that the presented system is a state-of-the-art system and, thus, a good foundation for future real-word ear recognition systems.", "Language": "en", "Citations": "0"},
{"Title": "Real-time ray casting of volumetric data", "Authors": ["Lesar Z."], "Keywords": [], "Date": "2015", "Abstract": "In this paper we present acceleration structures and techniques for real-time ray casting of large volumetric data sets, such as those obtained by CT or MRI scans. The techniques used include adaptive sampling and sparse casting. To improve rendering quality we use the regula falsi method and Monte-Carlo ambient occlusion estimation. To enhance the final rendering we use visual effects-screen-space ambient occlusion and depth of field. The algorithms have been parallelized with OpenCL. We compare the results obtained with different methods-isosurface extraction, maximum intensity projection and alpha compositing. We tested our methods on medical data sets, specifically angiograms.", "Language": "en", "Citations": "1"},
{"Title": "The turn after the spatial turn: The artistic research perspective Obrat po prostorskem obratu: Umetni\u0161koraziskovalni pristop", "Authors": ["Vaupotic A.", "Bovcon N."], "Keywords": ["Bakhtin", "Charles sanders", "Foucault", "Humanities", "Michel", "Mikhail", "Narrativization", "Networks", "New media", "Peirce", "Semiotics", "Spatial tum", "Theory of discourse", "Virtual space"], "Date": "2013", "Abstract": "This article addresses the consequences of the methodological approach of the \"spatial turn,\" whose \"object\" of research is the dispersion of atomic elements on a discursive surface. The theory of discourse developed by Mikhail Bakhtin and Michel Foucault is one of the centers of this methodological field. The research focused on spatial relations-in real spaces and in the spatialization of conceptual dispersions-brought about important insights; however, upon a closer look its limitations appear as well. Using the semiotics of Charles S. Peirce, the article asks whether the research on space and spatiality is a key to solving the diverse questions in the humanities and social sciences, or is the next \"turn\" already in view. In the second part, the article presents a model of artistic research on the problem of spatiality in the context of the changes brought about by the development of electronic and digital information technologies.", "Language": "en", "Citations": "0"},
{"Title": "Web services are not sufficient Spletne storitve niso dovolj", "Authors": ["Zrnec A."], "Keywords": ["Business process", "Electronic business", "Framework", "Mediator", "SOAP", "Web service", "WSMF", "XML"], "Date": "2004", "Abstract": "The current Web represents mainly a collection of information and doesn't yet provide support in processing this information. Recent standards, i.e. UDDI, WSDL and SOAP, try to raise the Web to a new level of service. Software programs can be accessed and executed via the internet based on the idea of Web services. A service can provide information (e.g. a weather forecast service), or it may have an effect in the real world (e.g. an on line flight-booking service). Web services can significantly increase the Web architecture's potential by providing a way of automated program communication, discovery of services and business process automation. The ultimate vision is that a program can use web services as support for its computation or processing. The program can discover web services and invoke them fully automated. If web services have a cost attached, the program knows when to search for a cheaper service and knows all the possible payment methods. In a business environment this translates into automatic cooperation between enterprises. Any enterprise requiring a business interaction with another enterprise can automatically discover and select appropriate optimal web services depending on selection policies. They can be invoked automatically and payment processes can be initiated. Any necessary mediation is applied based on data and process ontologies and the automatic translation of their concepts into each other. An example of an automatic business process would be supply chain relationships where an enterprise manufacturing goods has to frequently seek suppliers as well as buyers dynamically. Instead of employees constantly searching for suppliers and buyers, the web service infrastructure does it automatically within the defined constraints. Still, more work has to be done before the web service infrastructure can make this vision come true. The current technology around UDDI, WSDL, and SOAP pro vides limited support in mechanizing service recognition, service configuration and combination (i.e. realizing complex workflows and business logics with web services), service comparison and automated negotiation. Therefore, there are proposals such as WSFL that develops a language for describing complex web services. The Web Service Modeling Framework is a fully-fledged modeling framework for describing the various aspects related to web services. Fully enabled e-commerce based on workable web services requires a modeling framework that is centered around two complementary principles: \u2022 Strong de-coupling of the various components that realize an e-commerce application. \u2022 Strong mediation service enabling anybody to speak with everybody in a scalable manner.", "Language": "en", "Citations": "0"},
{"Title": "Selective Face Deidentification with End-to-End Perceptual Loss Learning", "Authors": ["Medcn B.", "Peer P.", "Struc V."], "Keywords": [], "Date": "2018", "Abstract": "Privacy is a highly debatable topic in the modern technological era. With the advent of massive video and image data (which in a lot of cases contains personal information on the recorded subjects), there is an imminent need for efficient privacy protection mechanisms. To this end, we develop in this work a novel Face Deidentification Network (FaDeNet) that is able to alter the input faces in such a way that automated recognition fail to recognize the subjects in the images, while this is still possible for human observers. FaDeNet is based an encoder-decoder architecture that is trained to auto-encode the input image, while (at the same time) minimizing the recognition performance of a secondary network that is used as an socalled identity critic in FaDeNet. We present experiments on the Radbound Faces Dataset and observe encouraging results.", "Language": "en", "Citations": "0"},
{"Title": "Mapping of marine meadows using multibeam sonar data Kartiranje morskih travnikov s podatki mnogosnopnega sonarja", "Authors": ["Moskon S.", "Zibert J.", "Kavsek B."], "Keywords": ["Hydrogeography", "Mapping of the seabed", "Marine meadows", "Multibeam sonar"], "Date": "2015", "Abstract": "This paper deals with the methodology of collecting acoustic data of the seabed and its use for mapping of marine meadows. The method enables fast and efficient mapping of large areas of marine meadows and provides continuous coverage throughout the region. In this paper we present various acoustic systems and their advantages and disadvantages for the mapping of the seabed. Furthermore we present acoustic data acquisition, acoustic data processing, calculation of the acoustic data features and machine learning methods used for mapping. The developed methodology was tested on real acoustic data, recorded in the Slovenian sea, in the area between Izola and Koper.", "Language": "en", "Citations": "2"},
{"Title": "Application of distributed SVM architectures in classifying forest data cover types", "Authors": ["Trebar M.", "Steele N."], "Keywords": ["Classification", "Distributed architecture", "Imbalanced data", "Support vector machine", "Training subsets"], "Date": "2008", "Abstract": "In many 'real-world' applications, a classification of large data sets, which are often also imbalanced, is difficult due to the small, but usually more interesting classes. In this study, a large data set, forest cover type classes, which is actually multi-class classification defined with seven imbalanced classes and used as a resource inventory information was analyzed and evaluated. The data set was transformed into seven new data sets and a support vector machine (SVM) was employed to solve a binary classification problem of balanced and imbalanced data sets with various sizes. In the two approaches considered, the use of distributed SVM architectures, which basically reduces the complexity of the quadratic optimization problem of very large data sets, and the use of two sampling approaches for classification of imbalanced data sets were combined and results presented. The experimental results of distributed SVM architectures show the improvement of the accuracy for larger data sets in comparison to a single SVM classifier and their ability to improve the correct classification of the minority class. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "27"},
{"Title": "Heuristic sampling for the subgraph isomorphism problem", "Authors": ["Cibej U.", "Mihelic J."], "Keywords": [], "Date": "2017", "Abstract": "Subgraph isomorphism is one of the fundamental search problems in computer science. In this article we consider the counting variation of this problem. The task is to count all instances of the pattern G occurring in a (usually larger) graph H. All algorithms for this problem use a variation of backtracking. Most commonly they assign one vertex of G to one vertex of H at each level of the search tree. The order of vertices for the assignment is the crucial factor determining the size of this search tree. But it is very hard to determine in advance the impact of the order for a particular instance. We use a method called heuristic sampling to estimate the size of the tree. We use this estimation to select the most suitable order of vertices of G which minimizes the expected tree size. This approach is empirically evaluated on a set of instances, showing the practical potential of the method.", "Language": "en", "Citations": "0"},
{"Title": "Modelling Lake Glums\u00f8 with Q2 learning", "Authors": ["Vladusic D.", "Kompare B.", "Bratko I."], "Keywords": ["Machine learning", "Qualitative modelling", "Qualitative reasoning"], "Date": "2006", "Abstract": "In this paper, we describe an application of Q", "Language": "en", "Citations": "4"},
{"Title": "A multi-attribute modelling approach to evaluate the efficient implementation of ICT in schools", "Authors": ["Campelj B.", "Karnet I.", "Brodnik A.", "Jereb E.", "Rajkovic U."], "Keywords": ["DEX methodology", "Multi-attribute decision-making", "School digitalization", "Self-assessment tool"], "Date": "2019", "Abstract": "Comprehensive implementation of Information and Communication Technologies in schools is a key factor in empowering students in the European Union (EU) for their future roles. The framework, DigCompOrg, was proposed in 2015 under the direction of the European Commission to encourage self-assessment within EU schools and to update the level of digitalization. This article presents a computer-supported model based on this framework and a multi-attribute decision-making methodology named, DEX. The model was built by a group of experts and tested on selected schools in Slovenia. The main advantages of the model are: the use of qualitative value scales for attributes which do not have exact values; the use of a hierarchical structure for attributes; a transparent presentation of the interconnectedness of these attributes; and the use of simple if\u2013then aggregation rules to allow the use of non-fixed weights. An application of our model to a selected school demonstrates the potential for knowledge modelling to facilitate upgrades of existing assessment tools and to provide a better understanding and analysis of assessment results.", "Language": "en", "Citations": "1"},
{"Title": "Interpreting and linguistic inclusion\u2013friends or foes? results from a field study", "Authors": ["K. Pokorn N.", "Cibej J."], "Keywords": ["asylum seekers", "community interpreting", "integration", "linguistic inclusion", "public service interpreting and translation", "Translational regime"], "Date": "2018", "Abstract": "The article responds to the existing political claims that translation and interpreting reduce the incentive of recent immigrants to learn the language(s) of the host country and thereby impede their integration. To verify these claims, quantitative and qualitative research was conducted among asylum seekers in Slovenia, i.e. a group of recent immigrants who have access to free interpreting and translation services and free courses in the dominant language of the host country. A questionnaire was used to gather quantitative data on the language profiles of 127 current and former residents of the asylum seeker centres in Slovenia, while qualitative data were obtained through semi-structured interviews conducted with a representative group of 38 asylum seekers. The results show that all surveyed migrants had a positive attitude towards the host country language and that all of the interviewed migrants who had been in the host country for 7\u00a0months or more, regardless of their educational attainment, also took the state-funded course of the host country language. Additionally, although the provision of translation and interpreting is recognised as essential in high-risk situations, it is not the preferred communication strategy of the migrants, and therefore does not hinder their functional linguistic inclusion.", "Language": "en", "Citations": "4"},
{"Title": "Self-supervised online learning of basic object push affordances", "Authors": ["Ridge B.", "Leonardis A.", "Ude A.", "Denisa M.", "Skocaj D."], "Keywords": ["Affordances", "Cognitive and Developmental Robotics", "Online Learning", "Self-supervised Learning"], "Date": "2015", "Abstract": "Continuous learning of object affordances in a cognitive robot is a challenging problem, the solution to which arguably requires a developmental approach. In this paper, we describe scenarios where robotic systems interact with household objects by pushing them using robot arms while observing the scene with cameras, and which must incrementally learn, without external supervision, both the effect classes that emerge from these interactions as well as a discriminative model for predicting them from object properties. We formalize the scenario as a multi-view learning problem where data co-occur over two separate data views over time, and we present an online learning framework that uses a self-supervised form of learning vector quantization to build the discriminative model. In various experiments, we demonstrate the effectiveness of this approach in comparison with related supervised methods using data from experiments performed using two different robotic platforms.", "Language": "en", "Citations": "6"},
{"Title": "Configurations of cycles and the apollonius problem", "Authors": ["Zlobec B.J.", "Kosta N.M."], "Keywords": [], "Date": "2001", "Abstract": "Given n+1 spheres andpla nes of dimension n\u22121 in R", "Language": "en", "Citations": "1"},
{"Title": "Search versus knowledge revisited again", "Authors": ["Sadikov A.", "Bratko I."], "Keywords": [], "Date": "2007", "Abstract": "The questions focusing on diminishing returns for additional search effort have been a burning issue in computer chess. Despite a lot of research in this field, there are still some open questions, e.g., what happens at search depths beyond 12 plies, and what is the effect of the program's knowledge on diminishing returns? The paper presents an experiment that attempts to answer these questions. The results (a) confirm that diminishing returns in chess exist, and more importantly (b) show that the amount of knowledge a program has influences when diminishing returns will start to manifest themselves. \u00a9 Springer-Verlag Berlin Heidelberg 2007.", "Language": "en", "Citations": "1"},
{"Title": "Comparison of frameworks and tools for test-driven development", "Authors": ["Pancur M.", "Ciglaric M.", "Trampus M.", "Vidmar T."], "Keywords": ["Agile Methods", "Software Tools", "Test-Driven Development (TDD)", "Unit-testing Frameworks and Extensions"], "Date": "2003", "Abstract": "Test-Driven Development (TDD) is one of the core practices in increasingly popular agile software development methodologies (e.g. Extreme Programming - XP). In TDD, automated tests are the driving force in development and they are always written prior to the code they test. Since tests are so important, their execution must be automated and run as often as possible. In this paper, we discuss and compare different frameworks, framework extensions and tools for TDD on the rival platforms: Sun's Java and Microsoft's .NET. We look at the current Open Source and commercial integrated development environments (IDEs) for both platforms and evaluate their readiness for TDD style development. We provide recommendations and links to resources for tools and unit-testing frameworks that worked best for us, were stable and relatively bug free.", "Language": "en", "Citations": "2"},
{"Title": "Quadratic mutual information feature selection", "Authors": ["Sluga D.", "Lotric U."], "Keywords": ["Cauchy-Schwarz divergence", "Feature selection", "Information-theoretic measures", "Quadratic mutual information"], "Date": "2017", "Abstract": "We propose a novel feature selection method based on quadratic mutual information which has its roots in Cauchy-Schwarz divergence and Renyi entropy. The method uses the direct estimation of quadratic mutual information from data samples using Gaussian kernel functions, and can detect second order non-linear relations. Its main advantages are: (i) unified analysis of discrete and continuous data, excluding any discretization; and (ii) its parameter-free design. The effectiveness of the proposed method is demonstrated through an extensive comparison with mutual information feature selection (MIFS), minimum redundancy maximum relevance (MRMR), and joint mutual information (JMI) on classification and regression problem domains. The experiments show that proposed method performs comparably to the other methods when applied to classification problems, except it is considerably faster. In the case of regression, it compares favourably to the others, but is slower.", "Language": "en", "Citations": "4"},
{"Title": "Skill modeling through symbolic reconstruction of operator's trajectories", "Authors": ["Suc D.", "Bratko I."], "Keywords": [], "Date": "2000", "Abstract": "Controlling a complex dynamic system, such as a plane or a crane, usually requires a skilled operator. Such control skill is typically hard to reconstruct through introspection. Therefore an attractive approach to the reconstruction of control skill involves machine learning from operator's control traces, also known as behavioral cloning. In the most common approach to behavioral cloning, a controller is induced as a direct mapping from system states to actions. Unfortunately, such controllers usually suffer from lack of robustness and lack typical elements of human control strategies, such as subgoals and substages of the control plan. In this paper, we investigate a novel approach. We apply the GoldHorn program to induce from the operator's trajectories a set of symbolic constraints. These are then used together with a locally weighted regression model to determine the next action. Using the Acrobot problem in a case study, this approach showed significant improvements both in terms of control performance and transparency of induced clones.", "Language": "en", "Citations": "14"},
{"Title": "Assessment of surveys for the management of hospital clinical pharmacy services", "Authors": ["Cufar A.", "Mrhar A.", "Robnik-Sikonja M."], "Keywords": ["Clinical pharmacy", "Human resource management", "Kano model", "OrdEval algorithm", "Survey analysis"], "Date": "2015", "Abstract": "Objective: Survey data sets are important sources of data, and their successful exploitation is of key importance for informed policy decision-making. We present how a survey analysis approach initially developed for customer satisfaction research in marketing can be adapted for an introduction of clinical pharmacy services into a hospital. Methods and material: We use a data mining analytical approach to extract relevant managerial consequences. We evaluate the importance of competences for users of a clinical pharmacy with the OrdEval algorithm and determine their nature according to the users' expectations. For this, we need substantially fewer questions than are required by the Kano approach. Results: From 52 clinical pharmacy activities we were able to identify seven activities with a substantial negative impact (i.e., negative reinforcement) on the overall satisfaction of clinical pharmacy services, and two activities with a strong positive impact (upward reinforcement). Using analysis of individual feature values, we identified six performance, 10 excitement, and one basic clinical pharmacists' activity. Conclusions: We show how the OrdEval algorithm can exploit the information hidden in the ordering of class and attribute values, and their inherent correlation using a small sample of highly relevant respondents. The visualization of the outputs turns out highly useful in our clinical pharmacy research case study.", "Language": "en", "Citations": "4"},
{"Title": "Framework for the Delivery of Information System Due Diligence", "Authors": ["Delak B.", "Bajec M."], "Keywords": ["decision model", "Information system due diligence", "information system quality", "information system risks"], "Date": "2013", "Abstract": "The IS field lacks a scientifically based analytical tool for delivering IS due diligence. Due Diligence is the activity of identifying and measuring the risks and increasing the likelihood of productive investment. In this article, we propose a framework for IS due diligence development based on our generalized experiences conducting IS due diligence in more than 60 banks and financial organizations in Europe. \u00a9 2013 Copyright Taylor & Francis Group, LLC.", "Language": "en", "Citations": "7"},
{"Title": "SWT voting-based color reduction for text detection in natural scene images", "Authors": ["Ikica A.", "Peer P."], "Keywords": ["Color reduction", "CVL OCR DB", "Natural scene images", "Stroke width transform", "SWT", "SWT direction determination", "SWT profiles", "SWT voting", "Text detection"], "Date": "2013", "Abstract": "In this article, we propose a novel stroke width transform (SWT) voting-based color reduction method for detecting text in natural scene images. Unlike other text detection approaches that mostly rely on either text structure or color, the proposed method combines both by supervising text-oriented color reduction process with additional SWT information. SWT pixels mapped to color space vote in favor of the color they correspond to. Colors receiving high SWT vote most likely belong to text areas and are blocked from being mean-shifted away. Literature does not explicitly address SWT search direction issue; thus, we propose an adaptive sub-block method for determining correct SWT direction. Both SWT voting-based color reduction and SWT direction determination methods are evaluated on binary (text/non-text) images obtained from a challenging Computer Vision Lab optical character recognition database. SWT voting-based color reduction method outperforms the state-of-the-art text-oriented color reduction approach. \u00a9 2013 Ikica and Peer; licensee Springer.", "Language": "en", "Citations": "4"},
{"Title": "Investigating the role of task engagement in mobile interruptibility", "Authors": ["Pejovic V.", "Musolesi M.", "Mehrotra A."], "Keywords": ["Context-aware computing", "Interruptibility", "Multitasking", "Notifications"], "Date": "2015", "Abstract": "Context-awareness of mobile phones is a cornerstone of recent efforts in automatic determination of user interruptibility. Modalities such as a user's location, her physical activity, time of day, can be used in machine learning models to infer if a user is going to welcome an incoming notification or not. However, the success of context-aware interruptibility systems questions the existing theory of interruptibility, that is based on the internal state of the user, not her surroundings. In this work we examine the role of a user's internal context, defined by her engagement in the current task, on the sentiment towards an interrupting mobile notification. We collect and analyse real-world data on interruptibility of twenty subjects over two weeks, and show that the internal state indeed impacts user interruptibility.", "Language": "en", "Citations": "13"},
{"Title": "Computer vision in contemporary art (introduction to the special session)", "Authors": ["Peer P.", "Batagelj B."], "Keywords": ["Depth recovery", "Face detection", "Human-computer interaction", "Social impact", "Tracking", "Transformations", "Virtual world"], "Date": "2008", "Abstract": "Contemporary art nowadays tries to exploit modern technology to better address and enlighten specific problems and ideas of our time. Our interest in wider impact of modern technology on society and the interest in contemporary art, brought our attention also to the applicative field of use of computer vision methods in art. The paper walks us through a few projects, proving that art is definitely a perfect testbed for our research: 15 seconds of fame, Virtual skiing, Dynamic anamorphosis and Virtual painter, from face detection, motion following, depth recovery, touchless human-computer interaction to pop-art, constant eye gaze of a person on the portrait, regardless of where the spectator stands, immersion into different virtual worlds without the need for any special equipment.", "Language": "en", "Citations": "0"},
{"Title": "Spatially-Adaptive Filter Units for Deep Neural Networks", "Authors": ["Tabemik D.", "Kristan M.", "Leonardis A."], "Keywords": [], "Date": "2018", "Abstract": "Classical deep convolutional networks increase receptive field size by either gradual resolution reduction or application of hand-crafted dilated convolutions to prevent increase in the number of parameters. In this paper we propose a novel displaced aggregation unit (DAU) that does not require hand-crafting. In contrast to classical filters with units (pixels) placed on a fixed regular grid, the displacement of the DAUs are learned, which enables filters to spatially-adapt their receptive field to a given problem. We extensively demonstrate the strength of DAUs on a classification and semantic segmentation tasks. Compared to ConvNets with regular filter, ConvNets with DAUs achieve comparable performance at faster convergence and up to 3-times reduction in parameters. Furthermore, DAUs allow us to study deep networks from novel perspectives. We study spatial distributions of DAU filters and analyze the number of parameters allocated for spatial coverage in a filter.", "Language": "en", "Citations": "0"},
{"Title": "Evaluation of prognostic factors and prediction of chronic wound healing rate by machine learning tools", "Authors": ["Robnik-Sikonja M.", "Cukjati D.", "Kononenko I."], "Keywords": [], "Date": "2001", "Abstract": "In more than a decade of clinical use of electrical stimulation to accelerate the chronic wound healing each patient and wound were registered and a wound healing process was weekly followed. The controlled study involved a conventional conservative treatment, sham treatment, biphasic pulsed current, and direct current electrical stimulation. A quantity of available data suffices for an analysis with machine learning methods. So far only a limited number of studies have investigated the wound and patient attributes which affect the chronic wound healing. There is none to our knowledge to include the treatment attributes. The aims of our study are to determine effects of the wound, patient and treatment attributes on the wound healing process and to propose a system for prediction of the wound healing rate. In the first step of our analysis we determined which wound and patient attributes play a predominant role in the wound healing process. Then we investigated a possibility to predict the wound healing rate at the beginning of the treatment based on the initial wound, patient and treatment attributes. Finally we discussed the possibility to enhance the wound healing rate prediction accuracy by predicting it after a few weeks of the wound healing follow-up. By using the attribute estimation algorithms ReliefF and RReliefF we obtained a ranking of the prognostic factors which was comprehensible to field experts. We also used regression and classification trees to build models for prediction of the wound healing rate. The obtained results are encouraging and may form a basis of an expert system for the chronic wound healing rate prediction. If the wound healing rate is known, then the provided information can help to formulate the appropriate treatment decisions and orient resources to those individuals with poor prognosis.", "Language": "en", "Citations": "2"},
{"Title": "VillageShare: Facilitating content generation and sharing in rural networks", "Authors": ["Johnson D.L.", "Pejovic V.", "Belding E.M.", "Van Stam G."], "Keywords": ["developing regions", "localization", "rural network", "social network", "traffic analysis"], "Date": "2012", "Abstract": "While broadband Internet connectivity has reached a significant part of the world's population, those living in rural areas of the developing world suffer from poor Internet connectivity over slow long distance links, if they even have connectivity at all. While this has a general negative impact on Internet utilization, our social survey of users in the community of Macha, Zambia shows that the severest impact is in the area of content generation and sharing. To this end, our work describes VillageShare, an integrated time-delayed proxy server and content-sharing Facebook application. Through these two components, VillageShare facilitates localization of traffic, protecting the bandwidth-limited Internet link from content shared between local users, and minimizes upload abortions by time-shifting large uploads to periods when the gateway link is under-utilized. In this work we analyze traffic traces from Macha to discern opportunities for improvement of connection utilization, and then describe and evaluate the VillageShare architecture. \u00a9 2012 ACM.", "Language": "en", "Citations": "14"},
{"Title": "Combining learning constraints and numerical regression", "Authors": ["Suc D.", "Bratko I."], "Keywords": [], "Date": "2005", "Abstract": "Usual numerical learning methods are primarily concerned with finding a good numerical fit to data and often make predictions that do not correspond to qualitative laws in the domain of modelling or expert intuition. In contrast, the idea of Q", "Language": "en", "Citations": "0"},
{"Title": "Probabilistic segmentation of folk music recordings", "Authors": ["Bohak C.", "Marolt M."], "Keywords": [], "Date": "2016", "Abstract": "The paper presents a novel method for automatic segmentation of folk music field recordings. The method is based on a distance measure that uses dynamic time warping to cope with tempo variations and a dynamic programming approach to handle pitch drifting for finding similarities and estimating the length of repeating segment. A probabilistic framework based on HMM is used to find segment boundaries, searching for optimal match between the expected segment length, between-segment similarities, and likely locations of segment beginnings. Evaluation of several current state-of-the-art approaches for segmentation of commercial music is presented and their weaknesses when dealing with folk music are exposed, such as intolerance to pitch drift and variable tempo. The proposed method is evaluated and its performance analyzed on a collection of 206 folk songs of different ensemble types: solo, two- and three-voiced, choir, instrumental, and instrumental with singing. It outperforms current commercial music segmentation methods for noninstrumental music and is on a par with the best for instrumental recordings. The method is also comparable to a more specialized method for segmentation of solo singing folk music recordings.", "Language": "en", "Citations": "2"},
{"Title": "Initial considerations in building a speech-to-speech translation system for the Slovenian-English language pair", "Authors": ["Zganec Gros J.", "Mihelic A.", "Zganec M.", "Mihelic F.", "Dobrisek S.", "Zibert J.", "Vintar S.", "Korosec T.", "Erjavec T.", "Romih M."], "Keywords": [], "Date": "2005", "Abstract": "The paper presents the design concept of the VoiceTRAN Communicator that integrates speech recognition, machine translation and text-to-speech synthesis using the DARPA Galaxy architecture. The aim of the project is to build a robust multimodal speech-to-speech translation communicator able to translate simple domain-specific sentences in the Slovenian-English language pair. The project represents a joint collaboration between several Slovenian research organizations that are active in human language technologies. We provide an overview of the task, describe the system architecture and individual servers. Further we describe the language resources that will be used and developed within the project. We conclude the paper with plans for evaluation of the VoiceTRAN Communicator.", "Language": "en", "Citations": "0"},
{},
{"Title": "Strategies for exploiting independent cloud implementations of biometric experts in multibiometric scenarios", "Authors": ["Peer P.", "Emersic Z.", "Bule J.", "Zganec-Gros J.", "Struc V."], "Keywords": [], "Date": "2014", "Abstract": "Cloud computing represents one of the fastest growing areas of technology and offers a new computing model for various applications and services. This model is particularly interesting for the area of biometric recognition, where scalability, processing power, and storage requirements are becoming a bigger and bigger issue with each new generation of recognition technology. Next to the availability of computing resources, another important aspect of cloud computing with respect to biometrics is accessibility. Since biometric cloud services are easily accessible, it is possible to combine different existing implementations and design new multibiometric services that next to almost unlimited resources also offer superior recognition performance and, consequently, ensure improved security to its client applications. Unfortunately, the literature on the best strategies of how to combine existing implementations of cloud-based biometric experts into a multibiometric service is virtually nonexistent. In this paper, we try to close this gap and evaluate different strategies for combining existing biometric experts into a multibiometric cloud service. We analyze the (fusion) strategies from different perspectives such as performance gains, training complexity, or resource consumption and present results and findings important to software developers and other researchers working in the areas of biometrics and cloud computing. The analysis is conducted based on two biometric cloud services, which are also presented in the paper. \u00a9 2014 P. Peer et al.", "Language": "en", "Citations": "5"},
{"Title": "The upper bound for the index of nilpotency for a matrix commuting with a given nilpotent matrix", "Authors": ["Oblak P."], "Keywords": ["Commuting nilpotent matrices", "Jordan canonical form"], "Date": "2007", "Abstract": "We study the set [image omitted]([image omitted]) of all possible Jordan canonical forms of nilpotent matrices commuting with a given nilpotent matrix B. We describe [image omitted]([image omitted]) in the special case when B has only one Jordan block and discuss some consequences. In the general case, we find the maximal possible index of nilpotency in the set of all nilpotent matrices commuting with a given nilpotent matrix. We consider several examples.", "Language": "en", "Citations": "8"},
{"Title": "Recursive splicing in long vertebrate genes", "Authors": ["Sibley C.R.", "Emmett W.", "Blazquez L.", "Faro A.", "Haberman N.", "Briese M.", "Trabzuni D.", "Ryten M.", "Weale M.E.", "Hardy J.", "Modic M.", "Curk T.", "Wilson S.W.", "Plagnol V.", "Ule J."], "Keywords": [], "Date": "2015", "Abstract": "It is generally believed that splicing removes introns as single units from precursor messenger RNA transcripts. However, some long Drosophila melanogaster introns contain a cryptic site, known as a recursive splice site (RS-site), that enables a multi-step process of intron removal termed recursive splicing. The extent to which recursive splicing occurs in other species and its mechanistic basis have not been examined. Here we identify highly conserved RS-sites in genes expressed in the mammalian brain that encode proteins functioning in neuronal development. Moreover, the RS-sites are found in some of the longest introns across vertebrates. We find that vertebrate recursive splicing requires initial definition of an 'RS-exon' that follows the RS-site. The RS-exon is then excluded from the dominant mRNA isoform owing to competition with a reconstituted 5\u2032 splice site formed at the RS-site after the first splicing step. Conversely, the RS-exon is included when preceded by cryptic promoters or exons that fail to reconstitute an efficient 5\u2032 splice site. Most RS-exons contain a premature stop codon such that their inclusion can decrease mRNA stability. Thus, by establishing a binary splicing switch, RS-sites demarcate different mRNA isoforms emerging from long genes by coupling cryptic elements with inclusion of RS-exons.", "Language": "en", "Citations": "40"},
{"Title": "Content networks: Distributed routing decisions in presence of repeated queries", "Authors": ["Ciglari M.", "Vidmar T.", "Trampus M.", "Pancur M."], "Keywords": ["Content network", "Content-based routing", "Flooding", "Peer-to-peer networking"], "Date": "2003", "Abstract": "Content networks are overlay networks, enabling access to distributed contents on centralized servers or individual computers. Since the flooding-based routing scheme features poor scalability, we present a modification, which reduces the total network traffic while retaining the original efficiency. In choosy routing, each node, while passing an answer, remembers where it came from. Subsequently repeated queries about the same content are forwarded only to one neighbor. This way, the network learns effective routes. The simulations on several topology types have shown the expected behavior, with up to three-fold reduction in the overall query traffic.", "Language": "en", "Citations": "0"},
{"Title": "Learning qualitative models from numerical data", "Authors": ["Zabkar J.", "Mozina M.", "Bratko I.", "Demsar J."], "Keywords": ["Monotone models", "Partial derivatives", "Qualitative modelling", "Regression"], "Date": "2011", "Abstract": "Qualitative models describe relations between the observed quantities in qualitative terms. In predictive modelling, a qualitative model tells whether the output increases or decreases with the input. We describe Pad\u00e9, a new method for qualitative learning which estimates partial derivatives of the target function from training data and uses them to induce qualitative models of the target function. We formulated three methods for computation of derivatives, all based on using linear regression on local neighbourhoods. The methods were empirically tested on artificial and real-world data. We also provide a case study which shows how the developed methods can be used in practice. \u00a9 2011 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "12"},
{"Title": "TA-clustering: Cluster analysis of gene expression profiles through Temporal Abstractions", "Authors": ["Sacchi L.", "Bellazzi R.", "Larizza C.", "Magni P.", "Curk T.", "Petrovic U.", "Zupan B."], "Keywords": ["Bioinformatics", "Clustering", "Data mining", "Gene expression analysis", "Temporal Abstractions"], "Date": "2005", "Abstract": "This paper describes a new technique for clustering short time series of gene expression data. The technique is a generalization of the template-based clustering and is based on a qualitative representation of profiles which are labelled using trend Temporal Abstractions (TAs); clusters are then dynamically identified on the basis of this qualitative representation. Clustering is performed in an efficient way at three different levels of aggregation of qualitative labels, each level corresponding to a distinct degree of qualitative representation. The developed TA-clustering algorithm provides an innovative way to cluster gene profiles. We show the developed method to be robust, efficient and to perform better than the standard hierarchical agglomerative clustering approach when dealing with temporal dislocations of time series. Results of the TA-clustering algorithm can be visualized as a three-level hierarchical tree of qualitative representations and as such easy to interpret. We demonstrate the utility of the proposed algorithm on a set of two simulated data sets and on a study of gene expression data from S. cerevisiae. \u00a9 2005 Elsevier Ireland Ltd. All rights reserved.", "Language": "en", "Citations": "17"},
{"Title": "WS-BPEL Extensions for Versioning", "Authors": ["Juric M.B.", "Sasa A.", "Rozman I."], "Keywords": ["BPEL", "Business processes", "SOA", "Versioning"], "Date": "2009", "Abstract": "This article proposes specific extensions for WS-BPEL (Business Process Execution Language) to support versioning of processes and partner links. It introduces new activities and extends existing activities, including partner links, invoke, receive, import, and onmessage activities. It proposes version-related extensions to variables and introduces version handlers. The proposed extensions represent a complete solution for process-level and scope-level versioning at development, deployment, and run-time. It also provides means to version applications that consist of several BPEL processes, and to put temporal constraints on versions. The proposed approach has been tested in real-world environment. It solves major challenges in BPEL versioning. \u00a9 2009 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "14"},
{"Title": "K6 and icosahedron minors in 5-connected projective planar graphs", "Authors": ["Fijavz G."], "Keywords": ["5-connected graphs", "Face-width", "Graph minors", "Projective-planar graphs"], "Date": "2000", "Abstract": "We show that every 5-connected graph admitting an embedding into the projective plane with face-width at least 3 contains K", "Language": "en", "Citations": "0"},
{"Title": "Robust real-time music transcription with a compositional hierarchical model", "Authors": ["Pesek M.", "Leonardis A.", "Marolt M."], "Keywords": [], "Date": "2017", "Abstract": "The paper presents a new compositional hierarchical model for robust music transcription. Its main features are unsupervised learning of a hierarchical representation of input data, transparency, which enables insights into the learned representation, as well as robustness and speed which make it suitable for real-world and real-time use. The model consists of multiple layers, each composed of a number of parts. The hierarchical nature of the model corresponds well to hierarchical structures in music. The parts in lower layers correspond to low-level concepts (e.g. tone partials), while the parts in higher layers combine lower-level representations into more complex concepts (tones, chords). The layers are learned in an unsupervised manner from music signals. Parts in each layer are compositions of parts from previous layers based on statistical co-occurrences as the driving force of the learning process. In the paper, we present the model's structure and compare it to other hierarchical approaches in the field of music information retrieval. We evaluate the model's performance for the multiple fundamental frequency estimation. Finally, we elaborate on extensions of the model towards other music information retrieval tasks.", "Language": "en", "Citations": "1"},
{"Title": "Using equality in the Krein conditions to prove nonexistence of certain distance-regular graphs", "Authors": ["Coolsaet K.", "Jurisic A."], "Keywords": ["Association scheme", "Distance-regular graph", "Krein parameters", "Strongly regular graph", "Triple intersection numbers"], "Date": "2008", "Abstract": "We prove the nonexistence of a distance-regular graph with intersection array {74, 54, 15 ; 1, 9, 60} and of distance-regular graphs with intersection arrays{4 r", "Language": "en", "Citations": "9"},
{"Title": "Argument based rule learning", "Authors": ["Mozina M.", "Zabkar J.", "Bratko I."], "Keywords": [], "Date": "2006", "Abstract": "We present a novel approach to machine learning, called ABML (argumentation based ML). This approach combines machine learning from examples with concepts from the field of argumentation. The idea is to provide expert's arguments, or reasons, for some of the learning examples. We require that the theory induced from the examples explains the examples in terms of the given reasons. Thus arguments constrain the combinatorial search among possible hypotheses, and also direct the search towards hypotheses that are more comprehensible in the light of expert's background knowledge. In this paper we implement ABCN2 as an extension of the CN2 rule learning algorithm, and analyze its advantages in comparison with the original CN2 algorithm. \u00a9 2006 The authors.", "Language": "en", "Citations": "2"},
{"Title": "Obtaining structural descriptions of building fa\u00e7ades", "Authors": ["Vracar P.", "Kononenko I.", "Robnik-Sikonja M."], "Keywords": ["Fa\u00e7ade segmentation", "Formal grammar", "Image segmentation", "Urban environment", "Window detection"], "Date": "2016", "Abstract": "We describe a method for learning and recognizing windows as basic structural elements of fa\u00e7ades and organizing them into interpretable models of building fa\u00e7ades. The method segments an input image into a hierarchical structure of window candidates. The candidates are used to create a likelihood map of window locations that is explained by a structural fa\u00e7ade model based on a formal grammar. We use a look-ahead greedy search method in the grammar derivation space to select the (sub)optimal fa\u00e7ade model. Empirical evaluation results reveal that, on average, the generated fa\u00e7ade model covers 45% of the actual windows present in the input image. On the other hand, 56% of the modeled windows actually cover fa\u00e7ade windows present in the input image.", "Language": "en", "Citations": "3"},
{"Title": "Organized flight in birds", "Authors": ["Bajec I.L.", "Heppner F.H."], "Keywords": ["animat", "bird flock", "boid", "Canada goose", "cluster formation", "European starling", "flight formation", "flock simulation", "line formation", "V formation"], "Date": "2009", "Abstract": "The organized flight of birds is one of the most easily observed, yet challenging to study, phenomena in biology. Birds that fly in organized groups generally do so in one of two fashions: Line formations and Cluster formations. The former groups are typical of large birds such as waterfowl, where birds fly arranged in single lines, often joined together. The scientific questions about these groups usually involve potential adaptive functions, such as why geese fly in a V. Cluster formations are typically made up of large numbers of smaller birds such as pigeons or starlings flying in more irregular arrangements that have a strong three-dimensional character. The groups are defined by synchronized and apparently simultaneous rapid changes in direction. Scientific questions about these groups are usually concerned with mechanism such as how synchrony is achieved. Although field observations about the phenomenon date to the origins of natural history, experimental studies did not begin until the 1970s. Early experimenters and theoreticians were primarily biologists, but more recently aeronautical engineers, mathematicians, computer scientists and, currently, physicists have been attracted to the study of organized flight. Computer modelling has recently generated striking visual representations of organized flight and a number of hypotheses about its functions and mechanisms, but the ability to test these hypotheses lags behind the capacity to generate them. We suggest that a multi disciplinary approach to the phenomenon will be necessary to resolve apparently conflicting current hypotheses. \u00a9 2009 The Association for the Study of Animal Behaviour.", "Language": "en", "Citations": "114"},
{"Title": "Collocations in the developmental corpus \u0161olar Kolokacije v Korpusu \u0160olar", "Authors": ["Rozman T.", "Holdt S.A.", "Pollak S.", "Kosem I."], "Keywords": ["Collocations", "Corpus analysis", "Language didactics", "Solar corpus", "Vocabulary development"], "Date": "2018", "Abstract": "The paper investigates the possibilities of using the developmental \u0160olar corpus for determining various ways in which collocations are acquired in Slovenian, and considers potential applications of the findings for teaching Slovenian as Li. Using the combinations adjective + noun and noun + noun, we tested two methods: a) quantitative comparative analysis of the Solar corpus and the Kres corpus was used to determine similarities and differences in collocation use in school essays and general written Slovenian; b) qualitative analysis of teacher corrections was made to determine the collocations that cause difficulties among students. The results show a number of atypical uses of collocations by students - uses that are detected by teachers only to a certain extent - and confirm the usefulness of a corpus-based approach in language acquisition.", "Language": "en", "Citations": "0"},
{"Title": "Correcting streaming predictions of an electricity load forecast system using a prediction reliability estimate", "Authors": ["Bosnic Z.", "Rodrigues P.P.", "Kononenko I.", "Gama J."], "Keywords": ["data stream", "online learning", "prediction accuracy", "prediction correction"], "Date": "2011", "Abstract": "Accurately predicting values for dynamic data streams is a challenging task in decision and expert systems, due to high data flow rates, limited storage and a requirement to quickly adapt a model to new data. We propose an approach for correcting predictions for data streams which is based on a reliability estimate for individual regression predictions. In our work, we implement the proposed technique and test it on a real-world problem: prediction of the electricity load for a selected European geographical region. For predicting the electricity load values we implement two regression models: the neural network and the k nearest neighbors algorithm. The results show that our method performs better than the referential method (i.e. the Kalman filter), significantly improving the original streaming predictions to more accurate values. \u00a9 2011 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "2"},
{"Title": "Multiple eigenspaces", "Authors": ["Leonardis A.", "Bischof H.", "Maver J."], "Keywords": ["Appearance-based object recognition", "Appearance-based object representation", "Dimensionality reduction", "Image grouping", "Multiple eigenspaces", "Principal component analysis (PCA)", "View-based navigation map", "Visual learning"], "Date": "2002", "Abstract": "In this paper, we propose a novel self-organizing framework to construct multiple, low-dimensional eigenspaces from a set of training images. Grouping of images is systematically and robustly performed via eigenspace-growing in terms of low-dimensional eigenspaces. To further increase the robustness, the eigenspace-growing is initiated independently with many small groups of images-seeds. All these grown eigenspaces are treated as hypotheses that are subject to a selection procedure eigenspace-selection, based on the MDL principle, which selects the final resulting set of eigenspaces as an efficient representation of the training set, taking into account the number of images encompassed by the eigenspaces, the dimensions of the eigenspaces, and their corresponding residual errors. We have tested the proposed method on a number of standard image sets, and the significance of the approach with respect to the recognition rate has been demonstrated. \u00a9 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.", "Language": "en", "Citations": "56"},
{"Title": "Open-source tool for interactive digitisation of pluviograph strip charts", "Authors": ["Susin N.", "Peer P."], "Keywords": [], "Date": "2018", "Abstract": "The analysis of weather requires data collected over long periods of time. Rainfall intensity is one of the basic weather measurements. Paper strip charts were used in the past, and in some parts of the world are used even today, to record rainfall intensity over a given period of time. Since most modern analysis takes place on computers, we need a way to digitise historical data to be able to process it. An existing automated algorithm was adapted and implemented in an interactive program to solve this task. The algorithm automatically processes images of rainfall charts and allows users to manually correct any errors, resulting in a very accurate reading. This paper documents how the program works, the results it gives and the underlying problem itself. It also offers some commentary on computer-aided digitisation of other strip charts. The software is freely available on the web under an open-source license and serves as a base for continued growth and evolution by contributions from the community.", "Language": "en", "Citations": "0"},
{"Title": "Consistency checking of UML business model Verslo modelio, i\u0161reik\u0161to UML kalba, darnos tikrinimas", "Authors": ["Vasilecas O.", "Dubauskaite R.", "Rupnik R."], "Keywords": ["Aspect model", "Consistency", "Consistency checking", "Consistency ensuring", "Model validation", "Rules", "UML diagram", "Uml model"], "Date": "2011", "Abstract": "Unified modelling language (UML) is often used in practice for modelling business system (BS) by various aspects. UML model of business system consists of different aspect models and their usage for information system (IS) design is related with inconsistency problem. It arises because ambiguous or even contradictory information are provided in different aspect models. The paper presents approach in ensuring UML model consistency. Several examples of consistency rules are included to the paper to illustrate how approach is working. Developed prototype of suggested approach is applied in a domain of enterprise manufacturing windows and doors. Obtained results are discussed. \u00a9 2011 Vilnius Gediminas Technical University (VGTU) Press Technika.", "Language": "en", "Citations": "5"},
{"Title": "Learning to explain with ABML", "Authors": ["Mozina M.", "Guid M.", "Krivec J.", "Sadikov A.", "Bratko I."], "Keywords": ["Argument based", "Chess", "Learning to explain", "Machine learning"], "Date": "2010", "Abstract": "The main advantage of machine learning algorithms that learn simple symbolic models is in their capability to trivially provide justifications for their decisions. However, there is no guarantee that these justifications will be understood by experts and other users. Induced models are often strange to the domain experts as they understand the problem in a different way. We suggest the use of argument based machine learning (ABML) to deal with this problem. This approach combines machine learning with explanations provided by domain experts. An ABML method is required to learn a model that correctly predicts learning examples and is consistent with the provided explanations. The present paper describes an application of ABML to learning a complex chess concept of an attack on the castled king. The explanation power of the learned model for this concept is especially important, as it will be used in a chess tutoring application.", "Language": "en", "Citations": "3"},
{"Title": "Simulating flocks on the wing: The fuzzy approach", "Authors": ["Lebar Bajec I.", "Zimic N.", "Mraz M."], "Keywords": ["Animat", "Artificial life", "Bird", "Boid", "Flock", "Fuzzy animat", "Fuzzy logic"], "Date": "2005", "Abstract": "Traditionally the systematic study of animal behaviour using simulations requires the construction of a suitable mathematical model. The construction of such models in most cases requires advanced mathematical skills and exact knowledge of the studied animal's behaviour. Exact knowledge is rarely available. Usually it is available in the form of the observer's linguistic explanations and descriptions of the perceived behaviour. Mathematical models thus require a transition from the linguistic description to a mathematical formula that is seldom straightforward. The substantial increase of the processing power of personal computers has had as a result a notable progress in the field of fuzzy logic. In this paper we present a novel approach to the construction of artificial animals (animats) that is based on fuzzy logic. Our leading hypothesis is, that by omitting the transition from linguistic descriptions to mathematical formulas, ethologists would gain a tool for testing the existing or forming new hypotheses about 'why' and 'how' animals behave the way they do. \u00a9 2004 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "28"},
{"Title": "The Image of the Monolingual Dictionary Across Europe. Results of the European Survey of Dictionary use and Culture", "Authors": ["Kosem I.", "Lew R.", "Muller-Spitzer C.", "Silveira M.R.", "Wolfer S.", "Dorn A.", "Gurrutxaga A.", "Ceberio K.", "Etxeberria E.", "Lefer M.-A.", "Geeraerts D.", "Despot K.S.", "Stojanov T.", "Ljubesic N.", "Skrabal M.", "Stepankova B.", "Vodrazkova V.", "Lorentzen H.", "Trap-Jensen L.", "Kallas J.", "Tuulik M.", "Koppel K.", "Langemets M.", "Heinonen T.", "Thomas I.", "Margilitadze T.", "Markantonatou S.", "Giouli V.", "Mulhall C.", "Kernerman I.", "Ben-Moshe Y.", "Sadan T.", "Abel A.", "Curcio M.N.", "Tanturovska L.", "Nikovska B.", "Tiberius C.", "Gronvik O.", "Hovdenak M.", "Berg-Olsen S.", "Karlsen K.E.", "Ore C.-E.S.", "Biesaga M.", "Kuhn T.Z.", "Silvestre J.", "Tamba E.I.", "Haja G.", "Clim M.-R.", "Patrascu M.-I.", "Tasovac T.", "Petrovic S.", "Holdt S.A.", "Riveiro C.V.", "Vazquez M.J.D.", "Volodina E.", "Pilan I.", "Skoldberg E.", "Holmer L.", "Nesi H."], "Keywords": [], "Date": "2019", "Abstract": "The article presents the results of a survey on dictionary use in Europe, focusing on general monolingual dictionaries. The survey is the broadest survey of dictionary use to date, covering close to 10,000 dictionary users (and non-users) in nearly thirty countries. Our survey covers varied user groups, going beyond the students and translators who have tended to dominate such studies thus far. The survey was delivered via an online survey platform, in language versions specific to each target country. It was completed by 9,562 respondents, over 300 respondents per country on average. The survey consisted of the general section, which was translated and presented to all participants, as well as country-specific sections for a subset of 11 countries, which were drafted by collaborators at the national level. The present report covers the general section.", "Language": "en", "Citations": "2"},
{"Title": "Elicitation of neurological knowledge with argument-based machine learning", "Authors": ["Groznik V.", "Guid M.", "Sadikov A.", "Mozina M.", "Georgiev D.", "Kragelj V.", "Ribaric S.", "Pirtosek Z.", "Bratko I."], "Keywords": ["Argument-based machine learning", "Decision support systems", "Essential tremor", "Knowledge elicitation", "Parkinsonian tremor"], "Date": "2013", "Abstract": "Objective: The paper describes the use of expert's knowledge in practice and the efficiency of a recently developed technique called argument-based machine learning (ABML) in the knowledge elicitation process. We are developing a neurological decision support system to help the neurologists differentiate between three types of tremors: Parkinsonian, essential, and mixed tremor (comorbidity). The system is intended to act as a second opinion for the neurologists, and most importantly to help them reduce the number of patients in the \" gray area\" that require a very costly further examination (DaTSCAN). We strive to elicit comprehensible and medically meaningful knowledge in such a way that it does not come at the cost of diagnostic accuracy. Materials and methods: To alleviate the difficult problem of knowledge elicitation from data and domain experts, we used ABML. ABML guides the expert to explain critical special cases which cannot be handled automatically by machine learning. This very efficiently reduces the expert's workload, and combines expert's knowledge with learning data. 122 patients were enrolled into the study. Results: The classification accuracy of the final model was 91%. Equally important, the initial and the final models were also evaluated for their comprehensibility by the neurologists. All 13 rules of the final model were deemed as appropriate to be able to support its decisions with good explanations. Conclusion: The paper demonstrates ABML's advantage in combining machine learning and expert knowledge. The accuracy of the system is very high with respect to the current state-of-the-art in clinical practice, and the system's knowledge base is assessed to be very consistent from a medical point of view. This opens up the possibility to use the system also as a teaching tool. \u00a9 2012 Elsevier B.V.", "Language": "en", "Citations": "14"},
{"Title": "Traversal and relations discovery among business entities and people using semantic web technologies and trust management", "Authors": ["Lavbic D.", "Zitnik S.", "Subelj L.", "Kumer A.", "Zrnec A.", "Bajec M."], "Keywords": ["ontologies", "personal information extraction", "semantic integration", "Semantic Web", "SocioLeaks", "trust management"], "Date": "2013", "Abstract": "There are several data silos containing information about business entities and people but are not semantically connected. If in integration process of data sources trust management is also employed than we can expect much higher success rate in relations discovery among entities. Majority of current mash-up approaches that deal with integration of information from several data sources omit or don't fully address the aspect of trust. In this paper we discuss semantic integration of personal and business information from various data sources coupled with trust layer. The resulting system has higher and more defined solidity while trust for single entity and also for data source is defined. The case study presented in the paper focuses on integration of personal information from data sources mainly maintained by government authorities who have higher trustability than information from social networks, but we also include other less trusted sources. The developed SocioLeaks system allows users traversal and further relation discovery in a graph based manner. \u00a9 2013 The authors and IOS Press. All rights reserved.", "Language": "en", "Citations": "1"},
{"Title": "E-HEALTHCARE for DIABETES MELLITUS TYPE 2 PATIENTS - A RANDOMISED CONTROLLED TRIAL in SLOVENIA", "Authors": ["Iljaz R.", "Brodnik A.", "Zrimec T.", "Cukjati I."], "Keywords": ["Diabetes mellitus", "family practice", "functional health status", "HbA1c", "telemedicine"], "Date": "2017", "Abstract": "Telemonitoring and web-based interventions are increasingly used in primary-care practices in many countries for more effective management of patients with diabetes mellitus (DM). A new approach in treating patients with diabetes mellitus in family practices, based on ICT use and nurse practitioners, has been introduced and evaluated in this study. Fifteen Slovene family practices enrolled 120 DM patients treated only with a diet regime and/or tablets into the study. 58 of them were included into the interventional group, and the other 62 DM patients into the control group, within one-year-long interventional, randomised controlled trial. Patients in the control group had conventional care for DM according to Slovenian professional guidelines, while the patients in the interventional group were using also the eDiabetes application. Patients were randomised through a balanced randomisation process. Significant reductions of glycated haemoglobin (HbA1c) values were found after 6 and 12 months among patients using this eDiabetes application (p<0.05). Among these patients, a significant correlation was also found between self-monitored blood pressure and the final HbA1c values. Diabetic patients' involvement in web-based intervention had only transient impact on their functional health status. This eDiabetes application was confirmed to be an innovative approach for better self-management of DM type 2 patients not using insulin. Both a significant reduction of HbA1c values and a significant correlation between the average self-measured blood pressure and the final HbA1c values in the interventional group were found. Nurse practitioners - as diabetes care coordinators - could contribute to better adherence in diabetes e-care.", "Language": "en", "Citations": "1"},
{"Title": "Panoramic depth imaging with a single standard camera", "Authors": ["Peer P.", "Solina F."], "Keywords": [], "Date": "2001", "Abstract": "In this article we present a panoramic depth imaging sys-. tern. The system is mosaic-based which means that we use a single rotating camera and assemble the captured images in a mosaic. Due to a setoff of the camera's optical center from the rotational center of the system we are able to capture the motion parallax effect which enables the stereo reconstruction. The camera is rotating on a circular path with the step defined by an angle, equivalent to one column of the captured image. The equation for depth estimation can be easily extracted from system geometry. To find the corresponding points on stereo pair of panoramic images the epipolar geometry needs to be determined. We focused mainly on the system analysis. The system performs well in the reconstruction of small indoor spaces.", "Language": "en", "Citations": "0"},
{"Title": "The visual object tracking VOT2014 challenge results", "Authors": ["Kristan M.", "Pflugfelder R.", "Leonardis A.", "Matas J.", "Cehovin L.", "Nebehay G.", "Vojir T.", "Fernandez G.", "Lukezic A.", "Dimitriev A.", "Petrosino A.", "Saffari A.", "Li B.", "Han B.", "Heng C.K.", "Garcia C.", "Pangersic D.", "Hager G.", "Khan F.S.", "Oven F.", "Possegger H.", "Bischof H.", "Nam H.", "Zhu J.", "Li J.J.", "Choi J.Y.", "Choi J.-W.", "Henriques J.F.", "van de Weijer J.", "Batista J.", "Lebeda K.", "Ofjall K.", "Yi K.M.", "Qin L.", "Wen L.", "Maresca M.E.", "Danelljan M.", "Felsberg M.", "Cheng M.-M.", "Torr P.", "Huang Q.", "Bowden R.", "Hare S.", "Lim S.Y.Y.", "Hong S.", "Liao S.", "Hadfield S.", "Li S.Z.", "Duffner S.", "Golodetz S.", "Mauthner T.", "Vineet V.", "Lin W.", "Li Y.", "Qi Y.", "Lei Z.", "Niu Z.H."], "Keywords": ["Performance evaluation", "Short-term single-object trackers", "VOT"], "Date": "2015", "Abstract": "The Visual Object Tracking challenge 2014, VOT2014, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 38 trackers are presented. The number of tested trackers makes VOT 2014 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2014 challenge that go beyond its VOT2013 predecessor are introduced: (i) a new VOT2014 dataset with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2013 evaluation methodology, (iii) a new unit for tracking speed assessment less dependent on the hardware and (iv) the VOT2014 evaluation toolkit that significantly speeds up execution of experiments. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http://votchallenge.net).", "Language": "en", "Citations": "95"},
{"Title": "Quantifying the consistency of scientific databases", "Authors": ["Subelj L.", "Bajec M.", "Boshkoska B.M.", "Kastrin A.", "Levnajic Z."], "Keywords": [], "Date": "2015", "Abstract": "Science is a social process with far-reaching impact on our modern society. In recent years, for the first time we are able to scientifically study the science itself. This is enabled by massive amounts of data on scientific publications that is increasingly becoming available. The data is contained in several databases such as Web of Science or PubMed, maintained by various public and private entities. Unfortunately, these databases are not always consistent, which considerably hinders this study. Relying on the powerful framework of complex networks, we conduct a systematic analysis of the consistency among six major scientific databases. We found that identifying a single \"best\" database is far from easy. Nevertheless, our results indicate appreciable differences in mutual consistency of different databases, which we interpret as recipes for future bibliometric studies.", "Language": "en", "Citations": "6"},
{"Title": "The key elements of logic design in ternary quantum-dot cellular automata", "Authors": ["Pecar P.", "Lebar Bajec I."], "Keywords": ["multi-valued Post logic", "ternary characteristic functions", "ternary functionally complete set", "ternary quantum-dot cellular automaton"], "Date": "2011", "Abstract": "The ternary Quantum-dot Cellular Automata (tQCA) were demonstrated to be a possible candidate for the implementation of a future multi-valued processing platform. Recent papers show that the application of adiabatic pipelining can be used to solve the issues of tQCA logic primitives. The architectures of the resulting tQCAs are similar to their binary counterparts and the physical design rules remain similar to those for the binary QCA domain. The design of complex processing structures is, however, usually based on logic design. The foundation of logic design is a functionally complete set of elementary logic primitives (functions). The currently available tQCA logic primitives, i.e. tQCA majority gate and tQCA inverter gate, do not constitute a functionally complete set. We here present a tQCA implementation of the ternary characteristic functions, which together with the tQCA majority gate and the ternary constants constitute a functionally complete set according to multi-valued Post logic. \u00a9 2011 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "6"},
{"Title": "Fast recovery of piled deformable objects using superquadrics", "Authors": ["Katsoulas D.", "Jaklic A."], "Keywords": [], "Date": "2002", "Abstract": "Fast robotic unloading of piled deformable box-like objects (e.g. box-like sacks), is undoubtedly of great importance to the industry. Existing systems although fast, can only deal with layered, neatly placed configurations of such objects. In this paper we discuss an approach which deals with both neatly placed and jumbled configurations of objects. We use a time of flight laser sensor mounted on the hand of a robot for data acquisition. Target objects are modeled with globally deformed superquadrics. Object vertices are detected and superquadric seeds are placed at these vertices. Seed refinement via region growing results in accurate object recovery. Our system exhibits a plethora of advantages the most important of which its speed. Experiments demonstrate that our system can be used for object unloading in real time, when a multi-processor computer is employed. \u00a9 Springer-Verlag Berlin Heidelberg 2002.", "Language": "en", "Citations": "3"},
{"Title": "Introduction of the automated assessment of homework assignments in a university-level programming course", "Authors": ["Pozenel M.", "Furst L.", "Mahnic V."], "Keywords": ["automated grading", "dynamic testing", "programming education"], "Date": "2015", "Abstract": "Modern teaching paradigms promote active student participation, encouraging teachers to adapt the teaching process to involve more practical work. In the introductory programming course at the Faculty of Computer and Information Science, University of Ljubljana, Slovenia, homework assignments contribute approximately one half to the total grade, requiring a significant investment of time and human resources in the assessment process. This problem was alleviated by the automated assessment of homework assignments. In this paper, we introduce an automated assessment system for programming assignments that includes dynamic testing of student programs, plagiarism detection, and a proper presentation of the results. We share our experience and compare the introduced system with the manual assessment approach used before.", "Language": "en", "Citations": "10"},
{"Title": "Extending information system integrability index with CMM model. A preliminary proposal", "Authors": ["Pusnik M.", "Sumak B.", "Juric M.B.", "Hericko M."], "Keywords": ["Capability maturity model", "Information systems", "Integrability index", "Integration"], "Date": "2007", "Abstract": "Development and maintenance of information systems often face integration and re-factoring challenges; therefore diminishing similar difficulties is stressed within this paper. Research was based on literature review of several integrability parameters, describing an information system, where the goal was finding optimal specifications that need to be met to build an integration friendly information system. As a result, a model of parameter groups, which should be included in development, was composed and we named it integrability index. Also a comparison between the index and the Capability Maturity Model was preformed and parallels between them were searched for. Both models are significant to information system improvement and indirectly to business processes they support.", "Language": "en", "Citations": "1"},
{"Title": "The COST278 broadcast news segmentation and speaker clustering evaluation - Overview, methodology, systems, results", "Authors": ["Zibert J.", "Mihelic F.", "Martens J.-P.", "Meinedo H.", "Neto J.", "Docio L.", "Garcia-Mateo C.", "David P.", "Zdansky J.", "Pleva M.", "Cizmar A.", "Zgank A.", "Kacic Z.", "Teleki C.", "Vicsi K."], "Keywords": [], "Date": "2005", "Abstract": "This paper describes a large scale experiment in which eight research institutions have tested their audio partitioning and labeling algorithms on the same data, a multi-lingual database of news broadcasts, using the same evaluation tools and protocols. The experiments have provide more insight in the cross-lingual robustness of the methods and they have demonstrated that by further collaborating in the domains of speaker change detection and speaker clustering it should be possible to achieve further technological progress in the near future.", "Language": "en", "Citations": "10"},
{"Title": "Aubergine iCLIP Reveals piRNA-Dependent Decay of mRNAs Involved in Germ Cell Development in the Early Embryo", "Authors": ["Barckmann B.", "Pierson S.", "Dufourt J.", "Papin C.", "Armenise C.", "Port F.", "Grentzinger T.", "Chambeyron S.", "Baronian G.", "Desvignes J.-P.", "Curk T.", "Simonelig M."], "Keywords": [], "Date": "2015", "Abstract": "The Piwi-interacting RNA (piRNA) pathway plays an essential role in the repression of transposons in the germline. Other functions of piRNAs such as post-transcriptional regulation of mRNAs are now emerging. Here, we perform iCLIP with the PIWI protein Aubergine (Aub) and identify hundreds of maternal mRNAs interacting with Aub in the early Drosophila embryo. Gene expression profiling reveals that a proportion of these mRNAs undergo Aub-dependent destabilization during the maternal-to-zygotic transition. Strikingly, Aub-dependent unstable mRNAs encode germ cell determinants. iCLIP with an Aub mutant that is unable to bind piRNAs confirms piRNA-dependent binding of Aub to mRNAs. Base pairing between piRNAs and mRNAs can induce mRNA cleavage and decay that are essential for embryonic development. These results suggest general regulation of maternal mRNAs by Aub and piRNAs, which plays a key developmental role in the embryo through decay and localization of mRNAs encoding germ cell determinants.", "Language": "en", "Citations": "38"},
{"Title": "Insights into the design and interpretation of iCLIP experiments", "Authors": ["Haberman N.", "Huppertz I.", "Attig J.", "Konig J.", "Wang Z.", "Hauer C.", "Hentze M.W.", "Kulozik A.E.", "Le Hir H.", "Curk T.", "Sibley C.R.", "Zarnack K.", "Ule J."], "Keywords": ["Binding site assignment", "ECLIP", "Eukaryotic initiation factor 4A-III (eIF4A3)", "Exon-junction complex", "High-throughput sequencing", "ICLIP", "IrCLIP", "Polypyrimidine tract binding protein 1 (PTBP1)", "Protein-RNA interactions"], "Date": "2017", "Abstract": "Background: Ultraviolet (UV) crosslinking and immunoprecipitation (CLIP) identifies the sites on RNAs that are in direct contact with RNA-binding proteins (RBPs). Several variants of CLIP exist, which require different computational approaches for analysis. This variety of approaches can create challenges for a novice user and can hamper insights from multi-study comparisons. Here, we produce data with multiple variants of CLIP and evaluate the data with various computational methods to better understand their suitability. Results: We perform experiments for PTBP1 and eIF4A3 using individual-nucleotide resolution CLIP (iCLIP), employing either UV-C or photoactivatable 4-thiouridine (4SU) combined with UV-A crosslinking and compare the results with published data. As previously noted, the positions of complementary DNA (cDNA)-starts depend on cDNA length in several iCLIP experiments and we now find that this is caused by constrained cDNA-ends, which can result from the sequence and structure constraints of RNA fragmentation. These constraints are overcome when fragmentation by RNase I is efficient and when a broad cDNA size range is obtained. Our study also shows that if RNase does not efficiently cut within the binding sites, the original CLIP method is less capable of identifying the longer binding sites of RBPs. In contrast, we show that a broad size range of cDNAs in iCLIP allows the cDNA-starts to efficiently delineate the complete RNA-binding sites. Conclusions: We demonstrate the advantage of iCLIP and related methods that can amplify cDNAs that truncate at crosslink sites and we show that computational analyses based on cDNAs-starts are appropriate for such methods.", "Language": "en", "Citations": "23"},
{"Title": "Software engineering in the cloud for reducing the application time-to-market Razvoj programske opreme v oblaku za skraj\u0161evanje \u010dasa vstopa na trg", "Authors": ["Zrnec A."], "Keywords": ["Cloud", "Cloud computing", "Cost", "Development process", "Software"], "Date": "2011", "Abstract": "In this paper we present the possibilities offered by cloud computing in development and testing of software. They allow us to shorten the time required for developing software and to shorten the time of software to become available on the market. We mainly focus on provisioning cloud resources on demand, accounting for the actual consumption of resources and adjusting the scale of the computing resources to contribute to more effective implementation of development and testing activities and ensure a higher level of quality of designed solutions.", "Language": "en", "Citations": "0"},
{"Title": "Simulating a basketball match with a homogeneous Markov model and forecasting the outcome", "Authors": ["Strumbelj E.", "Vracar P."], "Keywords": ["Monte Carlo", "National Basketball Association", "Probability forecasting", "Simulation", "Sports forecasting"], "Date": "2012", "Abstract": "We used a possession-based Markov model to model the progression of a basketball match. The model's transition matrix was estimated directly from NBA play-by-play data and indirectly from the teams' summary statistics. We evaluated both this approach and other commonly used forecasting approaches: logit regression of the outcome, a latent strength rating method, and bookmaker odds. We found that the Markov model approach is appropriate for modelling a basketball match and produces forecasts of a quality comparable to that of other statistical approaches, while giving more insight into basketball. Consistent with previous studies, bookmaker odds were the best probabilistic forecasts. \u00a9 2011 International Institute of Forecasters.", "Language": "en", "Citations": "26"},
{"Title": "Extreme value correction: a method for correcting optimistic estimations in rule learning", "Authors": ["Mozina M.", "Demsar J.", "Bratko I.", "Zabkar J."], "Keywords": ["Extreme value distribution", "Machine learning", "Multiple comparisons", "Rule learning"], "Date": "2019", "Abstract": "Machine learning algorithms rely on their ability to evaluate the constructed hypotheses for choosing the optimal hypothesis during learning and assessing the quality of the model afterwards. Since these estimates, in particular the former ones, are based on the training data from which the hypotheses themselves were constructed, they are usually optimistic. The paper shows three different solutions; two for the artificial boundary cases with the smallest and the largest optimism and a general correction procedure called extreme value correction (EVC) based on extreme value distribution. We demonstrate the application of the technique to rule learning, specifically to estimating classification accuracy of a single rule, and evaluate it on an artificial data set and on a number of UCI data sets. We observed that the correction successfully improved the accuracy estimates. We also describe an approach for combining rules into a linear global classifier and show that using EVC estimates leads to more accurate classifiers.", "Language": "en", "Citations": "0"},
{"Title": "Orthogonal matrix factorization enables integrative analysis of multiple RNA binding proteins", "Authors": ["Strazar M.", "Zitnik M.", "Zupan B.", "Ule J.", "Curk T."], "Keywords": [], "Date": "2016", "Abstract": "Motivation: RNA binding proteins (RBPs) play important roles in post-transcriptional control of gene expression, including splicing, transport, polyadenylation and RNA stability. To model protein-RNA interactions by considering all available sources of information, it is necessary to integrate the rapidly growing RBP experimental data with the latest genome annotation, gene function, RNA sequence and structure. Such integration is possible by matrix factorization, where current approaches have an undesired tendency to identify only a small number of the strongest patterns with overlapping features. Because protein-RNA interactions are orchestrated by multiple factors, methods that identify discriminative patterns of varying strengths are needed. Results: We have developed an integrative orthogonality-regularized nonnegative matrix factorization (iONMF) to integrate multiple data sources and discover non-overlapping, class-specific RNA binding patterns of varying strengths. The orthogonality constraint halves the effective size of the factor model and outperforms other NMF models in predicting RBP interaction sites on RNA. We have integrated the largest data compendium to date, which includes 31 CLIP experiments on 19 RBPs involved in splicing (such as hnRNPs, U2AF2, ELAVL1, TDP-43 and FUS) and processing of 3'UTR (Ago, IGF2BP). We show that the integration of multiple data sources improves the predictive accuracy of retrieval of RNA binding sites. In our study the key predictive factors of protein-RNA interactions were the position of RNA structure and sequence motifs, RBP co-binding and gene region type. We report on a number of protein-specific patterns, many of which are consistent with experimentally determined properties of RBPs. Availability and implementation: The iONMF implementation and example datasets are available at https://github.com/mstrazar/ionmf.", "Language": "en", "Citations": "25"},
{"Title": "It's time for a song - Transcribing recordings of bell-playing clocks", "Authors": ["Marolt M.", "Lefeber M."], "Keywords": [], "Date": "2010", "Abstract": "The paper presents an algorithm for automatic transcription of recordings of bell-playing clocks. Bell-playing clocks are clocks containing a hidden bell-playing mechanism that is periodically activated to play a melody. Clocks from the eighteenth century give us unique insight into the musical taste of their owners, so we are interested in studying their repertoire and performances - thus the need for automatic transcription. In the paper, we first present an analysis of acoustical properties of bells found in bell-playing clocks. We propose a model that describes positions of bell partials and an algorithm that discovers the number of bells and positions of their partials in a given recording. To transcribe a recording, we developed a probabilistic method that maximizes the joint probability of a note sequence given the recording and positions of bell partials. Finally, we evaluate our algorithms on a set of recordings of bell-playing clocks. \u00a9 2010 International Society for Music Information Retrieval.", "Language": "en", "Citations": "3"},
{"Title": "Automated diagnostics of coronary artery disease: Long-term results and recent advancements", "Authors": ["Kukar M.", "Kononenko I.", "Groselj C."], "Keywords": [], "Date": "2012", "Abstract": "The authors present results and the latest advancement in their long-term study on using image processing and data mining methods in medical image analysis in general, and in clinical diagnostics of coronary artery disease in particular. Since the evaluation of modern medical images is often difficult and time-consuming, authors integrate advanced analytical and decision support tools in diagnostic process. Partial diagnostic results, frequently obtained from tests with substantial imperfections, can be thus integrated in ultimate diagnostic conclusion about the probability of disease for a given patient. Authors study various topics, such as improving the predictive power of clinical tests by utilizing pre-test and post-test probabilities, texture representation, multi-resolution feature extraction, feature construction and data mining algorithms that significantly outperform the medical practice. During their long-term study (1995-2011) authors achieved, among other minor results, two really significant milestones. The first was achieved by using machine learning to significantly increase post-test diagnostic probabilities with respect to expert physicians. The second, even more significant result utilizes various advanced data analysis techniques, such as automatic multi-resolution image parameterization combined with feature extraction and machine learning methods to significantly improve on all aspects of diagnostic performance. With the proposed approach clinical results are significantly as well as fully automatically, improved throughout the study. Overall, the most significant result of the work is an improvement in the diagnostic power of the whole diagnostic process. The approach supports, but does not replace, physicians' diagnostic process, and can assist in decisions on the cost-effectiveness of diagnostic tests. \u00a9 2012, IGI Global.", "Language": "en", "Citations": "0"},
{"Title": "Space and time in new media objects - VideoSpace, Friedhof Laguna, Mouseion Serapeion, S.O.L.A.R.I.S., To Brecknock\u22ef, Data Dune", "Authors": ["Vaupotic A.", "Bovcon N."], "Keywords": ["Computer game", "Human-computer interaction", "Internet art", "New media art", "Virtual space"], "Date": "2008", "Abstract": "The space and time are two aspects that a new media object as a newly constructed communication model (by means of information technologies) reconfigures in each of its instances. The paper shows the different space and time forms in the new media art works by Narvika Bovcon and Ale\u0161 Vaupoti\u010d: VideoSpace, Friedhof Laguna, Mouseion Serapeion, S.O.L.A.R.I.S., To Brecknock, while my fearful head is on, If you look back, it won't be there anymore (on the Data Dune platform).", "Language": "en", "Citations": "0"},
{"Title": "Method for selection of motor insurance fraud management system components based on business performance", "Authors": ["Furlan S.", "Vasilecas O.", "Bajec M."], "Keywords": ["Business performance", "Fraud management", "Fraud management system", "Fraud management system development", "Key performance indicators", "Motor insurance"], "Date": "2011", "Abstract": "Fraud in motor insurance is assessed to incur annual losses in the range of 100 billion dollars. While much research exists in the fraud management field, majority only deals with partial problems and presupposes the independence of specific fraud management activities. Researches on components of fraud management system are rarely explicitly related to business performance improvements. These results in a common problem, which can be observed on the practitioners' side: only small amount of companies can objectively assess which of the many fraud management system components proposed by researchers and vendors will help to solve their problems in fraud management. The method proposed in this paper can be used as a strategic tool for improvement of fraud management process in motor insurance companies. The method is designed to be used for a selection of fraud management system components, and is based on business performance. The input for the method is a set of key performance indicators that an insurance companies wish to improve. The result is a set of activities, which should be improved, and a set of fraud management system components that should be used to improve these activities. The paper presents and explains the method and its components. The method components have been developed based on the data received from Slovenian motor insurance companies and method is evaluated in three case studies. \u00a9 2011 Vilnius Gediminas Technical University (VGTU) Press Technika.", "Language": "en", "Citations": "2"},
{"Title": "Predictive model for estimating risk of crush syndrome: A data mining approach", "Authors": ["Aoki N.", "Demsar J.", "Zupan B.", "Mozina M.", "Pretto E.A.", "Oda J.", "Tanaka H.", "Sugimoto K.", "Yoshioka T.", "Fukui T."], "Keywords": ["Crush injury", "Crush syndrome", "Data mining", "Disaster", "Earthquake", "Prognostic model", "Risk factors"], "Date": "2007", "Abstract": "BACKGROUND: There is no standard triage method for earthquake victims with crush injuries because of a scarcity of epidemiologic and quantitative data. We conducted a retrospective cohort study to develop predictive models based on clinical data for crush injury in the Kobe earthquake. METHODS: The medical records of 372 patients with crush injuries from the Kobe earthquake were retrospectively analyzed. Twenty-one risk factors were assessed with logistic regression analysis for three outcomes relating to crush syndrome. Two types of predictive triage models-initial evaluation in the field and secondary assessment at the hospital-were developed using logistic regression analysis. Classification accuracy, Brier score and area under the receiver operating characteristic curve (AUC) were used to evaluate the model. RESULTS: The initial triage model, which includes pulse rate, delayed rescue, and abnormal urine color, has an AUC of 0.73. The secondary model, which includes WBC, tachycardia, abnormal urine color, and hyperkalemia, shows an AUC of 0.76. CONCLUSIONS: These triage models may be especially useful to nondisaster experts for distinguishing earthquake victims at high risk of severe crush syndrome from those at lower risk. Application of the model may allow relief workers to better utilize limited medical and transportation resources in the aftermath of a disaster. \u00a9 2007 Lippincott Williams & Wilkins, Inc.", "Language": "en", "Citations": "15"},
{"Title": "Application of PKI in health care - Needs, ambitions, prospects", "Authors": ["Suselj M.", "Marcun T.", "Trcek D.", "Kandus G."], "Keywords": ["Health Cards", "Health Professional Cards", "PKI"], "Date": "2003", "Abstract": "Through continual development and considerable investment over the past years, Slovenia has established an information infrastructure providing efficient data links between all the health care actors. This includes furnishing all the citizens and health workers with microprocessor cards-health insurance card and health professional card. These tools have significantly simplified different procedures in the health care and brought services closer to insured persons. The know-how and experiences gathered to day have given rise to vivid discussions of further development steps: introduction of new contents on the infrastructure in place and technological upgrading, in particular progressive incorporation of the PKI concept and thereby integration of card and network solutions to provide an efficient and secure communication environment. This paper outlines key perspectives of the future developments in this segment. With the volume of health care data communications through internet growing steeply, and with the paramount importance of patient-doctor trust and confidence, security tools and solutions in the health care are a critical need.", "Language": "en", "Citations": "0"},
{"Title": "Vulkan abstraction layer for large data remote rendering system", "Authors": ["Lavric P.", "Bohak C.", "Marolt M."], "Keywords": ["Graphics library abstraction layer", "Real-time rendering", "Vulkan API"], "Date": "2018", "Abstract": "New graphics APIs require users to implement a lot of needed functionality, such as memory management, by themselves. In this paper we present an abstraction layer build on top of such API, in our case the Vulkan API, for purpose of off-screen rendering of large data. We also present a use case for such abstraction layer implementation \u2013 a remote rendering system for simple Path Tracing accessible through web-based client-side application. The preliminary evaluation results show that implementation of simple Path Tracer is significantly faster then comparable implementation in OpenCL. In conclusion we also present possible extension and improvements of the developed abstraction layer.", "Language": "en", "Citations": "0"},
{"Title": "Automatic design of optimal logic circuits based on ternary quantum-dot cellular automata", "Authors": ["Janez M.", "Bajec I.L.", "Pecar P.", "Jazbec A.", "Zimic N.", "Mraz M."], "Keywords": ["Computer-aided design", "Heuristics", "Iterative deepening", "Logic circuits", "Quantum-dot cellular automata", "Ternary logic", "Ternary quantum-dot cell"], "Date": "2008", "Abstract": "This paper treats the problems involved in the design of logic circuits based on novel processing platform. It begins with description of the ternary quantum-dot cell, an extended classic binary cell. These cells are basic building blocks of quantum-dot cellular automata. They are used to construct simple structures with inputs and output which implement some ternary logic function. These structures are employed as building blocks of larger and more complex circuits. The computer-aided design tool that finds an optimal implementation of a circuit by bottom-up approach is described. The search of a solution is based on iterative deepening. Since searching over all possible solutions is too computationally complex, heuristics are used to reduce the computation time.", "Language": "en", "Citations": "8"},
{"Title": "Combinations of susceptibility genes are associated with higher risk for multiple sclerosis and imply disease course specificity", "Authors": ["Akkad D.A.", "Olischewsky A.", "Reiner F.", "Hellwig K.", "Esser S.", "Epplen J.T.", "Curk T.", "Gold R.", "Haghikia A."], "Keywords": [], "Date": "2015", "Abstract": "Multiple sclerosis (MS) is a chronic autoimmune disease of the central nervous system that predominantly affects young adults. The genetic contributions to this multifactorial disease were underscored by a genome wide association study (GWAS) conducted by the International Multiple Sclerosis Genetic Consortium in a multinational cohort prompting the discovery of 57 non-MHC MS-associated common genetic variants. Hitherto, few of these newly reported variants have been replicated in larger independent patient cohorts. We genotyped a cohort of 1033 MS patients and 644 healthy controls with a consistent genetic background for the 57 non-MHC variants reported to be associated with MS by the first large GWAS as well as the HLA DRB1\u22171501 tagging SNP rs3135388. We robustly replicated three of the 57 non-MHC reported MS-associated single nucleotide polymorphisms (SNPs). In addition, our study revealed several genotype-genotype combinations with an evidently higher degree of disease association than the genotypes of the single SNPs. We further correlated well-defined clinical phenotypes, i.e. ataxia, visual impairment due to optic neuritis and paresis with single SNPs and genotype combinations, and identified several associations. The results may open new avenues for clinical implications of the MS associated genetic variants reported from large GWAS.", "Language": "en", "Citations": "2"},
{"Title": "Generalized information-theoretic measures for feature selection", "Authors": ["Sluga D.", "Lotric U."], "Keywords": ["Feature selection", "Information theory", "Renyi entropy", "Tsallis entropy"], "Date": "2013", "Abstract": "Information-theoretic measures are frequently employed to select the most relevant subset of features from datasets. This paper focuses on the analysis of continuous-valued features. We compare the common approach with discretization of features prior the analysis, to the direct usage of exact values. Due to the overwhelming costs of computing continuous information-theoretic measures based on Shannon entropy the Renyi and Tsallis generalized measures are considered. To enable computation with continuous Tsallis measures a novel modification of the information potential is introduced. The quality of the analysed measures was assessed indirectly through the classification accuracy in conjuction with the greedy feature selection process. The experiments on datasets from UCI repository show considerable improvements of the results when using both generalized continuous measures. \u00a9 2013 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "2"},
{"Title": "Testing data dependency for microprocessors with a short SIMD instruction set Ugotavljanje podatkovne odvisnosti za procesorje z naborom ukazov SIMD", "Authors": ["Bulic P.", "Dobravec T."], "Keywords": ["Data dependency", "Multimedia extensions", "SIMD instructions", "Vectorizing compilers"], "Date": "2009", "Abstract": "In this paper we present an algorithm for the data-dependency problem related to microprocessors with a multimedia extension (i.e., including short SIMD instructions). Actually, there is a number of data-dependency tests proposed in literature (Banerjee test [2], GCD test [11], Omega test [7], Power test [10], etc.) which are all based on solving the linear dependence system. These tests would prohibit any vectorization if dependence exists even though this dependence would not be violeted after the vectorization. As an extension to the Banerjee test, the presented method checks whether the vectorization affects any existing depenedence relation by checking the distance between two conflicting memory references. We assume a p-nested for loop. In short SIMD processing, we can process V", "Language": "en", "Citations": "0"},
{"Title": "Transcription of polyphonic vocal music with a repetitive melodic structure", "Authors": ["Bohak C.", "Marolt M."], "Keywords": [], "Date": "2016", "Abstract": "This paper presents a novel method for transcription of folk music that exploits its specifics to improve transcription accuracy. In contrast to most commercial music, folk music recordings may contain various inaccuracies as they are usually performed by amateur musicians and recorded in the field. If we use standard approaches for transcription, these inaccuracies are reflected in erroneous pitch estimates. On the other hand, the structure of western folk music is usually simple as songs are often composed of repeated melodic parts. In our approach we make use of these repetitions to increase transcription robustness and improve its accuracy. The proposed method fuses three sources of information: (1) frame-based multiple F0 estimates, (2) song structure, and (3) pitch drift estimates. It first selects a representative segment of the analyzed song and aligns all the other segments to it considering temporal as well as frequency deviations. Information from all segments is summarized and used in a two-layer probabilistic model based on explicit duration HMMs, to segment frame-based information into notes. The method is evaluated with state-of-the-art transcription methods where we show that significant improvement in accuracy can be achieved.", "Language": "en", "Citations": "2"},
{"Title": "Qualitative trust modeling in SOA", "Authors": ["Kovac D.", "Trcek D."], "Keywords": ["Reputation", "SOA", "Trust", "Web services", "XML"], "Date": "2009", "Abstract": "Trust among cooperating agents is an essential precondition for every e-business transaction. It is becoming increasingly vital in service oriented architectures (SOAs), where services from various administration domains are deployed. Traditional hard security mechanisms with different techniques of authorization, access control and information security services give a solid foundation, but they fail when cooperating entities act deceitfully. Trust as a soft social security mechanism can protect against such threats and consequently improves the quality of services and reliability of service providers. This paper presents an abstract trust model that applies complementary qualitative methodology which addresses the core of trust as socio-cognitive phenomenon. The model complements existing quantitative methodologies and is applied in the web services environment that enables trust management in SOAs. \u00a9 2009 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "42"},
{"Title": "Speeding up shortest path algorithms", "Authors": ["Brodnik A.", "Grgurovic M."], "Keywords": ["All pairs shortest path", "Single source shortest path"], "Date": "2012", "Abstract": "Given an arbitrary, non-negatively weighted, directed graph G = (V, E) we present an algorithm that computes all pairs shortest paths in time O(m*n + mlgn + nT", "Language": "en", "Citations": "0"},
{"Title": "Outcome of small cell lung cancer (SCLC) patients with brain metastases in a routine clinical setting", "Authors": ["Lekic M.", "Kovac V.", "Triller N.", "Knez L.", "Sadikov A.", "Cufer T."], "Keywords": ["brain metastases", "prophylactic cranial irradiation", "small-cell lung cancer"], "Date": "2012", "Abstract": "Background. Small cell lung cancer (SCLC) represents approximately 13 to 18% of all lung cancers. It is the most aggressive among lung cancers, mostly presented at an advanced stage, with median survival rates of 10 to12 months in patients treated with standard chemotherapy and radiotherapy. In approximately 15-20% of patients brain metastases are present already at the time of primary diagnosis; however, it is unclear how much it influences the outcome of disease according the other metastatic localisation. The objective of this analysis was to evaluate the median survival of SCLC patients treated by specific therapy (chemotherapy and/or radiotherapy) with regard to the presence or absence of brain metastases at the time of diagnosis. Patients and methods. All SCLC patients have been treated in a routine clinical practice and followed up at the University Clinic Golnik in Slovenia. In the retrospective study the medical files from 2002 to 2007 were review. All patients with cytological or histological confirmed disease and eligible for specific oncological treatment were included in the study. They have been treated according to the guidelines valid at the time. Chemotherapy and regular followed-up were carried out at the University Clinic Golnik and radiotherapy at the Institute of Oncology Ljubljana. Results. We found 251 patients eligible for the study. The median age of them was 65 years, majority were male (67%), smokers or ex-smokers (98%), with performance status 0 to 1 (83%). At the time of diagnosis no metastases were found in 64 patients (25.5%) and metastases outside the brain were presented in 153 (61.0%). Brain metastases, confirmed by a CT scan, were present in 34 patients (13.5%), most of them had also metastases at other localisations. All patients received chemotherapy and all patients with confirmed brain metastases received whole brain irradiation (WBRT). The radiotherapy with radical dose at primary tumour was delivered to 27 patients with limited disease and they got 4-6 cycles of chemotherapy. Median overall survival (OS) of 34 patients with brain metastases was 9 months (95% CI 6-12) while OS of 153 patients with metastases in other locations was 11 months (95% CI 10-12); the difference did not reach the level of significance (p = 0.62). As expected, the OS of patients without metastases at the time of primary diagnosis turned out to be significantly better compared to the survival of patients with either brain or other location metastases at the primary diagnosis (15 months vs 9 and 11 months, respectively, p < 0.001). Conclusions. In our investigated population, the prognosis of patients with extensive SCLS with brain metastases at the primary diagnosis treated with chemotherapy and WBRT was not significantly worse compared to the prognosis of patients with extensive SCLC and metastases outside the brain. In extensive SCLC brain metastases were not a negative prognostic factor per se if the patients were able to be treated appropriately. However, the survival rates of extensive SCLC with or without brain metastases remained poor and novel treatment approaches are needed. The major strength of this study is that it has been done on a population of patients treated in a routine clinical setting.", "Language": "en", "Citations": "22"},
{"Title": "Monitoring and automatic detection of the cold-ring patterns atop deep convective clouds using Meteosat data", "Authors": ["Zibert M.I.", "Zibert J."], "Keywords": ["Automatic detection algorithm", "Cold ring shape", "Cold U/V shape", "Deep convective storm", "Meteosat second generation", "Storm top"], "Date": "2013", "Abstract": "This paper presents a newly established database of deep convective storms that exhibit a cold ring at their cloud top, as observed in enhanced infrared (IR) window satellite imagery. The database consists of cold-ring patterns as seen on the Meteosat data over the summer period between 2006 and 2010 for Slovenia and parts of Italy, Austria, Hungary and Croatia. It includes 139 cold rings at different stages, where typically large hail was reported on the ground and as such it serves as an important source of information for any quantitative analysis of the cold rings in this region. The typical characteristics of cold rings in this database are presented and discussed. It was found that the median of the difference between the minimum brightness temperature in the cold ring and the maximum brightness temperature in the central warm spot is 7.1. K, and that the median distance between such pair is 27 km. In addition, the paper presents in detail a new objective satellite-based method for cold-ring or cold-U/V pattern detection on storm tops in infrared imagery. This method uses a combination of infrared brightness temperature from the Meteosat Spinning Enhanced Visible and Infrared Imager (SEVIRI) and the tropopause temperatures from radiosonde measurements. The method was built and evaluated on the cold-ring patterns from the presented database. \u00a9 2012 Elsevier B.V.", "Language": "en", "Citations": "8"},
{"Title": "Logarithmic multiplier in hardware implementation of neural networks", "Authors": ["Lotric U.", "Bulic P."], "Keywords": ["FPGA", "Iterative logarithmic multiplier", "Neural network"], "Date": "2011", "Abstract": "Neural networks on chip have found some niche areas of applications, ranging from massive consumer products requiring small costs to real-time systems requiring real time response. Speaking about latter, iterative logarithmic multipliers show a great potential in increasing performance of the hardware neural networks. By relatively reducing the size of the multiplication circuit, the concurrency and consequently the speed of the model can be greatly improved. The proposed hardware implementation of the multilayer perceptron with on chip learning ability confirms the potential of the concept. The experiments performed on a Proben1 benchmark dataset show that the adaptive nature of the proposed neural network model enables the compensation of the errors caused by inexact calculations by simultaneously increasing its performance and reducing power consumption. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "5"},
{"Title": "Adding discriminative power to hierarchical compositional models for object class detection", "Authors": ["Kristan M.", "Boben M.", "Tabernik D.", "Leonardis A."], "Keywords": ["categorization", "compositional models", "discriminative parts", "hierarchical models"], "Date": "2013", "Abstract": "In recent years, hierarchical compositional models have been shown to possess many appealing properties for the object class detection such as coping with potentially large number of object categories. The reason is that they encode categories by hierarchical vocabularies of parts which are shared among the categories. On the downside, the sharing and purely reconstructive nature causes problems when categorizing visually-similar categories and separating them from the background. In this paper we propose a novel approach that preserves the appealing properties of the generative hierarchical models, while at the same time improves their discrimination properties. We achieve this by introducing a network of discriminative nodes on top of the existing generative hierarchy. The discriminative nodes are sparse linear combinations of activated generative parts. We show in the experiments that the discriminative nodes consistently improve a state-of-the-art hierarchical compositional model. Results show that our approach considers only a fraction of all nodes in the vocabulary (less than 10%) which also makes the system computationally efficient. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "Roadmap to anticipatory mobile computing", "Authors": ["Pejovic V."], "Keywords": ["Anticipatory computing", "Mobile computing", "Pervasive computing", "Wireless networks"], "Date": "2014", "Abstract": "Growing pervasiveness and personalisation of mobile computing devices, have raised the need for their autonomous, yet intelligent proactive actioning. Such a functioning encapsulated in the concept of anticipatory computing has been a subject of academic research for more than thirty years. Yet, before the recent advances in ubiquitous computing, anticipatory computing remained confined primarily to theoretical explorations. In this paper we discuss the basics of anticipatory mobile computing, a novel concept that builds upon mobile sensing, machine learning and the intrinsic ways mobile devices are nowadays used, in order to enable novel proactive and personalised applications.", "Language": "en", "Citations": "0"},
{"Title": "BPMN extensions for automating cloud environments using a two-layer orchestration approach", "Authors": ["Dukaric R.", "Juric M.B."], "Keywords": ["Automation", "BPMN 2.0.2", "Cloud computing", "Cloud orchestration"], "Date": "2018", "Abstract": "Cloud orchestration describes the automated arrangement, coordination, and management of complex cloud systems, middleware and services, and is realized by orchestrating workflows. To achieve an end-to-end cloud orchestration, workflow designers usually have to cope with integration challenges between two different technologies \u2013 one that entails technical cloud orchestration and another comprising business-level orchestration. This however presents a complex undertaking for workflow designers, as they have to gain sufficient knowledge and expertise of two diverse technologies in order to automate cloud-specific tasks across two different domains. Introduction of a unified orchestration platform would solve these issues, as it would deliver a common vocabulary for different types of workflow designers and would provide them with a single platform for orchestrating both business and technical activities, without having to face the integration complexities. The main objective of this paper is to provide support for cloud-specific workflows in BPMN business process engines. To achieve this objective we (1) define a meta-model for modeling cloud workflows, (2) extend BPMN 2.0.2 specification to orchestrate cloud-specific workflow activities, and (3) implement a meta-model with BPMN extensions by showing how cloud orchestration workflow elements (i.e. activities and workflow control) map onto extended BPMN elements. As a part of the evaluation we measure process size and complexity of two process models using various process metrics. The results have shown that when using our proposed BPMN extensions, the overall size and complexity of the use case process under test has been reduced by more than half on an average. We also improve the readability of BPMN process.", "Language": "en", "Citations": "0"},
{"Title": "SONIC: A system for transcription of piano music", "Authors": ["Marolt M.", "Privosnik M."], "Keywords": ["Adaptive oscillators", "Music transcription", "Neural networks", "Polyphonic pitch recognition"], "Date": "2001", "Abstract": "This paper presents our system for transcription of polyphonic piano music - SONIC. SONIC takes an audio signal of a piano performance and tries to determine which notes were played by the performer, thus producing a list of notes and note onset times approximately matching the performance. SONIC is composed of three main parts: partial extractor, onset detector and note recognizer. We present these parts in more detail below, as well as some results obtained by using the system on recordings of piano music.", "Language": "en", "Citations": "3"},
{"Title": "Learning part-based spatial models for laser-vision-based room categorization", "Authors": ["Ursic P.", "Leonardis A.", "Skocaj D.", "Kristan M."], "Keywords": ["discriminative dictionary learning", "laser-vision fusion", "part-based models", "Room categorization"], "Date": "2017", "Abstract": "Room categorization, that is, recognizing the functionality of a never before seen room, is a crucial capability for a household mobile robot. We present a new approach for room categorization that is based on two-dimensional laser range data. The method is based on a novel spatial model consisting of mid-level parts that are built on top of a low-level part-based representation. The approach is then fused with a vision-based method for room categorization, which is also based on a spatial model consisting of mid-level visual parts. In addition, we propose a new discriminative dictionary learning technique that is applied for part-dictionary selection in both laser-based and vision-based modalities. Finally, we present a comparative analysis between laser-based, vision-based, and laser-vision-fusion-based approaches in a uniform part-based framework, which is evaluated on a large dataset with several categories of rooms from domestic environments.", "Language": "en", "Citations": "3"},
{"Title": "Stereo obstacle detection for unmanned surface vehicles by IMU-assisted semantic segmentation", "Authors": ["Bovcon B.", "Mandeljc R.", "Pers J.", "Kristan M."], "Keywords": ["Computer vision", "Inertial measurement unit", "Marine navigation", "Obstacle detection", "Semantic segmentation", "Sensor fusion", "Stereo vision", "Unmanned surface vehicles"], "Date": "2018", "Abstract": "A new obstacle detection algorithm for unmanned surface vehicles (USVs) is presented. A state-of-the-art graphical model for semantic segmentation is extended to incorporate boat pitch and roll measurements from the on-board inertial measurement unit (IMU), and a stereo verification algorithm that consolidates tentative detections obtained from the segmentation is proposed. The IMU readings are used to estimate the location of horizon line in the image, which automatically adjusts the priors in the probabilistic semantic segmentation model. We derive the equations for projecting the horizon into images, propose an efficient optimization algorithm for the extended graphical model, and offer a practical IMU\u2013camera\u2013USV calibration procedure. Using an USV equipped with multiple synchronized sensors, we captured a new challenging multi-modal dataset, and annotated its images with water edge and obstacles. Experimental results show that the proposed algorithm significantly outperforms the state of the art, with nearly 30% improvement in water-edge detection accuracy, an over 21% reduction of false positive rate, an almost 60% reduction of false negative rate, and an over 65% increase of true positive rate, while its Matlab implementation runs in real-time.", "Language": "en", "Citations": "5"},
{"Title": "sonicLamination \u2013 From a concept to artistic binding of visual and sound domains by using advanced technology", "Authors": ["Trcek D.", "Trcek G."], "Keywords": ["Concepts", "Digital technology", "Fine art", "Interactivity", "Lamination", "Layering", "Semantic enrichment", "Visual to audio transformations"], "Date": "2019", "Abstract": "Layers can be considered as powerful mental concept with proven suitability in numerous areas ranging from fine arts to engineering. To further expand their potential and apply them to computers-based art, sonicLamination method is presented here. The method builds upon computer-based (programmatic) transformations that take into account various enriching perspectives, e.g., physiological laws of human perception. By doing so, it achieves higher levels of semantic relationships when binding visual and sound domains. Put another way, sonicLamination is not just about a new kind of conceptual art, but about exploring new venues in fine arts through technology. Therefore, this paper presents how sonicLamination can be used to exceed its conceptual roots in order to lead to new ways of artistic deployment ranging from enhancing humans sensations to artificially generated soundscapes. This is due to the fact that the core principles of sonicLamination can be naturally extended to provide means for a plethora of artistic approaches when technologically transforming visual inputs into their sonic representations.", "Language": "en", "Citations": "0"},
{"Title": "Reliable classifications with machine learning", "Authors": ["Kukar M.", "Kononenko I."], "Keywords": [], "Date": "2002", "Abstract": "In the past decades Machine Learning algorithms have been successfully used in numerous classification problems. While they usually significantly outperform domain experts (in terms of classification accuracy or otherwise), they are mostly not being used in practice. A plausible reason for this is that it is difficult to obtain an unbiased estimation of a single classification\u2019s reliability. In the paper we propose a general transductive method for estimation of classification\u2019s reliability on single examples that is independent of the applied Machine Learning algorithm. We compare our method with existing approaches and discuss its advantages. We perform extensive testing on 14 domains and 6 Machine Learning algorithms and show that our approach can frequently yield more than 100% improvement in reliability estimation performance.", "Language": "en", "Citations": "41"},
{"Title": "Fixed-point multiplication and division in the logarithmic number system: A way to low-power design", "Authors": ["Bulic P."], "Keywords": ["Computer arithmetic", "Logarithm number system", "Power dissipation"], "Date": "2013", "Abstract": "In this article we present the use of the logarithmic number system (LNS) to implement fixed-point multiplication and division. LNS has recently attracted the interest of researchers for its low-power properties. The reduction of power dissipation in LNS arises from the simplification of basic arithmetic operations. In this paper we give a survey of the recently proposed digital circuits for logarithm and anti-logarithm conversion and multiplication and division in LNS. We also compare these methods in terms of accuracy, area, time and power. Finally, we give an overview of the real world applications that benefit form the use of LNS arithmetic.", "Language": "en", "Citations": "2"},
{"Title": "An iterative mitchell's algorithm based multiplier", "Authors": ["Babic Z.", "Avramovic A.", "Bulic P."], "Keywords": ["Computer arithmetic", "Digital signal processing", "Logarithmic number system", "Multiplier"], "Date": "2008", "Abstract": "This paper presents a new multiplier with possibility to achieve an arbitrary accuracy. The multiplier is based upon the same idea of numbers representation as Mitchell's algorithm, but does not use logarithm approximation. The proposed iterative algorithm is simple and efficient, achieving an error percentage as small as required, until the exact result. Hardware solution involves adders and shifters, so it is not gate and power consuming. Parallel circuits are used for error correction. The error summary for operands ranging from 8-bits to 16-bits operands indicates very low error percentage with only two parallel correction circuits. \u00a92008 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "Pessimistic heuristics beat optimistic ones in real-time search", "Authors": ["Sadikov A.", "Bratko I."], "Keywords": [], "Date": "2006", "Abstract": "Admissibility is a desired property of heuristic evaluation functions, because when these heuristics are used with complete search methods, such as A* and RBFS, they guarantee that an optimal solution will be found. Since every optimistic heuristic function is admissible, optimistic functions are widely used. We show, however, that with incomplete, real-time search, optimistic functions lose their appeal, and in fact they may hinder the search under quite reasonable conditions. Under these conditions the exact opposite is to be preferred, i.e. pessimistic heuristic functions that never underestimate the difficulty of the problem. We demonstrate that such heuristics behave better than optimistic ones of equal quality on a standard testbed using RTA* search method. \u00a9 2006 The authors.", "Language": "en", "Citations": "10"},
{"Title": "Tracking by identification using computer vision and radio", "Authors": ["Mandeljc R.", "Kovacic S.", "Kristan M.", "Pers J."], "Keywords": ["Computer vision", "Identification", "Multi-camera", "Person localization", "Radio", "Sensor fusion", "Tracking", "Tracking-by-identification"], "Date": "2013", "Abstract": "We present a novel system for detection, localization and tracking of multiple people, which fuses a multi-view computer vision approach with a radio-based localization system. The proposed fusion combines the best of both worlds, excellent computer-vision-based localization, and strong identity information provided by the radio system, and is therefore able to perform tracking by identification, which makes it impervious to propagated identity switches. We present comprehensive methodology for evaluation of systems that perform person localization in world coordinate system and use it to evaluate the proposed system as well as its components. Experimental results on a challenging indoor dataset, which involves multiple people walking around a realistically cluttered room, confirm that proposed fusion of both systems significantly outperforms its individual components. Compared to the radio-based system, it achieves better localization results, while at the same time it successfully prevents propagation of identity switches that occur in pure computer-vision-based tracking. \u00a9 2013 by the authors; licensee MDPI, Basel, Switzerland.", "Language": "en", "Citations": "11"},
{"Title": "Rigidity and separation indices of Paley graphs", "Authors": ["Fijavz G.", "Mohar B."], "Keywords": ["Paley graph", "Rigidity index", "Separation index"], "Date": "2004", "Abstract": "The rigidity and separation indices of Paley graphs were investigated. It was shown that the ratio between separation and rigidity indices of Paley graphs can be arbitrarily large. It was suggested that strongly regular graphs, and Paley graphs in particular, were basic graphs of coherent configurations with two non-trivial classes. The Paley graphs were suggested to attain the maximum possible ratio between separation and rigidity indices.", "Language": "en", "Citations": "8"},
{"Title": "Large-scale computational models of liver metabolism: How far from the clinics?", "Authors": ["Cvitanovic T.", "Reichert M.C.", "Moskon M.", "Mraz M.", "Lammert F.", "Rozman D."], "Keywords": [], "Date": "2017", "Abstract": "Understanding the dynamics of human liver metabolism is fundamental for effective diagnosis and treatment of liver diseases. This knowledge can be obtained with systems biology/medicine approaches that account for the complexity of hepatic responses and their systemic consequences in other organs. Computational modeling can reveal hidden principles of the system by classification of individual components, analyzing their interactions and simulating the effects that are difficult to investigate experimentally. Herein, we review the state-of-the-art computational models that describe liver dynamics from metabolic, gene regulatory, and signal transduction perspectives. We focus especially on large-scale liver models described either by genome scale metabolic networks or an object-oriented approach. We also discuss the benefits and limitations of each modeling approach and their value for clinical applications in diagnosis, therapy, and prevention of liver diseases as well as precision medicine in hepatology. (Hepatology 2017;66:1323-1334).", "Language": "en", "Citations": "10"},
{"Title": "Towards the bottom-up concept: Extended quantum-dot cellular automata", "Authors": ["Lebar Bajec I.", "Zimic N.", "Mraz M."], "Keywords": ["Multi-valued logic", "Quantum-dot cellular automata", "Switching structures"], "Date": "2006", "Abstract": "In this article we present an extended quantum-dot cellular automaton (QCA) cell. The classical QCA cell is extended in the sense of an enlarged range of its possible stable and usable states. Indeed, in the classical QCA cell the electrons, owing to electrostatic repulsion, align along one of the two diagonal configurations that correspond to their maximal spatial separation. The QCA cell thus has the ability to encode two states - two logic values (0 and 1). By extending the QCA cell with four additional quantum dots, we introduce the extended QCA (EQCA) cell and analyze its behavior, the analysis of which is based on the semi-classical modeling approach. Experiments showed that by using a special interpretation of electron configurations in the EQCA, the range of possible states can be increased from two to three, giving the EQCA cell the ability to encode the logic values (0, 1/2 and 1). The primary motive of this article is to promote the idea of finally switching focus from pure miniaturization and the top-down concept to the bottom-up concept and start extending the currently available approaches to allow for 'richer' processing and data storage capabilities without a major increase in space requirements. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "20"},
{"Title": "Experiments with cost-sensitive feature evaluation", "Authors": ["Robnik-Sikonja M."], "Keywords": [], "Date": "2003", "Abstract": "Many machine learning tasks contain feature evaluation as one of its important components. This work is concerned with attribute estimation in the problems where class distribution is unbalanced or the misclassification costs are unequal. We test some common attribute evaluation heuristics and propose their cost-sensitive adaptations. The new measures are tested on problems which can reveal their strengths and weaknesses.", "Language": "en", "Citations": "6"},
{"Title": "Impact of ERCC1 expression on treatment outcome in small-cell lung cancer patients treated with platinum-based chemotherapy", "Authors": ["Sodja E.", "Knez L.", "Kern I.", "Ovcaricek T.", "Sadikov A.", "Cufer T."], "Keywords": ["ERCC1 protein expression", "Platinum-based chemotherapy", "Response to therapy", "Small-cell lung cancer"], "Date": "2012", "Abstract": "Introduction: The excision repair cross-complementing 1 (ERCC1) protein is an extensively investigated molecular marker because it may decrease sensitivity to platinum-based chemotherapy. Low ERCC1 expression has already been correlated with better treatment efficacy in non-small-cell lung cancer patients treated with platinum-based chemotherapy. However, the data on a prognostic and/or predictive value of ERCC1 in small-cell lung cancer (SCLC) are still very limited. Methods: This retrospective pilot study evaluated the impact of ERCC1 expression levels on response to first-line platinum-based chemotherapy with or without radiotherapy and survival outcomes of 77 SCLC patients. ERCC1 protein expression was determined immunohistochemically in primary tumour tissue. Results: ERCC1 protein expression was positive in 40/77 (51.9%) of our patients. No significant association was found between ERCC1 protein expression and response rate to first-line platinum-based chemotherapy, progression-free survival (PFS), or overall survival (OS), either in the overall population or in patients stratified by disease stage. Conclusions: In our limited group of 77 SCLC patients, ERCC1 protein expression was not found to correlate with either response rate to platinum-based chemotherapy or survival outcomes. Multi-centric prospective trials using a validated method of ERCC1 determination are mandatory in order to obtain a definitive answer on the predictive value of ERCC1 in SCLC. \u00a9 2012 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "12"},
{"Title": "Discrimination between ischemic and artifactual ST segment events in Holter recordings", "Authors": ["Minchole A.", "Jager F.", "Laguna P."], "Keywords": ["Classification analysis", "ECG processing", "Holter recordings", "Ischemia detection"], "Date": "2010", "Abstract": "ST segment changes provide a sensitive marker in the diagnosis of myocardial ischemia in Holter recordings. However, not only do the mechanisms of ischemia result in ST segment deviation, but also heart rate related episodes, body position changes or conduction changes among others, which are considered artifactual events when ischemia is the target. In order to distinguish between them, the very similar signatures of ST modifications has led us to look for other ECG indices such as heart rate-based indices, correlation between the absolute ST segment deviation and heart rate series, the interval between the T", "Language": "en", "Citations": "10"},
{"Title": "Wi-Mind: Wireless mental effort inference", "Authors": ["Matkovic T.", "Pejovic V."], "Keywords": ["Cognitive load", "Signal processing", "Wireless sensing"], "Date": "2018", "Abstract": "From not disturbing a focused programmer, to entertaining a restless commuter waiting for a train, ubiquitous computing devices could greatly enhance their interaction with humans, should they only be aware of the user's cognitive load. However, current means of assessing cognitive load are, with a few exceptions, based on intrusive methods requiring physical contact of the measurement equipment and the user. In this paper we propose Wi-Mind, a system for remote cognitive load assessment through wireless sensing. Wi-Mind is based on a software-defined radio-based radar that measures sub-millimeter movements related to a person's breathing and heartbeats, which, in turn allow us to infer the person's cognitive load. We built and tested the system with 23 volunteers engaged in different tasks. Initial results show that while Wi-Mind manges to detect whether one is engaged in a cognitively demanding task, the inference of the exact cognitive load level remains challenging.", "Language": "en", "Citations": "0"},
{"Title": "Geometric realization of M\u00f6bius triangulations", "Authors": ["Chavez M.J.", "Aijavz G.", "Marquez A.", "Nakamoto A.", "Suarez E."], "Keywords": ["Geometric realization", "M\u00f6bius band", "Projective plane", "Triangulation"], "Date": "2008", "Abstract": "A M\u00f6bius triangulation is a triangulation on the M\u00f6bius band. A geometric realization of a map M on a surface \u03a3 is an embedding of \u03a3 into a Euclidean 3-space \u211d", "Language": "en", "Citations": "2"},
{"Title": "Continuous learning of simple visual concepts using incremental kernel density estimation", "Authors": ["Skocaj D.", "Kristan M.", "Leonardis A."], "Keywords": ["Continuous learning", "Incremental kernel density estimation", "Incremental learning", "Learning associations"], "Date": "2008", "Abstract": "In this paper we propose a method for continuous learning of simple visual concepts. The method continuously associates words describing observed scenes with automatically extracted visual features. Since in our setting every sample is labelled with multiple concept labels, and there are no negative examples, reconstructive representations of the incoming data are used. The associated features are modelled with kernel density probability distribution estimates, which are built incrementally. The proposed approach is applied to the learning of object properties and spatial relations.", "Language": "en", "Citations": "7"},
{"Title": "Argument based machine learning", "Authors": ["Mozina M.", "Zabkar J.", "Bratko I."], "Keywords": ["Argumentation", "Background knowledge", "Knowledge intensive learning", "Learning through arguments", "Machine learning"], "Date": "2007", "Abstract": "We present a novel approach to machine learning, called ABML (argumentation based ML). This approach combines machine learning from examples with concepts from the field of argumentation. The idea is to provide expert's arguments, or reasons, for some of the learning examples. We require that the theory induced from the examples explains the examples in terms of the given reasons. Thus arguments constrain the combinatorial search among possible hypotheses, and also direct the search towards hypotheses that are more comprehensible in the light of expert's background knowledge. In this paper we realize the idea of ABML as rule learning. We implement ABCN2, an argument-based extension of the CN2 rule learning algorithm, conduct experiments and analyze its performance in comparison with the original CN2 algorithm. \u00a9 2007 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "83"},
{"Title": "Tremor Tremor", "Authors": ["Kragelj V.", "Georgiev D.", "Pirtosek Z.", "Ribaric S."], "Keywords": ["Classification", "Mechanisms", "Syndroms"], "Date": "2012", "Abstract": "Tremor is one of the most common disorders in the population of patients diagnosed with movement disorders. In the literature we find several classifications and different types of tremors. Each tremor type has its own characteristics. The most frequently used and widely accepted tremor classification divides tremors according to clinical appearance. First, they are roughly divided into resting tremor and action tremor. Action tremor is then subdivided into postural, kinetic, intention, task specific and isometric tremor. Different types of tremor are further combined into tremor syndromes. Causes and mechanisms of tremor are still unclear. Tremor genesis is explained by four hypothetical mechanisms and one of them is assumed to be dominant for each type of tremor. Correct tremor recognition and diagnosis is necessary for appropriate treatment of tremor patients.", "Language": "en", "Citations": "1"},
{"Title": "Portal technology for integrated, user-centric university Information Systems", "Authors": ["Bajec M.", "Mahnic V.", "Krisper M."], "Keywords": ["E-business", "Portal technology", "Student records", "University information systems"], "Date": "2003", "Abstract": "The paper describes a project that was launched as the first step towards the realisation of the Information Systems (IS) strategy plan developed for the University of Ljubljana. The strategy plan revealed many weaknesses and disadvantages of the current university IS. One of them was the lack of the possibility to utilize e-business technology. As a response the University of Ljubljana decided to renovate the current Student records IS using a technology which seems to be very promising in developing integrated, usercentric IT solutions. In the paper, we shortly introduce the strategy plan and discuss some characteristics of the portal technology. In more detail we focus on the renovation of the Student records IS. In conclusion, we expose some difficulties that we had to deal with during the project. \u00a9 2003 by International Federation for Information Processing.", "Language": "en", "Citations": "0"},
{"Title": "Visual learning and recognition of a probabilistic spatio-temporal model of cyclic human locomotion", "Authors": ["Peternel M.", "Leonardis A."], "Keywords": [], "Date": "2004", "Abstract": "We present a novel representation of cyclic human locomotion based on a set of spatio-temporal curves of tracked points on the surface of a person. We start by extracting a set of continuous, phase aligned spatio-temporal curves from trajectories of random points tracked over several cycles of locomotion in a monocular video sequence. We analyze a PCA representation of a set of cyclic curves, pointing out properties of the representation which can be used for spatio-temporal alignment in tracking and recognition tasks. We model the curve distribution density by a mixture of Gaussians using expectation-maximization algorithm. For recognition, we use maximum a posteriori estimate combined with linear data adaptation. We tested the algorithms on CMU MoBo database with favourable results for the recognition of people \"by walking\" from monocular video sequences captured from the side view.", "Language": "en", "Citations": "6"},
{"Title": "Detection of ductus in mammary gland tissue by computer vision", "Authors": ["Mele K.", "Leonardis A."], "Keywords": [], "Date": "2003", "Abstract": "In cell nucleus images of mammary gland tissue, both features of the nucleus and nuclei group shapes serve as cancer diagnosis criteria. Also, further treatment is dependent on the information about the cell nucleus spatial arrangement formed by malignant, potentially malignant, and normal ductus. In this paper we present an automatic image analysis method that we developed for detecting the structures of nucleus clumps that represent boundaries of ductus. The method consist of the following stages applied in succession: segmentation with a threshold, greedy algorithm, relaxation, and graph search. We tested our algorithm on images of mammary glands tissue. The results indicate that the method can distinguish between healthy ducts that have regular shapes and the malignant ducts (ductal carcinoma in situ) that have irregular shapes with boundaries appearing also inside the ducts. This automatics procedure is a new approach in the area of cytometry and DCIS, which we believe will lead to a more reliable and objective evaluation of architectural characterization of nuclei group.", "Language": "en", "Citations": "0"},
{"Title": "Predicting multicellular function through multi-layer tissue networks", "Authors": ["Zitnik M.", "Leskovec J."], "Keywords": [], "Date": "2017", "Abstract": "Motivation: Understanding functions of proteins in specific human tissues is essential for insights into disease diagnostics and therapeutics, yet prediction of tissue-specific cellular function remains a critical challenge for biomedicine. Results: Here, we present OhmNet, a hierarchy-aware unsupervised node feature learning approach for multi-layer networks. We build a multi-layer network, where each layer represents molecular interactions in a different human tissue. OhmNet then automatically learns a mapping of proteins, represented as nodes, to a neural embedding-based low-dimensional space of features. OhmNet encourages sharing of similar features among proteins with similar network neighborhoods and among proteins activated in similar tissues. The algorithm generalizes prior work, which generally ignores relationships between tissues, by modeling tissue organization with a rich multiscale tissue hierarchy. We use OhmNet to study multicellular function in a multi-layer protein interaction network of 107 human tissues. In 48 tissues with known tissue-specific cellular functions, OhmNet provides more accurate predictions of cellular function than alternative approaches, and also generates more accurate hypotheses about tissue-specific protein actions. We show that taking into account the tissue hierarchy leads to improved predictive power. Remarkably, we also demonstrate that it is possible to leverage the tissue hierarchy in order to effectively transfer cellular functions to a functionally uncharacterized tissue. Overall, OhmNet moves from flat networks to multiscale models able to predict a range of phenotypes spanning cellular subsystems.", "Language": "en", "Citations": "23"},
{"Title": "Automatic attribute construction for basketball modelling", "Authors": ["Vracar P.", "Strumbelj E.", "Kononenko I."], "Keywords": ["Attribute construction", "Markov process", "Match simulation", "NBA", "Sports modelling"], "Date": "2019", "Abstract": "We address the problem of automatic extraction of patterns in the sequence of events in basketball games and construction of statistical models for generating a plausible simulation of a match between two distinct teams. We present a method for automatic construction of an attribute space which requires very little expert knowledge. The attributes are defined as the ratio between the number of entries and exits from higher-level concepts that are identified as groups of similar in-game events. The similarity between events is determined by the similarity between probability distributions describing the preceding and the following events in the observed sequences of game progression. The methodology is general and is applicable to any sports game that can be modelled as a random walk through the state space. Experiments on basketball show that automatically generated attributes are as informative as those derived using expert knowledge. Furthermore, the obtained simulations are in line with empirical data.", "Language": "en", "Citations": "0"},
{"Title": "Towards a real time panoramic depth sensor", "Authors": ["Peer P.", "Solina F."], "Keywords": [], "Date": "2003", "Abstract": "Recently we have presented a system for panoramic depth imaging with a single standard camera. One of the problems of such a system is the fact that we cannot generate a stereo pair of images in real time. This paper presents a possible solution to this problem. Based on a new sensor setup simulations were performed to establish the quality of new results in comparison to results obtained with the old sensor setup. The goal of the paper is to reveal whether the new setup can be used for real time capturing of panoramic depth images and consequently for autonomous navigation of a mobile robot in a room. \u00a9 Springer-Verlag Berlin Heidelberg 2003.", "Language": "en", "Citations": "1"},
{"Title": "High BMI1 mRNA expression in peripheral whole blood is associated with favorable prognosis in advanced non-small cell lung cancer patients", "Authors": ["Koren A.", "Rijavec M.", "Sodja E.", "Kern I.", "Sadikov A.", "Kovac V.", "Korosec P.", "Cufer T."], "Keywords": ["BMI1", "MRNA expression", "Non-small cell lung cancer", "Peripheral whole blood", "Prognosis"], "Date": "2017", "Abstract": "Polycomb group member protein BMI1 is involved in maintaining cell identity, proliferation, differentiation and human oncogenesis. In the present study, we determined BMI1 mRNA expression in whole blood and evaluated the impact of the expression level on the treatment response and survival of 96 advanced NSCLC patients treated with first-line platinum-based chemotherapy. We also determined BMI1 mRNA expression in primary tumors from 22 operable NSCLC patients treated with radical surgery. We found that compared with control subjects, BMI1 mRNA expression in whole blood of advanced NSCLC patients was decreased (P < 0.001). Similarly, we observed decreased BMI1 mRNA expression in primary tumors compared to normal lungs from operable NSCLC patients (P=0.001). We found high BMI1 mRNA expression in blood was associated with longer progression-free survival (PFS) (P=0.049) and overall survival (OS) (P=0.012) in advanced NSCLC patients treated with first-line platinum-based chemotherapy. However, no association between the BMI1 mRNA level and response to chemotherapy was found (P=0.21). Multivariate Cox proportional hazards regression analysis showed elevated BMI1 mRNA level in whole blood was an independent prognostic factor for longer PFS (P=0.012) and OS (P < 0.001). In conclusion, BMI1 mRNA expression in whole blood might represent a new biomarker for the diagnosis and prognosis of NSCLC.", "Language": "en", "Citations": "0"},
{"Title": "The Janes project: language resources and tools for Slovene user generated content", "Authors": ["Fiser D.", "Ljubesic N.", "Erjavec T."], "Keywords": ["Corpora", "Manually annotated datasets", "Slovene language", "Text normalisation", "User generated content"], "Date": "2018", "Abstract": "The paper presents the results of the Janes project, which aimed to develop language resources and tools for Slovene user generated content. The paper first describes the 200 million word Janes corpus, containing tweets, forum posts, news comments, user and talk pages from Wikipedia, and blogs and blog comments, where each text is accompanied by rich metadata. The developed processing tools for Slovene user generated content are presented next, which include a tokeniser, word-normaliser, part-of-speech tagger and lemmatiser, and a named entity recogniser. A set of manually annotated datasets was also produced, both for tool training as well as for linguistic research. The developed resources and tools are made publicly available under Creative Commons licences in the repository of the CLARIN.SI research infrastructure and on GitHub, while the corpora are also available through the CLARIN.SI concordancers.", "Language": "en", "Citations": "2"},
{"Title": "ParkinsonCheck smart phone app", "Authors": ["Sadikov A.", "Groznik V.", "Zabkar J.", "Mozina M.", "Georgiev D.", "Pirtosek Z.", "Bratko I."], "Keywords": [], "Date": "2014", "Abstract": "The paper introduces the ParkinsonCheck application. It is an app for smart phones based on spirography (spiral drawing) intended to detect signs of Parkinson's disease (PD) and essential tremor (ET), which is the main differential diagnosis from PD in the early stage of the disease. The app is equipped with an expert system and is the first such app to be completely automated. Its intended use is twofold: (a) to act as a standalone test for general population, advising potential patients to seek medical help as early as possible, and (b) to be used by neurologists as a portable and inexpensive fully digitalised clinical decision support system. ParkinsonCheck is currently freely available in Slovenia on four mobile platforms as a pilot study. After potentially upgrading its expert system with new learning data, the plan is for it to be translated into English and offered worldwide.", "Language": "en", "Citations": "6"},
{"Title": "Long-term analysis of elemental content in airborne particulate matter by PIXE and positive matrix factorization: Annual trends and seasonal variability during 2003 and 2008", "Authors": ["Praznikar J.", "Cepak F.", "Zibert J."], "Keywords": ["FLEXPART", "Inter and intra annual analysis", "Particulate matter", "PIXE", "Positive matrix factorization", "Trend and seasonal decomposition"], "Date": "2014", "Abstract": "In the presented study a comprehensive statistical analysis of the chemical composition of atmospheric particulate matter was carried out. The data were collected from April 2003 to August 2008 with a 7-day time resolution in the Northern Adriatic Port of Koper and analyzed by the Proton Induced X-ray method (PIXE). The Positive Matrix Factorization (PMF) analysis of fifteen chemical elements identified six source factors, three natural-regional sources and three local-anthropogenic sources. Heavy machinery, industry and iron ore factor were marked as anthropogenic sources. Heavy machinery source was represented by the elements V, Ni and Cu. The elements Fe and Mn are attributed to the Iron ore source and were explained by the proximity of the bulk-cargo warehouse and the intense handling of iron ore in Port of Koper. The heavy industry source represented by Pb and Zn was the only anthropogenic factor, which shows clear seasonal pattern. In contrast to the local-anthropogenic source factors, natural and regional source factors show significant negative trend. The reduction of the crustal elements Ca, Ti and Sr, joined in a soil source, and sulfur-biomass source, represented by elements K and S, have been attributed to more intense precipitation and to the negative trend of the North Atlantic Oscillation (NAO) index. The negative trend of the Cl and Br elements was in line with the negative trend of the wind speed above the sea surface and the significant sea-wave height. \u00a9 2014 Elsevier Ltd.", "Language": "en", "Citations": "5"},
{"Title": "High-Resolution RNA Maps Suggest Common Principles of Splicing and Polyadenylation Regulation by TDP-43", "Authors": ["Rot G.", "Wang Z.", "Huppertz I.", "Modic M.", "Lence T.", "Hallegger M.", "Haberman N.", "Curk T.", "von Mering C.", "Ule J."], "Keywords": ["3\u2032 mRNA sequencing", "alternative polyadenylation", "alternative splicing", "clustered sequence motifs", "expressRNA", "iCLIP", "positional regulatory principles", "RNA map", "RNAmotifs2", "TDP-43"], "Date": "2017", "Abstract": "Many RNA-binding proteins (RBPs) regulate both alternative exons and poly(A) site selection. To understand their regulatory principles, we developed\u00a0expressRNA, a web platform encompassing computational tools for integration of iCLIP and RNA motif analyses with RNA-seq and 3\u2032 mRNA sequencing. This reveals at nucleotide resolution the \u201cRNA maps\u201d describing how the RNA binding positions of RBPs relate to their regulatory functions. We use this approach to examine how TDP-43, an RBP involved in several neurodegenerative diseases, binds around its regulated poly(A) sites. Binding close to the poly(A) site generally represses, whereas binding further downstream enhances use of the site, which is similar to TDP-43 binding around regulated exons. Our RNAmotifs2 software also identifies sequence motifs that cluster together with the binding motifs of TDP-43. We conclude that TDP-43 directly regulates diverse types of pre-mRNA processing according to common position-dependent principles.", "Language": "en", "Citations": "17"},
{"Title": "Genepath: Software for genetic network exploration", "Authors": ["Zupan B.", "Demsar J.", "Bratko I.", "Juvan P.", "Halter J.A.", "Kuspa A.", "Shaulsky G."], "Keywords": ["Genetic", "Inference", "Network"], "Date": "2002", "Abstract": "Genetic pathways and networks are often used in the analysis of biological phenomena. They are constructed by determining the effect of mutations in different genes on the investigated phenomenon and by determining the relationships between the mutated genes. We describe a software tool named GenePath (http://genepath.org/) which serves as an assistant for genetic pathway discovery. The pathways and genetic networks are constructed in two steps. In the first step, genetic data are matched to a set of inference patterns of the type \"IF a combination of experiments is found in the data, THEN a relation between genes and phenotypic states can be assumed\". The discovered relations impose constraints over the plausible genetic network, which is constructed in the second step such that all the constraints are satisfied. In addition to automation of the network discovery process, GenePath provides an 'explanation', the tracing of any network relation back to the pertinent experimental data. GenePath was trained on a genetic pathway that regulates the transition from growth to development in the social amoeba Dictyostelium discoideum and tested successfully on several other problems, demonstrating its utility as a general-purpose intelligent assistant to biologists in their explorative analysis of genetic data.", "Language": "en", "Citations": "0"},
{"Title": "When is it better not to look ahead?", "Authors": ["Nau D.S.", "Lustrek M.", "Parker A.", "Bratko I.", "Gams M."], "Keywords": ["Game-tree search", "Lookahead pathology", "Minimax"], "Date": "2010", "Abstract": "In situations where one needs to make a sequence of decisions, it is often believed that looking ahead will help produce better decisions. However, it was shown 30 years ago that there are \"pathological\" situations in which looking ahead is counterproductive. Two long-standing open questions are (a) what combinations of factors have the biggest influence on whether lookahead pathology occurs, and (b) whether it occurs in real-world decision-making. This paper includes simulation results for several synthetic game-tree models, and experimental results for three well-known board games: two chess endgames, kalah (with some modifications to facilitate experimentation), and the 8-puzzle. The simulations show the interplay between lookahead pathology and several factors that affect it; and the experiments confirm the trends predicted by the simulation models. The experiments also show that lookahead pathology is more common than has been thought: all three games contain situations where it occurs. \u00a9 2010 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "17"},
{"Title": "The embedded left LR parser", "Authors": ["Slivnik B."], "Keywords": [], "Date": "2011", "Abstract": "A parser called the embedded left LR(k) parser is defined. It is capable of (a) producing the prefix of the left parse of the input string and (b) stopping not on the end-of-file marker but on any string from the set of lookahead strings fixed at the parser generation time. It is aimed at automatic construction of LL(k) parsers that use embedded LR(k) parsers to resolve LL(k) conflicts. The conditions regarding the termination of the embedded left LR(k) parser if used within LL(k) (and similar) parsers are defined and examined in-depth. As the embedded LR(k) parser produces the prefix of the left parse, the LL(k) parser augmented with embedded LR(k) parsers still produces the left parse and the compiler writer does not need to bother with different parsing strategies during the compiler implementation. \u00a9 2011 Polish Info Processing Soc.", "Language": "en", "Citations": "5"},
{"Title": "Convexity in scientific collaboration networks", "Authors": ["Subelj L.", "Fiala D.", "Ciglaric T.", "Kronegger L."], "Keywords": ["Centrality", "Co-authorship", "Convex skeletons", "Convexity", "Weak links"], "Date": "2019", "Abstract": "Convexity in a network (graph) has been recently defined as a property of each of its subgraphs to include all shortest paths between the nodes of that subgraph. It can be measured on the scale [0, 1] with 1 being assigned to fully convex networks. The largest convex component of a graph that emerges after the removal of the least number of edges is called a convex skeleton. It is basically a tree of cliques, which has been shown to have many interesting features. In this article the notions of convexity and convex skeletons in the context of scientific collaboration networks are discussed. More specifically, we analyze the co-authorship networks of Slovenian researchers in computer science, physics, sociology, mathematics, and economics and extract convex skeletons from them. We then compare these convex skeletons with the residual graphs (remainders) in terms of collaboration frequency distributions by various parameters such as the publication year and type, co-authors' birth year, status, gender, discipline, etc. We also show the top-ranked scientists by four basic centrality measures as calculated on the original networks and their skeletons and conclude that convex skeletons may help detect influential scholars that are hardly identifiable in the original collaboration network. As their inherent feature, convex skeletons retain the properties of collaboration networks. These include high-level structural properties but also the fact that the same authors are highlighted by centrality measures. Moreover, the most important ties and thus the most important collaborations are retained in the skeletons.", "Language": "en", "Citations": "0"},
{"Title": "Network intrusions research: Honeypot Simx Raziskovanje omre\u017enih napadov: Muholovec Simx", "Authors": ["Ciglaric M.", "Mavsar S."], "Keywords": ["Honeynet", "Honeypot", "Intrusion", "Intrusion detection system (IDS)", "Network security"], "Date": "2010", "Abstract": "Honeypot is a tool for capturing the intruder's activity, to be used later in forensic analysis. It enables better understanding of intrusion techniques, tools and motives. The paper first defines the honeypot concept and overviews the existing products. Their disadvantages are discussed and suggestions for better implementations are given. The third'section describes Simx - our prototype honeypot implementation, which pays due regards to the proposed suggestions. Its architecture is shown in Figure 1 and is described in detail in Section 3.2. Figure 2 shows the architecture of its main component - Simx driver. The main mechanisms are explained, especially the intrusion detection techniques and ways of hiding the honeypot functionality from intruders. Listing 1 shows how Simx detects port scanning attempts. Section 4 describes a prototype testing environment and results of real and simulated intrusion attempts. Figure 3 shows our test environment and Listing 3 shows sources of the captured unauthorized access attempts. The log of the simulated port scanning attempts is shown in Listing 4 and attack results are logged in Listing 5. Listings 6 and 7 show the logs of the captured activity at the terminal and on the file system, respectively. The last section overviews Simx and its main benefits. Although it is currently still in its prototype phase, we believe its future will be bright.", "Language": "en", "Citations": "1"},
{"Title": "Worst case constant time priority queue", "Authors": ["Brodnik A.", "Carlsson S.", "Fredman M.L.", "Karlsson J.", "Munro J.I."], "Keywords": ["Data structure", "Discrete priority queue", "Split tagged tree"], "Date": "2005", "Abstract": "We present a new data structure of size 3M bits, where M is the size of the universe at hand, for realizing a discrete priority queue. When this data structure is used in combination with a new memory topology it executes all discrete priority queue operations in O(1) worst case time. In doing so we demonstrate how an unconventional, but practically implementable, memory architecture can be employed to sidestep known lower bounds and achieve constant time performance. \u00a9 2004 Elsevier Inc. All rihgts reserved.", "Language": "en", "Citations": "5"},
{"Title": "Lightweight protocols and privacy for all-in-silicon objects", "Authors": ["Trcek D."], "Keywords": ["Lightweight protocols", "Privacy", "RFID technology", "Security", "Security policies", "Wireless sensor networks"], "Date": "2013", "Abstract": "Pervasive computing is already becoming a reality and one crucial consequence of this fact is endangered privacy. Now taking into account typical properties of pervasive computing devices, which are weak computing power and stringent energy or power consumption limitations, lightweight solutions are a must. This especially holds true for all-in-silicon objects like radio frequency identification tags, or RFIDs. Many solutions in this area are called lightweight, but being lightweight requires conformance to quantitative requirements using certain metrics. A solution that adheres to such requirements is a new privacy enabling protocol for RFIDs that outperforms other architecturally similar protocols, and this presents the first contribution of this paper. Further, privacy is not only a matter of technical solutions, but increasingly so a matter of organizational processes. This fact calls for further addressing of supporting its formal treatment in business contexts. This paper provides a basis for formal addressing of privacy from business processes perspective, and this is its second main contribution. \u00a9 2013 Elsevier B.V.", "Language": "en", "Citations": "4"},
{"Title": "Predicting Seam Appearance Quality", "Authors": ["Pavlinic D.Z.", "Gersak J.", "Demsar J.", "Bratko I."], "Keywords": ["garment engineering", "garment manufacture", "machine learning methods", "mechanical properties", "prediction", "seam quality"], "Date": "2006", "Abstract": "The appearance of a garment is affected by the quality of the fabrics used in its manufacture, as well as a number of factors determined by the technology of the garment manufacturing process. Since fabric quality, as the most important element of garment appearance, is determined by its mechanical properties, it is obvious that these properties directly impact fabric processing properties. It can be seen through various forms of fabric behavior under the loads that occur in sewing. Investigations of the correlations of the stress and fabric behavior are aimed at constructing a system to predict fabric behavior in garment manufacturing processes, as well as to predict the appearance of the garment to be manufactured. The investigation presented here deals with the impact of fabric mechanical properties on the quality of seam appearance, as defined by seam puckering and work-piece flotation. Machine learning methods included in the Orange software package were used to establish the importance of mechanical properties with respect to fabric behavior. \u00a9 2006, SAGE Publications. All rights reserved.", "Language": "en", "Citations": "28"},
{"Title": "Analysis of Slovenian research community through bibliographic networks", "Authors": ["Kastrin A.", "Klisara J.", "Luzar B.", "Povh J."], "Keywords": ["Interdisciplinarity", "Internationality", "Network analysis", "Productivity", "Research performance", "Scientific collaboration"], "Date": "2017", "Abstract": "Science is a societal process, designed on widely accepted general rules which facilitate its development. Productive researchers are viewed from the perspective of a social network of their interpersonal relations. In this paper we address performance of Slovenian research community using bibliographic networks between the years 1970 and 2015 from various aspects which determine prolific science. We focus on basic determinants of research performance including productivity, collaboration, internationality, and interdisciplinarity. For each of the determinants, we select a set of statistics and network measures to investigate the state of each in every year of the analyzed period. The analysis is based on high quality data from manually curated information systems. We interpret the results by relating them to important historical events impacting Slovenia and to domestic expenditure for research and development. Our results clearly demonstrate causal relations between the performance of research community and changes in wider society. Political and financial stability together with concise measuring of scientific productivity established soon after Slovenia won independence from Yugoslavia in 1991 had positive influence on all determinants. They were further leveraged by foundation of Slovenian research agency and joining EU and NATO. Publish and perish phenomenon, negative impacts of financial crisis in 2008\u20132014 and reshaping the domestic expenditure for research and development after 2008 have also clear response in scientific community. In the paper, we also study the researcher\u2019s career productivity cycles and present the analysis of the career productivity for all registered researchers in Slovenia.", "Language": "en", "Citations": "7"},
{"Title": "EbXML framework Ogrodje ebXML", "Authors": ["Zrnec A."], "Keywords": ["Business scenario", "Business service interface", "EbXML", "Electronic business", "Framework", "Scheme", "XML"], "Date": "2004", "Abstract": "In the last 25 years, the EDI standard hasn't fulfilled the promise of lowering the costs of business information exchange in the medium and small sized companies. Because of the very high costs, only the large enterprises have been able to implement the EDI standard. By today, the XML standard has become the most important format for business information exchange in modern web applications. Many people suspect that the EDI standard is of no value but it turns out that they are wrong. It is very important because we can learn a lot from its 25 years of experiences. They teach us how to build modern frameworks for electronic business, based on XML standards. The vision of the ebXML specifications is to provide a framework which enables a single global electronic marketplace where enterprises of any size and in any geographical location can meet and conduct business with each other through the exchange of XML-based messages. ebXML framework has been developed by two organizations: United Nations Center For Trade And Electronic Business (UN/CEFACT) and Organization for Advancement of Structural Information Standards (OASIS). It deals with both business and technical views of doing electronic business over the Internet. In this paper we present essential parts of the ebXML framework. Because this framework is based on an Open-EDI referential model, we devote special attention to business process specification scheme and core components which describe the business operational view of electronic business. We also introduce other parts of the framework, which are covered by functional service view. Functional service view concerns technical aspects of electronic business. We describe the message service which uses SOAP with the Attachments protocol, collaboration protocol profile and agreement, and the ebXML registry. The ebXML registry is the most important part of the framework because it stores data that are essential for business partners who want to use the ebXML framework.", "Language": "en", "Citations": "0"},
{"Title": "Polygenic analysis and targeted improvement of the complex trait of high acetic acid tolerance in the yeast Saccharomyces cerevisiae", "Authors": ["Meijnen J.-P.", "Randazzo P.", "Foulquie-Moreno M.R.", "Van Den Brink J.", "Vandecruys P.", "Stojiljkovic M.", "Dumortier F.", "Zalar P.", "Boekhout T.", "Gunde-Cimerman N.", "Kokosar J.", "Stajdohar M.", "Curk T.", "Petrovic U.", "Thevelein J.M."], "Keywords": ["Acetic acid tolerance", "Bioethanol production", "Inbreeding", "Polygenic analysis", "Pooled-segregant whole-genome sequence analysis", "QTL mapping", "Saccharomyces cerevisiae"], "Date": "2016", "Abstract": "Background: Acetic acid is one of the major inhibitors in lignocellulose hydrolysates used for the production of second-generation bioethanol. Although several genes have been identified in laboratory yeast strains that are required for tolerance to acetic acid, the genetic basis of the high acetic acid tolerance naturally present in some Saccharomyces cerevisiae strains is unknown. Identification of its polygenic basis may allow improvement of acetic acid tolerance in yeast strains used for second-generation bioethanol production by precise genome editing, minimizing the risk of negatively affecting other industrially important properties of the yeast. Results: Haploid segregants of a strain with unusually high acetic acid tolerance and a reference industrial strain were used as superior and inferior parent strain, respectively. After crossing of the parent strains, QTL mapping using the SNP variant frequency determined by pooled-segregant whole-genome sequence analysis revealed two major QTLs. All F1 segregants were then submitted to multiple rounds of random inbreeding and the superior F7 segregants were submitted to the same analysis, further refined by sequencing of individual segregants and bioinformatics analysis taking into account the relative acetic acid tolerance of the segregants. This resulted in disappearance in the QTL mapping with the F7 segregants of a major F1 QTL, in which we identified HAA1, a known regulator of high acetic acid tolerance, as a true causative allele. Novel genes determining high acetic acid tolerance, GLO1, DOT5, CUP2, and a previously identified component, VMA7, were identified as causative alleles in the second major F1 QTL and in three newly appearing F7 QTLs, respectively. The superior HAA1 allele contained a unique single point mutation that significantly improved acetic acid tolerance under industrially relevant conditions when inserted into an industrial yeast strain for second-generation bioethanol production. Conclusions: This work reveals the polygenic basis of high acetic acid tolerance in S. cerevisiae in unprecedented detail. It also shows for the first time that a single strain can harbor different sets of causative genes able to establish the same polygenic trait. The superior alleles identified can be used successfully for improvement of acetic acid tolerance in industrial yeast strains.", "Language": "en", "Citations": "32"},
{"Title": "Practical dependence analysis in a SIMD vectorizing compiler", "Authors": ["Bulic P.", "Gustin V."], "Keywords": ["Computational efficiency", "Computer graphics", "Costs", "Data analysis", "Equations", "Information science", "Linear programming", "Program processors", "Sufficient conditions", "Testing"], "Date": "2003", "Abstract": "In this paper we present a new, fast and accurate exact data dependence test method with linear cost for array references with linear subscripts, which solves the two-dimensional dependence equation by the use of the Bresenham incremental line algorithm which has been primarily used in computer graphics. The presented method uses only integer calculations and is implemented in our SIMD vectorizing compiler.", "Language": "en", "Citations": "0"},
{"Title": "Listeners' emotional engagement with performances of a Scriabin \u00e9tude: An explorative case study", "Authors": ["Timmers R.", "Marolt M.", "Camurri A.", "Volpe G."], "Keywords": ["Decision trees", "Motion", "Musical training", "Piano performance", "Serial correlation"], "Date": "2006", "Abstract": "In an explorative study, the variation in listeners' judgments of the emotionality of three performances of a Scriabin\u00e9tude was investigated. Three performances of the Scriabin\u00e9tude were recorded. Video and/or audio recordings were presented to 24 listeners who segmented the music in short phrases and indicated their emotional engagement with the music using a slider. The relation between the performance data and the listeners' responses was analysed as well as the effects of musical training, medium and musical structure. The analyses were done using multiple regression analyses and decision trees. The results confirmed the hypothesized influence of the performer's interpretation, the listener's background and the global phrase structure, though not always in the expected way. In particular the dynamics of the performance correlated with listeners' judgments of emotionality, while tempo correlated more strongly with the indications of phrase structure. This was more clearly the case for nonmusicians than for musicians. The movements of the pianist were related to the dynamics of the performance and seemed to aid the communication of emotional intensity. Copyright \u00a9 2006 Society for Education, Music and Psychology Research.", "Language": "en", "Citations": "18"},
{"Title": "A simple pipelined squaring circuit for DSP", "Authors": ["Risojevic V.", "Avramovic A.", "Babic Z.", "Bulic P."], "Keywords": [], "Date": "2011", "Abstract": "There are many digital signal processing applications where a shorter time delay of algorithms and efficient implementations are more important than accuracy. Since squaring is one of the fundamental operations widely used in digital signal processing algorithms, approximate squaring is proposed. We present a simple way of approximate squaring that allows achieving a desired accuracy. The proposed method uses the same simple combinational logic for the first approximation and correction terms. Performed analysis for various bit-length operands and level of approximation showed that maximum relative errors and average relative errors decrease significantly by adding more correction terms. The proposed squaring method can be implemented with a great level of parallelism. The pipelined implementation is also proposed in this paper. The proposed squarer achieved significant savings in area and power when compared to multiplier based squarer. As an example, an analysis of the impact of Euclidean distance calculation by approximate squaring on image retrieval is performed. \u00a9 2011 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "Gold-standard datasets for annotation of Slovene Computer-Mediated Communication", "Authors": ["Erjavec T.", "Cibej J.", "Holdt S.A.", "Ljubesic N.", "Fiser D."], "Keywords": ["Computer-mediated communication", "Lemmatisation", "Morphosyntactic tagging", "Slovene language", "Word normalisation"], "Date": "2016", "Abstract": "This paper presents the first publicly available, manually annotated gold-standard datasets for the annotation of Slovene Computer-Mediated Communication. In this type of language, diacritics, punctuation and spaces are often omitted, and phonetic spelling and slang words frequently used, which considerably deteriorates the performance of text processing tools that were trained on standard Slovene. Janes-Norm, which contains 7,816 texts or 184,766 tokens, is a gold-standard dataset for tokenisation, sentence segmentation and word normalisation, whereas Janes-Tag, comprising 2,958 texts or 75,276 tokens, was created for training and evaluating morphosyntactic tagging and lemmatisation tools for non-standard Slovene.", "Language": "en", "Citations": "1"},
{"Title": "Designing an interactive teaching tool with ABML knowledge refinement loop", "Authors": ["Zapusek M.", "Mozina M.", "Bratko I.", "Rugelj J.", "Guid M."], "Keywords": ["argument-based machine learning", "computer programming", "ill-defined concept", "intelligent tutoring", "knowledge elicitation", "programming style", "python"], "Date": "2014", "Abstract": "Argument-based machine learning (ABML) knowledge refinement loop offers a powerful knowledge elicitation tool, suitable for obtaining expert knowledge in difficult domains. In this paper, we first use it to conceptualize a difficult, even ill-defined concept: distinguishing between \"basic\" and \"advanced\" programming style in python programming language, and then to teach this concept in an interactive learning session between a student and the computer. We demonstrate that by automatically selecting relevant examples and counter examples to be explained by the student, the ABML knowledge refinement loop provides a valuable interactive teaching tool. \u00a9 2014 Springer International Publishing Switzerland.", "Language": "en", "Citations": "3"},
{"Title": "Message routing in pure peer-to-peer networks", "Authors": ["Ciglaric M.", "Trampus M.", "Pancur M.", "Vidmar T."], "Keywords": ["Distributed search", "File sharing protocol", "Message routing", "Peer-to-peer system"], "Date": "2003", "Abstract": "Pure peer-to-peer systems are loosely coupled, highly autonomous systems with a simple architecture, typically without hierarchy. Often, flooding-based routing mechanisms are used for sending messages through the overlay (application level) network. Due to the limited scalability and high network load experiences, the paper suggests an improvement to reduce the traffic and overall performance: if a peer remembers recently forwarded answers, it may route the next query messages asking about the same contents only to the relevant neighbour, not to all of them. The routing simulations have shown promising results.", "Language": "en", "Citations": "0"},
{"Title": "Learning positional features for annotating chess games: A case study", "Authors": ["Guid M.", "Mozina M.", "Krivec J.", "Sadikov A.", "Bratko I."], "Keywords": [], "Date": "2008", "Abstract": "By developing an intelligent computer system that will provide commentary of chess moves in a comprehensible, user-friendly, and instructive way, we are trying to use the power demonstrated by the current chess engines for tutoring chess and for annotating chess games. In this paper, we point out certain differences between the computer programs which are specialized for playing chess and our program which is aimed at providing quality commentary. Through a case study, we present an application of argument-based machine learning, which combines the techniques of machine learning and expert knowledge, to the construction of more complex positional features, in order to provide our annotating system with an ability to comment on various positional intricacies of positions in the game of chess. \u00a9 2008 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "7"},
{"Title": "Moments of superellipsoids and their application to range image registration", "Authors": ["Jaklic A.", "Solina F."], "Keywords": ["3-D cartesian moments", "Registration", "Superellipse", "Superellipsoid", "Transformations of 3-D moments"], "Date": "2003", "Abstract": "Cartesian moments are frequently used global geometrical features in computer vision for object pose estimation and recognition. In the paper we derive a closed form expression for 3-D cartesian moment of order p + q + r of a superellipsoid in its canonical coordinate system. We also show how 3-D cartesian moment of a globally deformed superellipsoid in general position and orientation can be computed as a linear combination of 3-D Cartesian moments of the corresponding nondeformed superellipsoid in canonical coordinate system. Additionally, moments of objects that are compositions of superellipsoids can be computed as simple sums of moments of individual parts. To demonstrate practical application of the derived results we register pairs of range images based on moments of recovered compositions of superellipsoids. We use a standard technique to find centers of gravity and principal axes in pairs of range images while third-order moments are used to resolve the four-way ambiguity. Experimental results show expected improvement of recovered rigid transformation based on moments of recovered superellipsoids as compared to the registration based on moments of raw range image data. Besides object pose estimation the presented results can be directly used for object recognition with moments and/or moment invariants as object features.", "Language": "en", "Citations": "15"},
{"Title": "Evaluation of ordinal attributes at value level", "Authors": ["Robnik-Sikonja M.", "Vanhoof K."], "Keywords": ["Attribute evaluation", "Attribute values", "Marketing", "Ordinal attributes", "Visualization"], "Date": "2007", "Abstract": "We propose a novel context sensitive algorithm for evaluation of ordinal attributes which exploits the information hidden in ordering of attributes' and class' values and provides a separate score for each value of the attribute. Similar to feature selection algorithm ReliefF, the proposed algorithm exploits the contextual information via selection of nearest instances. The ordEval algorithm outputs probabilistic factors corresponding to the effect an increase/decrease of attribute's value has on the class value. While the ordEval algorithm is general and can be used for analysis of any survey with graded answers, we show its utility on an important marketing problem of customer (dis)satisfaction. We develop a visualization technique and show how we can use it to detect and confirm several findings from marketing theory. \u00a9 Springer Science+Business Media, LLC 2007.", "Language": "en", "Citations": "11"},
{},
{"Title": "Learning from noisy data using a non-covering ILP algorithm", "Authors": ["Oblak A.", "Bratko I."], "Keywords": [], "Date": "2011", "Abstract": "In this paper we describe the non-covering inductive logic programming program HYPER/N, concentrating mainly on noise handling as well as some other mechanisms that improve learning. We perform some experiments with HYPER/N on synthetic weather data with artificially added noise, and on real weather data to learn to predict the movement of rain from radar rain images and synoptic data. \u00a9 2011 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "0"},
{"Title": "Hierarchical statistical learning of generic parts of object structure", "Authors": ["Fidler S.", "Berginc G.", "Leonardis A."], "Keywords": [], "Date": "2006", "Abstract": "With the growing interest in object categorization various methods have emerged that perform well in this challenging task, yet are inherently limited to only a moderate number of object classes. In pursuit of a more general categorization system this paper proposes a way to overcome the computational complexity encompassing the enormous number of different object categories by exploiting the statistical properties of the highly structured visual world. Our approach proposes a hierarchical acquisition of generic parts of object structure, varying from simple to more complex ones, which stem from the favorable statistics of natural images. The parts recovered in the individual layers of the hierarchy can be used in a top-down manner resulting in a robust statistical engine that could be efficiently used within many of the current categorization systems. The proposed approach has been applied to large image datasets yielding important statistical insights into the generic parts of object structure. \u00a9 2006 IEEE.", "Language": "en", "Citations": "38"},
{"Title": "Combining audio and video for detection of spontaneous emotions", "Authors": ["Gajsek R.", "Struc V.", "Dobrisek S.", "Zibert J.", "Mihelic F.", "Pavesic N."], "Keywords": ["Bimodal emotion database", "Emotion recognition", "Linear transformations"], "Date": "2009", "Abstract": "The paper presents our initial attempts in building an audio video emotion recognition system. Both, audio and video sub-systems are discussed, and description of the database of spontaneous emotions is given. The task of labelling the recordings from the database according to different emotions is discussed and the measured agreement between multiple annotators is presented. Instead of focusing on the prosody in audio emotion recognition, we evaluate the possibility of using linear transformations (CMLLR) as features. The classification results from audio and video sub-systems are combined using sum rule fusion and the increase in recognition results, when using both modalities, is presented. \u00a9 2009 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "Interactive and audience adaptive digital signage using real-time computer vision", "Authors": ["Ravnik R.", "Solina F."], "Keywords": ["Computer Vision", "Digital Signage", "Face Localization", "Information Interfaces"], "Date": "2013", "Abstract": "In this paper we present the development of an interactive, content-aware and cost-effective digital signage system. Using a monocular camera installed within the frame of a digital signage display, we employ real-time computer vision algorithms to extract temporal, spatial and demographic features of the observers, which are further used for observer-specific broadcasting of digital signage content. The number of observers is obtained by the Viola and Jones face detection algorithm, whilst facial images are registered using multi-view Active Appearance Models. The distance of the observers from the system is estimated from the interpupillary distance of registered faces. Demographic features, including gender and age group, are determined using SVM classifiers to achieve individual observer-specific selection and adaption of the digital signage broadcasting content. The developed system was evaluated at the laboratory study level and in a field study performed for audience measurement research. Comparison of our monocular localization module with the Kinect stereo-system reveals a comparable level of accuracy. The facial characterization module is evaluated on the FERET database with 95% accuracy for gender classification and 92% for age group. Finally, the field study demonstrates the applicability of the developed system in real-life environments. \u00a9 2013 Ravnik and Solina et al.", "Language": "en", "Citations": "9"},
{"Title": "Online discriminative kernel density estimator with gaussian kernels", "Authors": ["Kristan M.", "Leonardis A."], "Keywords": ["Gaussian mixture models (GMMs)", "kernel density estimation", "online discriminative models", "probability density estimation"], "Date": "2014", "Abstract": "We propose a new method for a supervised online estimation of probabilistic discriminative models for classification tasks. The method estimates the class distributions from a stream of data in the form of Gaussian mixture models (GMMs). The reconstructive updates of the distributions are based on the recently proposed online kernel density estimator (oKDE). We maintain the number of components in the model low by compressing the GMMs from time to time. We propose a new cost function that measures loss of interclass discrimination during compression, thus guiding the compression toward simpler models that still retain discriminative properties. The resulting classifier thus independently updates the GMM of each class, but these GMMs interact during their compression through the proposed cost function. We call the proposed method the online discriminative kernel density estimator (odKDE). We compare the odKDE to oKDE, batch state-of-the-art kernel density estimators (KDEs), and batch/incremental support vector machines (SVM) on the publicly available datasets. The odKDE achieves comparable classification performance to that of best batch KDEs and SVM, while allowing online adaptation from large datasets, and produces models of lower complexity than the oKDE. \u00a9 2014 IEEE.", "Language": "en", "Citations": "17"},
{"Title": "Application of argument based machine learning to law", "Authors": ["Mozina M.", "Zabkar J.", "Bench-Capon T.", "Bratko I."], "Keywords": [], "Date": "2005", "Abstract": "In this paper we discuss the application of a new machine learning approach - argumentation based machine learning - to the legal domain. Argumentation based machine learning is particularly suited to law as it makes use of the justifications of decisions to guide its learning. Importantly, where a large number of decided cases are available, it provides a way of identifying which need to be considered, so that only decisions which will have an influence are examined. Copyright 2005 ACM.", "Language": "en", "Citations": "1"},
{"Title": "Explaining machine learning models in sales predictions", "Authors": ["Bohanec M.", "Kljajic Borstnar M.", "Robnik-Sikonja M."], "Keywords": ["B2B Sales forecasting", "Black-box models", "Intelligent system", "Machine learning", "Prediction explanation"], "Date": "2017", "Abstract": "A complexity of business dynamics often forces decision-makers to make decisions based on subjective mental models, reflecting their experience. However, research has shown that companies perform better when they apply data-driven decision-making. This creates an incentive to introduce intelligent, data-based decision models, which are comprehensive and support the interactive evaluation of decision options necessary for the business environment. Recently, a new general explanation methodology has been proposed, which supports the explanation of state-of-the-art black-box prediction models. Uniform explanations are generated on the level of model/individual instance and support what-if analysis. We present a novel use of this methodology inside an intelligent system in a real-world case of business-to-business (B2B) sales forecasting, a complex task frequently done judgmentally. Users can validate their assumptions with the presented explanations and test their hypotheses using the presented what-if parallel graph representation. The results demonstrate effectiveness and usability of the methodology. A significant advantage of the presented method is the possibility to evaluate seller's actions and to outline general recommendations in sales strategy. This flexibility of the approach and easy-to-follow explanations are suitable for many different applications. Our well-documented real-world case shows how to solve a decision support problem, namely that the best performing black-box models are inaccessible to human interaction and analysis. This could extend the use of the intelligent systems to areas where they were so far neglected due to their insistence on comprehensible models. A separation of the machine learning model selection from model explanation is another significant benefit for expert and intelligent systems. Explanations unconnected to a particular prediction model positively influence acceptance of new and complex models in the business environment through their easy assessment and switching.", "Language": "en", "Citations": "8"},
{"Title": "Connectivity derived thalamic segmentation in deep brain stimulation for tremor", "Authors": ["Akram H.", "Dayal V.", "Mahlknecht P.", "Georgiev D.", "Hyam J.", "Foltynie T.", "Limousin P.", "De Vita E.", "Jahanshahi M.", "Ashburner J.", "Behrens T.", "Hariz M.", "Zrinzo L."], "Keywords": ["Connectivity", "DBS", "Deep brain stimulation", "Dentate nucleus", "Dentato-rubro-thalamic tract", "Diffusion weighted imaging", "DRT", "DWI", "Parkinson's disease", "PD", "Tremor", "Ventrointermedialis", "Ventrolateral nucleus", "VIM", "VL"], "Date": "2018", "Abstract": "The ventral intermediate nucleus (VIM) of the thalamus is an established surgical target for stereotactic ablation and deep brain stimulation (DBS) in the treatment of tremor in Parkinson's disease (PD) and essential tremor (ET). It is centrally placed on a cerebello-thalamo-cortical network connecting the primary motor cortex, to the dentate nucleus of the contralateral cerebellum through the dentato-rubro-thalamic tract (DRT). The VIM is not readily visible on conventional MR imaging, so identifying the surgical target traditionally involved indirect targeting that relies on atlas-defined coordinates. Unfortunately, this approach does not fully account for individual variability and requires surgery to be performed with the patient awake to allow for intraoperative targeting confirmation. The aim of this study is to identify the VIM and the DRT using probabilistic tractography in patients that will undergo thalamic DBS for tremor. Four male patients with tremor dominant PD and five patients (three female) with ET underwent high angular resolution diffusion imaging (HARDI) (128 diffusion directions, 1.5 mm isotropic voxels and b value = 1500) preoperatively. Patients received VIM-DBS using an MR image guided and MR image verified approach with indirect targeting. Postoperatively, using parallel Graphical Processing Unit (GPU) processing, thalamic areas with the highest diffusion connectivity to the primary motor area (M1), supplementary motor area (SMA), primary sensory area (S1) and contralateral dentate nucleus were identified. Additionally, volume of tissue activation (VTA) corresponding to active DBS contacts were modelled. Response to treatment was defined as 40% reduction in the total Fahn-Tolosa-Martin Tremor Rating Score (FTMTRS) with DBS-ON, one year from surgery. Three out of nine patients had a suboptimal, long-term response to treatment. The segmented thalamic areas corresponded well to anatomically known counterparts in the ventrolateral (VL) and ventroposterior (VP) thalamus. The dentate-thalamic area, lay within the M1-thalamic area in a ventral and lateral location. Streamlines corresponding to the DRT connected M1 to the contralateral dentate nucleus via the dentate-thalamic area, clearly crossing the midline in the mesencephalon. Good response was seen when the active contact VTA was in the thalamic area with highest connectivity to the contralateral dentate nucleus. Non-responders had active contact VTAs outside the dentate-thalamic area. We conclude that probabilistic tractography techniques can be used to segment the VL and VP thalamus based on cortical and cerebellar connectivity. The thalamic area, best representing the VIM, is connected to the contralateral dentate cerebellar nucleus. Connectivity based segmentation of the VIM can be achieved in individual patients in a clinically feasible timescale, using HARDI and high performance computing with parallel GPU processing. This same technique can map out the DRT tract with clear mesencephalic crossing.", "Language": "en", "Citations": "21"},
{"Title": "Induction of hypotheses concerning hip arthroplasty: A modified methodology for medical research", "Authors": ["Stankovski V.", "Bratko I.", "Demsar J.", "Smrke D."], "Keywords": ["Hip Arthroplosty", "Machine Learning", "Medical Data Analysis", "Regression Trees", "Research Methodology"], "Date": "2001", "Abstract": "Objectives: The objective of this study is to advocate a methodology for medical research that, in contrast to traditional medical methodology, exploits the flexibility of machine learning and retains the kind of statistical tests that are generally accepted in the medical field for the confirmation of hypotheses. Methods: First, the medical problem is defined and data for an observed population are collected; then a machine learning tool is used to generate hypotheses regarding the problem; finally, statistical methods are used to determine the validity of the generated hypotheses. Results: To illustrate this approach, the problem of defining indications for hip arthroplasty after an acute medial femoral neck fracture is investigated as a case study. Conclusions: The methodology is similar to the usual style of applying machine learning, but insists on a link to the techniques of statistical tests that are normally used in medicine. It aims at a more flexible and economical use of experimental data than in the usual medical research, which is enabled by techniques of machine learning. At the same time, by reference to traditional statistical tests, it is hoped that this approach will lead to improved acceptance of machine learning in the medical field.", "Language": "en", "Citations": "4"},
{"Title": "Weighted and robust learning of subspace representations", "Authors": ["Skocaj D.", "Leonardis A.", "Bischof H."], "Keywords": ["Appearance-based modeling", "Missing pixels", "Principal component analysis", "Robust learning", "Robust PCA", "Weighted PCA"], "Date": "2007", "Abstract": "A reliable system for visual learning and recognition should enable a selective treatment of individual parts of input data and should successfully deal with noise and occlusions. These requirements are not satisfactorily met when visual learning is approached by appearance-based modeling of objects and scenes using the traditional PCA approach. In this paper we extend standard PCA approach to overcome these shortcomings. We first present a weighted version of PCA, which, unlike the standard approach, considers individual pixels and images selectively, depending on the corresponding weights. Then we propose a robust PCA method for obtaining a consistent subspace representation in the presence of outlying pixels in the training images. The method is based on the EM algorithm for estimation of principal subspaces in the presence of missing data. We demonstrate the efficiency of the proposed methods in a number of experiments. \u00a9 2006 Pattern Recognition Society.", "Language": "en", "Citations": "48"},
{"Title": "An application of machine learning to haematological diagnosis", "Authors": ["Guncar G.", "Kukar M.", "Notar M.", "Brvar M.", "Cernelc P.", "Notar M.", "Notar M."], "Keywords": [], "Date": "2018", "Abstract": "Quick and accurate medical diagnoses are crucial for the successful treatment of diseases. Using machine learning algorithms and based on laboratory blood test results, we have built two models to predict a haematologic disease. One predictive model used all the available blood test parameters and the other used only a reduced set that is usually measured upon patient admittance. Both models produced good results, obtaining prediction accuracies of 0.88 and 0.86 when considering the list of five most likely diseases and 0.59 and 0.57 when considering only the most likely disease. The models did not differ significantly, which indicates that a reduced set of parameters can represent a relevant \"fingerprint\" of a disease. This knowledge expands the model's utility for use by general practitioners and indicates that blood test results contain more information than physicians generally recognize. A clinical test showed that the accuracy of our predictive models was on par with that of haematology specialists. Our study is the first to show that a machine learning predictive model based on blood tests alone can be successfully applied to predict haematologic diseases. This result and could open up unprecedented possibilities for medical diagnosis.", "Language": "en", "Citations": "8"},
{},
{"Title": "An implementation of a two-layered SVM classifier in Condor", "Authors": ["Trebar M.", "Steele N."], "Keywords": ["Classification", "Condor", "Distributed computing", "Support vector machine"], "Date": "2007", "Abstract": "Condor, as a high-throughput distributed computing system, is used in a two-layered Support Vector Machine (SVM) implementation of the classification problem. The complexity of the SVMs training algorithm increases with respect to the number of samples. The data are split into subsets and the solution described reduces the training time by optimizing the first layer SVMs separately on a cluster of computers. As a result, a smaller subset of support vectors from partial results is used to optimize the second layer SVM. For the experiments on a large data set (Forest data), the distributed implementation of two-layered SVMs in Condor shows a significant improvement of the training time by keeping or even improving the error performance of a single SVM classifier.", "Language": "en", "Citations": "1"},
{"Title": "Using ant colony optimization for searching in (dynamic) distributed datasets", "Authors": ["Jovanovic U.", "Slivnik B."], "Keywords": ["Ant colony optimization", "Data grids", "Distributed search"], "Date": "2007", "Abstract": "A method for searching in distributed datasets using a user-specified search agent (instead of a keyword) is considered. It implements (1) an efficient discoverey of data in distributed raw datasets and (2) collection of thus procured data. To prevent network flooding in case of dynamic distributed datasets, it is based on ant colony optimization (ACO). The user-specified search agent is a program which performs a search on a local chunk of a distributed raw dataset - the method described herein distributes this program and collects the data the program produces. The paper includes the detailed description of the modifications of the ACO algorithm needed for effective collection of the extracted data. The results of the extensive simulations for the different network topologies and for different rates of data extraction and aggregation are included.", "Language": "en", "Citations": "0"},
{"Title": "A local-global coupled-layer puppet model for robust online human pose tracking", "Authors": ["Ma M.", "Marturi N.", "Li Y.", "Stolkin R.", "Leonardis A."], "Keywords": ["Coupled-layer model", "Human pose tracking", "Human tracking", "Pose estimation", "Video tracking"], "Date": "2016", "Abstract": "This paper addresses the problem of online tracking of articulated human body poses in dynamic environments. Many previous approaches perform poorly in realistic applications: often future frames or entire sequences are used anticausally to mutually refine the poses in each individual frame, making online tracking impossible; tracking often relies on strong assumptions about e.g. clothing styles, body-part colours and constraints on body-part motion ranges, limiting such algorithms to a particular dataset; the use of holistic feature models limits the ability of optimisation-based matching to distinguish between pose errors of different body parts. We overcome these problems by proposing a coupled-layer framework, which uses the previous notions of deformable structure (DS) puppet models. The underlying idea is to decompose the global pose candidate in any particular frame into several local parts to obtain a refined pose. We introduce an adaptive penalty with our model to improve the searching scope for a local part pose, and also to overcome the problem of using fixed constraints. Since the pose is computed using only current and previous frames, our method is suitable for online sequential tracking. We have carried out empirical experiments using three different public benchmark datasets, comparing two variants of our algorithm against four recent state-of-the-art (SOA) methods from the literature. The results suggest comparatively strong performance of our method, regardless of weaker constraints and fewer assumptions about the scene, and despite the fact that our algorithm is performing online sequential tracking, whereas the comparison methods perform mutual optimisation backwards and forwards over all frames of the entire video sequence.", "Language": "en", "Citations": "4"},
{"Title": "Clustering scientific publications based on citation relations: A systematic comparison of different methods", "Authors": ["Subelj L.", "Van Eck N.J.", "Waltman L."], "Keywords": [], "Date": "2016", "Abstract": "Clustering methods are applied regularly in the bibliometric literature to identify research areas or scientific fields. These methods are for instance used to group publications into clusters based on their relations in a citation network. In the network science literature, many clustering methods, often referred to as graph partitioning or community detection techniques, have been developed. Focusing on the problem of clustering the publications in a citation network, we present a systematic comparison of the performance of a large number of these clustering methods. Using a number of different citation networks, some of them relatively small and others very large, we extensively study the statistical properties of the results provided by different methods. In addition, we also carry out an expert-based assessment of the results produced by different methods. The expert-based assessment focuses on publications in the field of scientometrics. Our findings seem to indicate that there is a trade-off between different properties that may be considered desirable for a good clustering of publications. Overall, map equation methods appear to perform best in our analysis, suggesting that these methods deserve more attention from the bibliometric community.", "Language": "en", "Citations": "21"},
{"Title": "Security metrics foundations for computer security", "Authors": ["Trek D."], "Keywords": ["computer security", "economics of security", "risk management", "security metrics"], "Date": "2010", "Abstract": "Security has been among top priority in computer information systems for more than a decade. Despite the importance of this area, it is interesting to note that the area still lacks (completeness of) one of its basic elements of scientific arsenal, which is metric. This paper therefore presents the situation in this field by giving an analysis of existing metrics that could serve the above-mentioned purpose. Further, it presents a generic risk management model, and gives an analysis of possibilities for application of these existing metrics to the model. It also introduces new metric elements, where these are lacking. As a result, means are provided that enable evaluation of security in information technology systems in a tangible way. Such an approach is essential for every organization in business areas ranging from economical justifications for new security implementations to customized security services with appropriate service costs calculations, and even development of new business models. \u00a9 The Author 2009. Published by Oxford University Press on behalf of The British Computer Society. All rights reserved.", "Language": "en", "Citations": "3"},
{"Title": "Comparison of machine learning for autonomous robot discovery", "Authors": ["Bratko I."], "Keywords": ["Autonomous learning", "Gaining insights", "Machine learning", "Robotic discovery"], "Date": "2010", "Abstract": "In this paper we consider autonomous robot discovery through experimentation in the robot's environment. We analyse the applicability of machine learning (ML) methods with respect to various levels of robot discovery tasks, from extracting simple laws among the observed variables, to discovering completely new notions that were never explicitly mentioned in the data directly. We first present some illustrative experiments in robot learning in the XPERO European project. Then we formulate criteria for a comparison of learning methods and a systematic list of types of learning or discovery tasks, and discuss the suitability of chosen ML methods for these tasks. \u00a9 2010 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "1"},
{"Title": "An integrated learning approach to environment modelling in mobile robot navigation", "Authors": ["Ster B."], "Keywords": ["Mobile robot navigation", "Recurrent neural networks", "Reinforcement learning"], "Date": "2004", "Abstract": "We extend the approach to learning a topological description of the environment with recurrent neural networks. Usually, a predetermined reactive behavior and a predefined criterion for decision points are used. In our extended approach, both the reactive behavior and the criterion for the decision points are adaptive and therefore more flexible. The reactive behavior is learnt using reinforcement learning supplemented by a new, psychologically grounded mechanism that enables the robot to autonomously explore the environment in a useful way for the purposes of modelling. Decision points or situations where a deviation from the reactive behavior is allowed are learnt on-line using a novel criterion based on the information theory. Results of experiments conducted with a simulated mobile robot equipped with proximity sensors and a color video camera show applicability of the proposed approach. \u00a9 2003 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "14"},
{"Title": "Using computer vision in a rehabilitation method of a human hand", "Authors": ["Katrasnik J.", "Veber M.", "Peer P."], "Keywords": ["Computer vision"], "Date": "2007", "Abstract": "We developed this program for the purpose of a rehabilitation method that requires a patient to move an object around with his hand. Using a black and white firewire camera the program determines the position and orientation of a black rectangle on a white plane. The user must enter the length and width of the rectangle before the start. With this information the position is determined even if a part of the rectangle is obscured by a user\u2019s hand. The program works in real-time (15 to 20 frames per second).", "Language": "en", "Citations": "5"},
{"Title": "Predictive value of ABCB1 polymorphisms G2677T/A, C3435T, and their haplotype in small cell lung cancer patients treated with chemotherapy", "Authors": ["Knez L.", "Kosnik M.", "Ovcaricek T.", "Sadikov A.", "Sodja E.", "Kern I.", "Cufer T."], "Keywords": ["ABCB1 protein", "Chemotherapy", "Genetic polymorphisms", "Multiple drug resistance", "Small cell lung cancer"], "Date": "2012", "Abstract": "Purpose: Multiple drug resistance limits the efficacy of numerous cytotoxic drugs used in the treatment of small cell lung cancer (SCLC). The drug efflux protein ATP-binding cassette transporter B1 (ABCB1) has an important role in this process, and its gene variability may affect chemotherapy outcomes. Patients and methods: This study aimed to evaluate the associations between ABCB1 polymorphisms G2677T/A, C3435T, and their haplotype with progression-free survival (PFS) and overall survival (OS) in 177 SCLC patients treated with cisplatin-etoposide or cyclophosphamide-epirubicin-vincristine chemotherapy. To determine the ABCB1 genotype, allelic speciWc TaqMan\u00ae probes were used in a RT-PCR. Results: Patients carrying the G2677T/A TT + TA + AA genotypes (24 %) or the C3435T CT + TT genotypes (72 %) or the 2677T/A-3435T haplotype (40 %) had a longer PFS (Cox regression, P = 0.052, 0.037 and 0.037, respectively); these associations persisted also in multivariate analyses (Cox regression, P = 0.028, 0.037 and 0.030, respectively). Moreover, patients with the C3435T CT + TT genotypes had a longer OS both in univariate and multivariate analysis (Cox regression, P = 0.022 and 0.028, respectively). A trend toward longer OS was noted for the 2677T/A-3435T haplotype (Cox regression, P = 0.051), but its independent value was not confirmed (Cox regression, P = 0.071). Conclusions: Our study reported a possible predictive value of ABCB1 polymorphisms G2677T/A, C3435T, and their haplotype for longer PFS and OS in Caucasian SCLC patients treated with chemotherapy. However, to be implemented into routine clinical practice, ABCB1 polymorphisms require further validation. \u00a9 Springer-Verlag 2012.", "Language": "en", "Citations": "3"},
{"Title": "A martingale on the zero-set of a holomorphic function", "Authors": ["Kink P."], "Keywords": ["Complex martingales", "Holomorphic functions", "Stochastic differential equations"], "Date": "2008", "Abstract": "We give a simple probabilistic proof of the classical fact from complex analysis that the zeros of a holomorphicfunction of several variables are never isolated and that they are not contained in any compact set. No facts from complex analysis are assumed other than the Cauchy-Riemann definition. From stochastic analysis only the It\u00f4 formula and the standard existencetheorem for stochastic differential equations are required. \u00a9 2008 Applied Probability Trust.", "Language": "en", "Citations": "1"},
{"Title": "Defending the need for a new global software approach: A literature review", "Authors": ["Trkman M.", "Vrhovec S.", "Vavpotic D.", "Krisper M."], "Keywords": [], "Date": "2013", "Abstract": "Software engineering has been affected by the globalization trend. Software companies embracing global software engineering can benefit from it by wisely approaching its challenges. This paper gives a literature review on general GSD's challenges by examining how the focus changed in the last decade. There exist some global software development approaches that focus on overcoming these challenges. For example, the loosely coupled team approach helps to overcome the coordination issues that the virtual teams have to deal with by modularization of development work. However, none of these approaches is focused on supporting the development project with scattered individual developers. \u00a9 2013 MIPRO.", "Language": "en", "Citations": "1"},
{"Title": "Hierarchical feature scheme for object recognition in visual sensor networks", "Authors": ["Sulic V.", "Pers J.", "Kristan M.", "Kovacic S."], "Keywords": ["Computer vision", "Object recognition", "Visual sensor networks"], "Date": "2009", "Abstract": "Visual sensor networks are the meeting point of two significantly different technologies: one is image processing with high computation and storage demands, and the other is distributed sensor approach with low power, low computational and storage capabilities. We propose a framework for hierarchical feature encoding scheme for a frequent computer vision task - object recognition. The key of our approach is the principle that individual nodes in the network hold only a small amount of information about objects seen by the network. However, this information is sufficient to efficiently route network query, when a new, unknown object is encountered. A set of criteria has to be fulfilled by the object recognition method to qualify for use in our framework. The paper provides examples of three widely known object recognition approaches that can be easily adapted for use in such hierarchical feature encoding scheme.", "Language": "en", "Citations": "0"},
{"Title": "Predictive model for survival at the conclusion of a damage control laparotomy", "Authors": ["Aoki N.", "Wall M.J.", "Demsar J.", "Zupan B.", "Granchi T.", "Schreiber M.A.", "Holcomb J.B.", "Byrne M.", "Liscum K.R.", "Goodwin G.", "Beck J.R.", "Mattox K.L."], "Keywords": [], "Date": "2000", "Abstract": "BACKGROUND: We employed modern statistical and data mining methods to model survival based on preoperative and intraoperative parameters for patients undergoing damage control surgery. METHODS: One hundred seventy-four parameters were collected from 68 damage control patients in prehospital, emergency center, operating room, and intensive care unit (ICU) settings. Data were analyzed with logistic regression and data mining. Outcomes were survival and death after the initial operation. RESULTS: Overall mortality was 66.2%. Logistic regression identified pH at initial ICU admission (odds ratio: 4.4) and worst partial thromboplastin time from hospital admission to ICU admission (odds ratio: 9.4) as significant. Data mining selected the same factors, and generated a simple algorithm for patient classification. Model accuracy was 83%. CONCLUSIONS: Inability to correct pH at the conclusion of initial damage-control laparotomy and the worst PTT can be predictive of death. These factors may be useful to identify patients with a high risk of mortality. \u00a9 2001 by Excerpta Medica, Inc.", "Language": "en", "Citations": "50"},
{"Title": "Arguments extracted from text in argument based machine learning: A case study", "Authors": ["Mozina M.", "Giuliano C.", "Bratko I."], "Keywords": [], "Date": "2009", "Abstract": "We introduce a novel approach to cross-media learning based on argument based machine learning (ABML). ABML is a recent method that combines argumentation and machine learning from examples, and its main idea is to provide expert's arguments for some of the learning examples. In this paper, we present an alternative approach, where arguments used in ABML are automatically extracted from text with a technique for relation extraction. We demonstrate and evaluate the approach through a case study of learning to classify animals by using arguments extracted from Wikipedia.", "Language": "en", "Citations": "0"},
{"Title": "Classification of the family AT4(qs,q,q) of antipodal tight graphs", "Authors": ["Jurisic A.", "Koolen J."], "Keywords": ["--graphs", "Antipodal graphs", "AT4 family", "Distance-regular graphs", "Locally strongly regular", "Tight graphs"], "Date": "2011", "Abstract": "Let \u0393 be an antipodal distance-regular graph with diameter 4 and eigenvalues \u03b80>\u03b81>\u03b82>\u03b83>\u03b84. Then its Krein parameter q114 vanishes precisely when \u0393 is tight in the sense of Juri\u0161i\u0107, Koolen and Terwilliger, and furthermore, precisely when \u0393 is locally strongly regular with nontrivial eigenvalues p:=\u03b82 and \u03b8q:=\u03b83. When this is the case, the intersection parameters of \u0393 can be parameterized by p, q and the size of the antipodal classes r of \u0393, hence we denote \u0393 by AT4(p,q,r).Juri\u0161i\u0107 conjectured that the AT4(p,q,r) family is finite and that, aside from the Conway-Smith graph, the Soicher2 graph and the 3.Fi24- graph, all graphs in this family have parameters belonging to one of the following four subfamilies:. (i)q|p,r=q,(ii)q|p,r=2,(iii)p=q-2,r=q-1,(iv)p=q-2,r=2. In this paper we settle the first subfamily. Specifically, we show that for a graph AT4(qs,q,q) there are exactly five possibilities for the pair (s,q), with an example for each: the Johnson graph J(8,4) for (1,2), the halved 8-cube for (2,2), the 3.O6-(3) graph for (1,3), the Meixner2 graph for (2,4) and the 3.O7(3) graph for (3,3). The fact that the --graphs of the graphs in this subfamily are completely multipartite is very crucial in this paper. \u00a9 2010 Elsevier Inc.", "Language": "en", "Citations": "8"},
{"Title": "Distinguishing ischemic from non-ischemic ST changes: The physionet/computers in cardiology challenge 2003", "Authors": ["Moody G.B.", "Jager F."], "Keywords": [], "Date": "2003", "Abstract": "Accurate diagnosis of myocardial ischemia during activities of daily living, if possible, would provide immediate and substantial benefits to affected subjects, by reducing the time needed to determine and evaluate therapeutic interventions intended to reduce or eliminate transient ischemia. Although ST changes provide a sensitive indicator of ischemia, a variety of non-ischemic mechanisms can also result in ST changes, so that ECG-based diagnosis of ischemia is regarded as non-specific and unreliable. It may be possible, by study of the dynamics of ST changes, to improve the specificity of ECG criteria consistent with ischemia. The fourth annual PhysioNet / Computers in Cardiology Challenge encouraged participants to develop and evaluate methods for distinguishing ischemic and non-ischemic ST changes using a standard, meticulously annotated data set. The training data provided to participants, and the algorithms they developed, are available at http://www.physionet.org/ challenge/2003/.", "Language": "en", "Citations": "9"},
{"Title": "Robust detection of heart beats in multimodal data using integer multiplier digital filters and morphological algorithms", "Authors": ["Pangerc U.", "Jager F."], "Keywords": [], "Date": "2014", "Abstract": "We developed a new, robust and efficient heart beat detector in multimodal data using an ECG signal, and one of the pulsatile signals such as blood pressure (BP), if present. To calculate the detection functions, simple and fast integer-multiplier sampling-frequency adjustable digital filters were developed. Using the morphological smoothing, the ECG and pulsatile-signal detection functions, and the noise detection function, are improved. Heart beats are detected using gain-independent adjustable detection thresholds. Streams of detected heart beats in the ECG and in pulsatile signal are linked together. Real time implementation is possible.", "Language": "en", "Citations": "8"},
{"Title": "Measuring progress of scrum-based software projects", "Authors": ["Mahnic V.", "Zabkar N."], "Keywords": ["Agile methods", "Scrum", "Software development management", "Software measurement"], "Date": "2012", "Abstract": "Possible loss of management control is one of the greatest concerns when adopting agile software development methods in industrial practice. Therefore, monitoring progress of agile projects is an important issue in the software industry. This paper describes a set of measures that provide IT management with continuous insight in the Scrum-based software development process. The proposed measures were applied within the scope of the project of rebuilding the web site of Slovenian daily newspaper with the highest circulation, which served as a case study for evaluation of their usability. The paper presents the measurement results and discusses their value for project management. The case study showed that each proposed measure describes a valuable process aspect and that data collecting does not require additional administrative work that would harm the agility of Scrum. \u00a9 Kauno technologijos universitetas, 2012.", "Language": "en", "Citations": "8"},
{"Title": "Development and evaluation of the emotional Slovenian speech database - EmoLUKS", "Authors": ["Justin T.", "Struc V.", "Zibert J.", "Mihelic F."], "Keywords": ["Database development", "Emotion recognition", "Emotional speech database"], "Date": "2015", "Abstract": "This paper describes a speech database built from 17 Slovenian radio dramas. The dramas were obtained from the national radio-and-television station (RTV Slovenia) and were given at the universities disposal with an academic license for processing and annotating the audio material. The utterances of one male and one female speaker were transcribed, segmented and then annotated with emotional states of the speakers. The annotation of the emotional states was conducted in two stages with our own web-based application for crowd sourcing. The final (emotional) speech database consists of 1385 recordings of one male (975 recordings) and one female (410 recordings) speaker and contains labeled emotional speech with a total duration of around 1 hour and 15 minutes. The paper presents the two-stage annotation process used to label the data and demonstrates the usefulness of the employed annotation methodology. Baseline emotion recognition experiments are also presented. The reported results are presented with the un-weighted as well as weighted average recalls and precisions for 2-class and 7-class recognition experiments.", "Language": "en", "Citations": "0"},
{"Title": "Distributed ensemble learning in text classification", "Authors": ["Silva C.", "Ribeiro B.", "Lotric U.", "Dobnikar A."], "Keywords": ["Cluster computing", "Text mining"], "Date": "2008", "Abstract": "In today's society, individuals and organizations are faced with an ever growing load and diversity of textual information and content, and with increasing demands for knowledge and skills. In this work we try to answer part of these challenges by addressing text classification problems, essential to managing knowledge, by combining several different pioneer kernel-learning machines, namely Support Vector Machines and Relevance Vector Machines. To excel complex learning procedures we establish a model of high-performance distributed computing environment to help tackling the tasks involved in the text classification problem. The presented approach is valuable in many practical situations where text classification is used. Reuters-21578 benchmark data set is used to demonstrate the strength of the proposed system while different ensemble based learning machines provide text classification models that are efficiently deployed in the Condor and Alchemi platforms.", "Language": "en", "Citations": "1"},
{"Title": "Producing the platform independent model of an existing web application", "Authors": ["Rozanc I.", "Slivnik B."], "Keywords": [], "Date": "2012", "Abstract": "A reverse engineering procedure for producing a platform independent model (PIM) of an existing Web application is presented using a case study. It focuses on extracting the domain knowledge built into the application and thus it produces the PIM, leaving the hypertext and presentation models aside. It is especially focused on reverse engineering of applications produced using agile software development methodology where documentation is scarce, and as it assumes that in large part the activity diagrams are produced and refined manually, it is particularly useful in environments where at least some developers of the original agile team are still available. Rather than being a result of a theoretical work, the method has crystallized during a lot of practical work. As such it is aimed at practitioners and following the spirit of its formulation, it is presented as a case study where it has been applied. \u00a9 2012 Polish Info Processing Socit.", "Language": "en", "Citations": "4"},
{"Title": "An ancient germ cell-specific RNA-binding protein protects the germline from cryptic splice site poisoning", "Authors": ["Ehrmann I.", "Crichton J.H.", "Gazzara M.R.", "James K.", "Liu Y.", "Grellscheid S.N.", "Curk T.", "de Rooij D.", "Steyn J.S.", "Cockell S.", "Adams I.R.", "Barash Y.", "Elliott D.J."], "Keywords": ["alternative splicing", "chromosomes", "gene expression", "meiosis", "mouse", "retrogene", "RNA", "spermatogenesis"], "Date": "2019", "Abstract": "Male germ cells of all placental mammals express an ancient nuclear RNA binding protein of unknown function called RBMXL2. Here we find that deletion of the retrogene encoding RBMXL2 blocks spermatogenesis. Transcriptome analyses of age-matched deletion mice show that RBMXL2 controls splicing patterns during meiosis. In particular, RBMXL2 represses the selection of aberrant splice sites and the insertion of cryptic and premature terminal exons. Our data suggest a Rbmxl2 retrogene has been conserved across mammals as part of a splicing control mechanism that is fundamentally important to germ cell biology. We propose that this mechanism is essential to meiosis because it buffers the high ambient concentrations of splicing activators, thereby preventing poisoning of key transcripts and disruption to gene expression by aberrant splice site selection.", "Language": "en", "Citations": "1"},
{"Title": "Computational Models for Prediction of Yeast Strain Potential for Winemaking from Phenotypic Profiles", "Authors": ["Mendes I.", "Franco-Duarte R.", "Umek L.", "Fonseca E.", "Drumonde-Neves J.", "Dequin S.", "Zupan B.", "Schuller D."], "Keywords": [], "Date": "2013", "Abstract": "Saccharomyces cerevisiae strains from diverse natural habitats harbour a vast amount of phenotypic diversity, driven by interactions between yeast and the respective environment. In grape juice fermentations, strains are exposed to a wide array of biotic and abiotic stressors, which may lead to strain selection and generate naturally arising strain diversity. Certain phenotypes are of particular interest for the winemaking industry and could be identified by screening of large number of different strains. The objective of the present work was to use data mining approaches to identify those phenotypic tests that are most useful to predict a strain's potential for winemaking. We have constituted a S. cerevisiae collection comprising 172 strains of worldwide geographical origins or technological applications. Their phenotype was screened by considering 30 physiological traits that are important from an oenological point of view. Growth in the presence of potassium bisulphite, growth at 40\u00b0C, and resistance to ethanol were mostly contributing to strain variability, as shown by the principal component analysis. In the hierarchical clustering of phenotypic profiles the strains isolated from the same wines and vineyards were scattered throughout all clusters, whereas commercial winemaking strains tended to co-cluster. Mann-Whitney test revealed significant associations between phenotypic results and strain's technological application or origin. Na\u00efve Bayesian classifier identified 3 of the 30 phenotypic tests of growth in iprodion (0.05 mg/mL), cycloheximide (0.1 \u03bcg/mL) and potassium bisulphite (150 mg/mL) that provided most information for the assignment of a strain to the group of commercial strains. The probability of a strain to be assigned to this group was 27% using the entire phenotypic profile and increased to 95%, when only results from the three tests were considered. Results show the usefulness of computational approaches to simplify strain selection procedures. \u00a9 2013 Mendes et al.", "Language": "en", "Citations": "14"},
{"Title": "Center location algorithms Algoritmi za razme\u0161\u010danje centrov", "Authors": ["Mihelic J.", "Robic B."], "Keywords": ["Approximation algorithms", "Clustering", "Combinatorial optimisation", "Facility location", "Heuristics"], "Date": "2003", "Abstract": "In this paper we discuss discrete combinatorial optimisation in the spatial location domain and draw general features of the location science, we then focus on the k-center problem and present several algorithms for its solving. We analyse space and time complexity of the k-center problem. We show the 2-approximation Gonzalez algorithm and Hochbaum-Shmoys approach in detail. We also present the greedy method, tabu search and a method for solving the center location problem with a set cover or maximal cover problem. Finally, several algorithms are experimentally tested on standard test problems and results are compared with similar tests found in literature.", "Language": "en", "Citations": "0"},
{"Title": "Semi-quantitative Modelling of Gene Regulatory Processes with Unknown Parameter Values Using Fuzzy Logic and Petri Nets", "Authors": ["Bordon J.", "Moskon M.", "Zimic N.", "Mraz M."], "Keywords": ["fuzzy logic", "modelling biological processes", "Petri nets", "unknown kinetic parameter values"], "Date": "2018", "Abstract": "Petri nets are a well-established modelling framework in life sciences and have been widely applied to systems and synthetic biology in recent years. With the various extensions they serve as graphical and simulation interface for both qualitative and quantitative modelling approaches. In terms of quantitative approaches, Stochastic and Continuous Petri nets are extensively used for modelling biological system's dynamics if underlying kinetic data are known. However, these are often only vaguely defined or even missing. In this paper we present a fuzzy approach, which can be used to model biological processes with unknown kinetic data in order to still obtain quantitatively relevant simulation results. We define fuzzy firing rate functions, which can be used in Continuous Petri nets and are able to describe different processes that govern the dynamics of gene expression networks. They can be used in combination with the conventional firing rate functions and applied only in the parts of the system for which the kinetic data are missing. The case study of the proposed approach is performed on models of a hypothetical repressilator and Neurospora circadian rhythm.", "Language": "en", "Citations": "0"},
{"Title": "The RNA-binding landscapes of two SR proteins reveal unique functions and binding to diverse RNA classes", "Authors": ["Anko M.-L.", "Muller-McNicoll M.", "Brandl H.", "Curk T.", "Gorup C.", "Henry I.", "Ule J.", "Neugebauer K.M."], "Keywords": [], "Date": "2012", "Abstract": "Background: The SR proteins comprise a family of essential, structurally related RNA binding proteins. The complexity of their RNA targets and specificity of RNA recognition in vivo is not well understood. Here we use iCLIP to globally analyze and compare the RNA binding properties of two SR proteins, SRSF3 and SRSF4, in murine cells.Results: SRSF3 and SRSF4 binding sites mapped to largely non-overlapping target genes, and in vivo consensus binding motifs were distinct. Interactions with intronless and intron-containing mRNAs as well as non-coding RNAs were detected. Surprisingly, both SR proteins bound to the 3' ends of the majority of intronless histone transcripts, implicating SRSF3 and SRSF4 in histone mRNA metabolism. In contrast, SRSF3 but not SRSF4 specifically bound transcripts encoding numerous RNA binding proteins. Remarkably, SRSF3 was shown to modulate alternative splicing of its own as well as three other transcripts encoding SR proteins. These SRSF3-mediated splicing events led to downregulation of heterologous SR proteins via nonsense-mediated decay.Conclusions: SRSF3 and SRSF4 display unique RNA binding properties underlying diverse cellular regulatory mechanisms, with shared as well as unique coding and non-coding targets. Importantly, CLIP analysis led to the discovery that SRSF3 cross-regulates the expression of other SR protein family members. \u00a9 2012 \u00c4nk\u00f6 et al.; licensee BioMed Central Ltd.", "Language": "en", "Citations": "126"},
{"Title": "Mobile robot localisation with incremental PCA", "Authors": ["Artac M."], "Keywords": ["PCA updating", "Repetitive learning", "View-based robot localisation", "Visual learning"], "Date": "2002", "Abstract": "Mobile robots can employ different sensors to collect data about the environment. We propose to use a special sensor (a catadioptric camera) which provides panoramic views at each robot position. To model the environment for later navigation and localisation, we build a representation of the appearance by compressing the set of panoramic images by Principal Component Analysis (PCA). Since the batch application of the PCA is inappropriate in this case, we propose to apply an incremental approach. This leads to novel aspects regarding the adaptation of compressed partial representation. We provide empirical results which indicate the performance of the proposed method is comparable to the performance of the batch method in terms of compression, computational cost, and, most importantly, precision of localisation.", "Language": "en", "Citations": "3"},
{"Title": "Unravelling the RNA-binding properties of SAFB proteins in breast cancer cells", "Authors": ["Hong E.", "Best A.", "Gautrey H.", "Chin J.", "Razdan A.", "Curk T.", "Elliott D.J.", "Tyson-Capper A.J."], "Keywords": [], "Date": "2015", "Abstract": "Scaffold attachment factor B1 (SAFB1) and SAFB2 proteins are oestrogen (ER) corepressors that bind to and modulate ER activity through chromatin remodelling or interaction with the basal transcription machinery. SAFB proteins also have an internal RNA-recognition motif but little is known about the RNA-binding properties of SAFB1 or SAFB2. We utilised crosslinking and immunoprecipitation (iCLIP) coupled with high-throughput sequencing to enable a transcriptome-wide mapping of SAFB1 protein-RNA interactions in breast cancer MCF-7 cells. Analysis of crosslinking frequency mapped to transcript regions revealed that SAFB1 binds to coding and noncoding RNAs (ncRNAs). The highest proportion of SAFB1 crosslink sites mapped to ncRNAs, followed by intergenic regions, open reading frames (ORFs), introns, and 3\u2032 or 5\u2032 untranslated regions (UTR). Furthermore, we reveal that SAFB1 binds directly to RNA and its binding is particularly enriched at purine-rich sequences not dissimilar to the RNA-binding motifs for SR proteins. Using RNAi, we also show, for the first time, that single depletion of either SAFB1 or SAFB2 leads to an increase in expression of the other SAFB protein in both MCF-7 and MDA-MD231 breast cancer cells.", "Language": "en", "Citations": "3"},
{"Title": "Robust Fusion of Color and Depth Data for RGB-D Target Tracking Using Adaptive Range-Invariant Depth Models and Spatio-Temporal Consistency Constraints", "Authors": ["Xiao J.", "Stolkin R.", "Gao Y.", "Leonardis A."], "Keywords": ["Clustered decision tree", "range-invariant depth models", "RGB-D tracking"], "Date": "2018", "Abstract": "This paper presents a novel robust method for single target tracking in RGB-D images, and also contributes a substantial new benchmark dataset for evaluating RGB-D trackers. While a target object's color distribution is reasonably motion-invariant, this is not true for the target's depth distribution, which continually varies as the target moves relative to the camera. It is therefore nontrivial to design target models which can fully exploit (potentially very rich) depth information for target tracking. For this reason, much of the previous RGB-D literature relies on color information for tracking, while exploiting depth information only for occlusion reasoning. In contrast, we propose an adaptive range-invariant target depth model, and show how both depth and color information can be fully and adaptively fused during the search for the target in each new RGB-D image. We introduce a new, hierarchical, two-layered target model (comprising local and global models) which uses spatio-temporal consistency constraints to achieve stable and robust on-the-fly target relearning. In the global layer, multiple features, derived from both color and depth data, are adaptively fused to find a candidate target region. In ambiguous frames, where one or more features disagree, this global candidate region is further decomposed into smaller local candidate regions for matching to local-layer models of small target parts. We also note that conventional use of depth data, for occlusion reasoning, can easily trigger false occlusion detections when the target moves rapidly toward the camera. To overcome this problem, we show how combining target information with contextual information enables the target's depth constraint to be relaxed. Our adaptively relaxed depth constraints can robustly accommodate large and rapid target motion in the depth direction, while still enabling the use of depth data for highly accurate reasoning about occlusions. For evaluation, we introduce a new RGB-D benchmark dataset with per-frame annotated attributes and extensive bias analysis. Our tracker is evaluated using two different state-of-the-art methodologies, VOT and object tracking benchmark, and in both cases it significantly outperforms four other state-of-the-art RGB-D trackers from the literature.", "Language": "en", "Citations": "4"},
{"Title": "An assessment of machine learning methods for robotic discovery", "Authors": ["Bratko I."], "Keywords": ["Autonomous learning", "Gaining insights", "Machine learning", "Robotic discovery"], "Date": "2008", "Abstract": "In this paper we consider autonomous robot discovery through experimentation in the robot's environment. We analyse the applicability of machine learning (ML) methods with respect to various levels of robot discovery tasks, from extracting simple laws among the observed variables, to discovering completely new notions that were never mentioned in the data directly. We first introduce the XPERO project, and present some illustrative initial experiments in robot learning in XPERO. Then we formulate a systematic list of types of learning or discovery tasks, and discuss the suitability of chosen ML methods for these tasks.", "Language": "en", "Citations": "4"},
{"Title": "MWELex - mwe lexica of croatian, slovene and serbian extracted from parsed corpora", "Authors": ["Ljubesic N.", "Dobrovoljc K.", "Fiser D."], "Keywords": ["Croatian", "English", "Multilingual lexical repository", "Slovenian"], "Date": "2015", "Abstract": "The paper presents MWELex, a multilingual lexical repository of Croatian, Slovene and Serbian multiword expressions that were extracted from parsed corpora. The lexica were built with the custom-built DepMWEx tool which uses dependency syntactic patterns to identify MWE candidates in parse trees. The extracted MWE candidates are subsequently scored by co-occurrence and organized by headwords producing a resource of 23 to 48 thousand headwords and 3.2 to 12 million MWE candidates per language. The evaluation of the lexicon, performed on Croatian and Slovene, shows an overall precision of just over 50% for Croatian but as high as 85% for Slovene. Similarly, precision over specific syntactic patterns varies greatly, 0.167-0.859 for Croatian, 0.158-1.00 for Slovene. The possible extension of the tool is demonstrated on a simplistic distributional-based extraction of non-transparent MWEs and cross-lingual linking of the extracted lexicons.", "Language": "en", "Citations": "1"},
{"Title": "The circular chromatic number of a digraph", "Authors": ["Bokal D.", "Fijavz G.", "Juvan M.", "Kayll P.M.", "Mohar B."], "Keywords": ["Acyclic homomorphism", "Chromatic number", "Circular chromatic number", "Digirth", "Digraph", "NP-completeness"], "Date": "2004", "Abstract": "We introduce the circular chromatic number \u03c7 ", "Language": "en", "Citations": "40"},
{"Title": "Common open representation of computer vision results in 2DGE research", "Authors": ["Peer P.", "Segura V.", "Solina F."], "Keywords": ["2DGE", "Algorithm", "Comparison", "Data sharing", "Image processing"], "Date": "2007", "Abstract": "Data standards are required to enable the development of public data repositories, to improve data sharing and also to enable sound and objective evaluation of different approaches. Here, we introduce a flexible data-format for computer vision results in 2DGE (2D Gel Electrophoresis), which meets data standard expectations. This data-format presents an attempt to standardize the communication between research groups in proteomics and computer vision communities, enabling them to share the data and simply compare automatically obtained image processing results of different computer vision algorithms. The proposed CVRML (Computer Vision Results Markup Language) for 2DGE format is modeled in UML (Unified Modeling Language) and implemented in XML (extensible Markup Language). Software is being developed, which helps us to easily and effectively annotate and visualize the data. We also discuss a possible framework around the annotated database structured in the CVRML manner, which we intend to make publicly available. All the material described in the paper is freely available on the CVRML web portal.", "Language": "en", "Citations": "1"},
{"Title": "\u201cDo I want to learn a language spoken by two million people?\u201d: Mediation choices by mid-term and long-term migrants", "Authors": ["Pokorn N.K.", "Cibej J."], "Keywords": ["Interpreting", "Long-term migrants", "Mediation choices", "Mid-term migrants", "Translation"], "Date": "2018", "Abstract": "Migrants' intended length of stay influences their choices between using a lingua franca, language technology, ad-hoc interpreters and translators, intercomprehension, or learning the host country's dominant language. To study this influence, data were collected through a questionnaire, semi-structured interviews, and a focus-group discussion from 15 long-term migrants (university language teachers) and eight mid-term migrants (teachers at two international schools) working in Slovenia. The results show that the long-term migrants all learned the host language, while the most common mediation strategy of the mid-term migrants was use of a lingua franca. Ad-hoc interpreters and translators were used not only in healthcare but also for the translations of official documents. Moreover, the French-speaking mid-term migrants attempted to learn the host language and often ended up learning English, while the group of native English speakers tended to form a linguistic enclave. It is argued that the preferred mediation strategy depends not just on the intended length of stay but also on the status of the migrant's L1 in the particular host country.", "Language": "en", "Citations": "0"},
{"Title": "A case study on agile estimating and planning using scrum", "Authors": ["Mahnic V."], "Keywords": [], "Date": "2011", "Abstract": "We describe a case study that was conducted at the University of Ljubljana with the aim of studying the behavior of development teams using Scrum for the first time, i.e., a situation typical for software companies trying to introduce Scrum into their development process. 13 student teams were required to develop an almost real project strictly using Scrum. The data on project management activities were collected in order to measure the amount of work completed, compliance with the release and iteration plans, and ability of effort estimation, thus contributing to evidence-based assessment of the typical Scrum processes. It was found that the initial plans and effort estimates were over-optimistic, but the abilities of estimating and planning improved from Sprint to Sprint. Most teams were able to define almost accurate Sprint plans after three Sprints. In the third Sprint the velocity stabilized and the actual achievement almost completely matched the plan.", "Language": "en", "Citations": "30"},
{"Title": "Likelihood based classification in Bayesian networks", "Authors": ["Stajduhar I.", "Bratko I."], "Keywords": ["Classification", "Machine learning", "Probabilistic networks"], "Date": "2007", "Abstract": "Learning directed probabilistic networks from data and using them for classification purposes is a well known problem. Many learning algorithms have been shown to be successful for various kinds of learning scenarios. Basically they all generate a single network from data, which is then used for classification purposes and possible domain understanding. In this paper we propose a simple method for inferring a model consisting of several Bayesian networks, each one representing data of one class. The data is divided into class subsets and from each subset a separate Bayesian network is learnt. Classification is done using prior and posterior probability distribution information in all networks. We thoroughly tested the proposed method on synthetic data and several repository datasets and compared it to other machine learning methods, to prove its effectiveness. We argue that with smaller modifications, the method can be used for learning from censored survival domains.", "Language": "en", "Citations": "1"},
{"Title": "Corpus-based diacritic restoration for south slavic languages", "Authors": ["Ljubesic N.", "Erjavec T.", "Fiser D."], "Keywords": ["Computer-mediated communication", "Diacritic restoration", "South-slavic languages"], "Date": "2016", "Abstract": "In computer-mediated communication, Latin-based scripts users often omit diacritics when writing. Such text is typically easily understandable to humans but very difficult for computational processing because many words become ambiguous or unknown. Letter-level approaches to diacritic restoration generalise better and do not require a lot of training data but word-level approaches tend to yield better results. However, they typically rely on a lexicon which is an expensive resource, not covering non-standard forms, and often not available for less-resourced languages. In this paper we present diacritic restoration models that are trained on easy-to-acquire corpora. We test three different types of corpora (Wikipedia, general web, Twitter) for three South Slavic languages (Croatian, Serbian and Slovene) and evaluate them on two types of text: standard (Wikipedia) and non-standard (Twitter). The proposed approach considerably outperforms charlifter, so far the only open source tool available for this task. We make the best performing systems freely available.", "Language": "en", "Citations": "6"},
{"Title": "Mobile vision for intelligent commuting: \"Interactive bus stop\" case study", "Authors": ["Krivic J.", "Jogan M.", "Leonardis A."], "Keywords": ["Mobile computer vision", "Object detection", "Object tracking", "User interaction with mobile device"], "Date": "2013", "Abstract": "In this paper we present a case study of context aware, on-site information retrieval using a computer-vision-based interaction on mobile phones with the goal of facilitating information access for urban commuters that use public transport. Our focus is on intuitive touchless interaction using contextual recognition of existing visual objects, such as signs and information plates. Object detection and localization are based on fast matching of local image features represented as Histogrammed Intensity Patches. We show that low-level image features can be learned, organized, and optimized for discrimination between similar, poorly textured object images. The system is integrated with existing software for information retrieval and experiments show that it achieves good performance in real-life situations.", "Language": "en", "Citations": "0"},
{"Title": "Comparison of selected performances of biological and electronic information processing structures Por\u00f3wnanie wybranych parametr\u00f3w struktur przep\u0142ywu informacji w systemie biologicznych i elektronicznym", "Authors": ["Moskon M.", "Zimic N.", "Strazar M.", "Mraz M."], "Keywords": ["Computational biology", "Gene regulatory networks", "Modelling", "Unconventional computing"], "Date": "2013", "Abstract": "We present the information processing perspective on biological systems. Several metrics, similar to the ones used in digital electronic circuits, are introduced. These metrics allow us to compare biological information processing structures with their electronic counterparts, to define the ones with the best dynamical properties, analyse their compatibility and most importantly, automatize their design. Regarding the metric values obtained and used on a simple example, target applications of synthetic information processing biological structures are discussed.", "Language": "en", "Citations": "0"},
{"Title": "Evaluating the aesthetics of endgame studies: A computational model of human aesthetic perception", "Authors": ["Iqbal A.", "Van Der Heijden H.", "Guid M.", "Makhmali A."], "Keywords": ["Aesthetics", "chess", "creativty", "endgame", "perception"], "Date": "2012", "Abstract": "In this paper, we explain how an existing computational aesthetics model for three-move mate problems was improved and adapted to suit the domain of chess endgame studies. Studies are typically longer and more sophisticated in terms of their perceived aesthetics or beauty. They are therefore likely a better test of the capability of machines to evaluate beauty in the game. Based on current validation methods for an aesthetics model such as this, the experimental results confirm that the adaptation was successful. In the first experiment, the new model enabled a computer program to distinguish correctly between composed studies and positions with sequences resembling studies taken from real games. In the second, the computational aesthetic evaluations were shown to correlate positively and well with human expert aesthetic assessment. The new model encompasses the previous three-mover one and can be used to evaluate beauty as perceived by humans in both domains. This technology pushes the boundaries of computational chess and can be of benefit to human players, composers, and judges. To some extent, it may also contribute to our understanding of the psychology of human aesthetic perception and the mechanics of human creativity in composing problems and studies. \u00a9 2009-2012 IEEE.", "Language": "en", "Citations": "6"},
{"Title": "On-line conservative learning for person detection", "Authors": ["Roth P.M.", "Grabner H.", "Skocaj D.", "Bischof H.", "Leonardis A."], "Keywords": [], "Date": "2005", "Abstract": "We present a novel on-line conservative learning framework for an object detection system. All algorithms operate in an on-line mode, in particular we also present a novel on-line AdaBoost method. The basic idea is to start with a very simple object detection system and to exploit a huge amount of unlabeled video data by being very conservative in selecting training examples. The key idea is to use reconstructive and discriminative classifiers in an iterative co-training fashion to arrive at increasingly better object detectors. We demonstrate the framework on a surveillance task where we learn person detectors that are tested on two surveillance video sequences. We start with a simple moving object classifier and proceed with incremental PCA (on shape and appearance) as a reconstructive classifier which in turn generates a training set for a discriminative on-line AdaBoost classifier. \u00a9 2005 IEEE.", "Language": "en", "Citations": "51"},
{"Title": "End-to-End Iris Segmentation Using U-Net", "Authors": ["Lozej J.", "Meden B.", "Struc V.", "Peer P."], "Keywords": ["Convolutional neural networks (CNN)", "Deep learning", "Iris", "Segmentation", "U-Net"], "Date": "2018", "Abstract": "Iris segmentation is an important research topic that received significant attention from the research community over the years. Traditional iris segmentation techniques have typically been focused on hand-crafted procedures that, nonetheless, achieved remarkable segmentation performance even with images captured in difficult settings. With the success of deep-learning models, researchers are increasingly looking towards convolutional neural networks (CNNs) to further improve on the accuracy of existing iris segmentation techniques and several CNN-based techniques have already been presented recently in the literature. In this paper we also consider deep-learning models for iris segmentation and present an iris segmentation approach based on the popular U-Net architecture. Our model is trainable end-to-end and, hence, avoids the need for hand designing the segmentation procedure. We evaluate the model on the CASIA dataset and report encouraging results in comparison to existing techniques used in this area.", "Language": "en", "Citations": "3"},
{"Title": "\u03ba-Same-Net: Neural-Network-Based Face Deidentification", "Authors": ["Meden B.", "Emersic Z.", "Struc V.", "Peer P."], "Keywords": [], "Date": "2017", "Abstract": "An increasing amount of video and image data is being shared between government entities and other relevant stakeholders and requires careful handling of personal information. A popular approach for privacy protection in such data is the use of deidentification techniques, which aim at concealing the identity of individuals in the imagery while still preserving certain aspects of the data deidentification. In this work, we propose a novel approach towards face deidentification, called \u03ba-Same-Net, which combines recent generative neural networks (GNNs) with the well-known \u03ba-anonymity mechanism and provides formal guarantees regarding privacy protection on a closed set of identities. Our GNN is able to generate synthetic surrogate face images for idedentification by seamlessly combining features of identities used to train the GNN mode. furthermore, it allows us to guide the image-generation process with a small set of appearance-related parameters that can be used to alter specific aspects (e.g., facial expressions, age, gender) of the synthesized surrogate images. We demonstrate the feasibility of \u03ba-Same-Net in comparative experiments with competing techniques on the XM2VTS dataset and discuss the main characteristics of our approach.", "Language": "en", "Citations": "3"},
{"Title": "A research tool for user preferences elicitation with facial expressions", "Authors": ["Tkalcic M.", "Maleki N.", "Pesek M.", "Elahi M.", "Ricci F.", "Marolt M."], "Keywords": ["Emotions", "Facial expressions", "Implicit preference elicitation", "Pairwise scores"], "Date": "2017", "Abstract": "We present a research tool for user preference elicitation that collects both explicit user feedback and unobtrusively acquired facial expressions. The concrete implementation is a web-based user interface where the user is presented with two music excerpts. After listening to both, the user provides a pairwise score (i.e. which of the two items is preferred) for each pair of music excerpts. The novelty of the demo is the integration of the unobtrusive acquisition of facial expressions through the webcam. During the listening of the music excerpts, the system extracts features related to the facial expressions of the user several times per second. The interaction runs as a web application, which allows for a large-scale remote acquisition of emotional data. Up to now, such acquisitions were usually done in controlled environments with few subjects, hence being of little use for the recommender systems community.", "Language": "en", "Citations": "5"},
{"Title": "A comparison of two approaches to bilingual HMM-based speech synthesis", "Authors": ["Pobar M.", "Justin T.", "Zibert J.", "Mihelic F.", "Ipsic I."], "Keywords": ["bilingual", "HMM", "Kullback-Leibler divergence", "phoneme mapping", "speaker adaptation", "speech synthesis", "state mapping"], "Date": "2013", "Abstract": "We compare the performance of two approaches when using cross-lingual data from different speakers to build bilingual speech synthesis systems capable of producing speech with the same speaker identity. One approach treats data from both languages as monolingual, by labeling all data with a manually joined phoneme set. Speaker independent voice is trained using the joined data, and adapted to the target speaker using the CMLLR adaptation. In the second approach, speaker independent voices are trained for each language separately. State mapping between these voices is derived automatically from minimum Kullback-Leibler divergence between state distributions. The mapping is used to apply the adaptation transformations calculated within one language across languages to the other speaker independent voice. We evaluate the quality of speech on MOS scale and similarity of synthesized speech characteristics to the target speaker using DMOS on the example of Croatian-Slovene language pair. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "3"},
{"Title": "Web user profiles with time-decay and prototyping", "Authors": ["Kosir D.", "Kononenko I.", "Bosnic Z."], "Keywords": ["Ontological profile", "Profile correction", "Profile quality", "Prototype profiles", "Time-decay", "User profiling"], "Date": "2014", "Abstract": "User profiling represents an important initial step in personalizing web services and in building recommendation systems. Non-invasive profiling methods monitor users\u2019 behavior and infer interest profiles from their past actions. Most existing profiling methods, which relate the users\u2019 interests to a given ontology, consider only the user\u2019s past actions when calculating his/her profile. The profiling algorithms use a time-decay function for users\u2019 past actions to adapt the profile to shifts in the user\u2019s interests over time. In our work, we propose a hybrid method that combines time-decay and profile correction using prototype profiles. The additional profile correction step considers the interests of similar users and expands the interest scores beyond the concepts detected in the user\u2019s past actions, which facilitates faster profile adaptation to the user\u2019s new interests. In our experimental work, we experimented extensively with two real data sets: data of an online advertising network and student data in an online e-learning environment. We measured the quality of the computed user profiles by correlating them to users\u2019 future actions. Experiments revealed that it is crucial to build the user\u2019s profile using a large number of events from his/her past and to update the profile regularly. When we are unable to do so, the profile correction can be used to keep the quality of the profile from dropping too low. The results show that our method significantly outperforms existing ontological profiling methods.", "Language": "en", "Citations": "2"},
{"Title": "Influence of daily individual meteorological parameters on the incidence of acute coronary syndrome", "Authors": ["Ravljen M.", "Bilban M.", "Kajfez-Bogataj L.", "Hovelja T.", "Vavpotic D."], "Keywords": [], "Date": "2014", "Abstract": "BACKGROUND: A nationwide study was conducted to explore the short term association between daily individual meteorological parameters and the incidence of acute coronary syndrome (ACS) treated with coronary emergency catheter interventions in the Republic of Slovenia, a south-central European country.", "Language": "en", "Citations": "9"},
{"Title": "Co-allocation with collective requests in grid systems", "Authors": ["Cankar M.", "Artac M.", "Sterk M.", "Lotric U.", "Slivnik B."], "Keywords": ["Advance reservations", "Concurrency", "Grid computing", "Parallel applications", "Resource co-allocation"], "Date": "2013", "Abstract": "We present a new algorithm for resource allocation in large, heterogeneous grids. Its main advantage over existing co-allocation algorithms is that it supports collective requests with partial resource reservation, where the focus is on better grid utilisation. Alongside the requests that must be fulfilled by each resource, a collective request specifies the total amount of a required resource property without a strict assumption with regard to its distribution. As a consequence, the job becomes much more flexible in terms of its resource assignment and the co-allocation algorithm may therefore start the job earlier. This flexibility increases grid utilisation as it allows an optimisation of job placement that leads to a greater number of accepted jobs. The proposed algorithm is implemented as a module in the XtreemOS grid operating system. Its performance and complexity have been assessed through experiments on the Grid'5000 infrastructure. The results reveal that in most cases the algorithm returns optimal start times for jobs and acceptable, but sometimes suboptimal resource sets. \u00a9 J.UCS.", "Language": "en", "Citations": "1"},
{"Title": "Does replication groups scoring reduce false positive rate in SNP interaction discovery?", "Authors": ["Toplak M.", "Curk T.", "Demsar J.", "Zupan B."], "Keywords": [], "Date": "2010", "Abstract": "Background: Computational methods that infer single nucleotide polymorphism (SNP) interactions from phenotype data may uncover new biological mechanisms in non-Mendelian diseases. However, practical aspects of such analysis face many problems. Present experimental studies typically use SNP arrays with hundreds of thousands of SNPs but record only hundreds of samples. Candidate SNP pairs inferred by interaction analysis may include a high proportion of false positives. Recently, Gayan et al. (2008) proposed to reduce the number of false positives by combining results of interaction analysis performed on subsets of data (replication groups), rather than analyzing the entire data set directly. If performing as hypothesized, replication groups scoring could improve interaction analysis and also any type of feature ranking and selection procedure in systems biology. Because Gayan et al. do not compare their approach to the standard interaction analysis techniques, we here investigate if replication groups indeed reduce the number of reported false positive interactions.Results: A set of simulated and false interaction-imputed experimental SNP data sets were used to compare the inference of SNP-SNP interactions by means of replication groups to the standard approach where the entire data set was directly used to score all candidate SNP pairs. In all our experiments, the inference of interactions from the entire data set (e.g. without using the replication groups) reported fewer false positives.Conclusions: With respect to the direct scoring approach the utility of replication groups does not reduce false positive rates, and may, depending on the data set, often perform worse. \u00a9 2010 Toplak et al; licensee BioMed Central Ltd.", "Language": "en", "Citations": "2"},
{"Title": "Systematic approach to computational designof gene regulatory networks with information processing capabilities", "Authors": ["Moskon M.", "Mraz M."], "Keywords": ["Computational biology", "computational design", "gene regulatory networks", "information processing", "modelling and simulation", "modular design", "synthetic biology"], "Date": "2014", "Abstract": "We present several measures that can be used in de novo computational design of biological systems with information processing capabilities. Their main purpose is to objectively evaluate the behavior and identify the biological information processing structures with the best dynamical properties. They can be used to define constraints that allow one to simplify the design of more complex biological systems. These measures can be applied to existent computational design approaches in synthetic biology, i.e., rational and automatic design approaches. We demonstrate their use on a) the computational models of several basic information processing structures implemented with gene regulatory networks and b) on a modular design of a synchronous toggle switch. \u00a9 2013 IEEE.", "Language": "en", "Citations": "3"},
{"Title": "Deep brain stimulation of the subthalamic nucleus does not affect the decrease of decision threshold during the choice process when there is no conflict, time pressure, or reward", "Authors": ["Leimbach F.", "Georgiev D.", "Litvak V.", "Antoniades C.", "Limousin P.", "Jahanshahi M.", "Bogacz R."], "Keywords": [], "Date": "2018", "Abstract": "During a decision process, the evidence supporting alternative options is integrated over time, and the choice is made when the accumulated evidence for one of the options reaches a decision threshold. Humans and animals have an ability to control the decision threshold, that is, the amount of evidence that needs to be gathered to commit to a choice, and it has been proposed that the subthalamic nucleus (STN) is important for this control. Recent behavioral and neurophysiological data suggest that, in some circumstances, the decision threshold decreases with time during choice trials, allowing overcoming of indecision during difficult choices. Here we asked whether this within-trial decrease of the decision threshold is mediated by the STN and if it is affected by disrupting information processing in the STN through deep brain stimulation (DBS). We assessed 13 patients with Parkinson disease receiving bilateral STN DBS six or more months after the surgery, 11 age-matched controls, and 12 young healthy controls. All participants completed a series of decision trials, in which the evidence was presented in discrete time points, which allowed more direct estimation of the decision threshold. The participants differed widely in the slope of their decision threshold, ranging from constant threshold within a trial to steeply decreasing. However, the slope of the decision threshold did not depend on whether STN DBS was switched on or off and did not differ between the patients and controls. Furthermore, there was no difference in accuracy and RT between the patients in the on and off stimulation conditions and healthy controls. Previous studies that have reported modulation of the decision threshold by STN DBS or unilateral subthalamotomy in Parkinson disease have involved either fast decision-making under conflict or time pressure or in anticipation of high reward. Our findings suggest that, in the absence of reward, decision conflict, or time pressure for decision-making, the STN does not play a critical role in modulating the within-trial decrease of decision thresholds during the choice process.", "Language": "en", "Citations": "2"},
{"Title": "Semi-automatic improvement of software development methods: Doctoral consortium paper", "Authors": ["Jankovic M."], "Keywords": [], "Date": "2013", "Abstract": "Empirical studies show that the use and adoption of software development methods (SDMs) in practice is still low. Not just that developers often avoid to follow what is prescribed in their organisations, very often the organisations don't even have prescribed and documented methods. This situation is recognized as one of the main reasons for failures in IT projects and a contributor to the low quality of software. In this paper we describe a PhD thesis that addresses this problem. We introduce an innovative approach and supporting tool that will semiautomatically monitor in-action methods during project performance, and capture the gained knowledge with only negligible involvement of the developers. Our expectation is that since the prescribed methods will be in this way based on the practice that is accepted in a certain organisation, its users will not perceive it as something imposed by their managers and will follow it much more rigorously. Furthermore, we intend to show that it is possible to establish a continuous software process improvement lifecycle with just a marginal user involvement. \u00a9 2013 IEEE.", "Language": "en", "Citations": "0"},
{"Title": "Model-driven architecture and its impact on the software development process Arhitektura za modelno voden razvoj in njen vpliv na proces razvoja programske opreme", "Authors": ["Vavpotic D.", "Krisper M."], "Keywords": ["MDA", "Model-driven development", "Software development methodologies", "Software development process"], "Date": "2006", "Abstract": "The idea of model-driven software development is not new. It has been practiced in certain development fields for several years (e.g. data modeling). The trials to spread the idea to other fields of software development have been quite unsuccessful and never widely used in practice (e.g. CASE). Based on the gained experience, the architecture for model-driven development of software systems, i.e. model-driven architecture (MDA), has been developed under the umbrella of the Object management group (OMG). MDA offers a theoretical basis that enables the development of software on a higher abstraction level. Its use also affects the software development process; on one hand, analysis and design become the most important parts of the development and on the other, implementation and testing turn to be less important. The first section of the paper briefly presents the basic concepts of MDA. The second section describes the impact of model-driven development and MDA on a software development process and especially focuses on a comparison between model driven development and agile processes.", "Language": "en", "Citations": "0"},
{"Title": "Wavelet analysis increases sensitivity and specificity of spirography for ambulatory tremor discrimination", "Authors": ["Kragelj V.", "Georgiev D.", "Pirtosek Z.", "Ribaric S."], "Keywords": [], "Date": "2014", "Abstract": "The most frequently seen types of tremor are essential (ET) and parkinsonian tremor (PT) and in some patients clinical characteristics of these tremor types overlap. It is vital to distinguish between these two types of tremor in order to reach the right diagnosis and select the appropriate treatment. One of the widely used methods for tremor detection and discrimination, appropriate for a quick ambulatory assessment of the patient's tremor, is spirography. With spirography, the tremor can be observed through several parameters, for example, tremor spectrum and spiral image, which give useful information for its identification. Standard spirography parameters of ET and PT can overlap; therefore, these parameters are often not enough for identification of the observed tremor. To increase the specificity and sensitivity of spirography for PT, ET and normal, tremor free controls, we used the wavelet analysis with Morlet wavelet transform. To facilitate analysis, comparison, storage, and retrieval of spirography tremor records we also developed an integrated computer assisted spirography system that increases the convenience of outpatient tremor identification and follow-up. We conclude that wavelet analysis of spirography records increases the sensitivity and specificity of the method, thus, facilitating the distinction between ET and PT.", "Language": "en", "Citations": "2"},
{"Title": "Application of method engineering principles in practice: Lessons learned and prospects for the future", "Authors": ["Bajec M."], "Keywords": [], "Date": "2011", "Abstract": "It seems that in IT sector we are all aware that for the development of non-trivial software the use of software methods is very important. They provides as with knowledge and guidance for the development process which otherwise might become too chaotic and out of control. It has been empirically proven that software development companies which have successfully established their software processes are more efficient, produce software of higher quality and have shorter time-to-market period; specifically if they are able to adapt their ways of working to specifics of a particular project. \u00a9 2011 IFIP International Federation for Information Processing.", "Language": "en", "Citations": "1"},
{"Title": "Qualitative assessment dynamics: QAD", "Authors": ["Trcek D."], "Keywords": [], "Date": "2013", "Abstract": "This chapter presents an anthropocentric method for trust management in the area of agreement technologies, called Qualitative Assessment Dynamics. As opposed to other approaches like Bayesian inference, game-theoretic approaches and alike, this method focuses on linguistic bases for modeling trust in agents societies. Therefore it deploys operands and operators that mimick trust dynamics by reflecting related mental processes through corresponding descriptions in ordinary languages.", "Language": "en", "Citations": "0"},
{"Title": "Practice-driven approach for creating project-specific software development methods", "Authors": ["Bajec M.", "Vavpotic D.", "Krisper M."], "Keywords": ["Method engineering", "Process engineering", "Project-specific method"], "Date": "2007", "Abstract": "Both practitioners and researchers agree that if software development methods were more adjustable to project-specific situations, this would increase their use in practice. Empirical investigations show that otherwise methods exist just on paper while in practice developers avoid them or do not follow them rigorously. In this paper we present an approach that deals with this problem. Process Configuration, as we named the approach, tells how to create a project-specific method from an existing one, taking into account the project circumstances. Compared to other approaches that deal with the creation of project-specific methods, our approach tends to be more flexible and easier to implement in practice as it introduces few simplifications. The proposed approach is practice-driven, i.e. it has been developed in cooperation with software development companies. \u00a9 2006 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "58"},
{"Title": "Dopaminergic pathway genes influence adverse events related to dopaminergic treatment in Parkinson's disease", "Authors": ["Redensek S.", "Flisar D.", "Kojovic M.", "Kramberger M.G.", "Georgiev D.", "Pirtosek Z.", "Trost M.", "Dolzan V."], "Keywords": ["Adverse events", "Dopaminergic pathway", "Genetic polymorphism", "Parkinson's disease", "Personalized medicine"], "Date": "2019", "Abstract": "Dopaminergic pathway is the most disrupted pathway in the pathogenesis of Parkinson's disease. Several studies reported associations of dopaminergic genes with the occurrence of adverse events of dopaminergic treatment. However, none of these studies adopted a pathway based approach. The aim of this study was to comprehensively evaluate the influence of selected single nucleotide polymorphisms of key dopaminergic pathway genes on the occurrence of motor and non-motor adverse events of dopaminergic treatment in Parkinson's disease. In total, 231 Parkinson's disease patients were enrolled. Demographic and clinical data were collected. Genotyping was performed for 16 single nucleotide polymorphisms from key dopaminergic pathway genes. Logistic and Cox regression analyses were used for evaluation. Results were adjusted for significant clinical data. We observed that carriers of at least one COMT rs165815 C allele had lower odds for developing visual hallucinations (OR = 0.34; 95% CI = 0.16-0.72; p = 0.004), while carriers of at least one DRD3 rs6280 C allele and CC homozygotes had higher odds for this adverse event (OR = 1.88; 95% CI = 1.00-3.54; p = 0.049 and OR = 3.31; 95% CI = 1.37-8.03; p = 0.008, respectively). Carriers of at least one DDC rs921451 C allele and CT heterozygotes had higher odds for orthostatic hypotension (OR = 1.86; 95% CI = 1.07-3.23; p = 0.028 and OR = 2.30; 95% CI = 1.26-4.20; p = 0.007, respectively). Heterozygotes for DDC rs3837091 and SLC22A1 rs628031 AA carriers also had higher odds for orthostatic hypotension (OR = 1.94; 95% CI = 1.07 - 3.51; p = 0.028 and OR = 2.57; 95% CI = 1.11 - 5.95; p = 0.028, respectively). Carriers of the SLC22A1 rs628031 AA genotype had higher odds for peripheral edema and impulse control disorders (OR = 4.00; 95% CI = 1.62 - 9.88; p = 0.003 and OR = 3.16; 95% CI = 1.03 - 9.72; p = 0.045, respectively). Finally, heterozygotes for SLC22A1 rs628031 and carriers of at least one SLC22A1 rs628031 A allele had lower odds for dyskinesia (OR = 0.48; 95% CI = 0.24 - 0.98, p = 0.043 and OR = 0.48; 95% CI = 0.25 - 0.92; p = 0.027, respectively). Gene-gene interactions, more specifically DDC-COMT, SLC18A2-SV2C, and SLC18A2-SLC6A3, also significantly influenced the occurrence of some adverse events. Additionally, haplotypes of COMT and SLC6A3 were associated with the occurrence of visual hallucinations (AT vs. GC: OR = 0.34; 95% CI = 0.16 - 0.72; p = 0.005) and orthostatic hypotension (ATG vs. ACG: OR = 2.48; 95% CI: 1.01 - 6.07; p = 0.047), respectively. Pathway based approach allowed us to identify new potential candidates for predictive biomarkers of adverse events of dopaminergic treatment in Parkinson's disease, which could contribute to treatment personalization.", "Language": "en", "Citations": "1"},
{"Title": "Imputation of quantitative genetic interactions in epistatic MAPs by interaction propagation matrix completion", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": ["epistatic miniarray profile", "genetic interaction", "interaction propagation", "matrix completion", "missing value imputation"], "Date": "2014", "Abstract": "A popular large-scale gene interaction discovery platform is the Epistatic Miniarray Profile (E-MAP). E-MAPs benefit from quantitative output, which makes it possible to detect subtle interactions. However, due to the limits of biotechnology, E-MAP studies fail to measure genetic interactions for up to 40% of gene pairs in an assay. Missing measurements can be recovered by computational techniques for data imputation, thus completing the interaction profiles and enabling downstream analysis algorithms that could otherwise be sensitive to largely incomplete data sets. We introduce a new interaction data imputation method called interaction propagation matrix completion (IP-MC). The core part of IP-MC is a low-rank (latent) probabilistic matrix completion approach that considers additional knowledge presented through a gene network. IP-MC assumes that interactions are transitive, such that latent gene interaction profiles depend on the profiles of their direct neighbors in a given gene network. As the IP-MC inference algorithm progresses, the latent interaction profiles propagate through the branches of the network. In a study with three different E-MAP data assays and the considered protein-protein interaction and Gene Ontology similarity networks, IP-MC significantly surpassed existing alternative techniques. Inclusion of information from gene networks also allows IP-MC to predict interactions for genes that were not included in original E-MAP assays, a task that could not be considered by current imputation approaches. \u00a9 2014 Springer International Publishing Switzerland.", "Language": "en", "Citations": "0"},
{"Title": "In Parkinson\u2019s disease on a probabilistic Go/NoGo task deep brain stimulation of the subthalamic nucleus only interferes with withholding of the most prepotent responses", "Authors": ["Georgiev D.", "Dirnberger G.", "Wilkinson L.", "Limousin P.", "Jahanshahi M."], "Keywords": ["Deep brain stimulation (DBS)", "Go/NoGo task", "Load-dependent effects", "Parkinson\u2019s disease (PD)", "Prepotency", "Subthalamic nucleus (STN)"], "Date": "2016", "Abstract": "The evidence on the impact of subthalamic nucleus deep brain stimulation (STN-DBS) on action restraint on Go/NoGO reaction time (RT) tasks in Parkinson\u2019s disease (PD) is inconsistent; with some studies reporting no effect and others finding that STN stimulation interferes with withholding of responses and results in more commission errors relative to STN-DBS off. We used a task in which the probability of Go stimuli varied from 100\u00a0% (simple RT task) to 80, 50 and 20\u00a0% (probabilistic Go/NoGo RT task), thus altering the prepotency of the response and the difficulty in withholding it on NoGo trials. Twenty PD patients with STN-DBS, ten unoperated PD patients and ten healthy controls participated in the study. All participants were tested twice; the order of on versus off stimulation for STN-DBS PD patients was counterbalanced. Both STN-DBS and unoperated PD patients were tested on medication. The results indicated that STN-DBS selectively decreased discriminability when the response was most prepotent (high\u201480\u00a0%, as compared to low Go probability trials\u201450 and 20\u00a0%). Movement times were faster with STN stimulation than with DBS off across different Go probability levels. There was neither an overall nor a selective effect of STN-DBS on RTs depending on the level of Go probability. Furthermore, compared to healthy controls, both STN-DBS and unoperated PD patients were more prone to making anticipatory errors; which was not influenced by STN stimulation. The results provide evidence for \u2018load-dependent\u2019 effects of STN stimulation on action restraint as a function of the prepotency of the Go response.", "Language": "en", "Citations": "7"},
{"Title": "Model selection with combining valid and optimal prediction intervals", "Authors": ["Pevec D.", "Kononenko I."], "Keywords": ["Estimation error", "Machine Learning", "Predictive models", "Regression analysis", "Supervised learning"], "Date": "2012", "Abstract": "In this paper we explore the possibility of automatic model selection in the supervised learning framework with the use of prediction intervals. First we compare two families of non-parametric approaches of constructing prediction intervals for arbitrary regression models. The first family of approaches is based on the idea of explaining the total prediction error as a sum of the model's error and the error caused by noise inherent to the data - the two are estimated independently. The second family assumes local similarity of the data and these approaches estimate the prediction intervals with use of the sample's nearest neighbors. The comparison shows that the first family strives to produce valid prediction intervals whereas the second family strives for optimality. We propose a statistic for model selection where we compare the discrepancy between valid and optimal prediction intervals. Experiments performed on a set of artificial datasets strongly support the hypothesis that for the correct model, this discrepancy is minimal among competing models. \u00a9 2012 IEEE.", "Language": "en", "Citations": "1"},
{"Title": "VillageCell: Cost effective cellular connectivity in rural areas", "Authors": ["Anand A.", "Pejovic V.", "Johnson D.L.", "Belding E.M."], "Keywords": ["Cellular communication", "Low-cost communication", "Mobile telephony", "OpenBTS", "Rural area networks"], "Date": "2012", "Abstract": "Mobile telephony brings clear economic and social benefits to its users. As handsets have become more affordable, ownership has reached staggering numbers, even in the most remote areas of the world. However, network coverage is often lacking in low population densities and low income rural areas of the developing world, where big telecoms often defer from deploying expensive infrastructure. To solve this coverage gap, we propose VillageCell, a low-cost alternative to high-end cell phone networks. VillageCell relies on software defined radios and open-source solutions to provide free local and cheap long-distance communication for remote regions. Our architecture is simple and easy to deploy, yet robust and requires no modification to GSM handsets. Through measuring the call quality metrics and the system capacity under a realistic rural-area network load, we show that VillageCell is indeed an attractive solution for rural area voice connectivity. Copyright 2012 ACM.", "Language": "en", "Citations": "37"},
{"Title": "Mobile robot localization under varying illumination", "Authors": ["Jogan M.", "Leonardis A.", "Wildenauer H.", "Bischof H."], "Keywords": [], "Date": "2002", "Abstract": "Methods for mobile robot localization that use eigenspaces of panoramic snapshots of the environment are in general sensitive to changes in the illumination of the environment. Therefore, we propose an approach which achieves a reliable localization under severe illumination conditions. The method uses gradient filtering of the eigenspace. After testing the approach on images obtained by a mobile robot, we show that it outperforms the standard eigenspace-based recognition method. \u00a9 2002 IEEE.", "Language": "en", "Citations": "17"},
{"Title": "Improving random forests", "Authors": ["Robnik-Sikonja M."], "Keywords": [], "Date": "2004", "Abstract": "Random forests are one of the most successful ensemble methods which exhibits performance on the level of boosting and support vector machines. The method is fast, robust to noise, does not overfit and offers possibilities for explanation and visualization of its output. We investigate some possibilities to increase strength or decrease correlation of individual trees in the forest. Using several attribute evaluation measures instead of just one gives promising results. On the other hand replacement of ordinary voting with voting weighted with margin achieved on most similar instances gives improvements which are statistically highly significant over several data sets. \u00a9 Springer-Verlag Berlin Heidelberg 2004.", "Language": "en", "Citations": "102"},
{"Title": "Simple and effective visual models for gene expression cancer diagnostics", "Authors": ["Leban G.", "Mramor M.", "Bratko I.", "Zupan B."], "Keywords": ["Cancer Diagnosis", "Data Mining", "Data Visualization", "Gene Expression Analysis", "Machine Learning"], "Date": "2005", "Abstract": "In the paper we show that diagnostic classes in cancer gene expression data sets, which most often include thousands of features (genes), may be effectively separated with simple two-dimensional plots such as scatterplot and radviz graph. The principal innovation proposed in the paper is a method called VizRank, which is able to score and identify the best among possibly millions of candidate projections for visualizations. Compared to recently much applied techniques in the field of cancer genomics that include neural networks, support vector machines and various ensemble-based approaches, VizRank is fast and finds visualization models that can be easily examined and interpreted by domain experts. Our experiments on a number of gene expression data sets show that VizRank was always able to find data visualizations with a small number of (two to seven) genes and excellent class separation. In addition to providing grounds for gene expression cancer diagnosis, VizRank and its visualizations also identify small sets of relevant genes, uncover interesting gene interactions and point to outliers and potential misclassifications in cancer data sets. Copyright 2005 ACM.", "Language": "en", "Citations": "1"},
{"Title": "Threshold-coloring and unit-cube contact representation of graphs", "Authors": ["Alam Md.J.", "Chaplick S.", "Fijavz G.", "Kaufmann M.", "Kobourov S.G.", "Pupyrev S."], "Keywords": [], "Date": "2013", "Abstract": "We study threshold coloring of graphs where the vertex colors, represented by integers, describe any spanning subgraph of the given graph as follows. Pairs of vertices with near colors imply the edge between them is present and pairs of vertices with far colors imply the edge is absent. Not all planar graphs are threshold-colorable, but several subclasses, such as trees, some planar grids, and planar graphs with no short cycles can always be threshold-colored. Using these results we obtain unit-cube contact representation of several subclasses of planar graphs. We show the NP-completeness for two variants of the threshold coloring problem and describe a polynomial-time algorithm for another. \u00a9 2013 Springer-Verlag.", "Language": "en", "Citations": "4"},
{"Title": "Survival regression by data fusion", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": ["Cancer data", "Collective matrix factorization", "Data fusion", "Latent models", "Survival regression"], "Date": "2014", "Abstract": "Any knowledge discovery could in principal benefit from the fusion of directly or even indirectly related data sources. In this paper we explore whether data fusion by simultaneous matrix factorization could be adapted for survival regression. We propose a new method that jointly infers latent data factors from a number of heterogeneous data sets and estimates regression coefficients of a survival model. We have applied the method to CAMDA 2014 largescale Cancer Genomes Challenge and modeled survival time as a function of gene, protein and miRNA expression data, and data on methylated and mutated regions. We find that both joint inference of data factors and regression coefficients and data fusion procedure are crucial for performance. Our approach is substantially more accurate than the baseline Aalen\u2019s additive model. Latent factors inferred by our approach could be mined further; for CAMDA challenge, we found that the most informative factors are related to known cancer processes.", "Language": "en", "Citations": "2"},
{"Title": "Co-bidding graphs for constrained paper clustering", "Authors": ["Skvorc T.", "Lavrac N.", "Robnik-Sikonja M."], "Keywords": ["Conference", "Constrained clustering", "Data fusion", "Scheduling", "Text mining"], "Date": "2016", "Abstract": "The information for many important problems can be found in various formats and modalities. Besides standard tabular form, these include also text and graphs. To solve such problems fusion of different data sources is required. We demonstrate a methodology which is capable to enrich textual information with graph based data and utilize both in an innovative machine learning application of clustering. The proposed solution is helpful in organization of academic conferences and automates one of its time consuming tasks. Conference organizers can currently use a small number of software tools that allow managing of the paper review process with no/little support for automated conference scheduling. We present a two-tier constrained clustering method for automatic conference scheduling that can automatically assign paper presentations into predefined schedule slots instead of requiring the program chairs to assign them manually. The method uses clustering algorithms to group papers into clusters based on similarities between papers. We use two types of similarities: text similarities (paper similarity with respect to their abstract and title), together with graph similarity based on reviewers' co-bidding information collected during the conference reviewing phase. In this way reviewers' preferences serve as a proxy for preferences of conference attendees. As a result of the proposed two-tier clustering process similar papers are assigned to predefined conference schedule slots. We show that using graph based information in addition to text based similarity increases clustering performance. The source code of the solution is freely available.", "Language": "en", "Citations": "3"},
{"Title": "Bilingual speech recognition for a weather information retrieval dialogue system", "Authors": ["Martincic-Ipsic S.", "Zibert J.", "Ipsic I.", "Mihelic F.", "Pavesic N."], "Keywords": [], "Date": "2003", "Abstract": "In the paper we present current activities and some preliminary results of a joint project in designing a spoken dialogue system for Slovenian and Croatian weather information retrieval. We give a brief description of the system design, of the procedures we have performed in order to obtain domain specific speech databases and monolingual and bilingual speech recognition experiments. Recognition results for Croatian and Slovenian speech are presented, as well as bilingual speech recognition results when using common acoustic models. We propose two different approaches to the language identification problem and show recognition results for the two acoustically similar languages like Slovenian and Croatian.", "Language": "en", "Citations": "1"},
{"Title": "Visual stability prediction for robotic manipulation", "Authors": ["Li W.", "Leonardis A.", "Fritz M."], "Keywords": [], "Date": "2017", "Abstract": "Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel objects and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage. In this paper, we contrast a more traditional approach of taking a model-based route with explicit 3D representations and physical simulation by an end-to-end approach that directly predicts stability from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way - bypassing the need for an explicit simulation at run-time. We present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. We first evaluate the approach on synthetic data and compared the results to human judgments on the same stimuli. Further, we extend this approach to reason about future states of such towers that in return enables successful stacking.", "Language": "en", "Citations": "8"},
{"Title": "Increasing accuracy of automated essay grading by grouping similar graders", "Authors": ["Zupanc K.", "Bosnic Z."], "Keywords": ["Automated essay evaluation", "Clustering", "Explanations of predictions", "Prediction accuracy"], "Date": "2018", "Abstract": "Automated essay evaluation is a widely used practical solution for replacing time-consuming manual grading of student essays. Automated systems are used in combination with human graders in different high-stake assessments, where grading models are learned on essays datasets scored by different graders. Despite the unified grading rules, human graders can unintentionally introduce subjective bias into scores. Consequently, a grading model has to learn from a data that represents a noisy relationship between essay attributes and its grade. We propose an approach for separating a set of essays into subsets that represent similar graders, which uses an explanation methodology and clustering. The results confirm our assumption that learning from the ensemble of separated models can significantly improve the average prediction accuracy on artificial and real-world datasets.", "Language": "en", "Citations": "0"},
{"Title": "Identification of the repertoire of eighteenth-century bell musical clocks based on automatic comparison Identificatie van repertoire van achttiendeeeuwse bellenspeelklokken op basis van automatische vergelijking", "Authors": ["Lefeber M.", "Van Kranenburg P.", "Marolt M."], "Keywords": [], "Date": "2011", "Abstract": "In the eighteenth century prosperous Dutch citizens possessed musical clocks that played melodies at fixed times of the day, using an automatic bell-playing mechanism. A large number of these clocks have survived, enabling us to research the repertoire of melodies in which Dutch eighteenth-century citizens were interested. This repertory has not been researched before. A major problem is the identification of the melodies. Sometimes titles are written on the clock, but in many cases these do not lead to identification. To find relationships among bell-clock melodies and between bell-clock melodies and other repertories in the Database of Dutch songs, which is hosted by the Meertens Institute (Amsterdam), we took a computational approach. The clock melodies are only available to us as recordings. To process a large amount of melodies, both automatic pitch recognition and a similarity measure of the melodies are necessary. Both problems have been researched within the field of Music Information Retrieval before. Until now no algorithm exists that is able to perform pitch and onset recognition in bell melodies. We developed an algorithm to transcribe a musical clock recording into a sequence of pitch-onset pairs. We analyzed a set of manually annotated bell sounds and developed a model for estimating the locations of the inharmonic bell partials based on positions of the fundamental and the fourth harmonic. Once the melodies were available as sequences of pitch-onset pairs, we computed the similarities between melodies using a variant of the Needleman-Wunsch sequence alignment algorithm. This algorithm finds an optimal alignment of two sequences of symbols according to a similarity measure for the symbols. In our case, we used the pitch-distance as a similarity measure for symbols. A score is computed for the alignment, which we interpreted as a similarity measure for melodies. This method enabled us to identify a large number of melodies and increased our insight in the musical clock repertoire. The repertory of the luxury clocks consists mainly of melodies which origin from lower social layers. An example of such a melody is the song Pierlala. On one of the musical clocks, the English title Dutch Skipper is used to indicate the Dutch melody Boerenballet. This shows that there was an exchange be-tween the Netherlands and England in clock making, and that a Dutch song became popular in England, which was quite unusual. This study demonstrates how methods from music information retrieval successfully can be employed for historical musico-logical research.", "Language": "en", "Citations": "0"},
{"Title": "Collective information extraction using first-order probabilistic models", "Authors": ["Zitnik S.", "Subelj L.", "Lavbic D.", "Zrnec A.", "Bajec M."], "Keywords": ["Conditional random fields text tagging", "Information extraction"], "Date": "2012", "Abstract": "Traditional information extraction (IE) tasks roughly consist of named-entity recognition, relation extraction and coreference resolution. Much work in this area focuses primarily on separate subtasks where best performance can be achieved only on specialized domains. In this paper we present a collective IE approach combining all three tasks by employing linear-chain conditional random fields. The usage of probabilistic models enables us to easily communicate between tasks on the fly and error correction during the iterative process execution. We introduce a novel iterative-based IE system architecture with additional semantic and collective feature functions. Proposed system is evaluated against real-world data set, introduced in the paper, and results are better over traditional approaches on two tested tasks by error reduction and performance improvements. Copyright 2012 ACM.", "Language": "en", "Citations": "3"},
{"Title": "Literary aspects of the new media art works by Jaka \u017deleznikar and Sre\u010do Dragan Literarni vidiki novomedijskih del Jake \u017deleznikarja in Sre\u010da Dragana", "Authors": ["Bovcon N."], "Keywords": ["Cybertext", "Digital art", "Dragan, Sre\u010do", "Interactive poetry", "New media", "Visual poetry", "\u017deleznikar, Jaka"], "Date": "2013", "Abstract": "Jaka \u017deleznikar's works are written as algorithms, which ensures that they function \"naturally\" on the computer. On the other hand, as literature, these works participate in creating a new-similar, but different-literary experience. \u017deleznikar writes visual poetry using literary algorithms. ASCII art using linguistic characters to produce images is a functional new medium for generative visual poetry. During this period, \u017deleznikar programmed several visual poem generators and typing machines, which focused on the interface layer of the new media object and on the gesture of typing. \u017deleznikar referred to his works from 1996 to 2005 as \"net. art,\" and afterwards he began writing browser extensions and \"networked narratives\" for the web 2.0 environment. Since 2008 he has been creating \"networked e-poetry\" incorporating on-line social media such as Twitter. Sreco Dragan, a new media artist with a background in conceptual art practices, video art, and painting, addressed the literary aspects of new media art in a series of techno-performances from 2005 to 2010. These projects involve verbal articulation of what is seen, or otherwise perceived, and are intended to actively integrate and change the mental archive of the visitor (i.e., previous experience, psychological condition, and social and cultural background) within the frameworks of a happening. Dragan's Mobile E-Book Flaneur references digital reading as a nomadic practice: the reader strolls through the database of the linguistic corpus streaming from the internet at one time, and on another occasion the reader is an urban nomad, where the city is layered with databases containing cultural artifacts. The city can thus be \"read\" at the level of the spatialization of the text on the map, at the level of memory and oblivion of past cultural events, and finally at the level of reading the literary texts displayed on mobile screen devices.", "Language": "en", "Citations": "1"},
{"Title": "Problems in unstructured peer-to-peer systems Problematika nestrukturiranih sistemov enak z enakim", "Authors": ["Ciglaric M.", "Vidmar T."], "Keywords": ["Content network", "Distributed search", "Peer-to-peer system"], "Date": "2005", "Abstract": "In recent years, peer-to-peer systems have become broadly accepted due to their simplicity, variability, self-organizing and load sharing ability, fault tolerance and high availability achieved through massive replication. Users emphasize their ability of joining and exploiting a large number of system resources, especially communication paths, storage and processing power, which can successfully replace usually inaccessible powerful and expensive supercomputers. Distributed contents are those that are not located on specialized servers only, but also on end computers. The main research issues in systems with distributed contents are: enabling content accessibility with no regard to its location; management and control over contents during their existence in the system, and effective collaboration among the system users. The toughest problems today are efficient use of system resources and acceptable security level since system users are unreliable, untrustworthy and highly autonomous. The autonomy issue interferes with the usage of known efficient mechanisms. In the paper we present general properties of peer-to-peer systems and specifically those related to search in unstructured systems with high local autonomy.", "Language": "en", "Citations": "0"},
{"Title": "Data quality: A prerequisite for successful data warehouse implementation", "Authors": ["Mahnic V.", "Rozanc I."], "Keywords": ["Data quality assessment", "Data warehouse", "Information quality", "Total quality management"], "Date": "2001", "Abstract": "Building a data warehouse for a large decentralized university such as the University of Ljubljana is an attractive challenge, but also a risky and demanding task. Experience has shown that projects attempting to integrate data are especially vulnerable to data quality issues. Therefore, before embarking on a data warehouse initiative a thorough quality assessment of the source data is necessary. We describe how the assessment criteria based on the Total Quality data Management Methodology were adapted to our specific needs and used to determine the quality of student records data at two member institutions, viz. the Faculty of Computer and Information Science, and the Faculty of Electrical Engineering. The most important results of the assessment are described and proposals are given for further activities. The assessment has shown that the student records data at the Faculty of Computer and Information Science and Faculty of Electrical Engineering are good enough to be used as source for the global warehouse at the university level after some data cleansing takes place. Additionally, special attention must be devoted to the integration of such data that are replicated at many individual departments (viz. employees, subjects taught, and students). Therefore, we propose that a unique coding scheme for all employees, students, and subjects taught be defined in the first step of the data warehouse design, and an ongoing data quality management process is established clearly defining the roles and responsibilities of all personnel involved.", "Language": "en", "Citations": "3"},
{"Title": "Slovene smart card and IP based health-care information system infrastructure", "Authors": ["Trcek D.", "Novak R.", "Kandus G.", "Suselj M."], "Keywords": ["EDI", "Health-care information system infrastructure", "Smart cards", "Web technology"], "Date": "2001", "Abstract": "Slovenia initiated a nation-wide project to introduce smart cards in the health sector in 1995 and its full-scale deployment started in September 2000. Although the basic aim of the project was to support insurance related procedures, the system was designed in a flexible and open manner to present an infrastructure for the whole health sector. The functionality of the current system is described in this paper along with lessons learned so far. The upgrade of the system is outlined, with emphasis on technical details, the objective being to provide a real-time EDI based environment for a general set of applications in the medical sector, supported by the flexibility and security of modern smart card technologies. Integration with similar systems in other EU countries is discussed. Copyright \u00a9 2001 Elsevier Science Ireland Ltd.", "Language": "en", "Citations": "12"},
{"Title": "AME-WPC: Advanced model for efficient workload prediction in the cloud", "Authors": ["Cetinski K.", "Juric M.B."], "Keywords": ["Cloud computing", "IaaS", "Infrastructure management", "Random forest", "Resource auto-scaling", "Workload prediction"], "Date": "2015", "Abstract": "Abstract Workload estimation and prediction has become a very relevant research area in the field of cloud computing. The reason lies in its many benefits, which include QoS (Quality of Service) satisfaction, automatic resource scaling, and job/task scheduling. It is very difficult to accurately predict the workload of cloud applications if they are varying drastically. To address this issue, existing solutions use either statistical methods, which effectively detect repeating patterns but provide poor accuracy for long-term predictions, or learning methods, which develop a complex prediction model but are mostly unable to detect unusual patterns. Some solutions use a combination of both methods. However, none of them address the issue of gathering system-specific information in order to improve prediction accuracy. We propose an Advanced Model for Efficient Workload Prediction in the Cloud (AME-WPC), which combines statistical and learning methods, improves accuracy of workload prediction for cloud computing applications and can be dynamically adapted to a particular system. The learning methods use an extended training dataset, which we define through the analysis of the system factors that have a strong influence on the application workload. We address the workload prediction problem with classification as well as regression and test our solution with the machine-learning method Random Forest on both - basic and extended - training data. To evaluate our proposed model, we compare empirical tests with the machine-learning method kNN (k-Nearest Neighbors). Experimental results demonstrate that combining statistical and learning methods makes sense and can significantly improve prediction accuracy of workload over time.", "Language": "en", "Citations": "18"},
{"Title": "Closed-world tracking of multiple interacting targets for indoor-sports applications", "Authors": ["Kristan M.", "Pers J.", "Perse M.", "Kovacic S."], "Keywords": ["Closed world", "Color histograms", "Dynamic models", "Local smoothing", "Multiple targets", "Particle filter", "Sport", "Tracking", "Voronoi partitioning"], "Date": "2009", "Abstract": "In this paper we present an efficient algorithm for tracking multiple players during indoor sports matches. A sports match can be considered as a semi-controlled environment for which a set of closed-world assumptions regarding the visual as well as the dynamical properties of the players and the court can be derived. These assumptions are then used in the context of particle filtering to arrive at a computationally fast, closed-world, multi-player tracker. The proposed tracker is based on multiple, single-player trackers, which are combined using a closed-world assumption about the interactions among players. With regard to the visual properties, the robustness of the tracker is achieved by deriving a novel sports-domain-specific likelihood function and employing a novel background-elimination scheme. The restrictions on the player's dynamics are enforced by employing a novel form of local smoothing. This smoothing renders the tracking more robust and reduces the computational complexity of the tracker. We evaluated the proposed closed-world, multi-player tracker on a challenging data set. In comparison with several similar trackers that did not utilize all of the closed-world assumptions, the proposed tracker produced better estimates of position and prediction as well as reducing the number of failures. \u00a9 2008 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "47"},
{"Title": "The visual object tracking VOT2016 challenge results", "Authors": ["Kristan M.", "Leonardis A.", "Matas J.", "Felsberg M.", "Pflugfelder R.", "Cehovin L.", "Vojir T.", "Hager G.", "Lukezic A.", "Fernandez G.", "Gupta A.", "Petrosino A.", "Memarmoghadam A.", "Martin A.G.", "Montero A.S.", "Vedaldi A.", "Robinson A.", "Ma A.J.", "Varfolomieiev A.", "Alatan A.", "Erdem A.", "Ghanem B.", "Liu B.", "Han B.", "Martinez B.", "Chang C.-M.", "Xu C.", "Sun C.", "Kim D.", "Chen D.", "Du D.", "Mishra D.", "Yeung D.-Y.", "Gundogdu E.", "Erdem E.", "Khan F.", "Porikli F.", "Zhao F.", "Bunyak F.", "Battistone F.", "Zhu G.", "Roffo G.", "Sai Subrahmanyam G.R.K.", "Bastos G.", "Seetharaman G.", "Medeiros H.", "Li H.", "Qi H.", "Bischof H.", "Possegger H.", "Lu H.", "Lee H.", "Nam H.", "Chang H.J.", "Drummond I.", "Valmadre J.", "Jeong J.-C.", "Cho J.-I.", "Lee J.-Y.", "Zhu J.", "Feng J.", "Gao J.", "Choi J.Y.", "Xiao J.", "Kim J.-W.", "Jeong J.", "Henriques J.F.", "Lang J.", "Choi J.", "Martinez J.M.", "Xing J.", "Gao J.", "Palaniappan K.", "Lebeda K.", "Gao K.", "Mikolajczyk K.", "Qin L.", "Wang L.", "Wen L.", "Bertinetto L.", "Rapuru M.K.", "Poostchi M.", "Maresca M.", "Danelljan M.", "Mueller M.", "Zhang M.", "Arens M.", "Valstar M.", "Tang M.", "Baek M.", "Khan M.H.", "Wang N.", "Fan N.", "Al-Shakarji N.", "Miksik O.", "Akin O.", "Moallem P.", "Senna P.", "Torr P.H.S.", "Yuen P.C.", "Huang Q.", "Nieto R.M.", "Pelapur R.", "Bowden R.", "Laganiere R.", "Stolkin R.", "Walsh R.", "Krah S.B.", "Li S.", "Zhang S.", "Yao S.", "Hadfield S.", "Melzi S.", "Lyu S.", "Li S.", "Becker S.", "Golodetz S.", "Kakanuru S.", "Choi S.", "Hu T.", "Mauthner T.", "Zhang T.", "Pridmore T.", "Santopietro V.", "Hu W.", "Li W.", "Hubner W.", "Lan X.", "Wang X.", "Li X.", "Li Y.", "Demiris Y.", "Wang Y.", "Qi Y.", "Yuan Z.", "Cai Z.", "Xu Z.", "He Z.", "Chi Z."], "Keywords": ["Performance evaluation", "Short-term single-object trackers", "VOT"], "Date": "2016", "Abstract": "The Visual Object Tracking challenge VOT2016 aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 70 trackers are presented, with a large number of trackers being published at major computer vision conferences and journals in the recent years. The number of tested state-of-the-art trackers makes the VOT 2016 the largest and most challenging benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the Appendix. The VOT2016 goes beyond its predecessors by (i) introducing a new semi-automatic ground truth bounding box annotation methodology and (ii) extending the evaluation system with the no-reset experiment. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http: //votchallenge.net).", "Language": "en", "Citations": "123"},
{"Title": "Leksikalne novosti v besedilih ra\u010dunalni\u0161ko posredovane komunikacije", "Authors": ["Gantar P.", "Holdt S.A.", "Pollak S."], "Keywords": ["Collocations", "Corpus analysis", "Dictionary database", "Lexicology", "Nonstandard Slovene"], "Date": "2018", "Abstract": "This article presents a lexical analysis of data extracted for a specific collocation window from the Janes and Kres corpora of Slovene. The results of the analysis are shown to be of interest in the monitoring of lexical innovations in Slovene vocabulary and for updating dictionaries. The extracted data were compared with existing dictionaries of Slovene in terms of new vocabulary, typical collocations, and set phrases, as well as semantic shifts. The linguistic analysis of the extracted collocations shows, among other things, that a contrastive comparison can be used to idenitfy the main characteristics and trends regarding lexical innovations, as well as to highlight their problematic aspects-e.g., when lexical innovations-particularly when under the influence of foreign language elements-also introduce changes in spelling and syntactic roles.", "Language": "en", "Citations": "0"},
{"Title": "Rule-based clustering for gene promoter structure discovery", "Authors": ["Curk T.", "Petrovic U.", "Shaulsky G.", "Zupan B."], "Keywords": ["Gene expression analysis", "Machine learning", "Promoter analysis", "Rule-based clustering"], "Date": "2009", "Abstract": "Background: The genetic cellular response to internal and external changes is determined by the sequence and structure of gene-regulatory promoter regions. Objectives: Using data on gene-regulatory elements (i.e., either putative or known transcription factor binding sites) and data on gene expression profiles we can discover structural elements in promoter regions and infer the underlying programs of gene regulation. Such hypotheses obtained in silico can greatly assist us in experiment planning. The principal obstacle for such approaches is the combinatorial explosion in different combinations of promoter elements to be examined. Methods; Stemming from several state-of the- art machine learning approaches we here propose a heuristic, rule-based clustering method that uses gene expression similarity to guide the search for informative structures in promoters, thus exploring only the most promising parts of the vast and expressively rich rule-space. Results: We present the utility of the method in the analysis of gene expression data on budding yeast S. cerevisiae where cells were induced to proliferate peroxisomes. Conclusions: We demonstrate that the proposed approach is able to infer informative relations uncovering relatively complex structures in gene promoter regions that regulate gene expression. \u00a9 2009 Schattauer.", "Language": "en", "Citations": "1"},
{"Title": "ICLIP - transcriptome-wide mapping of protein-RNA interactions with individual nucleotide resolution", "Authors": ["Konig J.", "Zarnack K.", "Rot G.", "Curk T.", "Kayikci M.", "Zupan B.", "Turner D.J.", "Luscombe N.M.", "Ule J."], "Keywords": [], "Date": "2011", "Abstract": "The unique composition and spatial arrangement of RNA-binding proteins (RBPs) on a transcript guide the diverse aspects of post-transcriptional regulation", "Language": "en", "Citations": "125"},
{"Title": "Predicting time series using neural networks with wavelet-based denoising layers", "Authors": ["Lotric U.", "Dobnikar A."], "Keywords": ["Denoising", "Feedforward and recurrent neural networks", "Gradient-based threshold adaptation", "Time series prediction", "Wavelet multiresolution analysis"], "Date": "2005", "Abstract": "To avoid the need to pre-process noisy data, two special denoising layers based on wavelet multiresolution analysis have been integrated into layered neural networks. A gradient-based learning algorithm has been developed that uses the same cost function to set both the neural network weights and the free parameters of the denoising layers. The denoising layers, when integrated into feedforward and recurrent neural networks, were validated on three time series prediction problems: the logistic map, a rubber hardness time series, and annual average sunspot numbers. Use of the denoising layers improved the prediction accuracy in both cases. \u00a9 Springer-Verlag London Limited 2004.", "Language": "en", "Citations": "22"},
{"Title": "Recognizing 2-tone images in grey-level parametric eigenspaces", "Authors": ["Maver J.", "Leonardis A."], "Keywords": ["2-tone images", "Appearance-based object recognition", "Binary images", "Parametric eigenspaces"], "Date": "2002", "Abstract": "Standard approaches to recognition using parametric eigenspaces have been designed under the assumption that the training images and the input images to be recognized are of the same type. In this paper we propose a novel approach which demonstrates that having an eigenspace encompassing a set of grey-level images, it is possible to recover the eigenspace coefficients and consequently recognize the input images even in the cases when the input images are 2-tone images of the objects in the training set. We present the recognition results of 2-tone images using the eigenspace built from a set of grey-level images. \u00a9 2002 Elsevier Science B.V. All rights reserved.", "Language": "en", "Citations": "4"},
{"Title": "Enterprise modelling with UML", "Authors": ["Zrnec A.", "Bajec M.", "Krisper M."], "Keywords": ["Business modelling", "Notation", "Standard", "Strategic planning", "Unified modelling language (UML)"], "Date": "2001", "Abstract": "Making effective project selection decisions in an enterprise requires a clear idea of where the enterprise is in the current state, what its vision for the future is, and how to make a transition to its desired future state possible. A strategic plan is a document that encompasses this information and is produced as an output of corporate strategic planning. In this paper we examine business modelling as an integral part of strategic planning, where models of a current and future enterprise are developed and refined. The question that we address in this regard is, whether the unified modelling language (UML) notation, the today's defacto standard in engineering modelling, can serve as an appropriate language for enterprise modelling, in particular for corporate strategic planning. In the paper we discuss types of models that are usually used in strategic planning to describe the enterprise from different perspectives and suggest some extensions to the UML notation to make it more suitable for this purpo se.", "Language": "en", "Citations": "0"},
{"Title": "TraX: The visual Tracking eXchange protocol and library", "Authors": ["Cehovin L."], "Keywords": ["Algorithm analysis", "Communication protocol", "Computer vision", "Performance evaluation", "Software library", "Visual tracking"], "Date": "2017", "Abstract": "In this paper we address the problem of developing on-line visual tracking algorithms. We present a specialized communication protocol that serves as a bridge between a tracker implementation and utilizing application. It decouples development of algorithms and application, encouraging re-usability. The primary use case is algorithm evaluation where the protocol facilitates more complex evaluation scenarios that are used nowadays thus pushing forward the field of visual tracking. We present a reference implementation of the protocol that makes it easy to use in several popular programming languages and discuss where the protocol is already used and some usage scenarios that we envision for the future.", "Language": "en", "Citations": "3"},
{"Title": "Empirical analysis of reliability estimates for individual regression predictions", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": ["Prediction accuracy", "Prediction error", "Regression", "Reliability", "Reliability estimate", "Sensitivity analysis"], "Date": "2008", "Abstract": "In machine learning, the reliability estimates for individual predictions provide more information about individual prediction error than the average accuracy of predictive model (e.g. relative mean squared error). Such reliability estimates may represent a decisive information in the risk-sensitive applications of machine learning (e.g. medicine, engineering, business), where they enable the users to distinguish between better and worse predictions. In this paper, we compare the sensitivity-based reliability estimates, developed in our previous work, with four other approaches, proposed or inspired by the ideas from the related work. The results, obtained using 5 regression models, indicate the potentials for the usage of the sensitivity-based and the local modeling approach, especially with the regression trees. \u00a9 2008 Springer-Verlag Berlin Heidelberg.", "Language": "en", "Citations": "1"},
{"Title": "Is my new tracker really better than yours?", "Authors": ["Cehovin L.", "Kristan M.", "Leonardis A."], "Keywords": [], "Date": "2014", "Abstract": "The problem of visual tracking evaluation is sporting an abundance of performance measures, which are used by various authors, and largely suffers from lack of consensus about which measures should be preferred. This is hampering the cross-paper tracker comparison and faster advancement of the field. In this paper we provide an overview of the popular measures and performance visualizations and their critical theoretical and experimental analysis. We show that several measures are equivalent from the point of information they provide for tracker comparison and, crucially, that some are more brittle than the others. Based on our analysis we narrow down the set of potential measures to only two complementary ones that can be intuitively interpreted and visualized, thus pushing towards homogenization of the tracker evaluation methodology. \u00a9 2014 IEEE.", "Language": "en", "Citations": "47"},
{"Title": "The sixth visual object tracking VOT2018 challenge results", "Authors": ["Kristan M.", "Leonardis A.", "Matas J.", "Felsberg M.", "Pflugfelder R.", "Zajc L.C.", "Vojir T.", "Bhat G.", "Lukezic A.", "Eldesokey A.", "Fernandez G.", "Garcia-Martin A.", "Iglesias-Arias A.", "Alatan A.A.", "Gonzalez-Garcia A.", "Petrosino A.", "Memarmoghadam A.", "Vedaldi A.", "Muhic A.", "He A.", "Smeulders A.", "Perera A.G.", "Li B.", "Chen B.", "Kim C.", "Xu C.", "Xiong C.", "Tian C.", "Luo C.", "Sun C.", "Hao C.", "Kim D.", "Mishra D.", "Chen D.", "Wang D.", "Wee D.", "Gavves E.", "Gundogdu E.", "Velasco-Salido E.", "Khan F.S.", "Yang F.", "Zhao F.", "Li F.", "Battistone F.", "De Ath G.", "Subrahmanyam G.R.K.S.", "Bastos G.", "Ling H.", "Galoogahi H.K.", "Lee H.", "Li H.", "Zhao H.", "Fan H.", "Zhang H.", "Possegger H.", "Li H.", "Lu H.", "Zhi H.", "Li H.", "Lee H.", "Chang H.J.", "Drummond I.", "Valmadre J.", "Martin J.S.", "Chahl J.", "Choi J.Y.", "Li J.", "Wang J.", "Qi J.", "Sung J.", "Johnander J.", "Henriques J.", "Choi J.", "van de Weijer J.", "Herranz J.R.", "Martinez J.M.", "Kittler J.", "Zhuang J.", "Gao J.", "Grm K.", "Zhang L.", "Wang L.", "Yang L.", "Rout L.", "Si L.", "Bertinetto L.", "Chu L.", "Che M.", "Maresca M.E.", "Danelljan M.", "Yang M.-H.", "Abdelpakey M.", "Shehata M.", "Kang M.", "Lee N.", "Wang N.", "Miksik O.", "Moallem P.", "Vicente-Monivar P.", "Senna P.", "Li P.", "Torr P.", "Raju P.M.", "Ruihe Q.", "Wang Q.", "Zhou Q.", "Guo Q.", "Martin-Nieto R.", "Gorthi R.K.", "Tao R.", "Bowden R.", "Everson R.", "Wang R.", "Yun S.", "Choi S.", "Vivas S.", "Bai S.", "Huang S.", "Wu S.", "Hadfield S.", "Wang S.", "Golodetz S.", "Ming T.", "Xu T.", "Zhang T.", "Fischer T.", "Santopietro V.", "Struc V.", "Wei W.", "Zuo W.", "Feng W.", "Wu W.", "Zou W.", "Hu W.", "Zhou W.", "Zeng W.", "Zhang X.", "Wu X.", "Wu X.-J.", "Tian X.", "Li Y.", "Lu Y.", "Law Y.W.", "Wu Y.", "Demiris Y.", "Yang Y.", "Jiao Y.", "Li Y.", "Zhang Y.", "Sun Y.", "Zhang Z.", "Zhu Z.", "Feng Z.-H.", "Wang Z.", "He Z."], "Keywords": [], "Date": "2019", "Abstract": "The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a \u201creal-time\u201d experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking subchallenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new long-term tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website (http://votchallenge.net).", "Language": "en", "Citations": "3"},
{"Title": "Automatic segmentation of whole-body bone scintigrams as a preprocessing step for computer assisted diagnostics", "Authors": ["Sajn L.", "Kukar M.", "Kononenko I.", "Milcinski M."], "Keywords": [], "Date": "2005", "Abstract": "Bone sciritigraphy or whole-body bone scan is one of the most common diagnostic procedures in nuclear medicine used in the last 25 years. Pathological conditions, technically poor quality images and artifacts necessitate that algorithms use sufficient background knowledge of anatomy and spatial relations of bones in order to work satisfactorily. We present a robust knowledge based methodology for detecting reference points of the main skeletal regions that simultaneously processes anterior and posterior whole-body bone scintigrams. Expert knowledge is represented as a set of parameterized rules which are used to support standard image processing algorithms. Our study includes 467 consecutive, non-selected scintigrams, which is to our knowledge the largest number of images ever used in such studies. Automatic analysis of whole-body bone scans using our knowledge based segmentation algorithm gives more accurate and reliable results than previous studies. Obtained reference points are used for automatic segmentation of the skeleton, which is used for automatic (machine learning) or manual (expert physicians) diagnostics. Preliminary experiments show that an expert system based on machine learning closely mimics the results of expert physicians. \u00a9 Springer-Vorlag Berlin Heidelberg 2005.", "Language": "en", "Citations": "2"},
{"Title": "Jordan forms for mutually annihilating nilpotent pairs", "Authors": ["Oblak P."], "Keywords": ["Commuting matrices", "Jordan canonical form", "Nilpotent matrices"], "Date": "2008", "Abstract": "In this paper we completely characterize all possible pairs of Jordan canonical forms for mutually annihilating nilpotent pairs, i.e. pairs (A, B) of nilpotent matrices such that AB = BA = 0. \u00a9 2007 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "3"},
{"Title": "V-RBNN based small drone detection in augmented datasets for 3D LADAR system", "Authors": ["Kim B.H.", "Khan D.", "Bohak C.", "Choi W.", "Lee H.J.", "Kim M.Y."], "Keywords": ["3D LADAR", "3D sensor", "Clustering", "Drone detection", "Fusion data", "LiDAR"], "Date": "2018", "Abstract": "A common countermeasure to detect threatening drones is the electro-optical infrared (EO/IR) system. However, its performance is drastically reduced in conditions of complex background, saturation and light reflection. 3D laser sensor LiDAR is used to overcome the problems of 2D sensors like EO/IR, but it is not enough to detect small drones at a very long distance because of low laser energy and resolution. To solve this problem, A 3D LADAR sensor is under development. In this work, we study the detection methodology adequate to the LADAR sensor which can detect small drones at up to 2 km. First, a data augmentation method is proposed to generate a virtual target considering the laser beam and scanning characteristics, and to augment it with the actual LADAR sensor data for various kinds of tests before full hardware system developed. Second, a detection algorithm is proposed to detect drones using voxel-based background subtraction and variable radially bounded nearest neighbor (V-RBNN) method. The results show that 0.2 m L2 distance and 60% expected average overlap (EAO) indexes are satisfied for the required specification to detect 0.3 m size of small drones.", "Language": "en", "Citations": "4"},
{"Title": "IPv4 address sharing mechanism classification and tradeoff analysis", "Authors": ["Skoberne N.", "Maennel O.", "Phillips I.", "Bush R.", "Zorz J.", "Ciglaric M."], "Keywords": ["Address family translation", "carrier grade NAT (CGN)", "IPv4 address sharing", "IPv6 transition", "network address translation (NAT)"], "Date": "2014", "Abstract": "The growth of the Internet has made IPv4 addresses a scarce resource. Due to slow IPv6 deployment, IANA-level IPv4 address exhaustion was reached before the world could transition to an IPv6-only Internet. The continuing need for IPv4 reachability will only be supported by IPv4 address sharing. This paper reviews ISP-level address sharing mechanisms, which allow Internet service providers to connect multiple customers who share a single IPv4 address. Some mechanisms come with severe and unpredicted consequences, and all of them come with tradeoffs. We propose a novel classification, which we apply to existing mechanisms such as NAT444 and DS-Lite and proposals such as 4rd, MAP, etc. Our tradeoff analysis reveals insights into many problems including: abuse attribution, performance degradation, address and port usage efficiency, direct intercustomer communication, and availability. \u00a9 2013 IEEE.", "Language": "en", "Citations": "14"},
{"Title": "An overview of advances in reliability estimation of individual predictions in machine learning", "Authors": ["Bosnic Z.", "Kononenko I."], "Keywords": ["Data perturbation", "Prediction accuracy", "Predictions", "Reliability", "Supervised learning", "Unlabeled examples"], "Date": "2009", "Abstract": "In Machine Learning, estimation of the predictive accuracy for a given model is most commonly approached by analyzing the average accuracy of the model. In general, the predictive models do not provide accuracy estimates for their individual predictions. The reliability estimates of individual predictions require the analysis of various model and instance properties. In the paper we make an overview of the approaches for estimation of individual prediction reliability. We start by summarizing three research fields, that provided ideas and motivation for our work: (a) approaches to perturbing learning data, (b) the usage of unlabeled data in supervised learning, and (c) the sensitivity analysis. The main part of the paper presents two classes of reliability estimation approaches and summarizes the relevant terminology, which is often used in this and related research fields. \u00a9 2009 IOS Press. All rights reserved.", "Language": "en", "Citations": "25"},
{"Title": "Perfect discrete morse functions on connected sums", "Authors": ["Varli H.", "Pamuk M.", "Kosta N.M."], "Keywords": ["Connected sum", "Discrete vector field", "Perfect discrete Morse function"], "Date": "2018", "Abstract": "We study perfect discrete Morse functions on closed, connected, oriented n-dimensional manifolds.We show how to compose such functions on connected sums of manifolds of arbitrary dimensions and how to decompose them on connected sums of closed oriented surfaces.", "Language": "en", "Citations": "1"},
{"Title": "Argument based machine learning in a medical domain", "Authors": ["Zabkar J.", "Mozina M.", "Videcnik J.", "Bratko I."], "Keywords": ["Argument Based Machine Learning", "Bacterial infections", "Geriatric population", "Rule learning"], "Date": "2006", "Abstract": "Argument Based Machine Learning (ABML) is a new approach to machine learning in which the learning examples can be accompanied by arguments. The arguments for specific examples are a special form of expert's knowledge which the expert uses to substantiate the class value for the chosen example. Mo\u017eina et al. developed the ABCN2 algorithm-an extension of the well known rule learning algorithm CN2-that can use argumented examples in the learning process. In this work we present an application of ABCN2 in the medical domain which deals with severe bacterial infections in geriatric population. The elderly population, people over 65 years of age, is rapidly growing as well as the costs of treating this population. In our study, we compare ABCN2 to CN2 and show that using arguments we improve the characteristics of the model. We also report the results that C4.5, Na\u00efve Bayes and Logistic Regression achieve in this domain.", "Language": "en", "Citations": "3"},
{"Title": "Impact of test-driven development on productivity, code and tests: A controlled experiment", "Authors": ["Pancur M.", "Ciglaric M."], "Keywords": ["Controlled experiment", "Empirical software engineering", "Iterative test-last development", "Test-driven development"], "Date": "2011", "Abstract": "Context: Test-driven development is an approach to software development, where automated tests are written before production code in highly iterative cycles. Test-driven development attracts attention as well as followers in professional environment; however empirical evidence of its superiority regarding its effect on productivity, code and tests compared to test-last development is still fairly limited. Moreover, it is not clear if the supposed benefits come from writing tests before code or maybe from high iterativity/short development cycles. Objective: This paper describes a family of controlled experiments comparing test-driven development to micro iterative test-last development with emphasis on productivity, code properties (external quality and complexity) and tests (code coverage and fault-finding capabilities). Method: Subjects were randomly assigned to test-driven and test-last groups. Controlled experiments were conducted for two years, in an academic environment and in different developer contexts (pair programming and individual programming contexts). Number of successfully implemented stories, percentage of successful acceptance tests, McCabe's code complexity, code coverage and mutation score indicator were measured. Results: Experimental results and their selective meta-analysis show no statistically significant differences between test-driven development and iterative test-last development regarding productivity (\u03c7", "Language": "en", "Citations": "26"},
{"Title": "Agents and people activities in Web-services based business processes", "Authors": ["Sasa A.", "Juric M.B.", "Krisper M."], "Keywords": ["Multiagent systems", "Ontology", "Service-oriented architecture"], "Date": "2007", "Abstract": "Business Processes Execution Language (BPEL) is primarily designed to support automated business processes based on Web services. The lack of support for human-performed activities led to specification of a BPEL extension called BPEL4People which tries to answer this problem. In this paper we propose an implementation of Web services with regard to this extension using the agent technology. We show not only how agents can implement such activities but also how they can improve their execution with acting on behalf of the users they are representing. Moreover the fact that agents' behaviour is based on their knowledge this is a promising approach to knowledge management in organisations. \u00a9 2007 IEEE.", "Language": "en", "Citations": "2"},
{"Title": "Feature mining and predictive model construction from severe trauma patient's data", "Authors": ["Demsar J.", "Zupan B.", "Aoki N.", "Wall M.J.", "Granchi T.H.", "Robert Beck J."], "Keywords": ["Damage control", "Data mining", "Feature mining", "Machine learning", "Medical prognostic models", "Severe traumatic injury"], "Date": "2001", "Abstract": "In management of severe trauma patients, trauma surgeons need to decide which patients are eligible for damage control. Such decision may be supported by utilizing models that predict the patient's outcome. The study described in this paper investigates the possibility to construct patient outcome prediction models from retrospective patient's data at the end of initial damage control surgery by using feature mining and machine learning techniques. As the data used comprises rather excessive number of features, special attention was paid to the problem of selecting only the most relevant features. We show that a small subset of features may carry enough information to construct reasonably accurate prognostic models. Furthermore, the techniques used in our study identified two factors, namely the pH value when admitted to ICU and the worst partial active thromboplastin time, to be of highest importance for prediction. This finding is pathophysiologically reasonable and represents two of three major problems with severe trauma patients, metabolic acidosis, hypothermia, and coagulopathy. \u00a9 2001 Elsevier Science Ireland Ltd.", "Language": "en", "Citations": "33"},
{"Title": "Evaluation of prediction reliability in regression using the transduction principle", "Authors": ["Bosnic Z.", "Kononenko I.", "Robnik-Sikonja M.", "Kukar M."], "Keywords": ["Prediction", "Regression", "Reliability", "Transduction"], "Date": "2003", "Abstract": "In machine learning community there are many efforts to improve overall reliability of predictors measured as an error on the testing set. But in contrast, very little research has been done concerning prediction reliability of a single answer. This article describes an algorithm that can be used for evaluation of prediction reliability In regression. The basic idea of the algorithm is based on construction of transductive predictors. Using them, the algorithm makes inference from the differences between initial and transductive predictions to the error on a single new case. The implementation of the algorithm with regression trees managed to significantly reduce the relative mean squared error on the majority of the tested domains.", "Language": "en", "Citations": "17"},
{"Title": "A formal apparatus for modeling trust in computing environments", "Authors": ["Trcek D."], "Keywords": ["Human behavior", "Modeling and simulation", "Security", "Services oriented architectures", "Trust management"], "Date": "2009", "Abstract": "Recent research in computer systems security has evolved into trust issues, which are now becoming an important topic. The majority of approaches for trust modeling addressed trust by actually focusing on security, and some of them addressed also trust as such. This paper presents a formal apparatus that concentrates on trust as such. It is flexible enough to accommodate the driving factors behind trust and consequently different trust-focused methodologies and technologies. The basic goal of the work presented in this paper is the definition of qualitative trust modeling methodology for trust management in contemporary computing environments that efficiently complements existing quantitative methodologies. Further, an open conceptual model for trust management is presented that accommodates various qualitative and quantitative trust management methodologies. This model has also been implemented in the web services environment, and this is discussed in this paper as well. \u00a9 2008 Elsevier Ltd. All rights reserved.", "Language": "en", "Citations": "32"},
{"Title": "Social media comparison and analysis: The best data source for research?", "Authors": ["Blagus N.", "Zitnik S."], "Keywords": ["application programming interface (API)", "Facebook", "Google+", "social data", "social media", "Tumblr", "Twitter", "YouTube"], "Date": "2018", "Abstract": "In the past decade, social media has become an important part of our everyday life. The employment of different social media changes the way we communicate, collaborate, gather information and consequently perceive the world around us. Thus, researchers from different fields exploit the social media to provide deeper insight into human behaviour. Each social media possesses its own privacy politics and access to publicly available data. In this paper, we present a generic framework along with the tools to analyse different social media. The analysis shows basic usage statistics, reach and engagement differences, language, sentiment and gender identification of each social network data. We compare data from Twitter, Facebook, Tumblr, Google+ and YouTube. The results reveal specifics of each social media, which to some extent also depend on the data available and the selected seed keywords. We uncover that popularity of selected topics in social media is proportional to the number of hits on Google, celebrities and politicians are the most talked topics and that behaviour of users across social media is different. For example, Twitter users prefer to post more, while Facebook and Youtube users prefer to comment. The majority of all social media posts are in English, larger number of them are negative and often written by male users. The results of the proposed framework should serve as a tool to identify the appropriate source of data for the representative analysis of social media.", "Language": "en", "Citations": "1"},
{"Title": "A general method for visualizing and explaining black-box regression models", "Authors": ["Strumbelj E.", "Kononenko I."], "Keywords": ["Neural networks", "prediction", "SVM", "transparency"], "Date": "2011", "Abstract": "We propose a method for explaining regression models and their predictions for individual instances. The method successfully reveals how individual features influence the model and can be used with any type of regression model in a uniform way. We used different types of models and data sets to demonstrate that the method is a useful tool for explaining, comparing, and identifying errors in regression models. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "7"},
{"Title": "Line graph operation and small worlds", "Authors": ["Govorcin J.", "Knor M.", "Skrekovski R."], "Keywords": ["Diameter", "Interconnection networks", "Large network", "Line graph", "Small world"], "Date": "2013", "Abstract": "Complex networks, such as small world networks, are the focus of recent interest because of their potential as models for the interaction networks of complex systems. Most of the well-known models of small world networks are stochastic. The randomness makes it more difficult to gain a visual understanding of how networks are shaped, and how different vertices relate to each other. In this paper, we present and study a method for constructing deterministic small worlds using the line graph operator. This operator introduces cliques at every vertex of the original graph, which may imply larger clustering coefficients. On the other hand, this operator can increase the diameter at most by one and assure the small world property. \u00a9 2013 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "2"},
{"Title": "Making reliable diagnoses with machine learning: A case study", "Authors": ["Kukar M."], "Keywords": ["Coronary artery disease", "Machine learning", "Medical diagnosis", "Reliability estimation", "Stepwise diagnostic process"], "Date": "2001", "Abstract": "In the past decades Machine Learning tools have been successfully used in several medical diagnostic problems. While they often significantly outperform expert physicians (in terms of diagnostic accuracy, sensitivity, and specificity), they are mostly not used in practice. One reason for this is that it is difficult to obtain an unbiased estimation of diagnose\u2019s reliability. We propose a general framework for reliability estimation, based on transductive inference. We show that our reliability estimation is closely connected with a general notion of significance tests. We compare our approach with classical stepwise diagnostic process where reliability of diagnose is presented as its post-test probability. The presented approach is evaluated in practice in the problem of clinical diagnosis of coronary artery disease, where significant improvements over existing techniques are achieved.", "Language": "en", "Citations": "8"},
{"Title": "An analysis of cell nucleus images in mammary gland tissue using computer vision Analiza slik celi\u010dnih jeder \u017eleznega tkiva dojk z ra\u010dunalni\u0161kim vidom", "Authors": ["Mele K.", "Leonardis A."], "Keywords": ["Breast cancer", "Computer vision", "Cytology", "Graph search", "Greedy algorithm", "Relaxation", "Segmentation"], "Date": "2003", "Abstract": "In cell nucleus images of the mammary gland tissue both features of the nucleus and group shapes serve as cancer diagnosis criteria. Also, further treatment is dependent on the information about the cell nucleus spatial arrangement and the repeatability of the shapes formed by a malignant, potentially malignant, and normal duct. In this paper we present an automatic image analysis method that we developed for detecting the structures of nucleus clumps that represent boundaries of a duct. The outline of the method is shown in Fig. 1. Our inputs are images showing cell smears that were stained by the Feulgen method. The images were captured and digitalized with an image cytometer. The first step in our method is thresholding segmentation to isolate the cell nucleus. Bright parts inside a nucelus, which segmentation assigns to the background (Fig. 2), are simply joined with the nucleus. We also eliminate small segmented blobs. The centers of gravity of isolated nucleus cells are used to form a graph. In the next steps we discard the graph connections that do not represent the boundary of the nucleus clumps. In the second step, different criteria are used to evaluate the connectivity and geometric properties. The criteria that we use are the distance between the nucleus and the criterion of angles (Eqs. 1-4, Fig. 3). While this step is based on local features, the remaining links are in the third assessed with an iterative relaxation algorithm (Egs. 5-6). Important features of the relaxation are the type of connection (Fig. 5), border lines (Fig. 6), and neighborhood of the links. The iterative procedure propagates the influence of local features over the global area. In the fourth step we retain only those links that form long connected rounding paths along the boundary (Fig. 4). These paths represent the boundaries of the nucleus clumps. Similar pairs of paths are evaluated by Eqs. 7 or 8. Only the best paths between them are preserved. We tested our algorithm on images of mammary glands. The results indicate that the method can distinguish between healthy ducts that have regular shapes and the malignant ducts (ductal carcinoma in situ) that have irregular shapes (Figs. 7, 8, and 9). We believe that our image processing method can contribute towards a more reliable and repeatable information extraction about malignant ducts.", "Language": "en", "Citations": "0"},
{"Title": "Dependability of Container-Based Data-Centric Systems", "Authors": ["Kochovski P.", "Stankovski V."], "Keywords": ["Big Data", "Container-based systems", "Dependability", "Storage"], "Date": "2018", "Abstract": "Nowadays, many Cloud computing systems and applications have to be designed to be able to cope with the four Vs of the Big Data problem: volume, variety, veracity, and velocity. In this context, an important property in such systems is dependability. Dependability is a measure of the availability, reliability, tractability, response time, durability, security, and other aspects of the system. Dependability has been well established as an important property in services science and systems engineering; however, data-centric systems may have some specific requirements due to a variety of technologies that are still maturing and evolving. The goal of the present work is to establish the main aspects of dependability in data-centric systems and to analyze to what extent various Cloud computing technologies can be used to achieve this property. Particular attention is given to the use of container-based technologies that represent a novel, lightweight form of virtualization. The practical part of the chapter presents examples of highly dependable data-centric systems, such as ENTICE and SWITCH, suitable for the Internet of Things (IoT) and the Big Data era.", "Language": "en", "Citations": "1"},
{"Title": "Face recognition in different subspaces: A comparative study", "Authors": ["Batagelj B.", "Solina F."], "Keywords": [], "Date": "2006", "Abstract": "Face recognition is one of the most successful applications of image analysis and understanding and has gained much attention in recent years. Among many approaches to the problem of face recognition, appearance-based subspace analysis still gives the most promising results. In this paper we study the three most popular appearance-based face recognition projection methods (PCA, LDA and ICA). All methods are tested in equal working conditions regarding preprocessing and algorithm implementation on the FERET data set with its standard tests. We also compare the ICA method with its whitening preprocess and find out that there is no significant difference between them. When we compare different projection with different metrics we found out that the LDA+COS combination is the most promising for all tasks. The L1 metric gives the best results in combination with PCA and ICA1, and COS is superior to any other metric when used with LDA and ICA2. Our results are compared to other studies and some discrepancies are pointed out.", "Language": "en", "Citations": "5"},
{"Title": "Evaluating reliability of single classifications of neural networks", "Authors": ["Pevec D.", "Strumbelj E.", "Kononenko I."], "Keywords": ["Classification", "Prediction accuracy", "Prediction error", "Reliability estimation"], "Date": "2011", "Abstract": "Current machine learning algorithms perform well on many problem domains, but in risk-sensitive decision making, for example in medicine and finance, common evaluation methods that give overall assessments of models fail to gain trust among experts, as they do not provide any information about single predictions. We continue the previous work on approaches for evaluating the reliability of single classifications where we focus on methods that are model independent. These methods have been shown to be successful in their narrow fields of application, so we constructed a testing methodology to evaluate these methods in straightforward, general-use test cases. For the evaluation, we had to derive a statistical reference function, which enables comparison between the reliability estimators and the model's own predictions. We compare five different approaches and evaluate them on a simple neural network with several artificial and real-world domains. The results indicate that reliability estimators CNK and LCV can be used to improve the model's predictions. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "1"},
{"Title": "The slWaC corpus of the Slovene web", "Authors": ["Erjavec T.", "Ljubesic N.", "Logar N."], "Keywords": ["Corpus linguistics", "Language technologies", "Slovene language", "World wide web"], "Date": "2015", "Abstract": "The availability of large collections of text (language corpora) is crucial for empirically supported linguistic investigations of various languages; however, such corpora are complicated and expensive to collect. In recent years corpora made from texts on the World Wide Web have become an attractive alternative to traditional corpora, as they can be made automatically, contain varied text types of contemporary language, and are quite large. The paper describes version 2 of slWaC, a web corpus of Slovene containing 1.2 billion tokens. The corpus extends the first version of slWaC with new materials and updates the corpus compilation pipeline. The paper describes the process of corpus compilation with a focus on near-duplicate removal, presents the linguistic annotation, format and accessibility of the corpus via web concordancers. It then investigates the content of the corpus using the method of frequency profiling, by comparing its lemma and part-of-speech annotations with three corpora: The first version of slWaC, with Gigafida, the one billion word reference corpus of Slovene, and KRES, the hundred million word reference balanced corpus of Slovene.", "Language": "en", "Citations": "1"},
{"Title": "Why minimax works: An alternative explanation", "Authors": ["Lustrek M.", "Gams M.", "Bratko I."], "Keywords": [], "Date": "2005", "Abstract": "In game-playing programs relying on the minimax principle, deeper searches generally produce better evaluations. Theoretical analyses, however, suggest that in many cases minimaxing amplifies the noise introduced by the heuristic function used to evaluate the leaves of the game tree, leading to what is known as pathological behavior, where deeper searches produce worse evaluations. In most of the previous research, positions were evaluated as losses or wins. Dependence between the values of positions close to each other was identified as the property of realistic game trees that eliminates the pathology and explains why minimax is successful in practice. In this paper we present an alternative explanation that does not rely on value dependence. We show that if real numbers are used for position values, position values tend to be further apart at lower levels of the game tree, which leads to a larger proportion of more extreme positions, where error is less probable. Decreased probability of error in searches to greater depths is sufficient to eliminate the pathology and no additional properties of game trees are required.", "Language": "en", "Citations": "6"},
{"Title": "Part-level object recognition using superquadrics", "Authors": ["Krivic J.", "Solina F."], "Keywords": ["Object recognition", "Part-level object modelling", "Range images", "Superquadrics"], "Date": "2004", "Abstract": "This paper proposes a technique for object recognition using superquadric built models. Superquadrics, which are three-dimensional models suitable for part-level representation of objects, are reconstructed from range images using the recover-and-select paradigm. Using interpretation trees, the presence of an object from the model database can be hypothesized. These hypotheses are verified by projecting and re-fitting the object model to the range image of the scene which at the same time enables a better localization of the object in the scene. \u00a9 2004 Elsevier Inc. All rights reserved.", "Language": "en", "Citations": "26"},
{"Title": "Performance comparison of CORBA and RMI", "Authors": ["Juric M.B.", "Rozman I.", "Hericko M."], "Keywords": [], "Date": "2000", "Abstract": "Distributed object architectures and Java are important for building modern, scalable, web-enabled applications. This paper is focused on qualitative and quantitative comparison of two distributed object models for use with Java: CORBA and RMI. We compare both models in terms of features, ease of development and performance. We present performance results based on real world scenarios that include single client and multi-client configurations, different data types and data sizes. We evaluate multithreading strategies and analyze code in order to identify the most time-consuming methods. We compare the results and give hints and conclusions. We have found that because of its complexity CORBA is slightly slower than RMI in simple scenarios. On the other hand, CORBA handles multiple simultaneous clients and larger data amounts better and suffers from far lower performance degradation under heavy client load. The article presents a solid basis for making a decision about the underlying distributed object model.", "Language": "en", "Citations": "32"},
{"Title": "Minimum 2-terminal routing in 2-jump circulant graphs", "Authors": ["Robic B.", "Zerovnik J."], "Keywords": ["Analysis of algorithms", "Circulant graph", "Optimal routing", "Parallel processing"], "Date": "2000", "Abstract": "A 2-jump circulant is an undirected graph whose nodes are integers 0, 1, . . ., n - 1 and each node u is adjacent to four nodes u \u00b1 h", "Language": "en", "Citations": "5"},
{"Title": "Network-based statistical comparison of citation topology of bibliographic databases", "Authors": ["Subelj L.", "Fiala D.", "Bajec M."], "Keywords": [], "Date": "2014", "Abstract": "Modern bibliographic databases provide the basis for scientific research and its evaluation. While their content and structure differ substantially, there exist only informal notions on their reliability. Here we compare the topological consistency of citation networks extracted from six popular bibliographic databases including Web of Science, CiteSeer and arXiv.org. The networks are assessed through a rich set of local and global graph statistics. We first reveal statistically significant inconsistencies between some of the databases with respect to individual statistics. For example, the introduced field bow-tie decomposition of DBLP Computer Science Bibliography substantially differs from the rest due to the coverage of the database, while the citation information within arXiv.org is the most exhaustive. Finally, we compare the databases over multiple graph statistics using the critical difference diagram. The citation topology of DBLP Computer Science Bibliography is the least consistent with the rest, while, not surprisingly, Web of Science is significantly more reliable from the perspective of consistency. This work can serve either as a reference for scholars in bibliometrics and scientometrics or a scientific evaluation guideline for governments and research agencies.", "Language": "en", "Citations": "16"},
{"Title": "On the optimal top-down evaluation of semantic rules O optimalnosti izra\u010duna semanti\u010dnih pravil po strategiji od-zgoraj-navzdol", "Authors": ["Slivnik B."], "Keywords": ["Computation interleaving", "Left parse", "Optimality critetion", "Parsing"], "Date": "2018", "Abstract": "In syntax-directed translation based on context-free grammars, a top-down construction of a derivation tree, where subtrees are constructed from left to right, is often preferred to other strategies. It makes formulation of semantic rules simpler and provides a better foundation for error recovery. In the parsing theory, it is modelled by computation producing the left parse of the input string derivation. In this paper, a criterion for the optimal printout of the left parse is defined regarding the interleaving of parser actions and printout of individual productions comprising the resulting left parse. In some cases, parsing cannot be done at all without at least some semantic rule evaluation, while in most cases interleaving of parsing and semantic rule evaluation can significantly improve error recovery and the quality of error messages. Finally, the criterion is applied to some of the most important contemporary parsing algorithms. It is shown that not all algorithms support full interleaving.", "Language": "en", "Citations": "0"},
{"Title": "A GPU implementation of a structural-similarity-based aerial-image classification", "Authors": ["Cesnovar R.", "Risojevic V.", "Babic Z.", "Dobravec T.", "Bulic P."], "Keywords": ["Aerial-image classification", "CUDA", "GPU", "Image processing", "Local images descriptors", "Structural texture similarity"], "Date": "2013", "Abstract": "There is an increasing need for fast and efficient algorithms for the automatic analysis of remote-sensing images. In this paper we address the implementation of the semantic classification of aerial images with general-purpose graphics-processing units (GPGPUs). We propose the calculation of a local Gabor-based structural texture descriptor and a structural texture similarity metric combined with a nearest-neighbor classifier and image-to-class similarity on CUDA supported graphics-processing units. We first present the algorithm and then describe the GPU implementation and optimization with the CUDA programming model. We then evaluate the results of the algorithm on a dataset of aerial images and present the execution times for the sequential and parallel implementations of the whole algorithm as well as measurements only for the selected steps of the algorithm. We show that the algorithms for the image classification can be effectively implemented on the GPUs. In our case, the presented algorithm is around 39 times faster on the Tesla C1060 unit than on the Core i5 650 CPU, while keeping the same success rate of classification. \u00a9 2013 Springer Science+Business Media New York.", "Language": "en", "Citations": "6"},
{"Title": "Illumination insensitive eigenspaces", "Authors": ["Bischof H.", "Wildenauer H.", "Leonardis A."], "Keywords": [], "Date": "2001", "Abstract": "Variations in illumination can have a dramatic effect on the appearance of an object in an image. In this paper we propose how to deal with illumination variations in eigenspace methods. We demonstrate that the eigenimages obtained by a training set under a single illumination condition (ambient light) can be used for recognition of objects taken under different illumination conditions. The major idea is to incorporate a set of gradient based filter banks into the eigenspace recognition framework. This can be achieved since the eigenimage coefficients are invariant for linearly filtered images (input and eigenimages). To achieve further illumination insensitivity we devised a robust procedure for coefficient recovery. The proposed approach has been extensively evaluated on a set of 2160 images and the results were compared to other approaches.", "Language": "en", "Citations": "23"},
{"Title": "Multiple-cloud platform monitoring", "Authors": ["Vicic J.", "Brodnik A."], "Keywords": ["Cloud", "Monitoring", "Multiple cloud"], "Date": "2014", "Abstract": "A research and a pilot implementation of a monitoring architecture for multiple-cloud infrastructures of VMware, HyperV and OpenStack are presented. A standardized set of monitoring attributes is selected and an efficient architecture to support monitoring itself is implemented. Two different monitoring architectures and their interfaces are described. The pilot is implemented by using the less flexible architecture due to the lack of time. The advantages and disadvantages of both architectures are analyzed. Furthermore two monitoring systems were implemented. One of them serves only as a proof of concept and the other is aa complete monitoring infrastructure for multiple different clouds. On the top of the monitoring system, a fully operational support for SLAs was implemented.", "Language": "en", "Citations": "0"},
{"Title": "Embodied concept discovery through qualitative action models", "Authors": ["Kosmerlj A.", "Bratko I.", "Zabkar J."], "Keywords": ["Cognitive robotics", "inductive logic programming", "predicate invention", "qualitative modeling", "symbolic modeling"], "Date": "2011", "Abstract": "We present a novel approach to embodied learning of qualitative models. We introduce algorithm STRUDEL that enables an autonomous robot to discover new concepts by performing experiments in its environment. The robot collects data about its actions and its observations of the environment. From the obtained data, the robot learns qualitative descriptive models of the effects that its actions have in the environment. Models are learned using inductive logic programming. We describe two experiments with a humanoid robot Nao in which Nao learns descriptive qualitative models which contain what can be interpreted as simple definitions of the concepts of movability and stability. \u00a9 2011 World Scientific Publishing Company.", "Language": "en", "Citations": "3"},
{"Title": "Diameters of commuting graphs of matrices over semirings", "Authors": ["Dolzan D.", "Bukovsek D.K.", "Oblak P."], "Keywords": ["Boolean semiring", "Commuting graph", "Diameter", "Semiring", "Tropical semiring"], "Date": "2012", "Abstract": "We calculate the diameters of commuting graphs of matrices over the binary Boolean semiring, the tropical semiring and an arbitrary nonentire commutative semiring. We also find the lower bound for the diameter of the commuting graph of the semigroup of matrices over an arbitrary commutative entire antinegative semiring. \u00a9 2012 Springer Science+Business Media, LLC.", "Language": "en", "Citations": "5"},
{"Title": "Issues and challanges in business rule-based information systems development", "Authors": ["Bajec M.", "Krisper M."], "Keywords": ["Business rule approach", "Business rule-based information systems development", "Business rules"], "Date": "2005", "Abstract": "An explicit manipulation of business rules in information systems development (ISD) is an old domain. There were many attempts in the last two decades to define how the rules should be dealt with throughout the ISD activities. Despite many results that have been achieved, several questions regarding business rule manipulation within ISD remain unresolved and present challenges for future research. The objective of this paper is to discuss these challenges and where possible to point out some directions for potential solutions.", "Language": "en", "Citations": "6"},
{"Title": "Overcoming unknown kinetic data for quantitative modelling of biological systems using fuzzy logic and Petri nets", "Authors": ["Bordon J.", "Moskon M.", "Mraz M."], "Keywords": ["Fuzzy logic", "Missing kinetic data", "Modelling biological systems", "ODE", "Petri nets"], "Date": "2014", "Abstract": "Biological system modelling is used to guide experimental work, therefore reducing the time and cost of in vivo implementation of newly designed systems. We introduce an improved modelling method, based on fuzzy logic and Petri nets. By using fuzzy logic to linguistically describe a biological process, we avoid the necessity to use kinetic rates, which are often unknown. We introduce a new set of transition functions to enable the use of our method with existing Continuous Petri nets. With this we achieve the extension of usability and applicability of current Continuous Petri nets definition even for biological systems for which exact kinetic data are unknown. We demonstrate the contribution of our approach by using it to model the translation in a simple transcription-translation system. We compare the results obtained to the results of exiting ODE approaches.", "Language": "en", "Citations": "0"},
{"Title": "A computer-aided program for congestion management Ra\u010dunalni\u0161ki program za razbremenitev kriti\u010dnih prenosnih poti", "Authors": ["Grgic D.", "Bajec M.", "Gubina F.", "Golob R.", "Sencar M."], "Keywords": ["Computer program", "Deregulated power systems", "Linear programming", "Power flows", "Power line congestion management"], "Date": "2003", "Abstract": "After the deregulation of the Slovenian power system, a countertrade congestion management approach was proposed to prevent transmission line overloading with active power redispatch. Since this is a mathematically intensive task, the national Energy Agency purchased a computer program for congestion management based on the proposed approach. The computer program consists of the main module with a graphic user interface and various submodules, such as database connection, calculation of the optimal active power production redispatch, report building, among others. Reports containing results of the congestion management are prepared and can be saved for a later use. They can also be printed and stored as a hard copy.", "Language": "en", "Citations": "0"},
{"Title": "Improving numerical prediction with qualitative constraints", "Authors": ["Suc D.", "Bratko I."], "Keywords": [], "Date": "2003", "Abstract": "The usual numerical learning methods, that are primarily concerned with finding a good numerical fit to the data, often make predictions that do not correspond to the qualitative mechanisms in the domain of modelling or a domain expert's intuition. Consistency of numerical predictions with a given qualitative model is helpful when a numerical model is used for explanation of phenomena in the modelled domain, but can also considerably improve numerical accuracy. In this paper we present a novel approach to numerical machine learning called Qfilter. Qfilter is a numerical regression method that can take into account qualitative background knowledge to give qualitatively faithful numerical prediction. The results on a set of domains including population dynamics show considerable prediction accuracy improvements compared to the usual numerical learners. As qualitative domain knowledge is often available in practice, Qfilter's ability to exploit such knowledge should be beneficial in many applications.", "Language": "en", "Citations": "5"},
{"Title": "Enhancing data stream predictions with reliability estimators and explanation", "Authors": ["Bosnic Z.", "Demsar J.", "Kespret G.", "Pereira Rodrigues P.", "Gama J.", "Kononenko I."], "Keywords": ["Data stream", "Incremental learning", "Prediction accuracy", "Prediction correction", "Prediction explanation"], "Date": "2014", "Abstract": "Incremental learning from data streams is increasingly attracting research focus due to many real streaming problems (such as learning from transactions, sensors or other sequential observations) that require processing and forecasting in the real time. In this paper we deal with two issues related to incremental learning - prediction accuracy and prediction explanation - and demonstrate their applicability on several streaming problems for predicting electricity load in the future. For improving prediction accuracy we propose and evaluate the use of two reliability estimators that allow us to estimate prediction error and correct predictions. For improving interpretability of the incremental model and its predictions we propose an adaptation of the existing prediction explanation methodology, which was originally developed for batch learning from stationary data. The explanation methodology is combined with a state-of-the-art concept drift detector and a visualization technique to enhance the explanation in dynamic streaming settings. The results show that the proposed approaches can improve prediction accuracy and allow transparent insight into the modeled concept. \u00a9 2014 Elsevier Ltd.", "Language": "en", "Citations": "5"},
{"Title": "A technology of adaptive link insertion in educational hypermedia", "Authors": ["Kavcic A."], "Keywords": ["Adaptation technologies", "Adaptive hypermedia", "Adaptive link insertion", "Educational hypermedia", "Evaluation", "User modelling"], "Date": "2001", "Abstract": "This paper describes an adaptive technology of link insertion, used in educational hypermedia. It also presents an adaptive hypermedia system that uses this technology and the results of system evaluation in a real environment. The described adaptive system integrates elements of knowledge uncertainty into the user model. This model is then used for system adaptation, which builds on adaptive link insertion, in addition to traditional adaptive navigation support technologies, like annotation and direct guidance.", "Language": "en", "Citations": "0"},
{"Title": "Search versus knowledge in human problem solving: A case study in chess", "Authors": ["Bratko I.", "Hristova D.", "Guid M."], "Keywords": [], "Date": "2016", "Abstract": "This paper contributes to the understanding of human problem solving involved in mental tasks that require exploration among alternatives. Examples of such tasks are theorem proving and classical games like chess. De Groot\u2019s largely used model of chess players\u2019 thinking conceptually consists of two stages: (1) detection of general possibilities, or \u201cmotifs\u201d, that indicate promising ideas the player may try to explore in a given chess position, and (2) calculation of concrete chess variations to establish whether any of the motifs can indeed be exploited to win the game. Strong chess players have to master both of these two components of chess problem solving skill. The first component reflects the player\u2019s chess-specific knowledge, whereas the second applies more generally in game playing and other combinatorial problems. In this paper, we studied experimentally the relative importance of the two components of problem solving skill in tactical chess problems. A possibly surprising conclusion of our experiments is that for our type of chess problems, and players over a rather large range of chess strength, it is the calculating ability, rather than chess-specific pattern-based knowledge, that better discriminates among the players regarding their success. We also formulated De Groot\u2019s model as a Causal Bayesian Network and set the probabilities in the network according to our experimental results.", "Language": "en", "Citations": "1"},
{"Title": "Robust speech detection based on phoneme recognition features", "Authors": ["Mihelic F.", "Zibert J."], "Keywords": [], "Date": "2006", "Abstract": "We introduce new method for discriminating speech and non-speech segments in audio signals based on the transcriptions produced by phoneme recognizers. Four measures based on consonant-vowels and voiced-unvoiced pairs obtained from different phonemes speech recognizers were proposed. They were constructed in a way to be recognizer and language independent and could be applied in different segmentation-classification frameworks. The segmentation systems were evaluated on different broadcast news datasets consisted of more than 60 hours of multilingual BN shows. The results of these evaluations illustrate the robustness of the proposed features in comparison to MFCC and posterior probability based features. The overall frame accuracies of the proposed approaches varied in range from 95% to 98% and remained stable through different test conditions and different phoneme recognizers. \u00a9 Springer-Verlag Berlin Heidelberg 2006.", "Language": "en", "Citations": "5"},
{"Title": "Learning to fly simple and robust", "Authors": ["Suc D.", "Bratko I.", "Sammut C."], "Keywords": [], "Date": "2004", "Abstract": "We report on new experiments with machine learning in the reconstruction of human sub-cognitive skill. The particular problem considered is to generate a clone of a human pilot performing a flying task on a simulated aircraft. The work presented here uses the human behaviour to create constraints for a search process that results in a controller - pilot's clone. Experiments in this paper indicate that this approach, called \"indirect controllers\", results in pilot clones that are, in comparison with those obtained with traditional \"direct controllers\", simpler, more robust and easier to understand. An important feature of indirect controllers in this paper is the use of qualitative constraints. \u00a9 Springer-Verlag Berlin Heidelberg 2004.", "Language": "en", "Citations": "4"},
{"Title": "Small network completion using frequent subnetworks", "Authors": ["Polajnar M.", "Demsar J."], "Keywords": ["Frequent patterns", "Network analysis", "Prediction"], "Date": "2015", "Abstract": "Prediction of missing or potential links and edges is currently the central theme in network analysis. Most of the work is focused on large unlabelled networks, with techniques based on global network models and, on a local level, on using patterns of temporal evolution. We define a problem of small network completion, which deals with sets of small networks, possibly with no recorded temporal dynamics. This problem requires a different set of methods and evaluation procedures. We present a method named Hyspan that extracts frequent patterns from small networks and uses them to predict missing vertices and edges in new networks. It ranks the predicted vertices and edges according to their likelihood estimated from the number and support of the patterns that suggest a particular missing part. Empirical evaluation on real and synthetic data sets shows that the method performs reasonably well. The quality of results depends upon the number and size of the used patterns; a larger number of patterns yields better results but requires longer - although still acceptable - running times.", "Language": "en", "Citations": "2"},
{"Title": "Conquering the curse of dimensionality in gene expression cancer diagnosis: Tough problem, simple models", "Authors": ["Mramor M.", "Leban G.", "Demsar J.", "Zupan B."], "Keywords": [], "Date": "2005", "Abstract": "In the paper we study the properties of cancer gene expression data sets from the perspective of classification and tumor diagnosis. Our findings and case studies are based on several recently published data sets. We find that these data sets typically include a subset of about 100 highly discriminating features of which predictive power can be further enhanced by exploring their interactions. This finding speaks against often used univariate feature selection methods, and may explain the superior performance of support vector machines recently reported in the related work. We argue that a much simpler technique that directly finds visualizations with clear separation of diagnostic classes may be used instead. Furthermore, it may perform better in inference of an understandable classifier that includes only a few relevant features. \u00a9 Springer-Verlag Berlin Heidelberg 2005.", "Language": "en", "Citations": "5"},
{"Title": "Ascending and descending regions of a discrete Morse function", "Authors": ["Jerse G.", "Mramor Kosta N."], "Keywords": ["Ascending and descending regions", "Discrete Morse theory", "Morse-Smale decomposition"], "Date": "2009", "Abstract": "We present an algorithm which produces a decomposition of a regular cellular complex with a discrete Morse function analogous to the Morse-Smale decomposition of a smooth manifold with respect to a smooth Morse function. The advantage of our algorithm compared to similar existing results is that it works, at least theoretically, in any dimension. Practically, there are dimensional restrictions due to the size of cellular complexes of higher dimensions, though. We prove that the algorithm is correct in the sense that it always produces a decomposition into descending and ascending regions of the critical cells in a finite number of steps, and that, after a finite number of subdivisions, all the regions are topological disks. The efficiency of the algorithm is discussed and its performance on several examples is demonstrated. \u00a9 2008 Elsevier B.V. All rights reserved.", "Language": "en", "Citations": "11"},
{"Title": "Protocol to assess robustness of ST analysers: A case study", "Authors": ["Jager F.", "Moody G.B.", "Mark R.G."], "Keywords": ["Assessing robustness", "Bootstrap evaluation of performance", "Critical performance boundaries", "Noise stress test", "Sensitivity analysis", "ST segment analyser"], "Date": "2004", "Abstract": "This paper proposes principles and methods for assessing the robustness of ST segment analysers and algorithms. We describe an evaluation protocol, procedures and performance measures suitable for assessing the robustness. An ST analyser is robust if its performance is not critically dependent on the variation of the noise content of input signals and on the choice of the database used for testing, and if its analysis parameters are not critically tuned to the database used for testing. The protocol to assess the robustness includes: (1) a noise stress test addressing the aspect of variation of input signals; (2) a bootstrap evaluation of algorithm performance addressing the aspect of distribution of input signals and (3) a sensitivity analysis addressing the aspect of variation of analyser's architecture parameters. An ST analyser is considered to be robust if the performance measurements obtained during these procedures remain above the predefined critical performance boundaries. We illustrate the use of the robustness protocol and robustness measures by a case study in which we assessed the robustness of our Karhunen- Lo\u00e8ve transform based ischaemic ST episode detection and quantification algorithm using the European Society of Cardiology ST-T database.", "Language": "en", "Citations": "3"},
{"Title": "\u201cIt's so vital to learn Slovene\u201d: Mediation choices by asylum seekers in Slovenia", "Authors": ["Pokorn N.K.", "Cibej J."], "Keywords": ["Asylum seekers", "Interpreting", "Mediation strategies", "Short-term migrants", "Translation"], "Date": "2018", "Abstract": "Short-time migrants, who stay in the host country from one to 12 months, use mediation strategies including lingua francas, public-service interpreting and translation, translation technologies, intercomprehension, and learning the host country's dominant language. The choices made by asylum seekers in Slovenia, a country of transit for the majority of asylum seekers, are analyzed on the basis of questionnaires answered by 127 current and former residents of the Slovene asylum seeker centers in 2016, followed up by semi-structured interviews with a representative group of 34 asylum seekers. The results show that the majority of newly arrived migrants regard the use of lingua francas as a helpful but not desired long-term strategy. They define host-country language learning as the most desirable strategy for linguistic and social inclusion. Surprisingly, they are reluctant to use translation technologies and interpreters because they either doubt the accuracy of the transfer or they consider such mediation (interpreting in particular) a hindrance to their independence.", "Language": "en", "Citations": "1"},
{"Title": "Novelties within the framework for information system due diligence", "Authors": ["Delak B.", "Vasilecas O.", "Bajec M."], "Keywords": ["Efficiency increase", "Framework for information system due diligence", "Information system", "Information system due diligence", "Time reduction"], "Date": "2017", "Abstract": "The information system (IS) field lacks a scientifically based analytical tool for rapid delivery of IS due diligence. In 2013, a framework for IS due diligence (FISDD) was published. The work on this framework was continued with proceeding development and evaluation. One of the most important objectives of today's business is making savings concerning various resources. The main novelty of the upgraded framework is the replacement of some of the manually conducted questionnaires with web-based questionnaires. This novelty reduces the total time required for on-site ISDD activities by 35-50%, as has been the result of several real cases. The results demonstrate and confirm the usefulness of upgrading FISDD, which increases its efficiency in DD processes, which is the main contribution of this article.", "Language": "en", "Citations": "0"},
{"Title": "Evaluating the effect of the automatic assessment introduction in the ADS1 course practical work", "Authors": ["Rozanc I.", "Mihelic J."], "Keywords": [], "Date": "2015", "Abstract": "The introduction of modern teaching and assessing approaches importantly enhance the quality of teaching. Apart beneficial effects such as effective knowledge presentation, better communication and easier practical work execution, changes include risk to ruin good results of well-established teaching routine. This paper is motivated by recent changes of the content, structure and assessing method of practical work of the Algorithms and Data Structures 1 course, which is taught at the FCIS at the University of Ljubljana. The most critical part was the introduction of automatic assessing of practical work as it considerably elevated the number of shorter tasks to be handed in on time by students. In our work the effect of such approach is objectively evaluated. First, the previous and current assessment data is used to examine the number and distribution of results for students involved in practical work. Second, the current grades are analyzed to check the correlation with previous grades obtained manually. Next, the results of a student questionnaire are presented, which confirm the successful introduction of new approach. Finally, our experience is summarized to support similar initiatives elsewhere.", "Language": "en", "Citations": "0"},
{"Title": "Triangle-free distance-regular graphs with an eigenvalue multiplicity equal to their valency and diameter 3", "Authors": ["Jurisic A.", "Koolen J.", "Zitnik A."], "Keywords": [], "Date": "2008", "Abstract": "In this paper, triangle-free distance-regular graphs with diameter 3 and an eigenvalue \u03b8 with multiplicity equal to their valency are studied. Let \u0393 be such a graph. We first show that \u03b8 = - 1 if and only if \u0393 is antipodal. Then we assume that the graph \u0393 is primitive. We show that it is formally self-dual (and hence Q-polynomial and 1-homogeneous), all its eigenvalues are integral, and the eigenvalue with multiplicity equal to the valency is either second largest or the smallest. Let x, y \u2208 V \u0393 be two adjacent vertices, and z \u2208 \u0393", "Language": "en", "Citations": "3"},
{"Title": "Automatic recognition of gait-related health problems in the elderly using machine learning", "Authors": ["Pogorelc B.", "Bosnic Z.", "Gams M."], "Keywords": ["Ambient assisted living", "Ambient intelligence", "Ambient media", "Data mining", "Elderly care", "Gait analysis", "Health-problems detection", "Human locomotion", "Human-motion analysis", "Machine learning", "Pervasive health", "Temporal data mining", "Time-series data mining", "Ubiquitous computing"], "Date": "2012", "Abstract": "This paper proposes a system for the early automatic recognition of health problems that manifest themselves in distinctive form of gait. Purpose of the system is to prolong the autonomous living of the elderly at home. When the system identifies a health problem, it automatically notifies a physician and provides an explanation of the automatic diagnosis. The gait of the elderly user is captured using a motion-capture system, which consists of body-worn tags and wall-mounted sensors. The positions of the tags are acquired by the sensors and the resulting time series of position coordinates are analyzed with machine-learning algorithms in order to recognize a specific health problem. Novel semantic features based on medical knowledge for training a machine-learning classifier are proposed in this paper. The classifier classifies the user's gait into: 1) normal, 2) with hemiplegia, 3) with Parkinson's disease, 4) with pain in the back and 5) with pain in the leg. The studies of 1) the feasibility of automatic recognition and 2) the impact of tag placement and noise level on the accuracy of the recognition of health problems are presented. The experimental results of the first study (12 tags, no noise) showed that the k-nearest neighbors and neural network algorithms achieved classification accuracies of 100%. The experimental results of the second study showed that classification accuracy of over 99% is achievable using several machine-learning algorithms and 8 or more tags with up to 15 mm standard deviation of noise. The results show that the proposed approach achieves high classification accuracy and can be used as a guide for further studies in the increasingly important area of Ambient Assisted Living. Since the system uses semantic features and an artificial-intelligence approach to interpret the health state, provides a natural explanation of the hypothesis and is embedded in the domestic environment of the elderly person; it is an example of the semantic ambient media for Ambient Assisted Living. \u00a9 2010 Springer Science+Business Media, LLC.", "Language": "en", "Citations": "59"},
{"Title": "Analysis of radiograph and detection of cardiomegaly", "Authors": ["Ilovar M.", "Sajn L."], "Keywords": [], "Date": "2011", "Abstract": "The paper presents the procedure which automatically and reliably determines the presence of heart enlargement, also known as cardiomegaly, from a chest radiograph. We took advantage of some well-established image processing methods and adapted a few of them to meet our needs. Methods which were used include image filtering with convolution masks, segmentation with thresholding and edge detection. The procedure to detect heart and chest cavity boundaries and the corresponding boundary points using modified and custom image processing methods is presented. The final result represents the confirmation or rejection of cardiomegaly. \u00a9 2011 MIPRO.", "Language": "en", "Citations": "1"},
{"Title": "User interfaces and methodology for gathering multimodal data about music Uporabni\u0161ki vmesniki in metodologija pridobivanja ve\u010dmodalnih podatkov o glasbi", "Authors": ["Pesek M.", "Strle G.", "Marolt M."], "Keywords": [], "Date": "2015", "Abstract": "Several studies dealing with music recommendation and visualization base their approaches on datasets gathered with user surveys. However, the gathering procedure is seldom the focus of music research, even though the user interfaces and methodology are an important part of gathering the music data and evaluation of the music information retrieval algorithms. The paper presents the main elements of gathering the Moodo dataset that combines the demographic data, the users' mood and perception of emotions with the users' emotional and color responses to music. For this purpose, two novel user interfaces were developed, i.e. the MoodStripe and MoodGraph, which have several advantages over the existing classical models, both in terms of intuitiveness and functionality. The proposed interfaces are also applicable to other domains dealing with the user data.", "Language": "en", "Citations": "0"},
{"Title": "Semantic web rule languages Pravila na semanti\u010dnem spletu", "Authors": ["Lavbic D.", "Bajec M.", "Krisper M."], "Keywords": ["RuleML", "Rules", "Semantic web", "SWRL"], "Date": "2006", "Abstract": "Semantic Web rule languages play an important role in making Semantic Web valued and are gaining support. Rules can be used for describing declarative knowledge of ontology languages and to express restrictions or transformations that can include the portion of description logic. On the other hand rules can express policy enforcement, define responses to changes, specify intelligent agent behaviors and be used for other purposes. Therefore, Semantic Web rule languages will play an important role for being lingua franca in interchanging rules among different systems and tools, enabling developing, running, publishing and interacting among Semantic Web rules. It is important to emphasize that reuse and interchange of rules at a higher abstract layer is required due to the fact that during the knowledge acquisition process rules are captured at the business level. To enable rules to be used at the information system level, some translation is required to establish several abstraction layers. In the paper we present a framework for knowledge representation that uses ontologies and rules. Our main focus is on the dynamic, heterogeneous environment and use of the knowledge base where ontologies and rules are combined thus extending OWL DL expressiveness. The merging phase is supported by converters, because there are presently no reasoning engines available to support the latest SWRL proposal. This paper also presents several approaches to rule representation on the Semantic Web, from SWRL, ORL to RuleML.", "Language": "en", "Citations": "0"},
{"Title": "QoS-Aware Orchestration of Network Intensive Software Utilities within Software Defined Data Centres: An Architecture and Implementation of a Global Cluster Manager", "Authors": ["Pascinski U.", "Trnkoczy J.", "Stankovski V.", "Cigale M.", "Gec S."], "Keywords": ["Autonomic orchestration", "Event-driven cloud applications", "Quality of Service", "Software-Defined Data Centre"], "Date": "2018", "Abstract": "Although the cloud computing domain is progressing rapidly, the deployment of various network intensive software utilities in the cloud is still a challenging task. The Quality of Service (QoS) for various gaming, simulations, videoconferencing, video streaming or even file uploading tasks may be significantly affected by the quality and geolocation of the selected underlying computing resources, which are available only when the specific functionality is required. This study presents a new architecture for geographic orchestration of network intensive software components which is designed for high QoS. Key elements of this architecture are a Global Cluster Manager operating within Software-Defined Data Centres (SDDCs), a runtime QoS Monitoring System, and a QoS Modeller and Decision Maker for automated orchestration of software utilities. The implemented system automatically selects the best geographically available computing resource within the SDDC according to the developed QoS model of the software component. This architecture is event-driven as the services are deployed and destroyed in real-time for every usage event. The utility of the implemented orchestration technology is verified qualitatively and in relation to the potential gains of selected QoS metrics by using two network intensive software utilities implemented as containers: an HTTP(S) File Upload service and a Jitsi Meet videoconferencing service. The study shows potential for QoS improvements in comparison to existing orchestration systems.", "Language": "en", "Citations": "4"},
{"Title": "Non-negative matrix factorization with selective sparsity constraints for transcription of bell chiming recordings", "Authors": ["Marolt M."], "Keywords": [], "Date": "2009", "Abstract": "The paper presents a method for automatic transcription of recordings of bell chiming performances. Bell chiming is a Slovenian folk music tradition involving performers playing tunes on church bells by holding the clapper and striking the rim of a stationary bell. The tunes played consist of repeated rhythmic patterns into which various changes are included. Because the sounds of bells are inharmonic and their tuning not known in advance, we propose a two step approach to transcription. First, by analyzing the covariance matrix of the time-frequency representation of a recording, we estimate the number of bells and their approximate spectra using prior knowledge of church bell acoustics and bell chiming performance rules. We then propose a non-negative matrix factorization algorithm with selective sparsity constraints that learns the basis vectors that approximate the previously estimated bell spectra. The algorithm also adapts the number of basis vectors during learning. We show how to apply the proposed method to bell chiming transcription and present results on a set of field recordings.", "Language": "en", "Citations": "3"},
{"Title": "Control of a neuronal morphology program by an RNA-binding zinc finger protein, Unkempt", "Authors": ["Murn J.", "Zarnack K.", "Yang Y.J.", "Durak O.", "Murphy E.A.", "Cheloufi S.", "Gonzalez D.M.", "Teplova M.", "Curk T.", "Zuber J.", "Patel D.J.", "Ule J.", "Luscombe N.M.", "Tsai L.-H.", "Walsh C.A.", "Shi Y."], "Keywords": ["Cell morphology", "Gene expression program", "Neurons", "RNA-binding proteins", "Translation", "Unkempt"], "Date": "2015", "Abstract": "Cellular morphology is an essential determinant of cellular function in all kingdoms of life, yet little is known about how cell shape is controlled. Here we describe a molecular program that controls the early morphology of neurons through a metazoan-specific zinc finger protein, Unkempt. Depletion of Unkempt in mouse embryos disrupts the shape of migrating neurons, while ectopic expression confers neuronal-like morphology to cells of different nonneuronal lineages. We found that Unkempt is a sequence-specific RNA-binding protein and identified its precise binding sites within coding regions of mRNAs linked to protein metabolism and trafficking. RNA binding is required for Unkempt-induced remodeling of cellular shape and is directly coupled to a reduced production of the encoded proteins. These findings link post-transcriptional regulation of gene expression with cellular shape and have general implications for the development and disease of multicellular organisms.", "Language": "en", "Citations": "11"},
{"Title": "Subgroup discovery in data sets with multi-dimensional responses: A method and a case study in traumatology", "Authors": ["Umek L.", "Zupan B.", "Toplak M.", "Morin A.", "Chauchat J.-H.", "Makovec G.", "Smrke D."], "Keywords": ["\u03c7 2 statistics", "Femoral neck fracture", "K-medoids clustering", "Multi-label prediction", "Subgroup discovery"], "Date": "2009", "Abstract": "Biomedical experimental data sets may often include many features both at input (description of cases, treatments, or experimental parameters) and output (outcome description). State-of-the-art data mining techniques can deal with such data, but would consider only one output feature at the time, disregarding any dependencies among them. In the paper, we propose the technique that can treat many output features simultaneously, aiming at finding subgroups of cases that are similar both in input and output space. The method is based on k-medoids clustering and analysis of contingency tables, and reports on case subgroups with significant dependency in input and output space. We have used this technique in explorative analysis of clinical data on femoral neck fractures. The subgroups discovered in our study were considered meaningful by the participating domain expert, and sparked a number of ideas for hypothesis to be further experimentally tested. \u00a9 2009 Springer Berlin Heidelberg.", "Language": "en", "Citations": "8"},
{"Title": "A model of influences of environmental stakeholders on strategic information systems planning success in an enterprise", "Authors": ["Hovelja T.", "Vasilecas O.", "Rupnik R."], "Keywords": ["enterprises", "environmental stakeholders", "IT deployment success", "IT investments", "strategic information systems planning"], "Date": "2013", "Abstract": "Worldwide spending of enterprises on information technologies (IT) in 2011 is projected to total USD 2.6 trillion with 350 enterprises each investing more than USD 1 billion. Despite such large and rapidly growing investment figures, the success rates of IT deployment projects over the past 20 years remained relatively low: approximately half of IT deployment projects were unsuccessful. Literature review reveals that inadequate strategic information system planning (SISP) is one of the main reasons for low deployment success rates of IT deployment projects and, thus, one of the current critical IT management issues. For this reason researchers and practitioners in the SISP field are currently directing their efforts into expanding the traditional understanding of SISP as a pure IT planning activity. In addition to the activities dedicated to the planning of IT investments, new SISP models should also include the contextual activities that enable adaptation of the deployed IT to the environment and knowledge transfer from the environment. In this paper, the authors propose an extended SISP model that includes influences of environmental stakeholders. The developed SISP model was empirically tested on a sample from the population of 1000 largest enterprises in Slovenia. The authors believe that the findings of their research and the suggested extended SISP model will improve understanding of SISP success factors in enterprises and consequently enable them to manage their IT investments with greater success. \u00a9 2013 Vilnius Gediminas Technical University (VGTU) Press.", "Language": "en", "Citations": "5"},
{"Title": "Basic sets in the digital plane", "Authors": ["Mramor-Kosta N.", "Trenklerova E."], "Keywords": ["Basic sets in the plane", "Digital topology"], "Date": "2008", "Abstract": "A set K in the plane \u211d", "Language": "en", "Citations": "0"},
{"Title": "Visualization of concurrent tones in music with colours", "Authors": ["Ciuha P.", "Klemenc B.", "Solina F."], "Keywords": ["colour", "concurrent tones", "midi", "music visualization"], "Date": "2010", "Abstract": "Visualizing music in a meaningful and intuitive way is a challenge. Our aim is to visualize music by interconnecting similar aspects in music and in visual perception. We focus on visualizing harmonic relationships between tones and colours. Related existing visualizations map tones or keys into a discrete set of colours. As concurrent (simultaneous) tones are not perceived as entirely separate, but also as a whole, we present a novel method for visualizing a group of concurrent tones (limited to the pitches of the 12-tone chromatic scale) with one colour for the whole group. The basis for calculation of colour is the assignment of key spanning circle of thirds to the colour wheel. The resulting colour is not limited to discrete set of colours: similar tones, chords and keys have similar colour hue; dissonance and consonance are represented by low and high colour saturation respectively. The proposed method is demonstrated as part of our prototype music visualization system using extended 3-dimensional piano roll notation. \u00a9 2010 ACM.", "Language": "en", "Citations": "17"},
{"Title": "Preservation of a computer-based art installation", "Authors": ["Solina F.", "Majcen G.", "Bovcon N.", "Batagelj B."], "Keywords": ["Case study", "Conservation", "Digital art", "Software maintenance"], "Date": "2014", "Abstract": "In contemporary digital art computer technology plays an integral part not only in the creation of art pieces but also in their functioning as art works. Such digital art works have usually a performative or interactive character and therefore rely on an underlying working computer system. Since computer and information technology advances with such unrelenting pace, hardware and software modules soon become obsolete. How to preserve such digital art works in these circumstances from a art conservation standpoint is much debated but not clear yet. In this article we present and discuss issues in the conservation of digital art works using a case study of a ten years old interactive art installation.", "Language": "en", "Citations": "1"},
{"Title": "A peer-to-peer search in data grids based on ant colony optimization", "Authors": ["Jovanovic U.", "Slivnik B."], "Keywords": ["Ant colony optimization", "Data grids", "Distributed search", "P2P"], "Date": "2006", "Abstract": "A method for (1) an efficient discovery of data in large distributed raw datasets and (2) collection of thus procured data is considered. It is a pure peer-to-peer method without any centralized control and is therefore primarily intended for a large-scale, dynamic (data)grid environments. It provides a simple but highly efficient mechanism for keeping the load it causes under control and proves especially usefull if data discovery and collection is to be performed simultaneoulsy with dataset generation. The method supports a user-specified extraction of structured metadata from raw datasets, and automatically performs aggregation of extracted metadata. It is based on the principle of ant colony optimization (ACO). The paper is focused on effective data aggregation and includes the detailed description of the modifications of the basic ACO algorithm that are needed for effective aggregation of the extracted data. Using a simulator, the method was vigorously tested on the wide set of different network topologies for different rates of data extraction and aggregation. Results of the most significant tests are included.", "Language": "en", "Citations": "1"},
{"Title": "Visual re-identification across large, distributed camera networks", "Authors": ["Kenk V.S.", "Mandeljc R.", "Kovacic S.", "Kristan M.", "Hajdinjak M.", "Pers J."], "Keywords": ["Distributed sensors", "Re-identification", "Smart cameras", "Surveillance", "Visual-sensor networks"], "Date": "2015", "Abstract": "We propose a holistic approach to the problem of re-identification in an environment of distributed smart cameras. We model the re-identification process in a distributed camera network as a distributed multi-class classifier, composed of spatially distributed binary classifiers. We treat the problem of re-identification as an open-world problem, and address novelty detection and forgetting. As there are many tradeoffs in design and operation of such a system, we propose a set of evaluation measures to be used in addition to the recognition performance. The proposed concept is illustrated and evaluated on a new many-camera surveillance dataset and SAIVT-SoftBio dataset.", "Language": "en", "Citations": "12"},
{"Title": "The effects of air mass transport, seasonality, and meteorology on pollutant levels at the Iskrba regional background station (1996-2014)", "Authors": ["Poberznik M.", "Strumbelj E."], "Keywords": ["Air pollutants", "Backward trajectory", "Bayesian statistics", "Climatology", "Clustering", "Long-range transport", "PM10\n                        ", "PM2.5\n                        ", "Slovenia"], "Date": "2016", "Abstract": "Our main goal was to estimate the effects of long-range air transport on pollutant concentrations measured at the Iskrba regional background station (Slovenia). We cluster back-trajectories into categories and simultaneously model the effects of meteorology, seasonality, trends, and air mass trajectory clusters using a Bayesian statistical approach. This simplifies the interpretation of results and allows us to better identify the effects of individual variables, which is important, because pollutant concentrations, meteorology, and trajectories are seasonal and correlated. Similar to related work from other European sites, we find that slow and faster moving trajectories from eastern Europe and the northern part of the Balkan peninsula are associated with higher pollutant levels, while fast-moving trajectories from the Atlantic are associated with lower pollutant concentration. Overall, pollutant concentrations have decreased in the studied period.", "Language": "en", "Citations": "1"},
{"Title": "A computer vision integration model for a multi-modal cognitive system", "Authors": ["Vrecko A.", "Skocaj D.", "Hawes N.", "Leonardis A."], "Keywords": [], "Date": "2009", "Abstract": "We present a general method for integrating visual components into a multi-modal cognitive system. The integration is very generic and can work with an arbitrary set of modalities. We illustrate our integration approach with a specific instantiation of the architecture schema that focuses on integration of vision and language: a cognitive system able to collaborate with a human, learn and display some understanding of its surroundings. As examples of cross-modal interaction we describe mechanisms for clarification and visual learning. \u00a9 2009 IEEE.", "Language": "en", "Citations": "10"},
{"Title": "Discovering disease-disease associations by fusing systems-level molecular data", "Authors": ["Zitnik M.", "Janjic V.", "Larminie C.", "Zupan B.", "Przulj N."], "Keywords": [], "Date": "2013", "Abstract": "The advent of genome-scale genetic and genomic studies allows new insight into disease classification. Recently, a shift was made from linking diseases simply based on their shared genes towards systems-level integration of molecular data. Here, we aim to find relationships between diseases based on evidence from fusing all available molecular interaction and ontology data. We propose a multi-level hierarchy of disease classes that significantly overlaps with existing disease classification. In it, we find 14 disease-disease associations currently not present in Disease Ontology and provide evidence for their relationships through comorbidity data and literature curation. Interestingly, even though the number of known human genetic interactions is currently very small, we find they are the most important predictor of a link between diseases. Finally, we show that omission of any one of the included data sources reduces prediction quality, further highlighting the importance in the paradigm shift towards systems-level data fusion.", "Language": "en", "Citations": "49"},
{"Title": "Rights management to enable a true Internet of Things", "Authors": ["Newman R.", "Doody P.", "Trebar M.", "Okoke U."], "Keywords": ["information mining", "Internet of Services", "Internet of Things", "rights management", "security"], "Date": "2016", "Abstract": "In this paper, we argue that the determining aspect of the 'Internet of Things' (IoT) is the accessibility of 'things' on the global Internet, as opposed to a simple interconnection of networked 'things'. We observe that most reported applications of the 'Internet of Things' would be more accurately described as 'Intranets of Things'. In large part due to understandable concerns about the security of. In the wider field of the Internet 'in the large', the open mining of the Web for information has become the mainstay of many genres of research. We give an example of how in one project the prospect of using IoT data in a similar way has been validated to an extent on the basis that data owners voluntarily made their data available. We suggest that there are two services that need to be provided in order for the generalized information mining that occurs on the Internet-At-large to occur in the Internet of Things. The first is a means of cataloguing available data, which is already being addressed by services such as HyperCAT. The second is an automatic rights management service (IoT-RM), which would manage the rights and permissions and allow data owners to determine in advance to whom their data should be released, for what purposes, subject to which restrictions (such as, for instance, anonymisation) and whether any remuneration should be involved. We make some concrete proposals about the form that such an IoT-RM should take.", "Language": "en", "Citations": "1"},
{"Title": "Rapid development of executable ontology for financial instruments and trading strategies", "Authors": ["Lavbic D.", "Bajec M."], "Keywords": ["financial instruments", "ontology", "rapid ontology development", "semantic web", "trading strategies"], "Date": "2011", "Abstract": "In this paper we employ Rapid Ontology Development approach (ROD) with constant evaluation of steps in the process of ontology construction for development of Financial Instruments and Trading Strategies (FITS) ontology. We show that ontology development process does not conclude with successful definition of schematic part of ontology but we continue with post development activities where additional axiomatic information and instances with dynamic imports from various sources are defined. The result is executable ontology as part of Semantic Web application that uses data from several semi structured sources. The overall process of construction is suitable for users without extensive technical and programming skills and those users are rather experts in the problem domain. \u00a9 2011 Springer-Verlag.", "Language": "en", "Citations": "0"},
{"Title": "Convexity in complex networks", "Authors": ["Marc T.", "Subelj L."], "Keywords": ["convex subgraphs", "convex subsets", "core-periphery structure", "network convexity"], "Date": "2018", "Abstract": "Metric graph properties lie in the heart of the analysis of complex networks, while in this paper we study their convexity through mathematical definition of a convex subgraph. A subgraph is convex if every geodesic path between the nodes of the subgraph lies entirely within the subgraph. According to our perception of convexity, convex network is such in which every connected subset of nodes induces a convex subgraph. We show that convexity is an inherent property of many networks that is not present in a random graph. Most convex are spatial infrastructure networks and social collaboration graphs due to their tree-like or clique-like structure, whereas the food web is the only network studied that is truly non-convex. Core-periphery networks are regionally convex as they can be divided into a non-convex core surrounded by a convex periphery. Random graphs, however, are only locally convex meaning that any connected subgraph of size smaller than the average geodesic distance between the nodes is almost certainly convex. We present different measures of network convexity and discuss its applications in the study of networks.", "Language": "en", "Citations": "3"},
{"Title": "The 10-cages and derived configurations", "Authors": ["Pisanski T.", "Boben M.", "Marusic D.", "Orbanic A.", "Graovac A."], "Keywords": ["Cages", "Configurations", "Graphs"], "Date": "2004", "Abstract": "Symmetry properties of the three 10-cages on 70 vertices are investigated. Being bipartite, these graphs are Levi graphs of triangle- and quadrangle-free (35", "Language": "en", "Citations": "20"},
{"Title": "Statistical machine translation of Croatian weather forecasts: How much data do we need?", "Authors": ["Ljubesic N.", "Bago P.", "Boras D."], "Keywords": ["Automatic evaluation", "Correlation between evaluation measures", "Manual evaluation", "Statistical machine translation"], "Date": "2010", "Abstract": "This research is the first step towards developing a system for translating Croatian weather forecasts into multiple languages. This step deals with the Croatian-English language pair. The parallel corpus consists of a one-year sample of the weather forecasts for the Adriatic, consisting of 7,893 sentence pairs. Evaluation is performed by the automatic evaluation measures BLUE, NIST and METEOR, as well as by manually evaluating a sample of 200 translations. We have shown that with a small-sized training set and the state-of-the artMoses system, decoding can be done with 96% accuracy concerning adequacy and fluency. Additional improvement is expected by increasing the training set size. Finally, the correlation of the recorded evaluation measures is explored.", "Language": "en", "Citations": "0"},
{"Title": "Driving information systems security through innovations - First indications", "Authors": ["Trcek D.", "Likar B."], "Keywords": ["Information security", "Information systems", "Innovations", "Multidisciplinary research"], "Date": "2014", "Abstract": "Modern organizations and even nations are increasingly dependent on information systems (IS) security, and their economic prosperity is strongly linked to innovation. Do these two important issues also relate one to another, and how? Can some lessons be learned that are important not only to security professionals but also to organizational and other important systems managing decision makers? Assuming that the answer is yes, how can we deploy innovation techniques to further improve IS security? Because this interdisciplinary area has not been addressed so far, this article presents one of the first attempts to address it on the basis of statistically relevant data on a national and international scale. It provides experimental results that imply some important statistical interdependencies that call for further study and also identifies systemic limitations, including those that exist on the European Union scale, that should be addressed to enable progress in this area. Copyright \u00a9 2014 Taylor & Francis Group, LLC.", "Language": "en", "Citations": "0"},
{"Title": "Data fusion by matrix factorization", "Authors": ["Zitnik M.", "Zupan B."], "Keywords": ["Bioinformatics", "Cheminformatics", "Data fusion", "Data mining", "Intermediate data integration", "Matrix factorization"], "Date": "2015", "Abstract": "For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.", "Language": "en", "Citations": "62"},
{"Title": "Effect of low versus high frequency subthalamic deep brain stimulation on speech intelligibility and verbal fluency in Parkinson's disease: A double-blind study", "Authors": ["Grover T.", "Georgiev D.", "Kalliola R.", "Mahlknecht P.", "Zacharia A.", "Candelario J.", "Hyam J.", "Zrinzo L.", "Hariz M.", "Foltynie T.", "Limousin P.", "Jahanshahi M.", "Tripoliti E."], "Keywords": ["deep brain stimulation", "frequency", "Parkinson's disease", "speech intelligibility", "Subthalamic nucleus", "verbal fluency"], "Date": "2019", "Abstract": "Background: Subthalamic deep brain stimulation (STN-DBS) is an established treatment for late stage Parkinson's disease (PD). Speech intelligibility (SI) and verbal fluency (VF) have been shown to deteriorate following chronic STN-DBS. It has been suggested that speech might respond favourably to low frequency stimulation (LFS). Objective: We examined how SI, perceptual speech characteristics, phonemic and semantic VF and processes underlying it (clustering and switching) respond to LFS of 60 and 80 Hz in comparison to high frequency stimulation (HFS) (110, 130 and 200 Hz). Methods: In this double-blind study, 15 STN-DBS PD patients (mean age 65, SD = 5.8, 14 right handed, three females), were assessed at five stimulation frequencies: 60 Hz, 80 Hz, 110 Hz, 130 Hz and 200 Hz. In addition to the clinical neurological assessment of speech, VF and SI were assessed. Results: SI and in particular articulation, respiration, phonation and prosody improved with LFS (all p < 0.05). Phonemic VF switching improved with LFS (p = 0.005) but this did not translate to an improved phonemic VF score. A trend for improved semantic VF was found. A negative correlation was found between perceptual characteristics of speech and duration of chronic stimulation (all p < 0.05). Conclusions: These findings highlight the need for meticulous programming of frequency to maximise SI in chronic STNDBS. The findings further implicate stimulation frequency in changes to specific processes underlying VF, namely phonemic switching and demonstrate the potential to address such deficits through advanced adjustment of stimulation parameters.", "Language": "en", "Citations": "0"},
{"Title": "Efficiently explaining the predictions of a probabilistic radial basis function classification network", "Authors": ["Robnik-Sikonja M.", "Strumbelj E.", "Kononenko I."], "Keywords": ["Data mining", "feature importance", "model interpretation", "model visualization", "neural nets"], "Date": "2013", "Abstract": "A probabilistic radial basis function (PRBF) network is an effective non-linear classifier. However, similar to most other neural network models it is non-transparent, which makes its predictions difficult to interpret. In this paper we show how a one-variable-at-a-time and an all-subsets explanation method can be modified for an equivalent and more efficient use with PRBF network classifiers. We use several artificial and real-life data sets to demonstrate the usefulness of the visualizations and explanations of the PRBF network classifier. \u00a9 2013 - IOS Press and the authors. All rights reserved.", "Language": "en", "Citations": "0"},
{"Title": "Intelligent agile method framework", "Authors": ["Jankovic M.", "Bajec M.", "Khodabandelou G.", "Deneckere R.", "Hug C.", "Salinesi C."], "Keywords": ["Process mining", "Situational method engineering", "Software development improvement"], "Date": "2013", "Abstract": "The paper addresses the problem of the low usage of software development methods in software development practice. This has been recognized as one of the key reasons for failures in software development projects and a contributor to the low quality of software. We introduce a novel approach that could help to improve the maturity of software development processes. The approach is based on the method engineering principles taking into account the limitations that hinder its use in practice. The main objective of our research is to show that the method engineering concepts are applicable in real settings and that could contribute to the higher quality of software development processes and their products. Copyright \u00a9 2013 SCITEPRESS.", "Language": "en", "Citations": "2"},
{"Title": "Automatic segmentation of ethnomusicological field recordings", "Authors": ["Marolt M.", "Bohak C.", "Kavcic A.", "Pesek M."], "Keywords": ["Audio segmentation", "Deep learning", "Field recordings", "Music information retrieval"], "Date": "2019", "Abstract": "The article presents a method for segmentation of ethnomusicological field recordings. Field recordings are integral documents of folk music performances captured in the field, and typically contain performances, intertwined with interviews and commentaries. As these are live recordings, captured in non-ideal conditions, they usually contain significant background noise. We present a segmentation method that segments field recordings into individual units labelled as speech, solo singing, choir singing, and instrumentals. Classification is based on convolutional deep networks, and is augmented with a probabilistic approach for segmentation. We describe the dataset gathered for the task and the tools developed for gathering the reference annotations. We outline a deep network architecture based on residual modules for labelling short audio segments and compare it to the more standard feature based approaches, where an improvement in classification accuracy of over 10% was obtained. We also present the SeFiRe segmentation tool that incorporates the presented segmentation method.", "Language": "en", "Citations": "0"},
{"Title": "Quantum-dot field programmable gate array: Enhanced routing", "Authors": ["Jazbec A.", "Zimic N.", "Bajec I.L.", "Pecar P.", "Mraz M."], "Keywords": ["FPGA", "QCA", "routing"], "Date": "2006", "Abstract": "FPGA (Field Programmable Gate Array) architecture contains configurable logic blocks (CLB) and programmable interconnects. CLB is used to implement most of the logic in FPGA. A hierarchy of programmable interconnects called Programmable Switch Matrix (PSM) allows the logic blocks of FPGA to be interconnected as needed by the system designer. Logic blocks and interconnects can be programmed after the manufacturing process by the customer/designer so that FPGA can perform whatever logical function is needed. Wherever a vertical and a horizontal line intersect there is a switch matrix interconnect point. In this article FPGA based on quantum-dot cellular automata (QCA) technology is presented. The research is dedicated mostly to PSM. \u00a9 2006 IEEE.", "Language": "en", "Citations": "4"},
{"Title": "Exploratory equivalence in graphs: Definition and algorithms", "Authors": ["Mihelic J.", "Furst L.", "Cibej U."], "Keywords": [], "Date": "2014", "Abstract": "Motivated by improving the efficiency of pattern matching on graphs, we define a new kind of equivalence on graph vertices. Since it can be used in various graph algorithms that explore graphs, we call it exploratory equivalence. The equivalence is based on graph automorphisms. Because many similar equivalences exist (some also based on automorphisms), we argue that this one is novel. For each graph, there are many possible exploratory equivalences, but for improving the efficiency of the exploration, some are better than others. To this end, we define a goal function that models the reduction of the search space in such algorithms. We describe two greedy algorithms for the underlying optimization problem. One is based directly on the definition using a straightforward greedy criterion, whereas the second one uses several practical speedups and a different greedy criterion. Finally, we demonstrate the huge impact of exploratory equivalence on a real application, i.e., graph grammar parsing.", "Language": "en", "Citations": "3"},
{"Title": "Ear recognition: More than a survey", "Authors": ["Emersic Z.", "Struc V.", "Peer P."], "Keywords": ["Biometry", "Dataset", "Descriptor-based method", "Ear recognition", "In-the-wild", "Open-source toolbox", "Unconstrained image"], "Date": "2017", "Abstract": "Automatic identity recognition from ear images represents an active field of research within the biometric community. The ability to capture ear images from a distance and in a covert manner makes the technology an appealing choice for surveillance and security applications as well as other application domains. Significant contributions have been made in the field over recent years, but open research problems still remain and hinder a wider (commercial) deployment of the technology. This paper presents an overview of the field of automatic ear recognition (from 2D images) and focuses specifically on the most recent, descriptor-based methods proposed in this area. Open challenges are discussed and potential research directions are outlined with the goal of providing the reader with a point of reference for issues worth examining in the future. In addition to a comprehensive review on ear recognition technology, the paper also introduces a new, fully unconstrained dataset of ear images gathered from the web and a toolbox implementing several state-of-the-art techniques for ear recognition. The dataset and toolbox are meant to address some of the open issues in the field and are made publicly available to the research community.", "Language": "en", "Citations": "56"},
{"Title": "Extremal 1-codes in distance-regular graphs of diameter 3", "Authors": ["Jurisic A.", "Vidali J."], "Keywords": ["1-codes", "Algebraic combinatorics", "Distance-regular graphs", "Krein condition", "Nonexistence", "Triple intersection numbers"], "Date": "2012", "Abstract": "We study 1-codes in distance-regular graphs of diameter 3 that achieve three different bounds. We show that the intersection array of a distance-regular graph containing such a code has the form {a(p+1), cp, a+1; 1, c, a p} or {a(p+1), (a+1)p,c; 1, c, a p} for c = c ", "Language": "en", "Citations": "7"},
{"Title": "Mining text enriched heterogeneous citation networks", "Authors": ["Kralj J.", "Valmarska A.", "Robnik-Sikonja M.", "Lavrac N."], "Keywords": ["Centroid classifier", "Document categorization", "Heterogeneous information networks", "Network analysis", "PageRank", "Text mining"], "Date": "2015", "Abstract": "The paper presents an approach to mining text enriched heterogeneous information networks, applied to a task of categorizing papers from a large citation network of scientific publications in the field of psychology. The methodology performs network propositionalization by calculating structural context vectors from homogeneous networks, extracted from the original network. The classifier is constructed from a table of structural context vectors, enriched with the bag-of-words vectors calculated from individual paper abstracts. A series of experiments was performed to examine the impact of increasing the number of publications in the network, and adding different types of structural context vectors. The results indicate that increasing the network size and combining both types of information is beneficial for improving the accuracy of paper categorization.", "Language": "en", "Citations": "3"},
{"Title": "The influence of diffusion of innovation theory factors on undergraduate students' adoption of scrum", "Authors": ["Mahnic V.", "Hovelja T."], "Keywords": ["Agile software development", "Capstone course", "Diffusion of innovation", "Scrum", "Software engineering education"], "Date": "2016", "Abstract": "Since Scrum is the most widespread agile software development method, teaching it is an important issue to prepare students for their professional careers. Scrum is often taught within the scope of a software engineering capstone course, which makes it possible for students to learn Scrum practices through practical project work. In this study, we use the Diffusion of Innovation theory (DOI) to analyze to what stage such a course enables students to assimilate the core Scrum practices and the factors that have the most impact on Scrum adoption. The study is based on the results of a survey that was conducted after each Sprint of the capstone course at the University of Ljubljana, Slovenia; the course has four Sprints and was attended by 88 undergraduates. It is shown that at the end of the course, all core Scrum practices reach either the acceptance or the routinization stage, and 11 most influential DOI factors are identified.", "Language": "en", "Citations": "3"},
{"Title": "Rewrite rules for debugging student programs in programming tutors", "Authors": ["Lazar T.", "Sadikov A.", "Bratko I."], "Keywords": ["Automatic debugging", "Computer-assisted instruction", "Intelligent tutoring systems", "Program synthesis"], "Date": "2018", "Abstract": "Data-driven intelligent tutoring systems learn to provide feedback based on past student behavior, reducing the effort required for their development. A major obstacle to applying data-driven methods in the programming domain is the lack of meaningful observable actions for describing the students problem-solving process. We propose rewrite rules as a language-independent formalization of programming actions in terms of code edits. We describe a method for automatically extracting rewrite rules from students program-writing traces, and a method for debugging new programs using these rules. We used these methods to automatically provide hints in a web application for learning programming. In-class evaluation showed that students receiving automatic feedback solved problems faster and submitted fewer incorrect programs. We believe that rewrite rules provide a good basis for further research into how humans write and debug programs.", "Language": "en", "Citations": "1"},
{"Title": "Automatic transcription of bell chiming recordings", "Authors": ["Marolt M."], "Keywords": ["Audio systems", "bell chiming", "music transcription", "signal analysis"], "Date": "2012", "Abstract": "Bell chiming is a folk music tradition that involves performers playing rhythmic patterns on church bells. The paper presents a method for automatic transcription of bell chiming recordings, where the goal is to detect the bells that were played and their onset times. We first present an algorithm that estimates the number of bells in a recording and their approximate spectra. The algorithm uses a modified version of the intelligent k-means algorithm, as well as some prior knowledge of church bell acoustics to find clusters of partials with synchronous onsets in the time-frequency representation of a recording. Cluster centers are used to initialize non-negative matrix factorization that factorizes the time-frequency representation into a set of basis vectors (bell spectra) and their activations. To transcribe a recording, we propose a probabilistic framework that integrates factorization and onset detection data with prior knowledge of bell chiming performance rules. Both parts of the algorithm are evaluated on a set of bell chiming field recordings. \u00a9 2006 IEEE.", "Language": "en", "Citations": "8"},
{"Title": "A survey of processors with explicit multithreading", "Authors": ["Ungerer T.", "Robic B.", "Silc J."], "Keywords": ["Blocked multithreading", "Interleaved multithreading", "Simultaneous multithreading"], "Date": "2003", "Abstract": "Hardware multithreading is becoming a generally applied technique in the next generation of microprocessors. Several multithreaded processors are announced by industry or already into production in the areas of high-performance microprocessors, media, and network processors. A multithreaded processor is able to pursue two or more threads of control in parallel within the processor pipeline. The contexts of two or more threads of control are often stored in separate on-chip register sets. Unused instruction slots, which arise from latencies during the pipelined execution of single-threaded programs by a contemporary microprocessor, are filled by instructions of other threads within a multithreaded processor. The execution units are multiplexed between the thread contexts that are loaded in the register sets. Underutilization of a superscalar processor due to missing instruction-level parallelism can be overcome by simultaneous multithreading, where a processor can issue multiple instructions from multiple threads each cycle. Simultaneous multithreaded processors combine the multithreading technique with a wide-issue superscalar processor to utilize a larger part of the issue bandwidth by issuing instructions from different threads simultaneously. Explicit multithreaded processors are multithreaded processors that apply processes or operating system threads in their hardware thread slots. These processors optimize the throughput of multiprogramming workloads rather than single-thread performance. We distinguish these processors from implicit multithreaded processors \u00a9 2003 ACM.", "Language": "en", "Citations": "113"},
{"Title": "On pairs of commuting nilpotent matrices", "Authors": ["Kosir T.", "Oblak P."], "Keywords": [], "Date": "2009", "Abstract": "Let B be a nilpotent matrix and suppose that its Jordan canonical form is determined by a partition \u03bb. Then it is known that its nilpotent commutator NB is an irreducible variety and that there is a unique partition \u03bc such that the intersection of the orbit of nilpotent matrices corresponding to \u03bc with NB is dense in NB . We prove that map D given by D (\u03bb) = \u03bc is an idempotent map. This answers a question of Basili and Iarrobino [9] and gives a partial answer to a question of Panyushev [18]. In the proof, we use the fact that for a generic matrix A NB the algebra generated by A and B is a Gorenstein algebra. Thus, a generic pair of commuting nilpotent matrices generates a Gorenstein algebra. We also describe D (\u03bb) in terms of \u03bb if D (\u03bb) has at most two parts. \u00a9 2008 Birkh\u00e4user Boston.", "Language": "en", "Citations": "7"},
{"Title": "Diurnal changes of heart rate and sympathovagal activity for temporal patterns of transient ischemic episodes in 24-hour electrocardiograms", "Authors": ["Smrdel A.", "Jager F."], "Keywords": [], "Date": "2007", "Abstract": "We test the hypothesis that different temporal patterns of transient ST segment changes compatible with ischemia (ischemic episodes) are a result of different physiologic mechanisms responsible for ischemia. We tested the hypothesis using records of the Long-Term ST Database. Each record was divided into three intervals of records: morning, day, and night intervals; and was inserted into one of three sets according to the temporal pattern of ischemia: salvo, periodic, and sporadic pattern. We derived time- and frequency-domain parameters of the heart rate time series in selected intervals in the neighborhood of ischemic episodes. We used the adaptive autoregressive method with a recursive least-square algorithm for consistent spectral tracking of heart rate time series and to study frequency-domain sympathovagal behavior during ischemia. The results support the hypothesis that there are at least two distinct populations, which differ according to mechanisms and temporal patterns of ischemia.", "Language": "en", "Citations": "1"}
]