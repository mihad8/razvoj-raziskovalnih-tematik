<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/29644438050</prism:url><dc:identifier>SCOPUS_ID:29644438050</dc:identifier><eid>2-s2.0-29644438050</eid><dc:title>Statistical comparisons of classifiers over multiple data sets</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>5271</citedby-count><prism:publicationName>Journal of Machine Learning Research</prism:publicationName><source-id>20969</source-id><prism:issn>15337928 15337928</prism:issn><prism:volume>7</prism:volume><prism:startingPage>1</prism:startingPage><prism:endingPage>30</prism:endingPage><prism:pageRange>1-30</prism:pageRange><prism:coverDate>2006-01-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="7004027053"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Demšar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7004027053</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/29644438050" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=29644438050&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=29644438050&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="7004027053"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Demšar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7004027053</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Comparative studies</author-keyword><author-keyword>Friedman test</author-keyword><author-keyword>Multiple comparisons tests</author-keyword><author-keyword>Statistical methods</author-keyword><author-keyword>Wilcoxon signed ranks test</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Multiple data sets</mainterm><mainterm weight="a" candidate="n">Parametric tests</mainterm><mainterm weight="a" candidate="n">Statistical comparisons</mainterm><mainterm weight="a" candidate="n">Wilcoxon signed ranks test</mainterm></idxterms><subject-areas><subject-area code="1712" abbrev="COMP">Software</subject-area><subject-area code="2207" abbrev="ENGI">Control and Systems Engineering</subject-area><subject-area code="2613" abbrev="MATH">Statistics and Probability</subject-area><subject-area code="1702" abbrev="COMP">Artificial Intelligence</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered day="13" month="07" timestamp="2019-07-13T07:56:43.000043-04:00" year="2019"/><ait:date-sort day="01" month="01" year="2006"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2008 Elsevier B.V., All rights reserved.</copyright><itemidlist><itemid idtype="PUI">43022939</itemid><itemid idtype="CPX">2006019629364</itemid><itemid idtype="SCP">29644438050</itemid><itemid idtype="SGR">29644438050</itemid></itemidlist><history><date-created day="06" month="01" year="2006"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">Comparative studies</author-keyword><author-keyword xml:lang="eng">Friedman test</author-keyword><author-keyword xml:lang="eng">Multiple comparisons tests</author-keyword><author-keyword xml:lang="eng">Statistical methods</author-keyword><author-keyword xml:lang="eng">Wilcoxon signed ranks test</author-keyword></author-keywords></citation-info><citation-title><titletext original="y" xml:lang="eng" language="English">Statistical comparisons of classifiers over multiple data sets</titletext></citation-title><author-group><author auid="7004027053" seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Demšar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><address-part>Tržaška 25</address-part><city-group>Ljubljana</city-group><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname></person><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><address-part>Tržaška 25</address-part><city-group>Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.</ce:para></abstract></abstracts><source country="usa" srcid="20969" type="j"><sourcetitle>Journal of Machine Learning Research</sourcetitle><sourcetitle-abbrev>J. Mach. Learn. Res.</sourcetitle-abbrev><issn type="print">15337928</issn><issn type="electronic">15337928</issn><volisspag><voliss volume="7"/><pagerange first="1" last="30"/></volisspag><publicationyear first="2006"/><publicationdate><year>2006</year><month>01</month><date-text xfab-added="true">January 2006</date-text></publicationdate><website><ce:e-address type="url">http://jmlr.csail.mit.edu/papers/volume7/demsar06a/demsar06a.pdf</ce:e-address></website></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification>721.1</classification><classification>723.2</classification><classification>723.4</classification><classification>921.4</classification><classification>922.2</classification></classifications><classifications type="ASJC"><classification>1712</classification><classification>2207</classification><classification>2613</classification><classification>1702</classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification><classification>ENGI</classification><classification>MATH</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="44"><reference id="1"><ref-info><ref-title><ref-titletext>Combined 5 × 2 F test for comparing supervised classification learning algorithms</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0033570831</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.</ce:initials><ce:indexed-name>Alpaydm E.</ce:indexed-name><ce:surname>Alpaydm</ce:surname></author></ref-authors><ref-sourcetitle>Neural Computation</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="11"/><pagerange first="1885" last="1892"/></ref-volisspag></ref-info><ref-fulltext>E. Alpaydm. Combined 5 × 2 F test for comparing supervised classification learning algorithms. Neural Computation, 11:1885-1892, 1999.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>The use of ROC curves in test performance evaluation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0022500965</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.R.</ce:initials><ce:indexed-name>Beck J.R.</ce:indexed-name><ce:surname>Beck</ce:surname></author><author seq="2"><ce:initials>E.K.</ce:initials><ce:indexed-name>Schultz E.K.</ce:indexed-name><ce:surname>Schultz</ce:surname></author></ref-authors><ref-sourcetitle>Arch Pathol Lab Med</ref-sourcetitle><ref-publicationyear first="1986"/><ref-volisspag><voliss volume="110"/><pagerange first="13" last="20"/></ref-volisspag></ref-info><ref-fulltext>J. R. Beck and E. K. Schultz. The use of ROC curves in test performance evaluation. Arch Pathol Lab Med, 110:13-20, 1986.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Intelligent data analysis in medicine and pharmacology: A position statement</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">29644445986</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Bellazzi R.</ce:indexed-name><ce:surname>Bellazzi</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Zupan B.</ce:indexed-name><ce:surname>Zupan</ce:surname></author></ref-authors><ref-sourcetitle>IDAMAP Workshop Notes at the 13th European Conference on Artificial Intelligence, ECAI-98</ref-sourcetitle><ref-publicationyear first="1998"/><ref-text>Brighton, UK</ref-text></ref-info><ref-fulltext>R. Bellazzi and B. Zupan. Intelligent data analysis in medicine and pharmacology: a position statement. In IDAMAP Workshop Notes at the 13th European Conference on Artificial Intelligence, ECAI-98, Brighton, UK, 1998.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>No unbiased estimator of the variance of k-fold cross-validation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84925604888</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Grandvalet Y.</ce:indexed-name><ce:surname>Grandvalet</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Machine Learning Research</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="5"/><pagerange first="1089" last="1105"/></ref-volisspag></ref-info><ref-fulltext>Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of k-fold cross-validation. Journal of Machine Learning Research, 5:1089-1105, 2004.</ref-fulltext></reference><reference id="5"><ref-info><refd-itemidlist><itemid idtype="SGR">0003408496</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.L.</ce:initials><ce:indexed-name>Blake C.L.</ce:indexed-name><ce:surname>Blake</ce:surname></author><author seq="2"><ce:initials>C.J.</ce:initials><ce:indexed-name>Merz C.J.</ce:indexed-name><ce:surname>Merz</ce:surname></author></ref-authors><ref-sourcetitle>UCI Repository of Machine Learning Databases</ref-sourcetitle><ref-publicationyear first="1998"/><ref-website><ce:e-address type="url">http://www.ics.uci.edu/~mlearn/MLRepository.html</ce:e-address></ref-website></ref-info><ref-fulltext>C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998. URL http://www.ics.uci.edu/~mlearn/MLRepository.html.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Choosing between two learning algorithms based on calibrated tests</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">1942452791</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.R.</ce:initials><ce:indexed-name>Bouckaert R.R.</ce:indexed-name><ce:surname>Bouckaert</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA</ref-sourcetitle><ref-publicationyear first="2003"/><ref-text>T. Fawcett and N. Mishra, editors, AAAI Press</ref-text></ref-info><ref-fulltext>R. R. Bouckaert. Choosing between two learning algorithms based on calibrated tests. In T. Fawcett and N. Mishra, editors, Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA. AAAI Press, 2003.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Estimating replicability of classifier learning experiments</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">14544275490</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.R.</ce:initials><ce:indexed-name>Bouckaert R.R.</ce:indexed-name><ce:surname>Bouckaert</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004)</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>C Brodley, editor, AAAI Press</ref-text></ref-info><ref-fulltext>R. R. Bouckaert. Estimating replicability of classifier learning experiments. In C Brodley, editor, Machine Learning, Proceedings of the Twenty-First International Conference (ICML 2004). AAAI Press, 2004.</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Evaluating the replicability of significance tests for comparing learning algorithms</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33644955890</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.R.</ce:initials><ce:indexed-name>Bouckaert R.R.</ce:indexed-name><ce:surname>Bouckaert</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Frank E.</ce:indexed-name><ce:surname>Frank</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Knowledge Discovery and Data Mining, 8th Pacific-Asia Conference, PAKDD 2004, Sydney, Australia, May 26-28, 2004, Proceedings</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>D. Honghua, R. Srikant, and C. Zhang, editors, Springer</ref-text></ref-info><ref-fulltext>R. R. Bouckaert and E. Frank. Evaluating the replicability of significance tests for comparing learning algorithms. In D. Honghua, R. Srikant, and C. Zhang, editors, Advances in Knowledge Discovery and Data Mining, 8th Pacific-Asia Conference, PAKDD 2004, Sydney, Australia, May 26-28, 2004, Proceedings. Springer, 2004.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>A comparison of ranking methods for classification algorithm selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0013058980</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.B.</ce:initials><ce:indexed-name>Brazdil P.B.</ce:indexed-name><ce:surname>Brazdil</ce:surname></author><author seq="2"><ce:initials>C.</ce:initials><ce:indexed-name>Soares C.</ce:indexed-name><ce:surname>Soares</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of 11th European Conference on Machine Learning</ref-sourcetitle><ref-publicationyear first="2000"/><ref-text>Springer Verlag</ref-text></ref-info><ref-fulltext>P. B. Brazdil and C. Soares. A comparison of ranking methods for classification algorithm selection. In Proceedings of 11th European Conference on Machine Learning. Springer Verlag, 2000.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Robust locally weighted regression and smoothing scatterplots</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84936916896</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>W.</ce:initials><ce:indexed-name>Cleveland W.</ce:indexed-name><ce:surname>Cleveland</ce:surname></author></ref-authors><ref-sourcetitle>Journal of the American Statistical Association</ref-sourcetitle><ref-publicationyear first="1979"/><ref-volisspag><voliss volume="74"/><pagerange first="329" last="336"/></ref-volisspag></ref-info><ref-fulltext>W. Cleveland. Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association, 74:329-336, 1979.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>The earth is round (p &lt;.05)</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0039802908</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Cohen J.</ce:indexed-name><ce:surname>Cohen</ce:surname></author></ref-authors><ref-sourcetitle>American Psychologist</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="49"/><pagerange first="997" last="1003"/></ref-volisspag></ref-info><ref-fulltext>J. Cohen. The earth is round (p &lt;.05). American Psychologist, 49:997 1003, 1994.</ref-fulltext></reference><reference id="12"><ref-info><refd-itemidlist><itemid idtype="SGR">13844283290</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Zupan B.</ce:indexed-name><ce:surname>Zupan</ce:surname></author></ref-authors><ref-sourcetitle>Orange: from Experimental Machine Learning to Interactive Data Mining, a White Paper</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>Faculty of Computer and Information Science, Ljubljana, Slovenia</ref-text></ref-info><ref-fulltext>J. Demšar and B. Zupan. Orange: From Experimental Machine Learning to Interactive Data Mining, A White Paper. Faculty of Computer and Information Science, Ljubljana, Slovenia, 2004.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Approximate statistical tests for comparing supervised classification learning algorithms</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000259511</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.G.</ce:initials><ce:indexed-name>Dietterich T.G.</ce:indexed-name><ce:surname>Dietterich</ce:surname></author></ref-authors><ref-sourcetitle>Neural Computation</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><voliss volume="10"/><pagerange first="1895" last="1924"/></ref-volisspag></ref-info><ref-fulltext>T. G. Dietterich. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10:1895-1924, 1998.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Multiple comparisons among means</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84943987463</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.J.</ce:initials><ce:indexed-name>Dunn O.J.</ce:indexed-name><ce:surname>Dunn</ce:surname></author></ref-authors><ref-sourcetitle>Journal of the American Statistical Association</ref-sourcetitle><ref-publicationyear first="1961"/><ref-volisspag><voliss volume="56"/><pagerange first="52" last="64"/></ref-volisspag></ref-info><ref-fulltext>O. J. Dunn. Multiple comparisons among means. Journal of the American Statistical Association, 56:52-64, 1961.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>A multiple comparison procedure for comparing several treatments with a control</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0009663357</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.W.</ce:initials><ce:indexed-name>Dunnett C.W.</ce:indexed-name><ce:surname>Dunnett</ce:surname></author></ref-authors><ref-sourcetitle>Journal of American Statistical Association</ref-sourcetitle><ref-publicationyear first="1980"/><ref-volisspag><voliss volume="50"/><pagerange first="1096" last="1121"/></ref-volisspag></ref-info><ref-fulltext>C. W. Dunnett. A multiple comparison procedure for comparing several treatments with a control. Journal of American Statistical Association, 50:1096-1121, 1980.</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Multi-interval discretization of continuous valued attributes for classification learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002593344</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>U.M.</ce:initials><ce:indexed-name>Fayyad U.M.</ce:indexed-name><ce:surname>Fayyad</ce:surname></author><author seq="2"><ce:initials>K.B.</ce:initials><ce:indexed-name>Irani K.B.</ce:indexed-name><ce:surname>Irani</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 13th International Joint Conference on Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="1993"/><ref-volisspag><pagerange first="1022" last="1029"/></ref-volisspag><ref-text>Chambery, France. Morgan-Kaufmann</ref-text></ref-info><ref-fulltext>U. M. Fayyad and K. B. Irani. Multi-interval discretization of continuous valued attributes for classification learning. In Proceedings of the 13th International Joint Conference on Artificial Intelligence, pages 1022-1029, Chambery, France, 1993. Morgan-Kaufmann.</ref-fulltext></reference><reference id="17"><ref-info><refd-itemidlist><itemid idtype="SGR">0003459504</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.A.</ce:initials><ce:indexed-name>Fisher R.A.</ce:indexed-name><ce:surname>Fisher</ce:surname></author></ref-authors><ref-sourcetitle>Statistical Methods and Scientific Inference (2nd Edition)</ref-sourcetitle><ref-publicationyear first="1959"/><ref-text>Hafner Publishing Co., New York</ref-text></ref-info><ref-fulltext>R. A. Fisher. Statistical methods and scientific inference (2nd edition). Hafner Publishing Co., New York, 1959.</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>The use of ranks to avoid the assumption of normality implicit in the analysis of variance</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84944811700</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Friedman M.</ce:indexed-name><ce:surname>Friedman</ce:surname></author></ref-authors><ref-sourcetitle>Journal of the American Statistical Association</ref-sourcetitle><ref-publicationyear first="1937"/><ref-volisspag><voliss volume="32"/><pagerange first="675" last="701"/></ref-volisspag></ref-info><ref-fulltext>M. Friedman. The use of ranks to avoid the assumption of normality implicit in the analysis of variance. Journal of the American Statistical Association, 32:675-701, 1937.</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>A comparison of alternative tests of significance for the problem of m rankings</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001837148</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Friedman M.</ce:indexed-name><ce:surname>Friedman</ce:surname></author></ref-authors><ref-sourcetitle>Annals of Mathematical Statistics</ref-sourcetitle><ref-publicationyear first="1940"/><ref-volisspag><voliss volume="11"/><pagerange first="86" last="92"/></ref-volisspag></ref-info><ref-fulltext>M. Friedman. A comparison of alternative tests of significance for the problem of m rankings. Annals of Mathematical Statistics, 11:86-92, 1940.</ref-fulltext></reference><reference id="20"><ref-info><refd-itemidlist><itemid idtype="SGR">0003644852</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.C.</ce:initials><ce:indexed-name>Hamilton L.C.</ce:indexed-name><ce:surname>Hamilton</ce:surname></author></ref-authors><ref-sourcetitle>Modern Data Analysis: A First Course in Applied Statistics</ref-sourcetitle><ref-publicationyear first="1990"/><ref-text>Wadsworth, Belmont, California</ref-text></ref-info><ref-fulltext>L. C. Hamilton. Modern Data Analysis: A First Course in Applied Statistics. Wadsworth, Belmont, California, 1990.</ref-fulltext></reference><reference id="21"><ref-info><refd-itemidlist><itemid idtype="SGR">0003706919</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.L.</ce:initials><ce:indexed-name>Harlow L.L.</ce:indexed-name><ce:surname>Harlow</ce:surname></author><author seq="2"><ce:initials>S.A.</ce:initials><ce:indexed-name>Mulaik S.A.</ce:indexed-name><ce:surname>Mulaik</ce:surname></author></ref-authors><ref-sourcetitle>What if There Were No Significance Tests?</ref-sourcetitle><ref-publicationyear first="1997"/><ref-text>Lawrence Erlbaum Associates, July</ref-text></ref-info><ref-fulltext>L. L. Harlow and S. A. Mulaik, editors. What If There Were No Significance Tests? Lawrence Erlbaum Associates, July 1997.</ref-fulltext></reference><reference id="22"><ref-info><ref-title><ref-titletext>A sharper Bonferroni procedure for multiple tests of significance</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33645762226</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Hochberg Y.</ce:indexed-name><ce:surname>Hochberg</ce:surname></author></ref-authors><ref-sourcetitle>Biometrika</ref-sourcetitle><ref-publicationyear first="1988"/><ref-volisspag><voliss volume="75"/><pagerange first="800" last="803"/></ref-volisspag></ref-info><ref-fulltext>Y. Hochberg. A sharper Bonferroni procedure for multiple tests of significance. Biometrika, 75: 800-803, 1988.</ref-fulltext></reference><reference id="23"><ref-info><ref-title><ref-titletext>On the application of three modified Bonferroni procedures to pairwise multiple comparisons in balanced repeated measures designs</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">18244400004</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Holland B.</ce:indexed-name><ce:surname>Holland</ce:surname></author></ref-authors><ref-sourcetitle>Computational Statistics Quarterly</ref-sourcetitle><ref-publicationyear first="1991"/><ref-volisspag><voliss volume="6"/><pagerange first="219" last="231"/></ref-volisspag></ref-info><ref-fulltext>B. Holland. On the application of three modified Bonferroni procedures to pairwise multiple comparisons in balanced repeated measures designs. Computational Statistics Quarterly, 6:219-231, 1991.</ref-fulltext></reference><reference id="24"><ref-info><ref-title><ref-titletext>A simple sequentially rejective multiple test procedure</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002294347</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Holm S.</ce:indexed-name><ce:surname>Holm</ce:surname></author></ref-authors><ref-sourcetitle>Scandinavian Journal of Statistics</ref-sourcetitle><ref-publicationyear first="1979"/><ref-volisspag><voliss volume="6"/><pagerange first="65" last="70"/></ref-volisspag></ref-info><ref-fulltext>S. Holm. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6:65-70, 1979.</ref-fulltext></reference><reference id="25"><ref-info><ref-title><ref-titletext>A stagewise rejective multiple test procedure based on a modified Bonferroni test</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001669952</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Hommel G.</ce:indexed-name><ce:surname>Hommel</ce:surname></author></ref-authors><ref-sourcetitle>Biometrika</ref-sourcetitle><ref-publicationyear first="1988"/><ref-volisspag><voliss volume="75"/><pagerange first="383" last="386"/></ref-volisspag></ref-info><ref-fulltext>G. Hommel. A stagewise rejective multiple test procedure based on a modified Bonferroni test. Biometrika, 75:383-386, 1988.</ref-fulltext></reference><reference id="26"><ref-info><refd-itemidlist><itemid idtype="SGR">19544363914</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.A.</ce:initials><ce:indexed-name>Hull D.A.</ce:indexed-name><ce:surname>Hull</ce:surname></author></ref-authors><ref-sourcetitle>Information Retrieval Using Statistical Classification</ref-sourcetitle><ref-publicationyear first="1994"/><ref-text>PhD thesis, Stanford University, November</ref-text></ref-info><ref-fulltext>D. A. Hull. Information Retrieval Using Statistical Classification. PhD thesis, Stanford University, November 1994.</ref-fulltext></reference><reference id="27"><ref-info><ref-title><ref-titletext>Approximations of the critical region of the Friedman statistic</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001750957</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.L.</ce:initials><ce:indexed-name>Iman R.L.</ce:indexed-name><ce:surname>Iman</ce:surname></author><author seq="2"><ce:initials>J.M.</ce:initials><ce:indexed-name>Davenport J.M.</ce:indexed-name><ce:surname>Davenport</ce:surname></author></ref-authors><ref-sourcetitle>Communications in Statistics</ref-sourcetitle><ref-publicationyear first="1980"/><ref-volisspag><pagerange first="571" last="595"/></ref-volisspag></ref-info><ref-fulltext>R. L. Iman and J. M. Davenport. Approximations of the critical region of the Friedman statistic. Communications in Statistics, pages 571-595, 1980.</ref-fulltext></reference><reference id="28"><ref-info><ref-title><ref-titletext>Crafting papers on machine learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">18744411494</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Langley P.</ce:indexed-name><ce:surname>Langley</ce:surname></author></ref-authors><ref-sourcetitle>Proc. of Seventeenth International Conference on Machine Learning (ICML-2000)</ref-sourcetitle><ref-publicationyear first="2000"/></ref-info><ref-fulltext>P. Langley. Crafting papers on machine learning. In Proc. of Seventeenth International Conference on Machine Learning (ICML-2000), 2000.</ref-fulltext></reference><reference id="29"><ref-info><ref-title><ref-titletext>Feature selection for unbalanced class distribution and naive bayes</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002551285</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Mladenic D.</ce:indexed-name><ce:surname>Mladenić</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Grobelnik M.</ce:indexed-name><ce:surname>Grobelnik</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning, Proceedings of the Sixteenth International Conference (ICML 1999), June 27-30, 2002, Bled, Slovenia</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="258" last="267"/></ref-volisspag><ref-text>I. Bratko and S. Džeroski, editors. Morgan Kaufmann</ref-text></ref-info><ref-fulltext>D. Mladenić and M. Grobelnik. Feature selection for unbalanced class distribution and naive bayes. In I. Bratko and S. Džeroski, editors, Machine Learning, Proceedings of the Sixteenth International Conference (ICML 1999), June 27-30, 2002, Bled, Slovenia, pages 258-267. Morgan Kaufmann, 1999.</ref-fulltext></reference><reference id="30"><ref-info><ref-title><ref-titletext>Inference for the generalization error</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0006494973</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Nadeau C.</ce:indexed-name><ce:surname>Nadeau</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="12"/><pagerange first="239" last="281"/></ref-volisspag></ref-info><ref-fulltext>C. Nadeau and Y. Bengio. Inference for the generalization error. Advances in Neural Information Processing Systems, 12:239-281, 2000.</ref-fulltext></reference><reference id="31"><ref-info><refd-itemidlist><itemid idtype="SGR">0003653217</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.B.</ce:initials><ce:indexed-name>Nemenyi P.B.</ce:indexed-name><ce:surname>Nemenyi</ce:surname></author></ref-authors><ref-sourcetitle>Distribution-free Multiple Comparisons</ref-sourcetitle><ref-publicationyear first="1963"/><ref-text>PhD thesis, Princeton University</ref-text></ref-info><ref-fulltext>P. B. Nemenyi. Distribution-free multiple comparisons. PhD thesis, Princeton University, 1963.</ref-fulltext></reference><reference id="32"><ref-info><ref-title><ref-titletext>Multiple comparison procedures applied to model selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036825523</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Pizarro J.</ce:indexed-name><ce:surname>Pizarro</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Guerrero E.</ce:indexed-name><ce:surname>Guerrero</ce:surname></author><author seq="3"><ce:initials>P.L.</ce:initials><ce:indexed-name>Galindo P.L.</ce:indexed-name><ce:surname>Galindo</ce:surname></author></ref-authors><ref-sourcetitle>Neurocomputing</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="48"/><pagerange first="155" last="173"/></ref-volisspag></ref-info><ref-fulltext>J. Pizarro, E. Guerrero, and P. L. Galindo. Multiple comparison procedures applied to model selection. Neurocomputing, 48:155-173, 2002.</ref-fulltext></reference><reference id="33"><ref-info><ref-title><ref-titletext>The case against accuracy estimation for comparing induction algorithms</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002900357</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Provost F.</ce:indexed-name><ce:surname>Provost</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Fawcett T.</ce:indexed-name><ce:surname>Fawcett</ce:surname></author><author seq="3"><ce:initials>R.</ce:initials><ce:indexed-name>Kohavi R.</ce:indexed-name><ce:surname>Kohavi</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Fifteenth International Conference on Machine Learning (ICML-1998)</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><pagerange first="445" last="453"/></ref-volisspag><ref-text>J. Shavlik, editor, San Francisco, CA. Morgan Kaufmann Publishers</ref-text></ref-info><ref-fulltext>F. Provost, T. Fawcett, and R. Kohavi. The case against accuracy estimation for comparing induction algorithms. In J. Shavlik, editor, Proceedings of the Fifteenth International Conference on Machine Learning (ICML-1998), pages 445-453, San Francisco, CA, 1998. Morgan Kaufmann Publishers.</ref-fulltext></reference><reference id="34"><ref-info><ref-title><ref-titletext>Bagging, boosting, and c4.5</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0030370417</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.R.</ce:initials><ce:indexed-name>Quinlan J.R.</ce:indexed-name><ce:surname>Quinlan</ce:surname></author></ref-authors><ref-sourcetitle>Proc. Thirteenth National Conference on Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><pagerange first="725" last="730"/></ref-volisspag><ref-text>Portland, OR. AAAI Press</ref-text></ref-info><ref-fulltext>J. R. Quinlan. Bagging, boosting, and c4.5. In Proc. Thirteenth National Conference on Artificial Intelligence, pages 725-730, Portland, OR, 1996. AAAI Press.</ref-fulltext></reference><reference id="35"><ref-info><ref-title><ref-titletext>On comparing classifiers: Pitfalls to avoid and a recommended approach</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">27144463192</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.L.</ce:initials><ce:indexed-name>Salzberg S.L.</ce:indexed-name><ce:surname>Salzberg</ce:surname></author></ref-authors><ref-sourcetitle>Data Mining and Knowledge Discovery</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="1"/><pagerange first="317" last="328"/></ref-volisspag></ref-info><ref-fulltext>S. L. Salzberg. On comparing classifiers: Pitfalls to avoid and a recommended approach. Data Mining and Knowledge Discovery, 1:317-328, 1997.</ref-fulltext></reference><reference id="36"><ref-info><ref-title><ref-titletext>Statistical significance testing and cumulative knowledge in psychology</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000724985</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.L.</ce:initials><ce:indexed-name>Schmidt F.L.</ce:indexed-name><ce:surname>Schmidt</ce:surname></author></ref-authors><ref-sourcetitle>Psychological Methods</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><voliss volume="1"/><pagerange first="115" last="129"/></ref-volisspag></ref-info><ref-fulltext>F. L. Schmidt. Statistical significance testing and cumulative knowledge in psychology. Psychological Methods, 1:115-129, 1996.</ref-fulltext></reference><reference id="37"><ref-info><ref-title><ref-titletext>A comparison of classifiers and document representations for the routing problem</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0029206376</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Schutze H.</ce:indexed-name><ce:surname>Schütze</ce:surname></author><author seq="2"><ce:initials>D.A.</ce:initials><ce:indexed-name>Hull D.A.</ce:indexed-name><ce:surname>Hull</ce:surname></author><author seq="3"><ce:initials>J.O.</ce:initials><ce:indexed-name>Pedersen J.O.</ce:indexed-name><ce:surname>Pedersen</ce:surname></author></ref-authors><ref-sourcetitle>SIGIR'95, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</ref-sourcetitle><ref-publicationyear first="1995"/><ref-volisspag><pagerange first="229" last="237"/></ref-volisspag><ref-text>E. A. Fox, P. Ingwersen, and R. Fidel, editors. ACM Press</ref-text></ref-info><ref-fulltext>H. Schütze, D. A. Hull, and J. O. Pedersen. A comparison of classifiers and document representations for the routing problem. In E. A. Fox, P. Ingwersen, and R. Fidel, editors, SIGIR'95, Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 229-237. ACM Press, 1995.</ref-fulltext></reference><reference id="38"><ref-info><ref-title><ref-titletext>Multiple hypothesis testing</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">11944262819</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.P.</ce:initials><ce:indexed-name>Shaffer J.P.</ce:indexed-name><ce:surname>Shaffer</ce:surname></author></ref-authors><ref-sourcetitle>Annual Review of Psychology</ref-sourcetitle><ref-publicationyear first="1995"/><ref-volisspag><voliss volume="46"/><pagerange first="561" last="584"/></ref-volisspag></ref-info><ref-fulltext>J. P. Shaffer. Multiple hypothesis testing. Annual Review of Psychology, 46:561-584, 1995.</ref-fulltext></reference><reference id="39"><ref-info><refd-itemidlist><itemid idtype="SGR">0003683294</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.J.</ce:initials><ce:indexed-name>Sheskin D.J.</ce:indexed-name><ce:surname>Sheskin</ce:surname></author></ref-authors><ref-sourcetitle>Handbook of Parametric and Nonparametric Statistical Procedures</ref-sourcetitle><ref-publicationyear first="2000"/><ref-text>Chapman &amp; Hall/CRC</ref-text></ref-info><ref-fulltext>D. J. Sheskin. Handbook of parametric and nonparametric statistical procedures. Chapman &amp; Hall/CRC, 2000.</ref-fulltext></reference><reference id="40"><ref-info><ref-title><ref-titletext>Comparing individual means in the analysis of variance</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002196917</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.W.</ce:initials><ce:indexed-name>Tukey J.W.</ce:indexed-name><ce:surname>Tukey</ce:surname></author></ref-authors><ref-sourcetitle>Biometrics</ref-sourcetitle><ref-publicationyear first="1949"/><ref-volisspag><voliss volume="5"/><pagerange first="99" last="114"/></ref-volisspag></ref-info><ref-fulltext>J. W. Tukey. Comparing individual means in the analysis of variance. Biometrics, 5:99-114, 1949.</ref-fulltext></reference><reference id="41"><ref-info><ref-title><ref-titletext>Repeated measures multiple comparison procedures applied to model selection in neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84902202785</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.G.</ce:initials><ce:indexed-name>Vazquez E.G.</ce:indexed-name><ce:surname>Vázquez</ce:surname></author><author seq="2"><ce:initials>A.Y.</ce:initials><ce:indexed-name>Escolano A.Y.</ce:indexed-name><ce:surname>Escolano</ce:surname></author><author seq="3"><ce:initials>J.P.</ce:initials><ce:indexed-name>Junquera J.P.</ce:indexed-name><ce:surname>Junquera</ce:surname></author><author seq="4"><ce:initials>P.G.</ce:initials><ce:indexed-name>Riano P.G.</ce:indexed-name><ce:surname>Riaño</ce:surname></author></ref-authors><ref-sourcetitle>Proc. of the 6th Intl. Conf. on Artificial and Natural Neural Networks (IWANN 2001)</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><pagerange first="88" last="95"/></ref-volisspag></ref-info><ref-fulltext>E. G. Vázquez, A. Y. Escolano, and J. P. Junquera P. G. Riaño. Repeated measures multiple comparison procedures applied to model selection in neural networks. In Proc. of the 6th Intl. Conf. On Artificial and Natural Neural Networks (IWANN 2001), pages 88-95, 2001.</ref-fulltext></reference><reference id="42"><ref-info><ref-title><ref-titletext>Multiboosting: A technique for combining boosting and wagging</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0034247206</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.I.</ce:initials><ce:indexed-name>Webb G.I.</ce:indexed-name><ce:surname>Webb</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="40"/><pagerange first="159" last="197"/></ref-volisspag></ref-info><ref-fulltext>G. I. Webb. Multiboosting: A technique for combining boosting and wagging. Machine Learning, 40:159-197, 2000.</ref-fulltext></reference><reference id="43"><ref-info><ref-title><ref-titletext>Individual comparisons by ranking methods</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001884644</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Wilcoxon F.</ce:indexed-name><ce:surname>Wilcoxon</ce:surname></author></ref-authors><ref-sourcetitle>Biometrics</ref-sourcetitle><ref-publicationyear first="1945"/><ref-volisspag><voliss volume="1"/><pagerange first="80" last="83"/></ref-volisspag></ref-info><ref-fulltext>F. Wilcoxon. Individual comparisons by ranking methods. Biometrics, 1:80-83, 1945.</ref-fulltext></reference><reference id="44"><ref-info><refd-itemidlist><itemid idtype="SGR">0004252445</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.H.</ce:initials><ce:indexed-name>Zar J.H.</ce:indexed-name><ce:surname>Zar</ce:surname></author></ref-authors><ref-sourcetitle>Biostatistical Analysis (4th Edition)</ref-sourcetitle><ref-publicationyear first="1998"/><ref-text>Prentice Hall, Englewood Clifs, New Jersey</ref-text></ref-info><ref-fulltext>J. H. Zar. Biostatistical Analysis (4th Edition). Prentice Hall, Englewood Clifs, New Jersey, 1998.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>