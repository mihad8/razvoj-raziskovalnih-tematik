<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/33846272371</prism:url><dc:identifier>SCOPUS_ID:33846272371</dc:identifier><eid>2-s2.0-33846272371</eid><dc:title>Statistical comparisons of classifiers in machine learning Statistične primerjave klasifikatorjev pri strojnem učenju</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Elektrotehniski Vestnik/Electrotechnical Review</prism:publicationName><source-id>16651</source-id><prism:issn>00135852</prism:issn><prism:volume>73</prism:volume><prism:issueIdentifier>5</prism:issueIdentifier><prism:startingPage>279</prism:startingPage><prism:endingPage>284</prism:endingPage><prism:pageRange>279-284</prism:pageRange><prism:coverDate>2006-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="7004027053"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Demšar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7004027053</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>Comparisons between classifiers are a crucial element in most studies that introduce new machine learning algorithms or modifications of the existing ones. Despite their importance, there is no consensus in the community regarding which test should be applied in a certain situation, and excellent machine learning papers quite often conclude with statistical tests that are conceptually or statistically inappropriate. The situation is especially bad in comparisons of multiple classifiers, where the tests designed for comparisons of two samples are often used on each pair of classifiers instead of using omnibus tests like ANOVA or at least applying the appropriate corrections for multiple hypotheses testing. We analyzed the tests which are or which should (in our opinion) be used in machine learning studies: the paired t-test, the Wilcoxon signed-ranks test and the sign test for comparison of two classifiers, and ANOVA and the Friedman test with appropriate post-hoc tests for comparisons of multiple classifiers [10]. We checked what the tests really measure and what assumptions they make about the data; specifically, the parametric tests require commensurability of the results accross different domains and assume that the results of classifiers are distributed normally. Since both of these conditions for the use of parametric tests are most probably violated, we dissuade from using the parametric tests. On the other hand, the described non-parametric tests suffer from none of these deficiencies. The same conclusion in favour of non-parametric tests is reached in the experimental part of the paper where we compare the tests on a selection of standard machine learning algorithms using the data sets from the UCI machine learning repository [1]. The non-parametric tests seem to have more power and be more replicable than the parametric ones both in comparisons between two (Fig. 1) and between multiple classifiers (Fig. 3), with the only exception of the Dunnet test for post-hoc comparisons of one classifier against all others, which rejects a somewhat larger number of hypotheses than the corresponding non-parametric alternative. Altogether, we recommend the use of non-parametric tests for comparisons of classifiers, but warn that other criteria beyond the grasp of statistics should be considered and possibly even favoured over the pure improvements in predictive power of classifiers.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/33846272371" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=33846272371&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=33846272371&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="7004027053"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Demšar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7004027053</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="slv"/><authkeywords><author-keyword>Artificial intelligence</author-keyword><author-keyword>Machine learning</author-keyword><author-keyword>Statistical tests</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Hypotheses testing</mainterm><mainterm weight="a" candidate="n">Post-hoc tests</mainterm><mainterm weight="a" candidate="n">Statistical comparisons</mainterm></idxterms><subject-areas><subject-area code="2208" abbrev="ENGI">Electrical and Electronic Engineering</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2018" month="10" day="24" timestamp="2018-10-24T08:07:22.000022-04:00"/><ait:date-sort year="2006" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2008 Elsevier B.V., All rights reserved.</copyright><itemidlist><itemid idtype="PUI">46113994</itemid><itemid idtype="CPX">20070410383972</itemid><itemid idtype="SCP">33846272371</itemid><itemid idtype="SGR">33846272371</itemid></itemidlist><history><date-created year="2007" month="01" day="23"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="slv" language="Slovenian"/><abstract-language xml:lang="eng" language="English"/><abstract-language xml:lang="slv" language="Slovenian"/><author-keywords><author-keyword xml:lang="eng">Artificial intelligence</author-keyword><author-keyword xml:lang="eng">Machine learning</author-keyword><author-keyword xml:lang="eng">Statistical tests</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="n" language="English">Statistical comparisons of classifiers in machine learning</titletext><titletext xml:lang="slv" original="y" language="Slovenian">Statistične primerjave klasifikatorjev pri strojnem učenju</titletext></citation-title><author-group><author auid="7004027053" seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Demšar J.</ce:indexed-name><ce:surname>Demšar</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name></author><affiliation afid="60031106" dptid="104580817" country="svn"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za Računalništvo in Informatiko</organization><address-part>Tržaška 25</address-part><city-group>1000 Ljubljana</city-group><affiliation-id afid="60031106" dptid="104580817"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname></person><affiliation country="svn"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za Računalništvo in Informatiko</organization><address-part>Tržaška 25</address-part><city-group>1000 Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>Comparisons between classifiers are a crucial element in most studies that introduce new machine learning algorithms or modifications of the existing ones. Despite their importance, there is no consensus in the community regarding which test should be applied in a certain situation, and excellent machine learning papers quite often conclude with statistical tests that are conceptually or statistically inappropriate. The situation is especially bad in comparisons of multiple classifiers, where the tests designed for comparisons of two samples are often used on each pair of classifiers instead of using omnibus tests like ANOVA or at least applying the appropriate corrections for multiple hypotheses testing. We analyzed the tests which are or which should (in our opinion) be used in machine learning studies: the paired t-test, the Wilcoxon signed-ranks test and the sign test for comparison of two classifiers, and ANOVA and the Friedman test with appropriate post-hoc tests for comparisons of multiple classifiers [10]. We checked what the tests really measure and what assumptions they make about the data; specifically, the parametric tests require commensurability of the results accross different domains and assume that the results of classifiers are distributed normally. Since both of these conditions for the use of parametric tests are most probably violated, we dissuade from using the parametric tests. On the other hand, the described non-parametric tests suffer from none of these deficiencies. The same conclusion in favour of non-parametric tests is reached in the experimental part of the paper where we compare the tests on a selection of standard machine learning algorithms using the data sets from the UCI machine learning repository [1]. The non-parametric tests seem to have more power and be more replicable than the parametric ones both in comparisons between two (Fig. 1) and between multiple classifiers (Fig. 3), with the only exception of the Dunnet test for post-hoc comparisons of one classifier against all others, which rejects a somewhat larger number of hypotheses than the corresponding non-parametric alternative. Altogether, we recommend the use of non-parametric tests for comparisons of classifiers, but warn that other criteria beyond the grasp of statistics should be considered and possibly even favoured over the pure improvements in predictive power of classifiers.</ce:para></abstract></abstracts><source srcid="16651" type="j" country="svn"><sourcetitle>Elektrotehniski Vestnik/Electrotechnical Review</sourcetitle><sourcetitle-abbrev>Elektroteh Vestn Electrotech Rev</sourcetitle-abbrev><issn type="print">00135852</issn><codencode>ELVEA</codencode><volisspag><voliss volume="73" issue="5"/><pagerange first="279" last="284"/></volisspag><publicationyear first="2006"/><publicationdate><year>2006</year><date-text xfab-added="true">2006</date-text></publicationdate></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification>716.1</classification><classification>723.4</classification><classification>922.2</classification></classifications><classifications type="ASJC"><classification>2208</classification></classifications><classifications type="SUBJABBR"><classification>ENGI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="12"><reference id="1"><ref-info><refd-itemidlist><itemid idtype="SGR">0003408496</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.L.</ce:initials><ce:indexed-name>Blake C.L.</ce:indexed-name><ce:surname>Blake</ce:surname></author><author seq="2"><ce:initials>C.J.</ce:initials><ce:indexed-name>Merz C.J.</ce:indexed-name><ce:surname>Merz</ce:surname></author></ref-authors><ref-sourcetitle>UCI Repository of machine learning databases</ref-sourcetitle><ref-publicationyear first="1998"/><ref-website><ce:e-address type="url">http://www.ics.uci.edu/~mlearn/MLRepository.html</ce:e-address></ref-website><ref-text>University of California, Irvine</ref-text></ref-info><ref-fulltext>C.L. Blake, C.J. Merz, UCI Repository of machine learning databases, http://www.ics.uci.edu/~mlearn/MLRepository.html, University of California, Irvine, 1998.</ref-fulltext></reference><reference id="2"><ref-info><refd-itemidlist><itemid idtype="SGR">33846283242</itemid></refd-itemidlist><ref-text>R. R. Bouckaert, Estimating Replicability of Classifier Learning Experiments, V C. Brodley, Machine Learning, Proceedings of the Twenty-First International Conference (ICML 2004), Menlo Part, CA, ZDA, AAAI Press, 2004.</ref-text></ref-info><ref-fulltext>R. R. Bouckaert, Estimating Replicability of Classifier Learning Experiments, V C. Brodley, Machine Learning, Proceedings of the Twenty-First International Conference (ICML 2004), Menlo Part, CA, ZDA, AAAI Press, 2004.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Statistical Comparisons of Classifiers over Multiple Data Sets</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">29644438050</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Machine Learning Research</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="7"/><pagerange first="1" last="31"/></ref-volisspag></ref-info><ref-fulltext>J. Demšar, Statistical Comparisons of Classifiers over Multiple Data Sets, Journal of Machine Learning Research 7, 2006, 1-31.</ref-fulltext></reference><reference id="4"><ref-info><refd-itemidlist><itemid idtype="SGR">13844283290</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Demsar J.</ce:indexed-name><ce:surname>Demšar</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Zupan B.</ce:indexed-name><ce:surname>Zupan</ce:surname></author></ref-authors><ref-sourcetitle>Orange: From Experimental Machine Learning to Interactive Data Mining, A White Paper</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>Faculty of Computer and Information Science, Ljubljana, Slovenia</ref-text></ref-info><ref-fulltext>J. Demšar, B. Zupan, Orange: From Experimental Machine Learning to Interactive Data Mining, A White Paper, Faculty of Computer and Information Science, Ljubljana, Slovenia, 2004.</ref-fulltext></reference><reference id="5"><ref-info><refd-itemidlist><itemid idtype="SGR">0003459504</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.A.</ce:initials><ce:indexed-name>Fisher R.A.</ce:indexed-name><ce:surname>Fisher</ce:surname></author></ref-authors><ref-sourcetitle>Statistical methods and scientific inference</ref-sourcetitle><ref-publicationyear first="1959"/><ref-text>2nd edition, New York, Hafner Publishing Co</ref-text></ref-info><ref-fulltext>R. A. Fisher, Statistical methods and scientific inference (2nd edition), New York, Hafner Publishing Co., 1959.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>The use of ranks to avoid the assumption of normality implicit in the analysis of variance</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84944811700</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Friedman M.</ce:indexed-name><ce:surname>Friedman</ce:surname></author></ref-authors><ref-sourcetitle>Journal of the American Statistical Association</ref-sourcetitle><ref-publicationyear first="1937"/><ref-volisspag><voliss volume="32"/><pagerange first="675" last="701"/></ref-volisspag></ref-info><ref-fulltext>M. Friedman, The use of ranks to avoid the assumption of normality implicit in the analysis of variance, Journal of the American Statistical Association (32), 675-701, 1937.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>The meaning and use of the area under a receiver operating characteristic (ROC) curve</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0020083498</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.A.</ce:initials><ce:indexed-name>Hanley J.A.</ce:indexed-name><ce:surname>Hanley</ce:surname></author><author seq="2"><ce:initials>B.J.</ce:initials><ce:indexed-name>McNeil B.J.</ce:indexed-name><ce:surname>McNeil</ce:surname></author></ref-authors><ref-sourcetitle>Radiology</ref-sourcetitle><ref-publicationyear first="1982"/><ref-volisspag><voliss volume="143" issue="1"/><pagerange first="29" last="36"/></ref-volisspag></ref-info><ref-fulltext>J. A. Hanley, B. J. McNeil, The meaning and use of the area under a receiver operating characteristic (ROC) curve, Radiology 143(1), 29-36, 1982.</ref-fulltext></reference><reference id="8"><ref-info><refd-itemidlist><itemid idtype="SGR">0003500248</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.R.</ce:initials><ce:indexed-name>Quinlan J.R.</ce:indexed-name><ce:surname>Quinlan</ce:surname></author></ref-authors><ref-sourcetitle>C4.5: Programs for Machine Learning</ref-sourcetitle><ref-publicationyear first="1993"/><ref-text>San Francisco, Morgan Kaufmann Publishers</ref-text></ref-info><ref-fulltext>J. R. Quinlan, C4.5: Programs for Machine Learning, San Francisco, Morgan Kaufmann Publishers, 1993.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>On comparing classifiers: Pitfalls to avoid and a recommended approach</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">27144463192</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.L.</ce:initials><ce:indexed-name>Salzberg S.L.</ce:indexed-name><ce:surname>Salzberg</ce:surname></author></ref-authors><ref-sourcetitle>Data Mining and Knowledge Discovery</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="1"/><pagerange first="317" last="328"/></ref-volisspag></ref-info><ref-fulltext>S. L. Salzberg, On comparing classifiers: Pitfalls to avoid and a recommended approach, Data Mining and Knowledge Discovery (1), 317-328, 1997.</ref-fulltext></reference><reference id="10"><ref-info><refd-itemidlist><itemid idtype="SGR">0003683294</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.J.</ce:initials><ce:indexed-name>Sheskin D.J.</ce:indexed-name><ce:surname>Sheskin</ce:surname></author></ref-authors><ref-sourcetitle>Handbook of parametric and nonparametric statistical procedures</ref-sourcetitle><ref-publicationyear first="2000"/><ref-text>Chapman &amp; Hall/CRC</ref-text></ref-info><ref-fulltext>D. J. Sheskin, Handbook of parametric and nonparametric statistical procedures, Chapman &amp; Hall/CRC, 2000.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>MultiBoosting: A Technique for Combining Boosting and Wagging</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0034247206</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.I.</ce:initials><ce:indexed-name>Webb G.I.</ce:indexed-name><ce:surname>Webb</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="40"/><pagerange first="159" last="197"/></ref-volisspag></ref-info><ref-fulltext>G. I. Webb, MultiBoosting: A Technique for Combining Boosting and Wagging, Machine Learning 40, 2000, 159-197.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Individual Comparisons by Ranking Methods</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001884644</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Wilcoxon F.</ce:indexed-name><ce:surname>Wilcoxon</ce:surname></author></ref-authors><ref-sourcetitle>Biometrics</ref-sourcetitle><ref-publicationyear first="1945"/><ref-volisspag><voliss volume="1"/><pagerange first="80" last="83"/></ref-volisspag></ref-info><ref-fulltext>F. Wilcoxon, Individual Comparisons by Ranking Methods, Biometrics 1, 80-83, 1945.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>