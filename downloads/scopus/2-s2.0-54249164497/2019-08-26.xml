<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/54249164497</prism:url><dc:identifier>SCOPUS_ID:54249164497</dc:identifier><eid>2-s2.0-54249164497</eid><prism:doi>10.1007/s10489-007-0084-9</prism:doi><dc:title>Estimation of individual prediction reliability using the local sensitivity analysis</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>34</citedby-count><prism:publicationName>Applied Intelligence</prism:publicationName><source-id>23674</source-id><prism:issn>0924669X</prism:issn><prism:volume>29</prism:volume><prism:issueIdentifier>3</prism:issueIdentifier><prism:startingPage>187</prism:startingPage><prism:endingPage>203</prism:endingPage><prism:pageRange>187-203</prism:pageRange><prism:coverDate>2008-12-01</prism:coverDate><openaccess>2</openaccess><openaccessFlag/><dc:creator><author seq="1" auid="23566763400"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/23566763400</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>For a given prediction model, some predictions may be reliable while others may be unreliable. The average accuracy of the system cannot provide the reliability estimate for a single particular prediction. The measure of individual prediction reliability can be important information in risk-sensitive applications of machine learning (e.g. medicine, engineering, business). We define empirical measures for estimation of prediction accuracy in regression. Presented measures are based on sensitivity analysis of regression models. They estimate reliability for each individual regression prediction in contrast to the average prediction reliability of the given regression model. We study the empirical sensitivity properties of five regression models (linear regression, locally weighted regression, regression trees, neural networks, and support vector machines) and the relation between reliability measures and distribution of learning examples with prediction errors for all five regression models. We show that the suggested methodology is appropriate only for the three studied models: regression trees, neural networks, and support vector machines, and test the proposed estimates with these three models. The results of our experiments on 48 data sets indicate significant correlations of the proposed measures with the prediction error. © 2007 Springer Science+Business Media, LLC.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/54249164497" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=54249164497&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=54249164497&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="23566763400"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/23566763400</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="57188535146"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57188535146</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords/><idxterms><mainterm weight="a" candidate="n">Data sets</mainterm><mainterm weight="a" candidate="n">Empirical measures</mainterm><mainterm weight="a" candidate="n">Individual predictions</mainterm><mainterm weight="a" candidate="n">Locally weighted regressions</mainterm><mainterm weight="a" candidate="n">Machine learnings</mainterm><mainterm weight="a" candidate="n">Prediction accuracies</mainterm><mainterm weight="a" candidate="n">Prediction errors</mainterm><mainterm weight="a" candidate="n">Prediction models</mainterm><mainterm weight="a" candidate="n">Regression models</mainterm><mainterm weight="a" candidate="n">Regression trees</mainterm><mainterm weight="a" candidate="n">Reliability measures</mainterm><mainterm weight="a" candidate="n">Reliable</mainterm><mainterm weight="a" candidate="n">Sensitive applications</mainterm><mainterm weight="a" candidate="n">Support vectors</mainterm></idxterms><subject-areas><subject-area code="1702" abbrev="COMP">Artificial Intelligence</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2017-11-08T14:02:53.965Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/nlp</xocs:funding-addon-type></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered day="01" month="07" timestamp="2019-07-01T10:55:49.000049-04:00" year="2019"/><ait:date-sort day="01" month="12" year="2008"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2008 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1007/s10489-007-0084-9</ce:doi><itemid idtype="PUI">352539326</itemid><itemid idtype="CPX">20084411666048</itemid><itemid idtype="SCP">54249164497</itemid><itemid idtype="SGR">54249164497</itemid></itemidlist><history><date-created day="27" month="10" year="2008"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/></citation-info><citation-title><titletext original="y" xml:lang="eng" language="English">Estimation of individual prediction reliability using the local sensitivity analysis</titletext></citation-title><author-group><author auid="23566763400" seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name></author><author auid="57188535146" seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>University of Ljubljana</organization><organization>Faculty of Computer and Information Science</organization><address-part>Tržaška 25</address-part><city-group>Ljubljana</city-group><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname></person><affiliation country="svn"><organization>University of Ljubljana</organization><organization>Faculty of Computer and Information Science</organization><address-part>Tržaška 25</address-part><city-group>Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>For a given prediction model, some predictions may be reliable while others may be unreliable. The average accuracy of the system cannot provide the reliability estimate for a single particular prediction. The measure of individual prediction reliability can be important information in risk-sensitive applications of machine learning (e.g. medicine, engineering, business). We define empirical measures for estimation of prediction accuracy in regression. Presented measures are based on sensitivity analysis of regression models. They estimate reliability for each individual regression prediction in contrast to the average prediction reliability of the given regression model. We study the empirical sensitivity properties of five regression models (linear regression, locally weighted regression, regression trees, neural networks, and support vector machines) and the relation between reliability measures and distribution of learning examples with prediction errors for all five regression models. We show that the suggested methodology is appropriate only for the three studied models: regression trees, neural networks, and support vector machines, and test the proposed estimates with these three models. The results of our experiments on 48 data sets indicate significant correlations of the proposed measures with the prediction error. © 2007 Springer Science+Business Media, LLC.</ce:para></abstract></abstracts><source country="nld" srcid="23674" type="j"><sourcetitle>Applied Intelligence</sourcetitle><sourcetitle-abbrev>Appl Intell</sourcetitle-abbrev><issn type="print">0924669X</issn><codencode>APITE</codencode><volisspag><voliss issue="3" volume="29"/><pagerange first="187" last="203"/></volisspag><publicationyear first="2008"/><publicationdate><year>2008</year><month>12</month><date-text xfab-added="true">December 2008</date-text></publicationdate></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification>732</classification><classification>741</classification><classification>912.2</classification><classification>913</classification><classification>922.2</classification><classification>913.3</classification><classification>921</classification><classification>921.1</classification><classification>922.1</classification><classification>914.1</classification><classification>731.5</classification><classification>731.1</classification><classification>421</classification><classification>422.2</classification><classification>461.1</classification><classification>461.4</classification><classification>723</classification><classification>723.2</classification><classification>723.4</classification><classification>723.5</classification></classifications><classifications type="ASJC"><classification>1702</classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="28"><reference id="1"><ref-info><ref-title><ref-titletext>Stability and generalization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038368335</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Elisseeff A.</ce:indexed-name><ce:surname>Elisseeff</ce:surname></author></ref-authors><ref-sourcetitle>J Mach Learn Res</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="2"/><pagerange first="499" last="526"/></ref-volisspag></ref-info><ref-fulltext>O Bousquet A Elisseeff 2002 Stability and generalization J Mach Learn Res 2 499 526</ref-fulltext></reference><reference id="2"><ref-info><refd-itemidlist><itemid idtype="SGR">54249155295</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.J.</ce:initials><ce:indexed-name>Crowder M.J.</ce:indexed-name><ce:surname>Crowder</ce:surname></author><author seq="2"><ce:initials>A.C.</ce:initials><ce:indexed-name>Kimber A.C.</ce:indexed-name><ce:surname>Kimber</ce:surname></author><author seq="3"><ce:initials>R.L.</ce:initials><ce:indexed-name>Smith R.L.</ce:indexed-name><ce:surname>Smith</ce:surname></author><author seq="4"><ce:initials>T.J.</ce:initials><ce:indexed-name>Sweeting T.J.</ce:indexed-name><ce:surname>Sweeting</ce:surname></author></ref-authors><ref-sourcetitle>Statistical Concepts in Reliability. Statistical Analysis of Reliability Data</ref-sourcetitle><ref-publicationyear first="1991"/><ref-volisspag><pagerange first="1" last="11"/></ref-volisspag><ref-text>Chapman &amp; Hall London</ref-text></ref-info><ref-fulltext>Crowder MJ, Kimber AC, Smith RL, Sweeting TJ (1991) Statistical concepts in reliability. Statistical analysis of reliability data. Chapman &amp; Hall, London, pp 1-11</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Algorithmic stability and generalization performance</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0012296113</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Elisseeff A.</ce:indexed-name><ce:surname>Elisseeff</ce:surname></author></ref-authors><ref-sourcetitle>NIPS</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><pagerange first="196" last="202"/></ref-volisspag></ref-info><ref-fulltext>Bousquet O, Elisseeff A (2000) Algorithmic stability and generalization performance. In: NIPS, pp 196-202</ref-fulltext></reference><reference id="4"><ref-info><refd-itemidlist><itemid idtype="SGR">54249110697</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Pontil M.</ce:indexed-name><ce:surname>Pontil</ce:surname></author></ref-authors><ref-sourcetitle>Leave-one-out Error and Stability of Learning Algorithms with Applications. In: Suykens JAK et Al, Advances in Learning Theory: Methods, Models and Applications</ref-sourcetitle><ref-publicationyear first="2003"/><ref-text>IOS Press Amsterdam</ref-text></ref-info><ref-fulltext>Bousquet O, Pontil M (2003) Leave-one-out error and stability of learning algorithms with applications. In: Suykens JAK et al, Advances in learning theory: methods, models and applications. IOS Press, Amsterdam</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Algorithmic stability and sanity-check bounds for leave-one-out cross-validation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0030654389</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.J.</ce:initials><ce:indexed-name>Kearns M.J.</ce:indexed-name><ce:surname>Kearns</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Ron D.</ce:indexed-name><ce:surname>Ron</ce:surname></author></ref-authors><ref-sourcetitle>Computational Learing Theory</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><pagerange first="152" last="162"/></ref-volisspag></ref-info><ref-fulltext>Kearns MJ, Ron D (1997) Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. In: Computational learing theory, pp 152-162</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Bagging predictors</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0030211964</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author></ref-authors><ref-sourcetitle>Mach Learn</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><voliss volume="24"/><pagerange first="123" last="140"/></ref-volisspag></ref-info><ref-fulltext>L Breiman 1996 Bagging predictors Mach Learn 24 123 140</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>A brief introduction to boosting</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84880692052</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.E.</ce:initials><ce:indexed-name>Schapire R.E.</ce:indexed-name><ce:surname>Schapire</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of IJCAI</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="1401" last="1406"/></ref-volisspag></ref-info><ref-fulltext>Schapire RE (1999) A brief introduction to boosting. In: Proceedings of IJCAI, pp 1401-1406</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Improving regressors using boosting techniques</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000201141</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Drucker H.</ce:indexed-name><ce:surname>Drucker</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning: Proceedings of the Fourteenth International Conference</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><pagerange first="107" last="115"/></ref-volisspag></ref-info><ref-fulltext>Drucker H (1997) Improving regressors using boosting techniques. In: Machine learning: proceedings of the fourteenth international conference, pp 107-115</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Boosting methodology for regression problems</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002311782</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Ridgeway G.</ce:indexed-name><ce:surname>Ridgeway</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Madigan D.</ce:indexed-name><ce:surname>Madigan</ce:surname></author><author seq="3"><ce:initials>T.</ce:initials><ce:indexed-name>Richardson T.</ce:indexed-name><ce:surname>Richardson</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Artificial Intelligence and Statistics</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="152" last="161"/></ref-volisspag></ref-info><ref-fulltext>Ridgeway G, Madigan D, Richardson T (1999) Boosting methodology for regression problems. In: Proceedings of the artificial intelligence and statistics, pp 152-161</ref-fulltext></reference><reference id="10"><ref-info><refd-itemidlist><itemid idtype="SGR">0007325881</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author></ref-authors><ref-sourcetitle>Pasting Bites Together for Prediction in Large Data Sets and On-line</ref-sourcetitle><ref-publicationyear first="1997"/><ref-text>Department of Statistics technical report, University of California, Berkeley</ref-text></ref-info><ref-fulltext>Breiman L (1997) Pasting bites together for prediction in large data sets and on-line. Department of Statistics technical report, University of California, Berkeley</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>The covariance inflation criterion for adaptive model selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0039724913</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Tibshirani R.</ce:indexed-name><ce:surname>Tibshirani</ce:surname></author><author seq="2"><ce:initials>K.</ce:initials><ce:indexed-name>Knight K.</ce:indexed-name><ce:surname>Knight</ce:surname></author></ref-authors><ref-sourcetitle>J Roy Stat Soc ser B</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="61"/><pagerange first="529" last="546"/></ref-volisspag></ref-info><ref-fulltext>R Tibshirani K Knight 1999 The covariance inflation criterion for adaptive model selection J Roy Stat Soc Ser B 61 529 546</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>On kernel principal component regression with covariance in action criterion for model selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">19544375922</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Rosipal R.</ce:indexed-name><ce:surname>Rosipal</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Girolami M.</ce:indexed-name><ce:surname>Girolami</ce:surname></author><author seq="3"><ce:initials>L.</ce:initials><ce:indexed-name>Trejo L.</ce:indexed-name><ce:surname>Trejo</ce:surname></author></ref-authors><ref-sourcetitle>Technical Report</ref-sourcetitle><ref-publicationyear first="2000"/><ref-text>University of Paisley</ref-text></ref-info><ref-fulltext>Rosipal R, Girolami M, Trejo L (2000) On kernel principal component regression with covariance in action criterion for model selection. Technical report, University of Paisley</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Data perturbation for escaping local maxima in learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036931049</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Elidan G.</ce:indexed-name><ce:surname>Elidan</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Ninio M.</ce:indexed-name><ce:surname>Ninio</ce:surname></author><author seq="3"><ce:initials>N.</ce:initials><ce:indexed-name>Friedman N.</ce:indexed-name><ce:surname>Friedman</ce:surname></author><author seq="4"><ce:initials>D.</ce:initials><ce:indexed-name>Shuurmans D.</ce:indexed-name><ce:surname>Shuurmans</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of AAAI/IAAI</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><pagerange first="132" last="139"/></ref-volisspag></ref-info><ref-fulltext>Elidan G, Ninio M, Friedman N, Shuurmans D (2002) Data perturbation for escaping local maxima in learning. In: Proceedings of AAAI/IAAI, pp 132-139</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Learning by transduction</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002947383</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Gammerman A.</ce:indexed-name><ce:surname>Gammerman</ce:surname></author><author seq="2"><ce:initials>V.</ce:initials><ce:indexed-name>Vovk V.</ce:indexed-name><ce:surname>Vovk</ce:surname></author><author seq="3"><ce:initials>V.</ce:initials><ce:indexed-name>Vapnik V.</ce:indexed-name><ce:surname>Vapnik</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><pagerange first="148" last="155"/></ref-volisspag><ref-text>Madison, WI</ref-text></ref-info><ref-fulltext>Gammerman A, Vovk V, Vapnik V (1998) Learning by transduction. In: Proceedings of the 14th conference on uncertainty in artificial intelligence, Madison, WI, pp 148-155</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Transduction with confidence and credibility</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84880657197</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Saunders C.</ce:indexed-name><ce:surname>Saunders</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Gammerman A.</ce:indexed-name><ce:surname>Gammerman</ce:surname></author><author seq="3"><ce:initials>V.</ce:initials><ce:indexed-name>Vovk V.</ce:indexed-name><ce:surname>Vovk</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of IJCAI</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="2"/><pagerange first="722" last="726"/></ref-volisspag></ref-info><ref-fulltext>Saunders C, Gammerman A, Vovk V (1999) Transduction with confidence and credibility. In: Proceedings of IJCAI, vol 2, pp 722-726</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Ridge regression confidence machine</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0003273622</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>I.</ce:initials><ce:indexed-name>Nouretdinov I.</ce:indexed-name><ce:surname>Nouretdinov</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Melluish T.</ce:indexed-name><ce:surname>Melluish</ce:surname></author><author seq="3"><ce:initials>V.</ce:initials><ce:indexed-name>Vovk V.</ce:indexed-name><ce:surname>Vovk</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 18th International Conference on Machine Learning</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><pagerange first="385" last="392"/></ref-volisspag><ref-text>Kaufmann San Francisco</ref-text></ref-info><ref-fulltext>Nouretdinov I, Melluish T, Vovk V (2001) Ridge regression confidence machine. In: Proceedings of the 18th international conference on machine learning. Kaufmann, San Francisco, pp 385-392</ref-fulltext></reference><reference id="17"><ref-info><refd-itemidlist><itemid idtype="SGR">0003450542</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>Vapnik V.</ce:indexed-name><ce:surname>Vapnik</ce:surname></author></ref-authors><ref-sourcetitle>The Nature of Statistical Learning Theory</ref-sourcetitle><ref-publicationyear first="1995"/><ref-text>Springer Berlin</ref-text></ref-info><ref-fulltext>Vapnik V (1995) The nature of statistical learning theory. Springer, Berlin</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Reliable classifications with machine learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84945287811</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Kukar M.</ce:indexed-name><ce:surname>Kukar</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Machine Learning: ECML-2002</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><pagerange first="219" last="231"/></ref-volisspag><ref-text>Springer Helsinki</ref-text></ref-info><ref-fulltext>Kukar M, Kononenko I (2002) Reliable classifications with machine learning. In: Proceedings of the machine learning: ECML-2002. Springer, Helsinki, pp 219-231</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>Evaluation of prediction reliability in regression using the transduction principle</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">62249148128</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Robnik-Sikonja M.</ce:indexed-name><ce:surname>Robnik-Šikonja</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Kukar M.</ce:indexed-name><ce:surname>Kukar</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of Eurocon 2003</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><pagerange first="99" last="103"/></ref-volisspag><ref-text>Ljubljana</ref-text></ref-info><ref-fulltext>Bosnić Z, Kononenko I, Robnik-Šikonja M, Kukar M (2003) Evaluation of prediction reliability in regression using the transduction principle. In Proceedings of Eurocon 2003, Ljubljana, pp 99-103</ref-fulltext></reference><reference id="20"><ref-info><ref-title><ref-titletext>The role of unlabelled data in supervised learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">8644236278</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Mitchell T.</ce:indexed-name><ce:surname>Mitchell</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 6th International Colloquium of Cognitive Science</ref-sourcetitle><ref-publicationyear first="1999"/><ref-text>San Sebastian, Spain</ref-text></ref-info><ref-fulltext>Mitchell T (1999) The role of unlabelled data in supervised learning. In: Proceedings of the 6th international colloquium of cognitive science, San Sebastian, Spain</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>Combining labeled and unlabeled data with co-training</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031620208</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Blum A.</ce:indexed-name><ce:surname>Blum</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Mitchell T.</ce:indexed-name><ce:surname>Mitchell</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 11th Annual Conference on Computational Learning Theory</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><pagerange first="92" last="100"/></ref-volisspag></ref-info><ref-fulltext>Blum A, Mitchell T (1998) Combining labeled and unlabeled data with co-training. In: Proceedings of the 11th annual conference on computational learning theory, pp 92-100</ref-fulltext></reference><reference id="22"><ref-info><refd-itemidlist><itemid idtype="SGR">0003680739</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Li M.</ce:indexed-name><ce:surname>Li</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Vitanyi P.</ce:indexed-name><ce:surname>Vitányi</ce:surname></author></ref-authors><ref-sourcetitle>An Introduction to Kolmogorov Complexity and Its Applications</ref-sourcetitle><ref-publicationyear first="1993"/><ref-text>Springer New York</ref-text></ref-info><ref-fulltext>Li M, Vitányi P (1993) An introduction to Kolmogorov complexity and its applications. Springer, New York</ref-fulltext></reference><reference id="23"><ref-info><refd-itemidlist><itemid idtype="SGR">0004161838</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>W.H.</ce:initials><ce:indexed-name>Press W.H.</ce:indexed-name><ce:surname>Press</ce:surname></author><et-al/></ref-authors><ref-sourcetitle>Numerical Recipes in C: The Art of Scientific Computing</ref-sourcetitle><ref-publicationyear first="2002"/><ref-text>Cambridge University Press Cambridge</ref-text></ref-info><ref-fulltext>Press WH et al. (2002) Numerical recipes in C: the art of scientific computing. Cambridge University Press, Cambridge</ref-fulltext></reference><reference id="24"><ref-info><refd-itemidlist><itemid idtype="SGR">33745834241</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.J.</ce:initials><ce:indexed-name>Newman D.J.</ce:indexed-name><ce:surname>Newman</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Hettich S.</ce:indexed-name><ce:surname>Hettich</ce:surname></author><author seq="3"><ce:initials>C.L.</ce:initials><ce:indexed-name>Blake C.L.</ce:indexed-name><ce:surname>Blake</ce:surname></author><author seq="4"><ce:initials>C.J.</ce:initials><ce:indexed-name>Merz C.J.</ce:indexed-name><ce:surname>Merz</ce:surname></author></ref-authors><ref-sourcetitle>UCI Repository of Machine Learning Databases</ref-sourcetitle><ref-publicationyear first="1998"/><ref-text>Department of Information and Computer Sciences, University of California, Irvine</ref-text></ref-info><ref-fulltext>Newman DJ, Hettich S, Blake CL, Merz CJ (1998) UCI repository of machine learning databases. Department of Information and Computer Sciences, University of California, Irvine</ref-fulltext></reference><reference id="25"><ref-info><refd-itemidlist><itemid idtype="SGR">39349111351</itemid></refd-itemidlist><ref-sourcetitle>StatLib-data, Software and News from the Statistics Community</ref-sourcetitle><ref-publicationyear first="2005"/><ref-text>Department of Statistics at Carnegie Mellon University</ref-text></ref-info><ref-fulltext>Department of Statistics at Carnegie Mellon University (2005) StatLib-data, software and news from the statistics community</ref-fulltext></reference><reference id="26"><ref-info><ref-title><ref-titletext>On estimating probabilities in tree pruning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85031805771</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Cestnik B.</ce:indexed-name><ce:surname>Cestnik</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of European Working Session on Learning (EWSL-91)</ref-sourcetitle><ref-publicationyear first="1991"/><ref-volisspag><pagerange first="138" last="150"/></ref-volisspag><ref-text>Porto, Portugal</ref-text></ref-info><ref-fulltext>Cestnik B, Bratko I (1991) On estimating probabilities in tree pruning. In: Proceedings of European working session on learning (EWSL-91), Porto, Portugal, pp 138-150</ref-fulltext></reference><reference id="27"><ref-info><refd-itemidlist><itemid idtype="SGR">0003710380</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Chang C.</ce:indexed-name><ce:surname>Chang</ce:surname></author><author seq="2"><ce:initials>C.</ce:initials><ce:indexed-name>Lin C.</ce:indexed-name><ce:surname>Lin</ce:surname></author></ref-authors><ref-sourcetitle>LIBSVM: A Library for Support Vector Machines</ref-sourcetitle><ref-publicationyear first="2001"/><ref-website><ce:e-address type="url">http://www.csie.ntu.edu.tw/~cjlin/libsvm</ce:e-address></ref-website></ref-info><ref-fulltext>Chang C, Lin C (2001) LIBSVM: a library for support vector machines. http://www.csie.ntu.edu.tw/~cjlin/libsvm</ref-fulltext></reference><reference id="28"><ref-info><refd-itemidlist><itemid idtype="SGR">25644459607</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.</ce:initials><ce:indexed-name>Alpaydin E.</ce:indexed-name><ce:surname>Alpaydin</ce:surname></author></ref-authors><ref-sourcetitle>Introduction to Machine Learning</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>MIT Press Cambridge</ref-text></ref-info><ref-fulltext>Alpaydin E (2004) Introduction to machine learning. MIT Press, Cambridge</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>