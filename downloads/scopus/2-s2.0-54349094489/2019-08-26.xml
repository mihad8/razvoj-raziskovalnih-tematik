<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/54349094489</prism:url><dc:identifier>SCOPUS_ID:54349094489</dc:identifier><eid>2-s2.0-54349094489</eid><pii>S0169023X08001080</pii><prism:doi>10.1016/j.datak.2008.08.001</prism:doi><dc:title>Comparison of approaches for estimating reliability of individual regression predictions</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>44</citedby-count><prism:publicationName>Data and Knowledge Engineering</prism:publicationName><source-id>24437</source-id><prism:issn>0169023X</prism:issn><prism:volume>67</prism:volume><prism:issueIdentifier>3</prism:issueIdentifier><prism:startingPage>504</prism:startingPage><prism:endingPage>516</prism:endingPage><prism:pageRange>504-516</prism:pageRange><prism:coverDate>2008-12-01</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="23566763400"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/23566763400</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>The paper compares different approaches to estimate the reliability of individual predictions in regression. We compare the sensitivity-based reliability estimates developed in our previous work with four approaches found in the literature: variance of bagged models, local cross-validation, density estimation, and local modeling. By combining pairs of individual estimates, we compose a combined estimate that performs better than the individual estimates. We tested the estimates by running data from 28 domains through eight regression models: regression trees, linear regression, neural networks, bagging, support vector machines, locally weighted regression, random forests, and generalized additive model. The results demonstrate the potential of a sensitivity-based estimate, as well as the local modeling of prediction error with regression trees. Among the tested approaches, the best average performance was achieved by estimation using the bagging variance approach, which achieved the best performance with neural networks, bagging and locally weighted regression. © 2008 Elsevier B.V. All rights reserved.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/54349094489" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=54349094489&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=54349094489&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="23566763400"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/23566763400</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="57188535146"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57188535146</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Prediction accuracy</author-keyword><author-keyword>Prediction error</author-keyword><author-keyword>Regression</author-keyword><author-keyword>Reliability estimate</author-keyword><author-keyword>Sensitivity analysis</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Average performances</mainterm><mainterm weight="a" candidate="n">Cross-validation</mainterm><mainterm weight="a" candidate="n">Density estimations</mainterm><mainterm weight="a" candidate="n">Do-mains</mainterm><mainterm weight="a" candidate="n">Generalized additive models</mainterm><mainterm weight="a" candidate="n">Individual predictions</mainterm><mainterm weight="a" candidate="n">Locally weighted regressions</mainterm><mainterm weight="a" candidate="n">Modeling</mainterm><mainterm weight="a" candidate="n">Prediction accuracy</mainterm><mainterm weight="a" candidate="n">Prediction error</mainterm><mainterm weight="a" candidate="n">Prediction errors</mainterm><mainterm weight="a" candidate="n">Random forests</mainterm><mainterm weight="a" candidate="n">Regression</mainterm><mainterm weight="a" candidate="n">Regression models</mainterm><mainterm weight="a" candidate="n">Regression trees</mainterm><mainterm weight="a" candidate="n">Support vectors</mainterm></idxterms><subject-areas><subject-area code="1802" abbrev="DECI">Information Systems and Management</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2017-12-18T03:12:26.056Z</xocs:funding-addon-generated-timestamp></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered year="2017" month="09" day="12" timestamp="2017-09-12T08:45:10.000010-04:00"/><ait:date-sort year="2008" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2008 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:pii>S0169023X08001080</ce:pii><ce:doi>10.1016/j.datak.2008.08.001</ce:doi><itemid idtype="PUI">50261129</itemid><itemid idtype="CPX">20084411668091</itemid><itemid idtype="SCP">54349094489</itemid><itemid idtype="SGR">54349094489</itemid></itemidlist><history><date-created year="2008" month="09" day="05"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword>Prediction accuracy</author-keyword><author-keyword>Prediction error</author-keyword><author-keyword>Regression</author-keyword><author-keyword>Reliability estimate</author-keyword><author-keyword>Sensitivity analysis</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Comparison of approaches for estimating reliability of individual regression predictions</titletext></citation-title><author-group><author auid="23566763400" seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name></author><author auid="57188535146" seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>University of Ljubljana</organization><organization>Faculty of Computer and Information Science</organization><address-part>Tržaška 25</address-part><city-group>Ljubljana</city-group><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname></person><affiliation country="svn"><organization>University of Ljubljana</organization><organization>Faculty of Computer and Information Science</organization><address-part>Tržaška 25</address-part><city-group>Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>The paper compares different approaches to estimate the reliability of individual predictions in regression. We compare the sensitivity-based reliability estimates developed in our previous work with four approaches found in the literature: variance of bagged models, local cross-validation, density estimation, and local modeling. By combining pairs of individual estimates, we compose a combined estimate that performs better than the individual estimates. We tested the estimates by running data from 28 domains through eight regression models: regression trees, linear regression, neural networks, bagging, support vector machines, locally weighted regression, random forests, and generalized additive model. The results demonstrate the potential of a sensitivity-based estimate, as well as the local modeling of prediction error with regression trees. Among the tested approaches, the best average performance was achieved by estimation using the bagging variance approach, which achieved the best performance with neural networks, bagging and locally weighted regression. © 2008 Elsevier B.V. All rights reserved.</ce:para></abstract></abstracts><source srcid="24437" type="j" country="nld"><sourcetitle>Data and Knowledge Engineering</sourcetitle><sourcetitle-abbrev>Data Knowl Eng</sourcetitle-abbrev><issn type="print">0169023X</issn><codencode>DKENE</codencode><volisspag><voliss volume="67" issue="3"/><pagerange first="504" last="516"/></volisspag><publicationyear first="2008"/><publicationdate><year>2008</year><month>12</month><date-text xfab-added="true">December 2008</date-text></publicationdate></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification>731.5</classification><classification>732</classification><classification>912.2</classification><classification>922.2</classification><classification>913</classification><classification>921</classification><classification>921.4</classification><classification>913.3</classification><classification>731.1</classification><classification>723.5</classification><classification>421</classification><classification>422.2</classification><classification>461.1</classification><classification>461.4</classification><classification>723</classification><classification>723.4</classification></classifications><classifications type="ASJC"><classification>1802</classification></classifications><classifications type="SUBJABBR"><classification>DECI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="61"><reference id="1"><ref-info><refd-itemidlist><itemid idtype="SGR">84882637032</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Kukar M.</ce:indexed-name><ce:surname>Kukar</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning and Data Mining: Introduction to Principles and Algorithms</ref-sourcetitle><ref-publicationyear first="2007"/><ref-text>Horwood Publishing Limited, UK</ref-text></ref-info><ref-fulltext>Kononenko I., and Kukar M. Machine Learning and Data Mining: Introduction to Principles and Algorithms (2007), Horwood Publishing Limited, UK</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Statistical concepts in reliability</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">15244360907</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.J.</ce:initials><ce:indexed-name>Crowder M.J.</ce:indexed-name><ce:surname>Crowder</ce:surname></author><author seq="2"><ce:initials>A.C.</ce:initials><ce:indexed-name>Kimber A.C.</ce:indexed-name><ce:surname>Kimber</ce:surname></author><author seq="3"><ce:initials>R.L.</ce:initials><ce:indexed-name>Smith R.L.</ce:indexed-name><ce:surname>Smith</ce:surname></author><author seq="4"><ce:initials>T.J.</ce:initials><ce:indexed-name>Sweeting T.J.</ce:indexed-name><ce:surname>Sweeting</ce:surname></author></ref-authors><ref-sourcetitle>Statistical Analysis of Reliability Data</ref-sourcetitle><ref-publicationyear first="1991"/><ref-text>Chapman &amp; Hall, London, UK</ref-text></ref-info><ref-fulltext>Crowder M.J., Kimber A.C., Smith R.L., and Sweeting T.J. Statistical concepts in reliability. Statistical Analysis of Reliability Data (1991), Chapman &amp; Hall, London, UK</ref-fulltext></reference><reference id="3"><ref-info><refd-itemidlist><itemid idtype="SGR">54349099755</itemid></refd-itemidlist><ref-text>A. Gammerman, V. Vovk, V. Vapnik, Learning by transduction, in: Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence, Madison, Wisconsin, 1998, pp. 148-155.</ref-text></ref-info><ref-fulltext>A. Gammerman, V. Vovk, V. Vapnik, Learning by transduction, in: Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence, Madison, Wisconsin, 1998, pp. 148-155.</ref-fulltext></reference><reference id="4"><ref-info><refd-itemidlist><itemid idtype="SGR">54349119828</itemid></refd-itemidlist><ref-text>C. Saunders, A. Gammerman, V. Vovk, Transduction with confidence and credibility, in: Proceedings of IJCAI'99, vol. 2, 1999, pp. 722-726.</ref-text></ref-info><ref-fulltext>C. Saunders, A. Gammerman, V. Vovk, Transduction with confidence and credibility, in: Proceedings of IJCAI'99, vol. 2, 1999, pp. 722-726.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Ridge regression confidence machine</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0003273622</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>I.</ce:initials><ce:indexed-name>Nouretdinov I.</ce:indexed-name><ce:surname>Nouretdinov</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Melluish T.</ce:indexed-name><ce:surname>Melluish</ce:surname></author><author seq="3"><ce:initials>V.</ce:initials><ce:indexed-name>Vovk V.</ce:indexed-name><ce:surname>Vovk</ce:surname></author></ref-authors><ref-sourcetitle>Proc. 18th International Conf. on Machine Learning</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><pagerange first="385" last="392"/></ref-volisspag><ref-text>Morgan Kaufman, San Francisco, CA</ref-text></ref-info><ref-fulltext>Nouretdinov I., Melluish T., and Vovk V. Ridge regression confidence machine. Proc. 18th International Conf. on Machine Learning (2001), Morgan Kaufman, San Francisco, CA 385-392</ref-fulltext></reference><reference id="6"><ref-info><refd-itemidlist><itemid idtype="SGR">54349122057</itemid></refd-itemidlist><ref-text>A. Weigend, D. Nix, Predictions with confidence intervals (local error bars), in: Proceedings of the International Conference on Neural Information Processing (ICONIP'94), Seoul, Korea, 1994, pp. 847-852.</ref-text></ref-info><ref-fulltext>A. Weigend, D. Nix, Predictions with confidence intervals (local error bars), in: Proceedings of the International Conference on Neural Information Processing (ICONIP'94), Seoul, Korea, 1994, pp. 847-852.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Practical confidence and prediction intervals</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898947879</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Heskes T.</ce:indexed-name><ce:surname>Heskes</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="9"/><pagerange first="176" last="182"/></ref-volisspag><ref-text>Mozer M.C., Jordan M.I., and Petsche T. (Eds), The MIT Press</ref-text></ref-info><ref-fulltext>Heskes T. Practical confidence and prediction intervals. In: Mozer M.C., Jordan M.I., and Petsche T. (Eds). Advances in Neural Information Processing Systems vol. 9 (1997), The MIT Press 176-182</ref-fulltext></reference><reference id="8"><ref-info><refd-itemidlist><itemid idtype="SGR">54349106208</itemid></refd-itemidlist><ref-text>J. Carney, P. Cunningham, Confidence and prediction intervals for neural network ensembles, in: Proceedings of IJCNN'99, The International Joint Conference on Neural Networks, Washington, USA, 1999, pp. 1215-1218.</ref-text></ref-info><ref-fulltext>J. Carney, P. Cunningham, Confidence and prediction intervals for neural network ensembles, in: Proceedings of IJCNN'99, The International Joint Conference on Neural Networks, Washington, USA, 1999, pp. 1215-1218.</ref-fulltext></reference><reference id="9"><ref-info><refd-itemidlist><itemid idtype="SGR">54349088045</itemid></refd-itemidlist><ref-text>M. Birattari, H. Bontempi, H. Bersini, Local learning for data analysis, in: Proceedings of the Eighth Belgian-Dutch Conference on Machine Learning, 1998, pp. 55-61.</ref-text></ref-info><ref-fulltext>M. Birattari, H. Bontempi, H. Bersini, Local learning for data analysis, in: Proceedings of the Eighth Belgian-Dutch Conference on Machine Learning, 1998, pp. 55-61.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Dynamic classifier selection based on multiple classifier behaviour</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84994037050</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Giacinto G.</ce:indexed-name><ce:surname>Giacinto</ce:surname></author><author seq="2"><ce:initials>F.</ce:initials><ce:indexed-name>Roli F.</ce:indexed-name><ce:surname>Roli</ce:surname></author></ref-authors><ref-sourcetitle>Pattern Recognition</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><voliss volume="34" issue="9"/><pagerange first="1879" last="1881"/></ref-volisspag></ref-info><ref-fulltext>Giacinto G., and Roli F. Dynamic classifier selection based on multiple classifier behaviour. Pattern Recognition 34 9 (2001) 1879-1881</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Learning to predict the leave-one-out error of kernel based classifiers</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84958985297</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Tsuda K.</ce:indexed-name><ce:surname>Tsuda</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Ratsch G.</ce:indexed-name><ce:surname>Rätsch</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Mika S.</ce:indexed-name><ce:surname>Mika</ce:surname></author><author seq="4"><ce:initials>K.</ce:initials><ce:indexed-name>Muller K.</ce:indexed-name><ce:surname>Müller</ce:surname></author></ref-authors><ref-sourcetitle>Lecture Notes in Computer Science</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><pagerange first="331"/></ref-volisspag></ref-info><ref-fulltext>Tsuda K., Rätsch G., Mika S., and Müller K. Learning to predict the leave-one-out error of kernel based classifiers. Lecture Notes in Computer Science (2001) 331</ref-fulltext></reference><reference id="12"><ref-info><refd-itemidlist><itemid idtype="SGR">0003450542</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>Vapnik V.</ce:indexed-name><ce:surname>Vapnik</ce:surname></author></ref-authors><ref-sourcetitle>The Nature of Statistical Learning Theory</ref-sourcetitle><ref-publicationyear first="1995"/><ref-text>Springer</ref-text></ref-info><ref-fulltext>Vapnik V. The Nature of Statistical Learning Theory (1995), Springer</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Reliable classifications with machine learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84945287811</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Kukar M.</ce:indexed-name><ce:surname>Kukar</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Proc. Machine Learning: ECML-2002</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><pagerange first="219" last="231"/></ref-volisspag><ref-text>Elomaa T., Manilla H., and Toivonen H. (Eds), Springer-Verlag, Helsinki Finland</ref-text></ref-info><ref-fulltext>Kukar M., and Kononenko I. Reliable classifications with machine learning. In: Elomaa T., Manilla H., and Toivonen H. (Eds). Proc. Machine Learning: ECML-2002 (2002), Springer-Verlag, Helsinki Finland 219-231</ref-fulltext></reference><reference id="14"><ref-info><refd-itemidlist><itemid idtype="SGR">54349105458</itemid></refd-itemidlist><ref-text>Z. Bosnić, I. Kononenko, M. Robnik-Šikonja, M. Kukar, Evaluation of prediction reliability in regression using the transduction principle, in: B. Zajc, M. Tkalčič (Eds.), Proceedings of Eurocon 2003, Ljubljana, 2003, pp. 99-103.</ref-text></ref-info><ref-fulltext>Z. Bosnić, I. Kononenko, M. Robnik-Šikonja, M. Kukar, Evaluation of prediction reliability in regression using the transduction principle, in: B. Zajc, M. Tkalčič (Eds.), Proceedings of Eurocon 2003, Ljubljana, 2003, pp. 99-103.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Estimation of regressor reliability</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">39349103103</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Systems</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="17" issue="1-3"/><pagerange first="297" last="311"/></ref-volisspag></ref-info><ref-fulltext>Bosnić Z., and Kononenko I. Estimation of regressor reliability. Journal of Intelligent Systems 17 1/3 (2008) 297-311</ref-fulltext></reference><reference id="16"><ref-info><refd-itemidlist><itemid idtype="SGR">54349124130</itemid></refd-itemidlist><ref-text>Z. Bosnić, I. Kononenko, Estimation of individual prediction reliability using the local sensitivity analysis, Applied Intelligence, in press [Online edition] http://www.springerlink.com/content/e27p2584387532g8/.</ref-text></ref-info><ref-fulltext>Z. Bosnić, I. Kononenko, Estimation of individual prediction reliability using the local sensitivity analysis, Applied Intelligence, in press [Online edition] http://www.springerlink.com/content/e27p2584387532g8/.</ref-fulltext></reference><reference id="17"><ref-info><refd-itemidlist><itemid idtype="SGR">0003680739</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Li M.</ce:indexed-name><ce:surname>Li</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Vitanyi P.</ce:indexed-name><ce:surname>Vitányi</ce:surname></author></ref-authors><ref-sourcetitle>An Introduction to Kolmogorov Complexity and its Applications</ref-sourcetitle><ref-publicationyear first="1993"/><ref-text>Springer-Verlag, New York</ref-text></ref-info><ref-fulltext>Li M., and Vitányi P. An Introduction to Kolmogorov Complexity and its Applications (1993), Springer-Verlag, New York</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Bagging predictors</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0030211964</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><voliss volume="24" issue="2"/><pagerange first="123" last="140"/></ref-volisspag></ref-info><ref-fulltext>Breiman L. Bagging predictors. Machine Learning 24 2 (1996) 123-140</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>Stacked generalization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0026692226</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Wolpert D.</ce:indexed-name><ce:surname>Wolpert</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="5"/><pagerange first="241" last="259"/></ref-volisspag><ref-text>Pergamon Press</ref-text></ref-info><ref-fulltext>Wolpert D. Stacked generalization. Neural Networks vol. 5 (1992), Pergamon Press 241-259</ref-fulltext></reference><reference id="20"><ref-info><ref-title><ref-titletext>Model search and inference by bootstrap bumping</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0033266602</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Tibshirani R.</ce:indexed-name><ce:surname>Tibshirani</ce:surname></author><author seq="2"><ce:initials>K.</ce:initials><ce:indexed-name>Knight K.</ce:indexed-name><ce:surname>Knight</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Computational and Graphical Statistics</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="8"/><pagerange first="671" last="686"/></ref-volisspag></ref-info><ref-fulltext>Tibshirani R., and Knight K. Model search and inference by bootstrap bumping. Journal of Computational and Graphical Statistics 8 (1999) 671-686</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>A decision-theoretic generalization of on-line learning and an application to boosting</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031211090</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Freund Y.</ce:indexed-name><ce:surname>Freund</ce:surname></author><author seq="2"><ce:initials>R.</ce:initials><ce:indexed-name>Schapire R.</ce:indexed-name><ce:surname>Schapire</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Computer and System Sciences</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="55" issue="1"/><pagerange first="119" last="139"/></ref-volisspag></ref-info><ref-fulltext>Freund Y., and Schapire R. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences 55 1 (1997) 119-139</ref-fulltext></reference><reference id="22"><ref-info><refd-itemidlist><itemid idtype="SGR">54349099502</itemid></refd-itemidlist><ref-text>G. Elidan M. Ninio N. Friedman D. Schuurmans, Data perturbation for escaping local maxima in learning, 2002.</ref-text></ref-info><ref-fulltext>G. Elidan M. Ninio N. Friedman D. Schuurmans, Data perturbation for escaping local maxima in learning, 2002.</ref-fulltext></reference><reference id="23"><ref-info><ref-title><ref-titletext>Active learning with statistical models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001341901</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.A.</ce:initials><ce:indexed-name>Cohn D.A.</ce:indexed-name><ce:surname>Cohn</ce:surname></author><author seq="2"><ce:initials>Z.</ce:initials><ce:indexed-name>Ghahramani Z.</ce:indexed-name><ce:surname>Ghahramani</ce:surname></author><author seq="3"><ce:initials>M.I.</ce:initials><ce:indexed-name>Jordan M.I.</ce:indexed-name><ce:surname>Jordan</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1995"/><ref-volisspag><voliss volume="7"/><pagerange first="705" last="712"/></ref-volisspag><ref-website><ce:e-address type="url">http://citeseer.ist.psu.edu/cohn95active.html</ce:e-address></ref-website><ref-text>Tesauro G., Touretzky D., and Leen T. (Eds), The MIT Press</ref-text></ref-info><ref-fulltext>Cohn D.A., Ghahramani Z., and Jordan M.I. Active learning with statistical models. In: Tesauro G., Touretzky D., and Leen T. (Eds). Advances in Neural Information Processing Systems vol. 7 (1995), The MIT Press 705-712. citeseer.ist.psu.edu/cohn95active.html</ref-fulltext></reference><reference id="24"><ref-info><refd-itemidlist><itemid idtype="SGR">54349106716</itemid></refd-itemidlist><ref-text>L.A.D.A. Cohn and, R. Ladner, Training connectionist networks with queries and selective sampling, in: M.K.D. Touretzky, (Ed.), Advances in Neural Information Processing Systems, vol. 2, 1990, pp. 566-573.</ref-text></ref-info><ref-fulltext>L.A.D.A. Cohn and, R. Ladner, Training connectionist networks with queries and selective sampling, in: M.K.D. Touretzky, (Ed.), Advances in Neural Information Processing Systems, vol. 2, 1990, pp. 566-573.</ref-fulltext></reference><reference id="25"><ref-info><refd-itemidlist><itemid idtype="SGR">54349119827</itemid></refd-itemidlist><ref-text>A. Linden, F. Weber, Implementing inner drive by competence reflection, in: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, Hawaii, 1992, pp. 321-326.</ref-text></ref-info><ref-fulltext>A. Linden, F. Weber, Implementing inner drive by competence reflection, in: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, Hawaii, 1992, pp. 321-326.</ref-fulltext></reference><reference id="26"><ref-info><refd-itemidlist><itemid idtype="SGR">54349119348</itemid></refd-itemidlist><ref-text>J. Schmidhuber, J. Storck, Reinforcement driven information acquisition in nondeterministic environments, Fakultat fur Informatik, Technische Universit at Munchen, Technical Report, 1993.</ref-text></ref-info><ref-fulltext>J. Schmidhuber, J. Storck, Reinforcement driven information acquisition in nondeterministic environments, Fakultat fur Informatik, Technische Universit at Munchen, Technical Report, 1993.</ref-fulltext></reference><reference id="27"><ref-info><refd-itemidlist><itemid idtype="SGR">54349099020</itemid></refd-itemidlist><ref-text>S.D. Whitehead, A complexity analysis of cooperative mechanisms in reinforcement learning, in: AAAI, 1991, pp. 607-613.</ref-text></ref-info><ref-fulltext>S.D. Whitehead, A complexity analysis of cooperative mechanisms in reinforcement learning, in: AAAI, 1991, pp. 607-613.</ref-fulltext></reference><reference id="28"><ref-info><refd-itemidlist><itemid idtype="SGR">54349095664</itemid></refd-itemidlist><ref-text>M. Seeger, Learning with labeled and unlabeled data, Technical Report, http://www.dai.ed.ac.uk/seeger/papers.html, 2000.</ref-text></ref-info><ref-fulltext>M. Seeger, Learning with labeled and unlabeled data, Technical Report, http://www.dai.ed.ac.uk/seeger/papers.html, 2000.</ref-fulltext></reference><reference id="29"><ref-info><refd-itemidlist><itemid idtype="SGR">54349114075</itemid></refd-itemidlist><ref-text>A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training, in: Proceedings of the 11th Annual Conference on Computational Learning Theory, 1998, pp. 92-100.</ref-text></ref-info><ref-fulltext>A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training, in: Proceedings of the 11th Annual Conference on Computational Learning Theory, 1998, pp. 92-100.</ref-fulltext></reference><reference id="30"><ref-info><refd-itemidlist><itemid idtype="SGR">54349114726</itemid></refd-itemidlist><ref-text>T. Mitchell, The role of unlabelled data in supervised learning, in: Proceedings of the Sixth International Colloquium of Cognitive Science, San Sebastian, Spain, 1999.</ref-text></ref-info><ref-fulltext>T. Mitchell, The role of unlabelled data in supervised learning, in: Proceedings of the Sixth International Colloquium of Cognitive Science, San Sebastian, Spain, 1999.</ref-fulltext></reference><reference id="31"><ref-info><ref-title><ref-titletext>Learning classification with unlabeled data</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0005986550</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>de Sa V.</ce:indexed-name><ce:surname>de Sa</ce:surname></author></ref-authors><ref-sourcetitle>Proc. NIPS'93, Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1993"/><ref-volisspag><pagerange first="112" last="119"/></ref-volisspag><ref-text>Cowan J.D., Tesauro G., and Alspector J. (Eds), Morgan Kaufmann Publishers, San Francisco, CA</ref-text></ref-info><ref-fulltext>de Sa V. Learning classification with unlabeled data. In: Cowan J.D., Tesauro G., and Alspector J. (Eds). Proc. NIPS'93, Neural Information Processing Systems (1993), Morgan Kaufmann Publishers, San Francisco, CA 112-119</ref-fulltext></reference><reference id="32"><ref-info><ref-title><ref-titletext>Enhancing supervised learning with unlabeled data</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0007950880</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Goldman S.</ce:indexed-name><ce:surname>Goldman</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Zhou Y.</ce:indexed-name><ce:surname>Zhou</ce:surname></author></ref-authors><ref-sourcetitle>Proc. 17th International Conf. on Machine Learning</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><pagerange first="327" last="334"/></ref-volisspag><ref-text>Morgan Kaufman, San Francisco, CA</ref-text></ref-info><ref-fulltext>Goldman S., and Zhou Y. Enhancing supervised learning with unlabeled data. Proc. 17th International Conf. on Machine Learning (2000), Morgan Kaufman, San Francisco, CA 327-334</ref-fulltext></reference><reference id="33"><ref-info><refd-itemidlist><itemid idtype="SGR">54349095663</itemid></refd-itemidlist><ref-text>L. Breierova, M. Choudhari, An introduction to sensitivity analysis, MIT System Dynamics in Education Project, September 1996.</ref-text></ref-info><ref-fulltext>L. Breierova, M. Choudhari, An introduction to sensitivity analysis, MIT System Dynamics in Education Project, September 1996.</ref-fulltext></reference><reference id="34"><ref-info><refd-itemidlist><itemid idtype="SGR">54349125616</itemid></refd-itemidlist><ref-text>J. Kleijnen, Experimental designs for sensitivity analysis of simulation models, in: Tutorial at the Eurosim 2001 Conference.</ref-text></ref-info><ref-fulltext>J. Kleijnen, Experimental designs for sensitivity analysis of simulation models, in: Tutorial at the Eurosim 2001 Conference.</ref-fulltext></reference><reference id="35"><ref-info><ref-title><ref-titletext>Stability and generalization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038368335</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Elisseeff A.</ce:indexed-name><ce:surname>Elisseeff</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Machine Learning Research</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="2"/><pagerange first="499" last="526"/></ref-volisspag></ref-info><ref-fulltext>Bousquet O., and Elisseeff A. Stability and generalization. Journal of Machine Learning Research 2 (2002) 499-526</ref-fulltext></reference><reference id="36"><ref-info><ref-title><ref-titletext>Algorithmic stability and sanity-check bounds for leave-one-out cross-validation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0030654389</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.J.</ce:initials><ce:indexed-name>Kearns M.J.</ce:indexed-name><ce:surname>Kearns</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Ron D.</ce:indexed-name><ce:surname>Ron</ce:surname></author></ref-authors><ref-sourcetitle>Computational Learning Theory</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><pagerange first="152" last="162"/></ref-volisspag></ref-info><ref-fulltext>Kearns M.J., and Ron D. Algorithmic stability and sanity-check bounds for leave-one-out cross-validation. Computational Learning Theory (1997) 152-162</ref-fulltext></reference><reference id="37"><ref-info><refd-itemidlist><itemid idtype="SGR">54349084094</itemid></refd-itemidlist><ref-text>O. Bousquet, A. Elisseeff, Algorithmic stability and generalization performance, in: NIPS, 2000, pp. 196-202.</ref-text></ref-info><ref-fulltext>O. Bousquet, A. Elisseeff, Algorithmic stability and generalization performance, in: NIPS, 2000, pp. 196-202.</ref-fulltext></reference><reference id="38"><ref-info><ref-title><ref-titletext>Leave-one-out error and stability of learning algorithms with applications</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">54349123122</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Pontil M.</ce:indexed-name><ce:surname>Pontil</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Learning Theory: Methods, Models and Applications</ref-sourcetitle><ref-publicationyear first="2003"/><ref-text>Suykens J.A.K., et al. (Ed), IOS Press</ref-text></ref-info><ref-fulltext>Bousquet O., and Pontil M. Leave-one-out error and stability of learning algorithms with applications. In: Suykens J.A.K., et al. (Ed). Advances in Learning Theory: Methods, Models and Applications (2003), IOS Press</ref-fulltext></reference><reference id="39"><ref-info><refd-itemidlist><itemid idtype="SGR">5444223864</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Saltelli A.</ce:indexed-name><ce:surname>Saltelli</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Tarantola S.</ce:indexed-name><ce:surname>Tarantola</ce:surname></author><author seq="3"><ce:initials>F.</ce:initials><ce:indexed-name>Campolongo F.</ce:indexed-name><ce:surname>Campolongo</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Ratto M.</ce:indexed-name><ce:surname>Ratto</ce:surname></author></ref-authors><ref-sourcetitle>Sensitivity Analysis in Practice: A Guide to Assessing Scientific Models</ref-sourcetitle><ref-publicationyear first="2003"/><ref-text>John Wiley &amp; Sons Ltd., England</ref-text></ref-info><ref-fulltext>Saltelli A., Tarantola S., Campolongo F., and Ratto M. Sensitivity Analysis in Practice: A Guide to Assessing Scientific Models (2003), John Wiley &amp; Sons Ltd., England</ref-fulltext></reference><reference id="40"><ref-info><ref-title><ref-titletext>Constructive incremental learning from only local information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001108227</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Schaal S.</ce:indexed-name><ce:surname>Schaal</ce:surname></author><author seq="2"><ce:initials>C.G.</ce:initials><ce:indexed-name>Atkeson C.G.</ce:indexed-name><ce:surname>Atkeson</ce:surname></author></ref-authors><ref-sourcetitle>Neural Computation</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><voliss volume="10" issue="8"/><pagerange first="2047" last="2084"/></ref-volisspag></ref-info><ref-fulltext>Schaal S., and Atkeson C.G. Constructive incremental learning from only local information. Neural Computation 10 8 (1998) 2047-2084</ref-fulltext></reference><reference id="41"><ref-info><ref-title><ref-titletext>Combination of multiple classifiers using local accuracy estimates</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031121318</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Woods K.</ce:indexed-name><ce:surname>Woods</ce:surname></author><author seq="2"><ce:initials>W.P.</ce:initials><ce:indexed-name>Kegelmeyer W.P.</ce:indexed-name><ce:surname>Kegelmeyer</ce:surname></author><author seq="3"><ce:initials>K.</ce:initials><ce:indexed-name>Bowyer K.</ce:indexed-name><ce:surname>Bowyer</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on PAMI</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="19" issue="4"/><pagerange first="405" last="410"/></ref-volisspag></ref-info><ref-fulltext>Woods K., Kegelmeyer W.P., and Bowyer K. Combination of multiple classifiers using local accuracy estimates. IEEE Transactions on PAMI 19 4 (1997) 405-410</ref-fulltext></reference><reference id="42"><ref-info><ref-title><ref-titletext>Assessing the quality of learned local models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0343486227</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Schaal S.</ce:indexed-name><ce:surname>Schaal</ce:surname></author><author seq="2"><ce:initials>C.G.</ce:initials><ce:indexed-name>Atkeson C.G.</ce:indexed-name><ce:surname>Atkeson</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="6"/><pagerange first="160" last="167"/></ref-volisspag><ref-text>Cowan J.D., Tesauro G., and Alspector J. (Eds), Morgan Kaufman Publishers, Inc.</ref-text></ref-info><ref-fulltext>Schaal S., and Atkeson C.G. Assessing the quality of learned local models. In: Cowan J.D., Tesauro G., and Alspector J. (Eds). Advances in Neural Information Processing Systems vol. 6 (1994), Morgan Kaufman Publishers, Inc. 160-167</ref-fulltext></reference><reference id="43"><ref-info><refd-itemidlist><itemid idtype="SGR">0004236801</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.P.</ce:initials><ce:indexed-name>Wand M.P.</ce:indexed-name><ce:surname>Wand</ce:surname></author><author seq="2"><ce:initials>M.C.</ce:initials><ce:indexed-name>Jones M.C.</ce:indexed-name><ce:surname>Jones</ce:surname></author></ref-authors><ref-sourcetitle>Kernel Smoothing</ref-sourcetitle><ref-publicationyear first="1995"/><ref-text>Chapman and Hall, London</ref-text></ref-info><ref-fulltext>Wand M.P., and Jones M.C. Kernel Smoothing (1995), Chapman and Hall, London</ref-fulltext></reference><reference id="44"><ref-info><refd-itemidlist><itemid idtype="SGR">0042309838</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.W.</ce:initials><ce:indexed-name>Silverman B.W.</ce:indexed-name><ce:surname>Silverman</ce:surname></author></ref-authors><ref-sourcetitle>Monographs on Statistics and Applied Probability</ref-sourcetitle><ref-publicationyear first="1986"/><ref-text>Chapman and Hall, London</ref-text></ref-info><ref-fulltext>Silverman B.W. Density Estimation for Statistics and Data Analysis. Monographs on Statistics and Applied Probability (1986), Chapman and Hall, London</ref-fulltext></reference><reference id="45"><ref-info><ref-title><ref-titletext>Parzen density estimation using clustering-based branch and bound</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0028499745</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Jeon B.</ce:indexed-name><ce:surname>Jeon</ce:surname></author><author seq="2"><ce:initials>D.A.</ce:initials><ce:indexed-name>Landgrebe D.A.</ce:indexed-name><ce:surname>Landgrebe</ce:surname></author></ref-authors><ref-sourcetitle>Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><pagerange first="950" last="954"/></ref-volisspag></ref-info><ref-fulltext>Jeon B., and Landgrebe D.A. Parzen density estimation using clustering-based branch and bound. Transactions on Pattern Analysis and Machine Intelligence (1994) 950-954</ref-fulltext></reference><reference id="46"><ref-info><refd-itemidlist><itemid idtype="SGR">54349084843</itemid></refd-itemidlist><ref-text>R Development Core Team, A Language and Environment for Statistical Computing, R Foundation for Statistical Computing,Vienna, Austria, 2006.</ref-text></ref-info><ref-fulltext>R Development Core Team, A Language and Environment for Statistical Computing, R Foundation for Statistical Computing,Vienna, Austria, 2006.</ref-fulltext></reference><reference id="47"><ref-info><refd-itemidlist><itemid idtype="SGR">0003802343</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author><author seq="2"><ce:initials>J.H.</ce:initials><ce:indexed-name>Friedman J.H.</ce:indexed-name><ce:surname>Friedman</ce:surname></author><author seq="3"><ce:initials>R.A.</ce:initials><ce:indexed-name>Olshen R.A.</ce:indexed-name><ce:surname>Olshen</ce:surname></author><author seq="4"><ce:initials>C.J.</ce:initials><ce:indexed-name>Stone C.J.</ce:indexed-name><ce:surname>Stone</ce:surname></author></ref-authors><ref-sourcetitle>Classification and Regression Trees</ref-sourcetitle><ref-publicationyear first="1984"/><ref-text>Wadsworth International Group, Belmont CA</ref-text></ref-info><ref-fulltext>Breiman L., Friedman J.H., Olshen R.A., and Stone C.J. Classification and Regression Trees (1984), Wadsworth International Group, Belmont CA</ref-fulltext></reference><reference id="48"><ref-info><ref-title><ref-titletext>A logical calculus of the ideas imminent in nervous activity</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">51249194645</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>W.S.</ce:initials><ce:indexed-name>McCulloch W.S.</ce:indexed-name><ce:surname>McCulloch</ce:surname></author><author seq="2"><ce:initials>W.</ce:initials><ce:indexed-name>Pitts W.</ce:indexed-name><ce:surname>Pitts</ce:surname></author></ref-authors><ref-sourcetitle>Bulletin of Mathematical Biophysics</ref-sourcetitle><ref-publicationyear first="1943"/><ref-volisspag><voliss volume="5"/><pagerange first="115" last="133"/></ref-volisspag></ref-info><ref-fulltext>McCulloch W.S., and Pitts W. A logical calculus of the ideas imminent in nervous activity. Bulletin of Mathematical Biophysics 5 (1943) 115-133</ref-fulltext></reference><reference id="49"><ref-info><refd-itemidlist><itemid idtype="SGR">54349121348</itemid></refd-itemidlist><ref-text>A.J. Smola, B. Schölkopf, A tutorial on support vector regression, NeuroCOLT2 Technical Report NC2-TR-1998-030, 1998.</ref-text></ref-info><ref-fulltext>A.J. Smola, B. Schölkopf, A tutorial on support vector regression, NeuroCOLT2 Technical Report NC2-TR-1998-030, 1998.</ref-fulltext></reference><reference id="50"><ref-info><refd-itemidlist><itemid idtype="SGR">0003798635</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.</ce:initials><ce:indexed-name>Christiannini N.</ce:indexed-name><ce:surname>Christiannini</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Shawe-Taylor J.</ce:indexed-name><ce:surname>Shawe-Taylor</ce:surname></author></ref-authors><ref-sourcetitle>Support Vector Machines and Other Kernel-Based Learning Methods</ref-sourcetitle><ref-publicationyear first="2000"/><ref-text>Cambridge University Press</ref-text></ref-info><ref-fulltext>Christiannini N., and Shawe-Taylor J. Support Vector Machines and Other Kernel-Based Learning Methods (2000), Cambridge University Press</ref-fulltext></reference><reference id="51"><ref-info><refd-itemidlist><itemid idtype="SGR">54349112140</itemid></refd-itemidlist><ref-text>C. Chang, C. Lin, LIBSVM: a library for support vector machines, software available at http://www.csie.ntu.edu.tw/cjlin/libsvm, 2001.</ref-text></ref-info><ref-fulltext>C. Chang, C. Lin, LIBSVM: a library for support vector machines, software available at http://www.csie.ntu.edu.tw/cjlin/libsvm, 2001.</ref-fulltext></reference><reference id="52"><ref-info><ref-title><ref-titletext>Random forests</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0035478854</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><voliss volume="45" issue="1"/><pagerange first="5" last="32"/></ref-volisspag></ref-info><ref-fulltext>Breiman L. Random forests. Machine Learning 45 1 (2001) 5-32</ref-fulltext></reference><reference id="53"><ref-info><refd-itemidlist><itemid idtype="SGR">33745837600</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.N.</ce:initials><ce:indexed-name>Wood S.N.</ce:indexed-name><ce:surname>Wood</ce:surname></author></ref-authors><ref-sourcetitle>Generalized Additive Models: An Introduction with R</ref-sourcetitle><ref-publicationyear first="2006"/><ref-text>Chapman &amp; Hall, CRC</ref-text></ref-info><ref-fulltext>Wood S.N. Generalized Additive Models: An Introduction with R (2006), Chapman &amp; Hall, CRC</ref-fulltext></reference><reference id="54"><ref-info><refd-itemidlist><itemid idtype="SGR">0003598526</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Hastie T.</ce:indexed-name><ce:surname>Hastie</ce:surname></author><author seq="2"><ce:initials>R.</ce:initials><ce:indexed-name>Tibshirani R.</ce:indexed-name><ce:surname>Tibshirani</ce:surname></author></ref-authors><ref-sourcetitle>Generalized Additive Models</ref-sourcetitle><ref-publicationyear first="1990"/><ref-text>Chapman and Hall, London</ref-text></ref-info><ref-fulltext>Hastie T., and Tibshirani R. Generalized Additive Models (1990), Chapman and Hall, London</ref-fulltext></reference><reference id="55"><ref-info><refd-itemidlist><itemid idtype="SGR">54349117750</itemid></refd-itemidlist><ref-text>A. Asuncion D.J. Newman, UCI machine learning repository, 2007.</ref-text></ref-info><ref-fulltext>A. Asuncion D.J. Newman, UCI machine learning repository, 2007.</ref-fulltext></reference><reference id="56"><ref-info><refd-itemidlist><itemid idtype="SGR">54349110925</itemid></refd-itemidlist><ref-text>Department of Statistics at Carnegie Mellon University, Statlib - data, software and news from the statistics community, http://lib.stat.cmu.edu/, 2005.</ref-text></ref-info><ref-fulltext>Department of Statistics at Carnegie Mellon University, Statlib - data, software and news from the statistics community, http://lib.stat.cmu.edu/, 2005.</ref-fulltext></reference><reference id="57"><ref-info><ref-title><ref-titletext>Detection and prediction of errors in epcs of the sap reference model</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">36049019276</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Mendling J.</ce:indexed-name><ce:surname>Mendling</ce:surname></author><author seq="2"><ce:initials>H.M.W.</ce:initials><ce:indexed-name>Verbeek H.M.W.</ce:indexed-name><ce:surname>Verbeek</ce:surname></author><author seq="3"><ce:initials>B.F.</ce:initials><ce:indexed-name>van Dongen B.F.</ce:indexed-name><ce:surname>van Dongen</ce:surname></author><author seq="4"><ce:initials>W.M.P.</ce:initials><ce:indexed-name>van der Aalst W.M.P.</ce:indexed-name><ce:surname>van der Aalst</ce:surname></author></ref-authors><ref-sourcetitle>Data and Knowledge Engineering</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="64" issue="1"/><pagerange first="312" last="329"/></ref-volisspag></ref-info><ref-fulltext>Mendling J., Verbeek H.M.W., van Dongen B.F., and van der Aalst W.M.P. Detection and prediction of errors in epcs of the sap reference model. Data and Knowledge Engineering 64 1 (2008) 312-329</ref-fulltext></reference><reference id="58"><ref-info><ref-title><ref-titletext>Tests and variables selection on regression analysis for massive datasets</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">34548697522</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.H.</ce:initials><ce:indexed-name>Fan T.H.</ce:indexed-name><ce:surname>Fan</ce:surname></author><author seq="2"><ce:initials>K.F.</ce:initials><ce:indexed-name>Cheng K.F.</ce:indexed-name><ce:surname>Cheng</ce:surname></author></ref-authors><ref-sourcetitle>Data and Knowledge Engineering</ref-sourcetitle><ref-publicationyear first="2007"/><ref-volisspag><voliss volume="63" issue="4"/><pagerange first="811" last="819"/></ref-volisspag></ref-info><ref-fulltext>Fan T.H., and Cheng K.F. Tests and variables selection on regression analysis for massive datasets. Data and Knowledge Engineering 63 4 (2007) 811-819</ref-fulltext></reference><reference id="59"><ref-info><ref-title><ref-titletext>Utilizing hierarchical feature domain values for prediction</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">34147132802</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Han Y.</ce:indexed-name><ce:surname>Han</ce:surname></author><author seq="2"><ce:initials>W.</ce:initials><ce:indexed-name>Lam W.</ce:indexed-name><ce:surname>Lam</ce:surname></author></ref-authors><ref-sourcetitle>Data and Knowledge Engineering</ref-sourcetitle><ref-publicationyear first="2007"/><ref-volisspag><voliss volume="61" issue="3"/><pagerange first="540" last="553"/></ref-volisspag></ref-info><ref-fulltext>Han Y., and Lam W. Utilizing hierarchical feature domain values for prediction. Data and Knowledge Engineering 61 3 (2007) 540-553</ref-fulltext></reference><reference id="60"><ref-info><ref-title><ref-titletext>epsilon-ssvr: A smooth support vector machine for epsilon-insensitive regression</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">19944407892</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.J.</ce:initials><ce:indexed-name>Lee Y.J.</ce:indexed-name><ce:surname>Lee</ce:surname></author><author seq="2"><ce:initials>W.F.</ce:initials><ce:indexed-name>Hsieh W.F.</ce:indexed-name><ce:surname>Hsieh</ce:surname></author><author seq="3"><ce:initials>C.M.</ce:initials><ce:indexed-name>Huang C.M.</ce:indexed-name><ce:surname>Huang</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions of Knowledge and Data Engineering</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="17" issue="5"/><pagerange first="678" last="685"/></ref-volisspag></ref-info><ref-fulltext>Lee Y.J., Hsieh W.F., and Huang C.M. epsilon-ssvr: A smooth support vector machine for epsilon-insensitive regression. IEEE Transactions of Knowledge and Data Engineering 17 5 (2005) 678-685</ref-fulltext></reference><reference id="61"><ref-info><ref-title><ref-titletext>Semi-supervised regression with co-training style algorithms</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">35348881683</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.H.</ce:initials><ce:indexed-name>Zhou Z.H.</ce:indexed-name><ce:surname>Zhou</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Li M.</ce:indexed-name><ce:surname>Li</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions of Knowledge and Data Engineering</ref-sourcetitle><ref-publicationyear first="2007"/><ref-volisspag><voliss volume="19" issue="11"/><pagerange first="1479" last="1493"/></ref-volisspag></ref-info><ref-fulltext>Zhou Z.H., and Li M. Semi-supervised regression with co-training style algorithms. IEEE Transactions of Knowledge and Data Engineering 19 11 (2007) 1479-1493</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>