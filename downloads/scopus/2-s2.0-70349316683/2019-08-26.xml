<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/70349316683</prism:url><dc:identifier>SCOPUS_ID:70349316683</dc:identifier><eid>2-s2.0-70349316683</eid><dc:title>Fast training of recurrent neural networks with the LRP reinforcement scheme Hitro u훾enje rekurentnih nevronskih mre탑 s korekcijsko shemo LRP</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Elektrotehniski Vestnik/Electrotechnical Review</prism:publicationName><source-id>16651</source-id><prism:issn>00135852</prism:issn><prism:volume>75</prism:volume><prism:issueIdentifier>5</prism:issueIdentifier><prism:startingPage>311</prism:startingPage><prism:endingPage>316</prism:endingPage><prism:pageRange>311-316</prism:pageRange><prism:coverDate>2008-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="6507593861"><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Ster</ce:surname><ce:given-name>Branko</ce:given-name><preferred-name><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Ster</ce:surname><ce:given-name>Branko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6507593861</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"> <ce:para>Recurrent neural networks (RNNs) [3] are neural networks containing feedback connections which create loops and enable the functionality of memorizing. RNNs are able to solve time-dependent tasks, such as modeling dynamical systems, predicting time-series [4], learning sequences, induction of finite state automata [1, 2], etc. Two algorithms for training recurrent neural networks were compared, i.e. the standard gradient RTRL (Real Time Recurrent Learning) algorithm and heuristic LRP (Linear Reward-Penalty) algorithm, which is otherwise used as a reinforcement scheme in learning automata [5]. The RTRL algorithm [8] may be used in two variants: the batch RTRL, where the same sequence of input/output samples is applied continually for calculation of gradients and consequently for updating the weights, and the on-line RTRL, where the weight updating is done after each presented sample. The results on different variants of the delayed XOR(r) task at r=2, 3, 4 (see Figure 2 for r = 2 and Figure 3 for r = 3) and different RNN sizes show that LRP operates much faster than the batch RTRL (Table 1). The course of the mean squared error (MSE) of the batch RTRL and of the LRP algorithm is also quite different. Figure 4 shows the course of the MSE for different sizes of RNN. The batch RTRL requires a long time before the error begins decreasing; as it finally happens, it decreases quickly and smoothly. The MSE of LRP decreases more monotonically, but on the other hand much less smoothly. By using a moving window, LRP was also compared to the on-line RTRL. LRP was of a comparable speed in RNNs of up to 40 neurons, but much faster in larger RNNs (Table 2).</ce:para> </abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/70349316683" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=70349316683&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=70349316683&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="6507593861"><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Ster</ce:surname><ce:given-name>Branko</ce:given-name><preferred-name><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Ster</ce:surname><ce:given-name>Branko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6507593861</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="slv"/><authkeywords><author-keyword>Finite state automata</author-keyword><author-keyword>Gradient learning</author-keyword><author-keyword>Heuristic algorithms</author-keyword><author-keyword>Learning automata</author-keyword><author-keyword>Recurrent neural networks</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Different sizes</mainterm><mainterm weight="a" candidate="n">Feedback connection</mainterm><mainterm weight="a" candidate="n">Finite state automata</mainterm><mainterm weight="a" candidate="n">Gradient learning</mainterm><mainterm weight="a" candidate="n">Input/output</mainterm><mainterm weight="a" candidate="n">Learning automata</mainterm><mainterm weight="a" candidate="n">Learning sequences</mainterm><mainterm weight="a" candidate="n">Mean squared error</mainterm><mainterm weight="a" candidate="n">Moving window</mainterm><mainterm weight="a" candidate="n">RTRL (real time recurrent learning) algorithm</mainterm><mainterm weight="a" candidate="n">RTRL algorithm</mainterm><mainterm weight="a" candidate="n">Time-dependent</mainterm></idxterms><subject-areas><subject-area code="2208" abbrev="ENGI">Electrical and Electronic Engineering</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered day="19" month="12" timestamp="2016-12-19T17:43:41.000041+00:00" year="2016"/><ait:date-sort day="01" month="12" year="2008"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2009 Elsevier B.V., All rights reserved.</copyright><itemidlist> <itemid idtype="PUI">355312065</itemid> <itemid idtype="CPX">20094012349737</itemid> <itemid idtype="SCP">70349316683</itemid> <itemid idtype="SGR">70349316683</itemid> </itemidlist><history> <date-created day="28" month="09" year="2009"/> </history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="slv" language="Slovenian"/><abstract-language xml:lang="eng" language="English"/><abstract-language xml:lang="slv" language="Slovenian"/><author-keywords> <author-keyword>Finite state automata</author-keyword> <author-keyword>Gradient learning</author-keyword> <author-keyword>Heuristic algorithms</author-keyword> <author-keyword>Learning automata</author-keyword> <author-keyword>Recurrent neural networks</author-keyword> </author-keywords></citation-info><citation-title><titletext original="n" xml:lang="eng" language="English">Fast training of recurrent neural networks with the LRP reinforcement scheme</titletext><titletext original="y" xml:lang="slv" language="Slovenian">Hitro u훾enje rekurentnih nevronskih mre탑 s korekcijsko shemo LRP</titletext></citation-title><author-group><author auid="6507593861" seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Ster</ce:surname><ce:given-name>Branko</ce:given-name><preferred-name> <ce:initials>B.</ce:initials> <ce:indexed-name>Ster B.</ce:indexed-name> <ce:surname>Ster</ce:surname> <ce:given-name>Branko</ce:given-name> </preferred-name></author><affiliation afid="60031106" country="svn" dptid="104580817"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za racunalnistvo in informatiko</organization><address-part>Trzaska 25</address-part><city-group>1000 Ljubljana</city-group><country>Slovenia</country></affiliation></author-group><correspondence><person> <ce:initials>B.</ce:initials> <ce:indexed-name>Ster B.</ce:indexed-name> <ce:surname>Ster</ce:surname> </person><affiliation country="svn"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za racunalnistvo in informatiko</organization><address-part>Trzaska 25</address-part><city-group>1000 Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"> <ce:para>Recurrent neural networks (RNNs) [3] are neural networks containing feedback connections which create loops and enable the functionality of memorizing. RNNs are able to solve time-dependent tasks, such as modeling dynamical systems, predicting time-series [4], learning sequences, induction of finite state automata [1, 2], etc. Two algorithms for training recurrent neural networks were compared, i.e. the standard gradient RTRL (Real Time Recurrent Learning) algorithm and heuristic LRP (Linear Reward-Penalty) algorithm, which is otherwise used as a reinforcement scheme in learning automata [5]. The RTRL algorithm [8] may be used in two variants: the batch RTRL, where the same sequence of input/output samples is applied continually for calculation of gradients and consequently for updating the weights, and the on-line RTRL, where the weight updating is done after each presented sample. The results on different variants of the delayed XOR(r) task at r=2, 3, 4 (see Figure 2 for r = 2 and Figure 3 for r = 3) and different RNN sizes show that LRP operates much faster than the batch RTRL (Table 1). The course of the mean squared error (MSE) of the batch RTRL and of the LRP algorithm is also quite different. Figure 4 shows the course of the MSE for different sizes of RNN. The batch RTRL requires a long time before the error begins decreasing; as it finally happens, it decreases quickly and smoothly. The MSE of LRP decreases more monotonically, but on the other hand much less smoothly. By using a moving window, LRP was also compared to the on-line RTRL. LRP was of a comparable speed in RNNs of up to 40 neurons, but much faster in larger RNNs (Table 2).</ce:para> </abstract></abstracts><source country="svn" srcid="16651" type="j"><sourcetitle>Elektrotehniski Vestnik/Electrotechnical Review</sourcetitle><sourcetitle-abbrev>Elektroteh Vestn Electrotech Rev</sourcetitle-abbrev><issn type="print">00135852</issn><codencode>ELVEA</codencode><volisspag> <voliss issue="5" volume="75"/> <pagerange first="311" last="316"/> </volisspag><publicationyear first="2008"/><publicationdate> <year>2008</year> <date-text xfab-added="true">2008</date-text></publicationdate><website> <ce:e-address type="url">http://ev.fe.uni-lj.si/5-2008/Ster.pdf</ce:e-address> </website></source><enhancement><classificationgroup><classifications type="CPXCLASS"> <classification> <classification-code>731.1</classification-code> <classification-description>Control Systems</classification-description> </classification> <classification> <classification-code>731.5</classification-code> <classification-description>Robotics</classification-description> </classification> <classification> <classification-code>731.6</classification-code> <classification-description>Robot Applications</classification-description> </classification> <classification> <classification-code>816.1</classification-code> <classification-description>Plastics Processing</classification-description> </classification> <classification> <classification-code>901.2</classification-code> <classification-description>Education</classification-description> </classification> <classification> <classification-code>903.1</classification-code> <classification-description>Information Sources and Analysis</classification-description> </classification> <classification> <classification-code>921</classification-code> <classification-description>Applied Mathematics</classification-description> </classification> <classification> <classification-code>922.2</classification-code> <classification-description>Mathematical Statistics</classification-description> </classification> <classification> <classification-code>931</classification-code> <classification-description>Applied Physics Generally</classification-description> </classification> <classification> <classification-code>931.1</classification-code> <classification-description>Mechanics</classification-description> </classification> <classification> <classification-code>951</classification-code> <classification-description>Materials Science</classification-description> </classification> <classification> <classification-code>723.5</classification-code> <classification-description>Computer Applications</classification-description> </classification> <classification> <classification-code>412.2</classification-code> <classification-description>Concrete Reinforcements</classification-description> </classification> <classification> <classification-code>415</classification-code> <classification-description>Metals, Plastics, Wood and Other Structural Materials</classification-description> </classification> <classification> <classification-code>461.1</classification-code> <classification-description>Biomedical Engineering</classification-description> </classification> <classification> <classification-code>716</classification-code> <classification-description>Electronic Equipment, Radar, Radio and Television</classification-description> </classification> <classification> <classification-code>717</classification-code> <classification-description>Electro-Optical Communication</classification-description> </classification> <classification> <classification-code>718</classification-code> <classification-description>Telephone and Other Line Communications</classification-description> </classification> <classification> <classification-code>721.1</classification-code> <classification-description>Computer Theory (Includes Formal Logic, Automata Theory, Switching Theory and Programming Theory)</classification-description> </classification> <classification> <classification-code>722.4</classification-code> <classification-description>Digital Computers and Systems</classification-description> </classification> <classification> <classification-code>723</classification-code> <classification-description>Computer Software, Data Handling and Applications</classification-description> </classification> <classification> <classification-code>723.1</classification-code> <classification-description>Computer Programming</classification-description> </classification> <classification> <classification-code>723.4</classification-code> <classification-description>Artificial Intelligence</classification-description> </classification> </classifications><classifications type="GEOCLASS"> <classification> <classification-code>Related Topics</classification-code> </classification> </classifications><classifications type="ASJC"> <classification>2208</classification> </classifications><classifications type="SUBJABBR"><classification>ENGI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="8"> <reference id="1"> <ref-info> <ref-title> <ref-titletext>Fi-nite state automata and simple recurrent networks</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">0000111307</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>A.</ce:initials> <ce:indexed-name>Cleeremans A.</ce:indexed-name> <ce:surname>Cleeremans</ce:surname> </author> <author seq="2"> <ce:initials>D.</ce:initials> <ce:indexed-name>Servan-Schreiber D.</ce:indexed-name> <ce:surname>Servan-Schreiber</ce:surname> </author> <author seq="3"> <ce:initials>J.L.</ce:initials> <ce:indexed-name>Mcclelland J.L.</ce:indexed-name> <ce:surname>Mcclelland</ce:surname> </author> </ref-authors> <ref-sourcetitle>Neural Computation</ref-sourcetitle> <ref-publicationyear first="1989"/> <ref-volisspag> <voliss issue="3" volume="1"/> <pagerange first="372" last="381"/> </ref-volisspag> </ref-info> <ref-fulltext>A. Cleeremans, D. Servan-Schreiber, J. L. McClelland, Fi-nite State Automata and Simple Recurrent Networks, Neural Computation 1(3): 372-381, 1989.</ref-fulltext> </reference> <reference id="2"> <ref-info> <ref-title> <ref-titletext>On-line identification and recon-struction of finite automata with generalized recurrent neural networks</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">0037254330</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>I.</ce:initials> <ce:indexed-name>Gabrijel I.</ce:indexed-name> <ce:surname>Gabrijel</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Dobnikar A.</ce:indexed-name> <ce:surname>Dobnikar</ce:surname> </author> </ref-authors> <ref-sourcetitle>Neural Networks</ref-sourcetitle> <ref-publicationyear first="2003"/> <ref-volisspag> <voliss volume="16"/> <pagerange first="101" last="120"/> </ref-volisspag> </ref-info> <ref-fulltext>I. Gabrijel, A. Dobnikar, On-line identification and recon-struction of finite automata with generalized recurrent neural networks, Neural Networks 16: 101-120, 2003.</ref-fulltext> </reference> <reference id="3"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">0003413187</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>S.</ce:initials> <ce:indexed-name>Haykin S.</ce:indexed-name> <ce:surname>Haykin</ce:surname> </author> </ref-authors> <ref-sourcetitle>Neural Networks: A Comprehensive Foundation</ref-sourcetitle> <ref-publicationyear first="1994"/> <ref-text>Macmillan College Publishing Company</ref-text> </ref-info> <ref-fulltext>S. Haykin, Neural Networks: A Comprehensive Foundation, Macmillan College Publishing Company, 1994.</ref-fulltext> </reference> <reference id="4"> <ref-info> <ref-title> <ref-titletext>Predicting time series using neural networks with wavelet-based denoising layers</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">17444402426</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>U.</ce:initials> <ce:indexed-name>Lotric U.</ce:indexed-name> <ce:surname>Lotric</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Dobnikar A.</ce:indexed-name> <ce:surname>Dobnikar</ce:surname> </author> </ref-authors> <ref-sourcetitle>Neural Com-puting And Applications</ref-sourcetitle> <ref-publicationyear first="2005"/> <ref-volisspag> <voliss issue="1" volume="14"/> <pagerange first="11" last="17"/> </ref-volisspag> </ref-info> <ref-fulltext>U. Lotric, A. Dobnikar, Predicting time series using neural networks with wavelet-based denoising layers, Neural com-puting and applications 14(1): 11-17, 2005.</ref-fulltext> </reference> <reference id="5"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">0003891507</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>K.</ce:initials> <ce:indexed-name>Narendra K.</ce:indexed-name> <ce:surname>Narendra</ce:surname> </author> <author seq="2"> <ce:initials>M.</ce:initials> <ce:indexed-name>Thathachar M.</ce:indexed-name> <ce:surname>Thathachar</ce:surname> </author> </ref-authors> <ref-sourcetitle>Learning Automata: An Intro-duction</ref-sourcetitle> <ref-publicationyear first="1989"/> <ref-text>Prentice-Hall, Englewood Cliffs, New Jersey</ref-text> </ref-info> <ref-fulltext>K. Narendra, M. Thathachar, Learning Automata: An Intro-duction, Prentice-Hall, Englewood Cliffs, New Jersey, 1989.</ref-fulltext> </reference> <reference id="6"> <ref-info> <ref-title> <ref-titletext>Learning in-ternal representations by error propagation</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">0000646059</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>D.E.</ce:initials> <ce:indexed-name>Rumelhart D.E.</ce:indexed-name> <ce:surname>Rumelhart</ce:surname> </author> <author seq="2"> <ce:initials>G.E.</ce:initials> <ce:indexed-name>Hinton G.E.</ce:indexed-name> <ce:surname>Hinton</ce:surname> </author> <author seq="3"> <ce:initials>R.J.</ce:initials> <ce:indexed-name>Williams R.J.</ce:indexed-name> <ce:surname>Williams</ce:surname> </author> </ref-authors> <ref-sourcetitle>Parallel Distributed Process-ing</ref-sourcetitle> <ref-publicationyear first="1986"/> <ref-volisspag> <voliss volume="1"/> <pagerange first="318" last="362"/> </ref-volisspag> <ref-text>D. E. Rumel-hart, J. L. McClelland (ur.):, The MIT Press, Cambridge, Massachusetts</ref-text> </ref-info> <ref-fulltext>D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning in-ternal representations by error propagation, v D. E. Rumel-hart, J. L. McClelland (ur.): Parallel Distributed Process-ing 1: 318-362, The MIT Press, Cambridge, Massachusetts, 1986.</ref-fulltext> </reference> <reference id="7"> <ref-info> <ref-title> <ref-titletext>Recurrent neural network training by a learning automaton approach for trajectory learning and control system design</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">0032072476</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>M.</ce:initials> <ce:indexed-name>Sundareshan M.</ce:indexed-name> <ce:surname>Sundareshan</ce:surname> </author> <author seq="2"> <ce:initials>T.</ce:initials> <ce:indexed-name>Condarcure T.</ce:indexed-name> <ce:surname>Condarcure</ce:surname> </author> </ref-authors> <ref-sourcetitle>IEEE Transactions on Neural Networks</ref-sourcetitle> <ref-publicationyear first="1998"/> <ref-volisspag> <voliss issue="3" volume="9"/> <pagerange first="354" last="368"/> </ref-volisspag> </ref-info> <ref-fulltext>M. Sundareshan, T. Condarcure, Recurrent neural network training by a learning automaton approach for trajectory learning and control system design, IEEE Transactions on Neural Networks 9(3): 354-368, 1998.</ref-fulltext> </reference> <reference id="8"> <ref-info> <ref-title> <ref-titletext>A learning algorithm for con-tinually running fully recurrent neural networks</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">0001202594</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>R.J.</ce:initials> <ce:indexed-name>Williams R.J.</ce:indexed-name> <ce:surname>Williams</ce:surname> </author> <author seq="2"> <ce:initials>D.</ce:initials> <ce:indexed-name>Zipser D.</ce:indexed-name> <ce:surname>Zipser</ce:surname> </author> </ref-authors> <ref-sourcetitle>Neural Computation</ref-sourcetitle> <ref-publicationyear first="1989"/> <ref-volisspag> <voliss issue="2" volume="1"/> <pagerange first="270" last="280"/> </ref-volisspag> </ref-info> <ref-fulltext>R. J. Williams, D. Zipser, A Learning Algorithm for Con-tinually Running Fully Recurrent Neural Networks. Neural Computation 1(2): 270-280, 1989.</ref-fulltext> </reference> </bibliography></tail></bibrecord></item></abstracts-retrieval-response>