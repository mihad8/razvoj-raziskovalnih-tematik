<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/74849088359</prism:url><dc:identifier>SCOPUS_ID:74849088359</dc:identifier><eid>2-s2.0-74849088359</eid><dc:title>Time-series modeling using information-theory techniques Modeliranje časovnih vrst z metodami teorije informacij</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Elektrotehniski Vestnik/Electrotechnical Review</prism:publicationName><source-id>16651</source-id><prism:issn>00135852</prism:issn><prism:volume>76</prism:volume><prism:issueIdentifier>4</prism:issueIdentifier><prism:startingPage>240</prism:startingPage><prism:endingPage>245</prism:endingPage><prism:pageRange>240-245</prism:pageRange><prism:coverDate>2009-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="34167587100"><ce:initials>M.</ce:initials><ce:indexed-name>Bratina M.</ce:indexed-name><ce:surname>Bratina</ce:surname><ce:given-name>Marko</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Bratina M.</ce:indexed-name><ce:surname>Bratina</ce:surname><ce:given-name>Marko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/34167587100</author-url><affiliation id="106423050" href="https://api.elsevier.com/content/affiliation/affiliation_id/106423050"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>The paper analyzes the possibility of using measures originating from the information theory in modeling dynamic systems from time series. Our modeling was based on a multilayered perceptron neural network into which several information measures were integrated. The presented information measures are based on the Shannon's definitions of entropy and the divergence given in Eqs. (1) and (2). To simplify the extremely computationally intensive methods, the Renyi's counterparts defined by Eqs. (6) and (7) are preferred instead. For example, the information potential given in (9) can be calculated even without integrations. Besides the information potential, the Renyi's approximations of the measures presented by Eqs. (3) and (5) are very useful in modeling. The measures originating from the information theory were included in two ways: in the data preprocessing and as the criterion function during the process of neural network learning. The basic aim of preprocessing is to reduce the number of inputs to the model and/or reformulate them and thus improve the model performance and generalization ability. Besides classical methods for feature extraction, including the appropriate number of the last values in a time series that results in the best model (H1), the greedy algorithm for finding the combination of inputs, enabling the best model performance (H2) or making a linear combination of original inputs using the principal component analysis (PCA), two methods based on the information theory were tested: the independent component analysis (ICA) and the maximally discriminative projection (MDP). The neural network learning is usually based on minimizing the mean square error ε(E). As an alternative, maximizing information potential V<inf>R</inf>(E) is considered as the learning criterion. The proposed ideas were tested on five different time series. Fig. 1 gives results for predicting the future value in the time series, while average results are summarized in Table 1. The model performance is estimated by several measures: the normalized root mean square error (NRMSE), the normalized information potential (NIP) and the number of model parameters (N<inf>PAR</inf>). The ICA preprocessing combined with learning based on information potential yields results comparable to method H2. The latter is, however, computationally extremely intensive. In Fig. 2 and Table 2, classification of the predicted values of time series is presented. As a performance measure, the proportion of correct classification (P<inf>OK</inf>) is added. The ICA and MDP methods perform equally well in this case. We show that the methods based on the information theory can be efficiently used in retrieval of relevant features from data. Besides, the usage of the information potential as a criterion function in the neural-network learning process is also promising.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/74849088359" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=74849088359&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=74849088359&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><affiliation id="106423050" href="https://api.elsevier.com/content/affiliation/affiliation_id/106423050"><affilname>d.o.o.</affilname><affiliation-city>Kranj</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="34167587100"><ce:initials>M.</ce:initials><ce:indexed-name>Bratina M.</ce:indexed-name><ce:surname>Bratina</ce:surname><ce:given-name>Marko</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Bratina M.</ce:indexed-name><ce:surname>Bratina</ce:surname><ce:given-name>Marko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/34167587100</author-url><affiliation id="106423050" href="https://api.elsevier.com/content/affiliation/affiliation_id/106423050"/></author><author seq="2" auid="26643142700"><ce:initials>A.</ce:initials><ce:indexed-name>Dobnikar A.</ce:indexed-name><ce:surname>Dobnikar</ce:surname><ce:given-name>Andrej</ce:given-name><preferred-name><ce:initials>A.</ce:initials><ce:indexed-name>Dobnikar A.</ce:indexed-name><ce:surname>Dobnikar</ce:surname><ce:given-name>Andrej</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/26643142700</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="6506205187"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name><preferred-name><ce:initials>U.</ce:initials><ce:indexed-name>Lotrič U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6506205187</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="slo"/><authkeywords><author-keyword>Information theory</author-keyword><author-keyword>Neural networks</author-keyword><author-keyword>Pre-processing</author-keyword><author-keyword>Time-series modeling</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Best model</mainterm><mainterm weight="a" candidate="n">Classical methods</mainterm><mainterm weight="a" candidate="n">Criterion functions</mainterm><mainterm weight="a" candidate="n">Data preprocessing</mainterm><mainterm weight="a" candidate="n">Dynamic Systems</mainterm><mainterm weight="a" candidate="n">Generalization ability</mainterm><mainterm weight="a" candidate="n">Greedy algorithms</mainterm><mainterm weight="a" candidate="n">Information measures</mainterm><mainterm weight="a" candidate="n">Information potential</mainterm><mainterm weight="a" candidate="n">Learning criterion</mainterm><mainterm weight="a" candidate="n">Linear combinations</mainterm><mainterm weight="a" candidate="n">Minimizing the mean square errors</mainterm><mainterm weight="a" candidate="n">Model parameters</mainterm><mainterm weight="a" candidate="n">Model performance</mainterm><mainterm weight="a" candidate="n">Multi-layered</mainterm><mainterm weight="a" candidate="n">Neural network learning</mainterm><mainterm weight="a" candidate="n">Perceptron neural networks</mainterm><mainterm weight="a" candidate="n">Performance measure</mainterm><mainterm weight="a" candidate="n">Pre-processing</mainterm><mainterm weight="a" candidate="n">Root mean square errors</mainterm></idxterms><subject-areas><subject-area code="2208" abbrev="ENGI">Electrical and Electronic Engineering</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2017" month="11" day="09" timestamp="2017-11-09T18:44:29.000029-05:00"/><ait:date-sort year="2009" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2010 Elsevier B.V., All rights reserved.</copyright><itemidlist><itemid idtype="PUI">358154830</itemid><itemid idtype="CPX">20100412665108</itemid><itemid idtype="SCP">74849088359</itemid><itemid idtype="SGR">74849088359</itemid></itemidlist><history><date-created year="2010" month="01" day="28"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="slo" language="Slovak"/><abstract-language xml:lang="eng" language="English"/><abstract-language xml:lang="slo" language="Slovak"/><author-keywords><author-keyword>Information theory</author-keyword><author-keyword>Neural networks</author-keyword><author-keyword>Pre-processing</author-keyword><author-keyword>Time-series modeling</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="n" language="English">Time-series modeling using information-theory techniques</titletext><titletext xml:lang="slo" original="y" language="Slovak">Modeliranje časovnih vrst z metodami teorije informacij</titletext></citation-title><author-group><author auid="34167587100" seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Bratina M.</ce:indexed-name><ce:surname>Bratina</ce:surname><ce:given-name>Marko</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Bratina M.</ce:indexed-name><ce:surname>Bratina</ce:surname><ce:given-name>Marko</ce:given-name></preferred-name></author><affiliation afid="106423050" country="svn"><organization>Savatech, d.o.o.</organization><address-part>Škofjeloška 6</address-part><city-group>Kranj</city-group><affiliation-id afid="106423050"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="26643142700" seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Dobnikar A.</ce:indexed-name><ce:surname>Dobnikar</ce:surname><ce:given-name>Andrej</ce:given-name><preferred-name><ce:initials>A.</ce:initials><ce:indexed-name>Dobnikar A.</ce:indexed-name><ce:surname>Dobnikar</ce:surname><ce:given-name>Andrej</ce:given-name></preferred-name></author><author auid="6506205187" seq="3"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name><preferred-name><ce:initials>U.</ce:initials><ce:indexed-name>Lotrič U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name></preferred-name></author><affiliation afid="60031106" dptid="104580817" country="svn"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za Računalništvo in Informatiko</organization><address-part>Tržaška 25</address-part><city-group>Ljubljana</city-group><affiliation-id afid="60031106" dptid="104580817"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>M.</ce:initials><ce:indexed-name>Bratina M.</ce:indexed-name><ce:surname>Bratina</ce:surname></person><affiliation country="svn"><organization>Savatech, d.o.o.</organization><address-part>Škofjeloška 6</address-part><city-group>Kranj</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>The paper analyzes the possibility of using measures originating from the information theory in modeling dynamic systems from time series. Our modeling was based on a multilayered perceptron neural network into which several information measures were integrated. The presented information measures are based on the Shannon's definitions of entropy and the divergence given in Eqs. (1) and (2). To simplify the extremely computationally intensive methods, the Renyi's counterparts defined by Eqs. (6) and (7) are preferred instead. For example, the information potential given in (9) can be calculated even without integrations. Besides the information potential, the Renyi's approximations of the measures presented by Eqs. (3) and (5) are very useful in modeling. The measures originating from the information theory were included in two ways: in the data preprocessing and as the criterion function during the process of neural network learning. The basic aim of preprocessing is to reduce the number of inputs to the model and/or reformulate them and thus improve the model performance and generalization ability. Besides classical methods for feature extraction, including the appropriate number of the last values in a time series that results in the best model (H1), the greedy algorithm for finding the combination of inputs, enabling the best model performance (H2) or making a linear combination of original inputs using the principal component analysis (PCA), two methods based on the information theory were tested: the independent component analysis (ICA) and the maximally discriminative projection (MDP). The neural network learning is usually based on minimizing the mean square error ε(E). As an alternative, maximizing information potential V<inf>R</inf>(E) is considered as the learning criterion. The proposed ideas were tested on five different time series. Fig. 1 gives results for predicting the future value in the time series, while average results are summarized in Table 1. The model performance is estimated by several measures: the normalized root mean square error (NRMSE), the normalized information potential (NIP) and the number of model parameters (N<inf>PAR</inf>). The ICA preprocessing combined with learning based on information potential yields results comparable to method H2. The latter is, however, computationally extremely intensive. In Fig. 2 and Table 2, classification of the predicted values of time series is presented. As a performance measure, the proportion of correct classification (P<inf>OK</inf>) is added. The ICA and MDP methods perform equally well in this case. We show that the methods based on the information theory can be efficiently used in retrieval of relevant features from data. Besides, the usage of the information potential as a criterion function in the neural-network learning process is also promising.</ce:para></abstract></abstracts><source srcid="16651" type="j" country="svn"><sourcetitle>Elektrotehniski Vestnik/Electrotechnical Review</sourcetitle><sourcetitle-abbrev>Elektroteh Vestn Electrotech Rev</sourcetitle-abbrev><issn type="print">00135852</issn><codencode>ELVEA</codencode><volisspag><voliss volume="76" issue="4"/><pagerange first="240" last="245"/></volisspag><publicationyear first="2009"/><publicationdate><year>2009</year><date-text xfab-added="true">2009</date-text></publicationdate><website><ce:e-address type="url">http://ev.fe.uni-lj.si/4-2009/Bratina.pdf</ce:e-address></website></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification> <classification-code>931</classification-code> <classification-description>Applied Physics Generally</classification-description> </classification><classification> <classification-code>731.1</classification-code> <classification-description>Control Systems</classification-description> </classification><classification> <classification-code>741.1</classification-code> <classification-description>Light and Optics</classification-description> </classification><classification> <classification-code>751.1</classification-code> <classification-description>Acoustic Waves</classification-description> </classification><classification> <classification-code>903.3</classification-code> <classification-description>Information Retrieval and Use</classification-description> </classification><classification> <classification-code>921</classification-code> <classification-description>Applied Mathematics</classification-description> </classification><classification> <classification-code>922.2</classification-code> <classification-description>Mathematical Statistics</classification-description> </classification><classification> <classification-code>723.5</classification-code> <classification-description>Computer Applications</classification-description> </classification><classification> <classification-code>723.1</classification-code> <classification-description>Computer Programming</classification-description> </classification><classification> <classification-code>716.1</classification-code> <classification-description>Information and Communication Theory</classification-description> </classification><classification> <classification-code>716</classification-code> <classification-description>Electronic Equipment, Radar, Radio and Television</classification-description> </classification><classification> <classification-code>631.1</classification-code> <classification-description>Fluid Flow, General</classification-description> </classification><classification> <classification-code>461.1</classification-code> <classification-description>Biomedical Engineering</classification-description> </classification><classification> <classification-code>723.4</classification-code> <classification-description>Artificial Intelligence</classification-description> </classification></classifications><classifications type="GEOCLASS"><classification> <classification-code>Related Topics</classification-code> </classification></classifications><classifications type="ASJC"><classification>2208</classification></classifications><classifications type="SUBJABBR"><classification>ENGI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="17"><reference id="1"><ref-info><ref-title><ref-titletext>Modeliranje nelinearnih dinamičnih sistemov na osnovi teorije informacij</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">74849086518</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Dobnikar A.</ce:indexed-name><ce:surname>Dobnikar</ce:surname></author></ref-authors><ref-sourcetitle>Znanje za trajnostni razvoj: Zbornik povzetkov referatov 27. mednarodne konference o razvoju organizacijskih znanosti</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><pagerange first="32" last="45"/></ref-volisspag><ref-text>Slovenija, Portorož</ref-text></ref-info><ref-fulltext>A. Dobnikar, Modeliranje nelinearnih dinamičnih sistemov na osnovi teorije informacij, Znanje za trajnostni razvoj: zbornik povzetkov referatov 27. mednarodne konference o razvoju organizacijskih znanosti, Slovenija, Portorož, 32-45, 2008.</ref-fulltext></reference><reference id="2"><ref-info><refd-itemidlist><itemid idtype="SGR">0004027973</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.C.A.</ce:initials><ce:indexed-name>van der Lubbe J.C.A.</ce:indexed-name><ce:surname>van der Lubbe</ce:surname></author></ref-authors><ref-sourcetitle>Information Theory</ref-sourcetitle><ref-publicationyear first="1997"/><ref-text>Cambridge, Cambridge University</ref-text></ref-info><ref-fulltext>J. C. A. van der Lubbe, Information Theory, Cambridge, Cambridge University, 1997.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>An Error-Entropy Minimization Algorithm for Supervised Training of Nonlinear Adaptive Systems</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036647905</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Erdogmus D.</ce:indexed-name><ce:surname>Erdogmus</ce:surname></author><author seq="2"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Principe</ce:surname></author></ref-authors><ref-sourcetitle>IEEE trasactions on signal processing</ref-sourcetitle><ref-publicationyear first="1780" last="1786"/><ref-volisspag><voliss volume="50"/></ref-volisspag><ref-text>2002</ref-text></ref-info><ref-fulltext>D. Erdogmus, J. C. Principe, An Error-Entropy Minimization Algorithm for Supervised Training of Nonlinear Adaptive Systems, IEEE trasactions on signal processing, 50, 1780-1786, 2002.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Extracting cellular automation rules directly from experimental data</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">44949289686</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.C.</ce:initials><ce:indexed-name>Richards F.C.</ce:indexed-name><ce:surname>Richards</ce:surname></author><author seq="2"><ce:initials>T.P.</ce:initials><ce:indexed-name>Meyer T.P.</ce:indexed-name><ce:surname>Meyer</ce:surname></author><author seq="3"><ce:initials>N.H.</ce:initials><ce:indexed-name>Packard N.H.</ce:indexed-name><ce:surname>Packard</ce:surname></author></ref-authors><ref-sourcetitle>Physica D</ref-sourcetitle><ref-publicationyear first="1990"/><ref-volisspag><voliss volume="45"/><pagerange first="189" last="202"/></ref-volisspag></ref-info><ref-fulltext>F. C. Richards, T. P. Meyer, N. H. Packard, Extracting cellular automation rules directly from experimental data, Physica D, 45, 189-202, 1990.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Blind source separation using Renyi's Mutual Information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0035369829</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.E.</ce:initials><ce:indexed-name>Hild K.E.</ce:indexed-name><ce:surname>Hild</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Erdogmus D.</ce:indexed-name><ce:surname>Erdogmus</ce:surname></author><author seq="3"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Principe</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Signal Processing Letters</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><voliss volume="8"/></ref-volisspag></ref-info><ref-fulltext>K. E. Hild, D. Erdogmus, J. C. Principe, Blind source separation using Renyi's Mutual Information, IEEE Signal Processing Letters, 8, 2001.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Matrix formulation of the multilayered perceptron with a denoising unit</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0346307309</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotrič</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Dobnikar A.</ce:indexed-name><ce:surname>Dobnikar</ce:surname></author></ref-authors><ref-sourcetitle>Elektrotehniški vestnik</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><voliss volume="70" issue="4"/><pagerange first="221" last="226"/></ref-volisspag></ref-info><ref-fulltext>U. Lotrič, A. Dobnikar, Matrix formulation of the multilayered perceptron with a denoising unit, Elektrotehniški vestnik, 70, 4, 221-226, 2003.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>From Adaptive Filtering to Nonlinear Information Processing</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85032751845</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Erdogmus D.</ce:indexed-name><ce:surname>Erdogmus</ce:surname></author><author seq="2"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Principe</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Signal Processing Magazine</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="23" issue="6"/><pagerange first="14" last="33"/></ref-volisspag></ref-info><ref-fulltext>D. Erdogmus, J. C. Principe, From Adaptive Filtering to Nonlinear Information Processing, IEEE Signal Processing Magazine, 23, 6, 14-33, 2006.</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Nonparametric entropy estimation: An overview</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001787422</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Beirlant J.</ce:indexed-name><ce:surname>Beirlant</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Dudewicz E.</ce:indexed-name><ce:surname>Dudewicz</ce:surname></author><author seq="3"><ce:initials>L.</ce:initials><ce:indexed-name>Gyorfi L.</ce:indexed-name><ce:surname>Gyorfi</ce:surname></author><author seq="4"><ce:initials>E.</ce:initials><ce:indexed-name>van der Meulen E.</ce:indexed-name><ce:surname>van der Meulen</ce:surname></author></ref-authors><ref-sourcetitle>International Journal of Mathematical and Statistical Sciences</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="80" issue="1"/><pagerange first="17" last="39"/></ref-volisspag></ref-info><ref-fulltext>J. Beirlant, E. Dudewicz, L. Gyorfi, E. van der Meulen, Nonparametric entropy estimation: an overview, International Journal of Mathematical and Statistical Sciences, 80, 1, 17-39, 1997.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>LEGClustA Clustering Algorithm Based on Layered Entropic Subgraphs</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">36448984894</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.M.</ce:initials><ce:indexed-name>Santos J.M.</ce:indexed-name><ce:surname>Santos</ce:surname></author><author seq="2"><ce:initials>J.M.</ce:initials><ce:indexed-name>de Sa J.M.</ce:indexed-name><ce:surname>de Sa</ce:surname></author><author seq="3"><ce:initials>L.A.</ce:initials><ce:indexed-name>Alexandre L.A.</ce:indexed-name><ce:surname>Alexandre</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="30" issue="1"/><pagerange first="62" last="75"/></ref-volisspag></ref-info><ref-fulltext>J. M. Santos, J. M. de Sa, L. A. Alexandre, LEGClustA Clustering Algorithm Based on Layered Entropic Subgraphs, IEEE Transactions on Pattern Analysis and Machine Intelligence, 30, 1, 62-75, 2008.</ref-fulltext></reference><reference id="10"><ref-info><refd-itemidlist><itemid idtype="SGR">0242704414</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.A.</ce:initials><ce:indexed-name>Freitas A.A.</ce:indexed-name><ce:surname>Freitas</ce:surname></author></ref-authors><ref-sourcetitle>Data Mining and Knowledge Discovery with Evolutionary Algorithms</ref-sourcetitle><ref-publicationyear first="2002"/><ref-text>Berlin Heidelberg, Springer</ref-text></ref-info><ref-fulltext>A. A. Freitas, Data Mining and Knowledge Discovery with Evolutionary Algorithms, Berlin Heidelberg, Springer, 2002.</ref-fulltext></reference><reference id="11"><ref-info><refd-itemidlist><itemid idtype="SGR">53949100479</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Gorban A.</ce:indexed-name><ce:surname>Gorban</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Kegl B.</ce:indexed-name><ce:surname>Kegl</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Wunsch D.</ce:indexed-name><ce:surname>Wunsch</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Zinovyev A.</ce:indexed-name><ce:surname>Zinovyev</ce:surname></author></ref-authors><ref-sourcetitle>Principal Manifolds for Data Visualisation and Dimension Reduction</ref-sourcetitle><ref-publicationyear first="2007"/><ref-text>New York, Springer</ref-text></ref-info><ref-fulltext>A. Gorban, B. Kegl, D. Wunsch, A. Zinovyev, Principal Manifolds for Data Visualisation and Dimension Reduction, New York, Springer, 2007.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Independent Component Analysis: A new concept?</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0028416938</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Comon P.</ce:indexed-name><ce:surname>Comon</ce:surname></author></ref-authors><ref-sourcetitle>Signal Processing</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="36" issue="3"/><pagerange first="287" last="314"/></ref-volisspag></ref-info><ref-fulltext>P. Comon, Independent Component Analysis: a new concept?, Signal Processing, 36, 3, 287-314, 1994.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>High-order contrasts for independent component analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0032612381</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.-F.</ce:initials><ce:indexed-name>Cardoso J.-F.</ce:indexed-name><ce:surname>Cardoso</ce:surname></author></ref-authors><ref-sourcetitle>Neural Computation</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="11"/><pagerange first="157" last="192"/></ref-volisspag></ref-info><ref-fulltext>J.-F. Cardoso, High-order contrasts for independent component analysis, Neural Computation, 11, 157-192, 1999.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Fast and Robust Fixed-Point Algorithms for Independent Component Analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0032629347</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Hyvarinen A.</ce:indexed-name><ce:surname>Hyvarinen</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Neural Networks</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="10" issue="3"/><pagerange first="626" last="634"/></ref-volisspag></ref-info><ref-fulltext>A. Hyvarinen, Fast and Robust Fixed-Point Algorithms for Independent Component Analysis, IEEE Transactions on Neural Networks, 10, 3, 626-634, 1999.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Independent component analysis based on marginal density estimation using weighted Parzen windows</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">51049096370</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.-M.</ce:initials><ce:indexed-name>Wu J.-M.</ce:indexed-name><ce:surname>Wu</ce:surname></author><author seq="2"><ce:initials>M.-H.</ce:initials><ce:indexed-name>Chen M.-H.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="3"><ce:initials>Z.-H.</ce:initials><ce:indexed-name>Lin Z.-H.</ce:indexed-name><ce:surname>Lin</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="21" issue="7"/><pagerange first="914" last="924"/></ref-volisspag></ref-info><ref-fulltext>J.-M. Wu, M.-H. Chen, Z.-H. Lin, Independent component analysis based on marginal density estimation using weighted Parzen windows, Neural Networks, 21, 7, 914-924, 2008.</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Sequential Feature Extraction Using Information Theoretic Learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33746475260</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.E.</ce:initials><ce:indexed-name>Hild K.E.</ce:indexed-name><ce:surname>Hild</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Erdogmus D.</ce:indexed-name><ce:surname>Erdogmus</ce:surname></author><author seq="3"><ce:initials>K.</ce:initials><ce:indexed-name>Torkkola K.</ce:indexed-name><ce:surname>Torkkola</ce:surname></author><author seq="4"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Principe</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="28" issue="9"/><pagerange first="1385" last="1393"/></ref-volisspag></ref-info><ref-fulltext>K. E. Hild, D. Erdogmus, K. Torkkola, J. C. Principe, Sequential Feature Extraction Using Information Theoretic Learning, IEEE Transactions on Pattern Analysis and Machine Intelligence, 28, 9, 1385-1393, 2006.</ref-fulltext></reference><reference id="17"><ref-info><refd-itemidlist><itemid idtype="SGR">0003413187</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Haykin S.</ce:indexed-name><ce:surname>Haykin</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks: A Comprehensive Foundation</ref-sourcetitle><ref-publicationyear first="1999"/><ref-text>New Jersey, Prentice-Hall</ref-text></ref-info><ref-fulltext>S. Haykin, Neural Networks: A Comprehensive Foundation, New Jersey, Prentice-Hall, 1999.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>