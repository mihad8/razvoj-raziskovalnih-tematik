<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/77952017592</prism:url><dc:identifier>SCOPUS_ID:77952017592</dc:identifier><eid>2-s2.0-77952017592</eid><prism:doi>10.1007/978-3-642-04391-8_15</prism:doi><dc:title>Combining audio and video for detection of spontaneous emotions</dc:title><prism:aggregationType>Book Series</prism:aggregationType><srctype>k</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</prism:publicationName><source-id>25674</source-id><prism:isbn>3642043909</prism:isbn><prism:isbn>9783642043901</prism:isbn><prism:issn>03029743 16113349</prism:issn><prism:volume>5707 LNCS</prism:volume><prism:startingPage>114</prism:startingPage><prism:endingPage>121</prism:endingPage><prism:pageRange>114-121</prism:pageRange><prism:coverDate>2009-12-01</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="25121212700"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname><ce:given-name>Rok</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Gajšek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname><ce:given-name>Rok</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/25121212700</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>The paper presents our initial attempts in building an audio video emotion recognition system. Both, audio and video sub-systems are discussed, and description of the database of spontaneous emotions is given. The task of labelling the recordings from the database according to different emotions is discussed and the measured agreement between multiple annotators is presented. Instead of focusing on the prosody in audio emotion recognition, we evaluate the possibility of using linear transformations (CMLLR) as features. The classification results from audio and video sub-systems are combined using sum rule fusion and the increase in recognition results, when using both modalities, is presented. © 2009 Springer-Verlag.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/77952017592" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=77952017592&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=77952017592&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="25121212700"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname><ce:given-name>Rok</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Gajšek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname><ce:given-name>Rok</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/25121212700</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="17347474600"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Štruc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/17347474600</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="6505886301"><ce:initials>S.</ce:initials><ce:indexed-name>Dobrisek S.</ce:indexed-name><ce:surname>Dobrišek</ce:surname><ce:given-name>Simon</ce:given-name><preferred-name><ce:initials>S.</ce:initials><ce:indexed-name>Dobrišek S.</ce:indexed-name><ce:surname>Dobrišek</ce:surname><ce:given-name>Simon</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6505886301</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="4" auid="6507417859"><ce:initials>J.</ce:initials><ce:indexed-name>Zibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Žibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6507417859</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="5" auid="6604057186"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name><preferred-name><ce:initials>F.</ce:initials><ce:indexed-name>Mihelič F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6604057186</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="6" auid="6603816127"><ce:initials>N.</ce:initials><ce:indexed-name>Pavesic N.</ce:indexed-name><ce:surname>Pavešić</ce:surname><ce:given-name>Nikola</ce:given-name><preferred-name><ce:initials>N.</ce:initials><ce:indexed-name>Pavešić N.</ce:indexed-name><ce:surname>Pavešić</ce:surname><ce:given-name>Nikola</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603816127</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Bimodal emotion database</author-keyword><author-keyword>Emotion recognition</author-keyword><author-keyword>Linear transformations</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Audio and video</mainterm><mainterm weight="a" candidate="n">Audio videos</mainterm><mainterm weight="a" candidate="n">Bimodal emotion database</mainterm><mainterm weight="a" candidate="n">Classification results</mainterm><mainterm weight="a" candidate="n">Emotion recognition</mainterm><mainterm weight="a" candidate="n">In-buildings</mainterm><mainterm weight="a" candidate="n">Sub-systems</mainterm><mainterm weight="a" candidate="n">Sum rule</mainterm></idxterms><subject-areas><subject-area code="2614" abbrev="MATH">Theoretical Computer Science</subject-area><subject-area code="1700" abbrev="COMP">Computer Science (all)</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2017" month="09" day="15" timestamp="2017-09-15T03:48:48.000048-04:00"/><ait:date-sort year="2009" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2010 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1007/978-3-642-04391-8_15</ce:doi><itemid idtype="PUI">358778899</itemid><itemid idtype="CPX">20101912924286</itemid><itemid idtype="SCP">77952017592</itemid><itemid idtype="SGR">77952017592</itemid></itemidlist><history><date-created year="2010" month="05" day="14"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword>Bimodal emotion database</author-keyword><author-keyword>Emotion recognition</author-keyword><author-keyword>Linear transformations</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Combining audio and video for detection of spontaneous emotions</titletext></citation-title><author-group><author auid="25121212700" seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname><ce:given-name>Rok</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Gajšek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname><ce:given-name>Rok</ce:given-name></preferred-name></author><author auid="17347474600" seq="2"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Štruc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name></preferred-name></author><author auid="6505886301" seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Dobrisek S.</ce:indexed-name><ce:surname>Dobrišek</ce:surname><ce:given-name>Simon</ce:given-name><preferred-name><ce:initials>S.</ce:initials><ce:indexed-name>Dobrišek S.</ce:indexed-name><ce:surname>Dobrišek</ce:surname><ce:given-name>Simon</ce:given-name></preferred-name></author><author auid="6507417859" seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Zibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Žibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name></author><author auid="6604057186" seq="5"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name><preferred-name><ce:initials>F.</ce:initials><ce:indexed-name>Mihelič F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name></preferred-name></author><author auid="6603816127" seq="6"><ce:initials>N.</ce:initials><ce:indexed-name>Pavesic N.</ce:indexed-name><ce:surname>Pavešić</ce:surname><ce:given-name>Nikola</ce:given-name><preferred-name><ce:initials>N.</ce:initials><ce:indexed-name>Pavešić N.</ce:indexed-name><ce:surname>Pavešić</ce:surname><ce:given-name>Nikola</ce:given-name></preferred-name></author><affiliation afid="60031106" dptid="104580649" country="svn"><organization>Faculty of Electrical Engineering</organization><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city-group>SI-1000 Ljubljana</city-group><affiliation-id afid="60031106" dptid="104580649"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname></person><affiliation country="svn"><organization>Faculty of Electrical Engineering</organization><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city-group>SI-1000 Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>The paper presents our initial attempts in building an audio video emotion recognition system. Both, audio and video sub-systems are discussed, and description of the database of spontaneous emotions is given. The task of labelling the recordings from the database according to different emotions is discussed and the measured agreement between multiple annotators is presented. Instead of focusing on the prosody in audio emotion recognition, we evaluate the possibility of using linear transformations (CMLLR) as features. The classification results from audio and video sub-systems are combined using sum rule fusion and the increase in recognition results, when using both modalities, is presented. © 2009 Springer-Verlag.</ce:para></abstract></abstracts><source srcid="25674" type="k" country="deu"><sourcetitle>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</sourcetitle><sourcetitle-abbrev>Lect. Notes Comput. Sci.</sourcetitle-abbrev><issuetitle>Biometric ID Management and Multimodal Communication - Joint COST 2101 and 2102 International Conference, BioID_MultiComm 2009, Proceedings</issuetitle><issn type="print">03029743</issn><issn type="electronic">16113349</issn><isbn length="10" level="volume">3642043909</isbn><isbn length="13" level="volume">9783642043901</isbn><volisspag><voliss volume="5707 LNCS"/><pagerange first="114" last="121"/></volisspag><publicationyear first="2009"/><publicationdate><year>2009</year><date-text xfab-added="true">2009</date-text></publicationdate><additional-srcinfo><conferenceinfo><confevent><confname>Joint COST 2101 and 2102 International Conference on Biometric ID Management and Multimodal Communication, BioID_MultiComm 2009</confname><conflocation country="esp"><city-group>Madrid</city-group></conflocation><confdate><startdate year="2009" month="09" day="16"/><enddate year="2009" month="09" day="18"/></confdate><confcode>80162</confcode></confevent><confpublication><procpagerange>var.pagings</procpagerange></confpublication></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification> <classification-code>921.3</classification-code> <classification-description>Mathematical Transformations</classification-description> </classification><classification> <classification-code>752.1</classification-code> <classification-description>Acoustic Devices</classification-description> </classification><classification> <classification-code>751</classification-code> <classification-description>Acoustics, Noise and Sound</classification-description> </classification><classification> <classification-code>732</classification-code> <classification-description>Control Devices</classification-description> </classification><classification> <classification-code>723.3</classification-code> <classification-description>Database Systems</classification-description> </classification><classification> <classification-code>723</classification-code> <classification-description>Computer Software, Data Handling and Applications</classification-description> </classification><classification> <classification-code>461</classification-code> <classification-description>Bioengineering</classification-description> </classification></classifications><classifications type="GEOCLASS"><classification> <classification-code>Related Topics</classification-code> </classification></classifications><classifications type="ASJC"><classification>2614</classification><classification>1700</classification></classifications><classifications type="SUBJABBR"><classification>MATH</classification><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="9"><reference id="1"><ref-info><ref-title><ref-titletext>Audio-visual based emotion recognition using tripled hidden Markov model</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">4544263979</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Song M.</ce:indexed-name><ce:surname>Song</ce:surname></author><author seq="2"><ce:initials>C.</ce:initials><ce:indexed-name>Chen C.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>You M.</ce:indexed-name><ce:surname>You</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of Acoustics, Speech, and Signal Processing (ICASSP 2004)</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="5"/><pagerange first="877" last="880"/></ref-volisspag></ref-info><ref-fulltext>Song, M., Chen, C., You, M.: Audio-visual based emotion recognition using tripled hidden Markov model. In: Proceedings of Acoustics, Speech, and Signal Processing (ICASSP 2004), vol. 5, pp. 877-880 (2004)</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Multi-Modal Emotional Database: AvID</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">64249151844</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname></author><et-al/></ref-authors><ref-sourcetitle>Informatica</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="33"/><pagerange first="101" last="106"/></ref-volisspag></ref-info><ref-fulltext>Gajšek, R., et al.: Multi-Modal Emotional Database: AvID. Informatica 33, 101-106 (2009)</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Analysis of emotion recognition using facial expressions, speech and multimodal information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">14944351245</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Busso C.</ce:indexed-name><ce:surname>Busso</ce:surname></author><author seq="2"><ce:initials>Z.</ce:initials><ce:indexed-name>Deng Z.</ce:indexed-name><ce:surname>Deng</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Yildirim S.</ce:indexed-name><ce:surname>Yildirim</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Bulut M.</ce:indexed-name><ce:surname>Bulut</ce:surname></author><author seq="5"><ce:initials>C.M.</ce:initials><ce:indexed-name>Lee C.M.</ce:indexed-name><ce:surname>Lee</ce:surname></author><author seq="6"><ce:initials>A.</ce:initials><ce:indexed-name>Kazemzadeh A.</ce:indexed-name><ce:surname>Kazemzadeh</ce:surname></author><author seq="7"><ce:initials>S.</ce:initials><ce:indexed-name>Lee S.</ce:indexed-name><ce:surname>Lee</ce:surname></author><author seq="8"><ce:initials>U.</ce:initials><ce:indexed-name>Neumann U.</ce:indexed-name><ce:surname>Neumann</ce:surname></author><author seq="9"><ce:initials>S.</ce:initials><ce:indexed-name>Narayanan S.</ce:indexed-name><ce:surname>Narayanan</ce:surname></author></ref-authors><ref-sourcetitle>ICMI'04 - Sixth International Conference on Multimodal Interfaces</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><pagerange first="205" last="211"/></ref-volisspag><ref-text>ICMI'04 - Sixth International Conference on Multimodal Interfaces</ref-text></ref-info><ref-fulltext>Busso, C., et al.: Analysis of emotion recognition using facial expressions, speech and multimodal information. In: ICMI 2004: Proceedings of the 6th international conference on Multimodal interfaces, pp. 205-211. ACM, New York (2004) (Pubitemid 40371911)</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Strong Evidence for universals in facial expressions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0028390852</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Eckman P.</ce:indexed-name><ce:surname>Eckman</ce:surname></author></ref-authors><ref-sourcetitle>Psychol. Bull.</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="115" issue="2"/><pagerange first="268" last="287"/></ref-volisspag></ref-info><ref-fulltext>Eckman, P.: Strong Evidence for universals in facial expressions. Psychol. Bull. 115(2), 268-287 (1994)</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Automatic analysis of facial expressions: The state of the art</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0034498178</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Pantic M.</ce:indexed-name><ce:surname>Pantic</ce:surname></author><author seq="2"><ce:initials>L.U.M.</ce:initials><ce:indexed-name>Rothkrantz L.U.M.</ce:indexed-name><ce:surname>Rothkrantz</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="22" issue="12"/><pagerange first="1424" last="1445"/></ref-volisspag><ref-text>DOI 10.1109/34.895976</ref-text></ref-info><ref-fulltext>Pantic, M., Rothkrantz, L.J.M.: Automatic analysis of facial expressions: the state of the art. IEEE TPAMI 22(12), 1424-1445 (2000) (Pubitemid 32088469)</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Robust real-time object detection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0004308492</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Viola P.</ce:indexed-name><ce:surname>Viola</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Jones M.</ce:indexed-name><ce:surname>Jones</ce:surname></author></ref-authors><ref-sourcetitle>Proc. of the Second Intenrnational Workshop on Statistical and Computational Theories of Vision - Modeling, Learning, Computing and Sampling, Vancouver, Canada (2001)</ref-sourcetitle></ref-info><ref-fulltext>Viola, P., Jones, M.: Robust real-time object detection. In: Proc. of the Second Intenrnational Workshop on Statistical and Computational Theories of Vision - Modeling, Learning, Computing and Sampling, Vancouver, Canada (2001)</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Prosody-based automatic detection of annoyance and frustration in human-computer dialog</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85009145332</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Ang J.</ce:indexed-name><ce:surname>Ang</ce:surname></author><et-al/></ref-authors><ref-sourcetitle>Proc. ICSLP 2002</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="3"/><pagerange first="2037" last="2040"/></ref-volisspag></ref-info><ref-fulltext>Ang, J., et al.: Prosody-based automatic detection of annoyance and frustration in human-computer dialog. In: Proc. ICSLP 2002, vol. 3, pp. 2037-2040 (2002)</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Maximum likelihood linear transformations for HMM-based speech recognition</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0032050110</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.J.F.</ce:initials><ce:indexed-name>Gales M.J.F.</ce:indexed-name><ce:surname>Gales</ce:surname></author></ref-authors><ref-sourcetitle>Computer Speech and Language</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><voliss volume="12" issue="2"/><pagerange first="75" last="98"/></ref-volisspag></ref-info><ref-fulltext>Gales, M.J.F.: Maximum likelihood linear transformations for HMM-based speech recognition. Computer Speech and Language 12(2), 75-98 (1998)</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Spoken language resources at LUKS of the University of Ljubljana</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0037939771</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname></author><et-al/></ref-authors><ref-sourcetitle>Int. J. of Speech Technology</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="6" issue="3"/><pagerange first="221" last="232"/></ref-volisspag></ref-info><ref-fulltext>Mihelič, F., et al.: Spoken language resources at LUKS of the University of Ljubljana. Int. J. of Speech Technology 6(3), 221-232 (2006)</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>