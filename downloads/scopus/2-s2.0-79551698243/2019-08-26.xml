<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/79551698243</prism:url><dc:identifier>SCOPUS_ID:79551698243</dc:identifier><eid>2-s2.0-79551698243</eid><dc:title>A hardware/software view of CUDA Strojni in programski vidiki arhitekture CUDA</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>1</citedby-count><prism:publicationName>Elektrotehniski Vestnik/Electrotechnical Review</prism:publicationName><source-id>16651</source-id><prism:issn>00135852</prism:issn><prism:volume>77</prism:volume><prism:issueIdentifier>5</prism:issueIdentifier><prism:startingPage>267</prism:startingPage><prism:endingPage>272</prism:endingPage><prism:pageRange>267-272</prism:pageRange><prism:coverDate>2010-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="6508356094"><ce:initials>T.</ce:initials><ce:indexed-name>Dobravec T.</ce:indexed-name><ce:surname>Dobravec</ce:surname><ce:given-name>Tomaž</ce:given-name><preferred-name><ce:initials>T.</ce:initials><ce:indexed-name>Dobravec T.</ce:indexed-name><ce:surname>Dobravec</ce:surname><ce:given-name>Tomaž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6508356094</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"> <ce:para>Extended abstract. This paper presents a hardware/software view of CUDA. A Graphical Processing Unit (GPU) has up to 480 streaming processors (SP), organized as up to 30 streaming multiprocessors (SM). Each SM has 8 or 16 SPs [2, 3]. SM is an independent processing unit. SM consists of a Fetch/Issue unit (Figure 1) and an execution unit (Figure 2). SM uses two different clock domains for the Fetch/Issue unit and the execution unit, respectively. SM manages and executes threads in groups of 32 parallel threads called warps. An issued warp executes in the execution unit as a set of 32 threads over four processor cycles (Figure 3). Besides a shared memory and a register set for each SM, each SM has a uniform access to a global memory. The global memory (up to 4GB DDR3 SDRAM) is connected to SMs via a 384-bit wide bus and is used for comunication between CPU and GPU and for sharing data among threads in different SMs. From the programmers point of view, CUDA is a device which can be used through a parallel programming model. The code is split into the so called kernels which are executed in up to several thousand parallel and/or asynchronous threads. Parallel threads can cooperate using synchronization mechanisms and shared memory, while asynchronous threads can only use a common global memory.</ce:para> </abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/79551698243" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=79551698243&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=79551698243&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="6508356094"><ce:initials>T.</ce:initials><ce:indexed-name>Dobravec T.</ce:indexed-name><ce:surname>Dobravec</ce:surname><ce:given-name>Tomaž</ce:given-name><preferred-name><ce:initials>T.</ce:initials><ce:indexed-name>Dobravec T.</ce:indexed-name><ce:surname>Dobravec</ce:surname><ce:given-name>Tomaž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6508356094</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="6603205527"><ce:initials>P.</ce:initials><ce:indexed-name>Bulic P.</ce:indexed-name><ce:surname>Bulić</ce:surname><ce:given-name>Patricio</ce:given-name><preferred-name><ce:initials>P.</ce:initials><ce:indexed-name>Bulić P.</ce:indexed-name><ce:surname>Bulić</ce:surname><ce:given-name>Patricio</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603205527</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="slo"/><authkeywords><author-keyword>CUDA</author-keyword><author-keyword>Graphics processing units</author-keyword><author-keyword>Parallel programming</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Clock domains</mainterm><mainterm weight="a" candidate="n">CUDA</mainterm><mainterm weight="a" candidate="n">Execution units</mainterm><mainterm weight="a" candidate="n">Extended abstracts</mainterm><mainterm weight="a" candidate="n">Graphical processing units</mainterm><mainterm weight="a" candidate="n">Graphics Processing Unit</mainterm><mainterm weight="a" candidate="n">Hardware/software</mainterm><mainterm weight="a" candidate="n">Parallel programming model</mainterm><mainterm weight="a" candidate="n">Processing units</mainterm><mainterm weight="a" candidate="n">Shared memories</mainterm><mainterm weight="a" candidate="n">Synchronization mechanisms</mainterm></idxterms><subject-areas><subject-area code="2208" abbrev="ENGI">Electrical and Electronic Engineering</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered day="18" month="05" timestamp="2018-05-18T22:39:14.000014-04:00" year="2018"/><ait:date-sort day="01" month="12" year="2010"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2011 Elsevier B.V., All rights reserved.</copyright><itemidlist> <itemid idtype="PUI">361225439</itemid> <itemid idtype="CPX">20110613654747</itemid> <itemid idtype="SCP">79551698243</itemid> <itemid idtype="SGR">79551698243</itemid> </itemidlist><history> <date-created day="11" month="02" year="2011"/> </history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="slo" language="Slovak"/><abstract-language xml:lang="eng" language="English"/><abstract-language xml:lang="slo" language="Slovak"/><author-keywords> <author-keyword>CUDA</author-keyword> <author-keyword>Graphics processing units</author-keyword> <author-keyword>Parallel programming</author-keyword> </author-keywords></citation-info><citation-title><titletext original="n" xml:lang="eng" language="English">A hardware/software view of CUDA</titletext><titletext original="y" xml:lang="slo" language="Slovak">Strojni in programski vidiki arhitekture CUDA</titletext></citation-title><author-group><author auid="6508356094" seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Dobravec T.</ce:indexed-name><ce:surname>Dobravec</ce:surname><ce:given-name>Tomaž</ce:given-name><preferred-name> <ce:initials>T.</ce:initials> <ce:indexed-name>Dobravec T.</ce:indexed-name> <ce:surname>Dobravec</ce:surname> <ce:given-name>Tomaž</ce:given-name> </preferred-name></author><author auid="6603205527" seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Bulic P.</ce:indexed-name><ce:surname>Bulić</ce:surname><ce:given-name>Patricio</ce:given-name><preferred-name> <ce:initials>P.</ce:initials> <ce:indexed-name>Bulić P.</ce:indexed-name> <ce:surname>Bulić</ce:surname> <ce:given-name>Patricio</ce:given-name> </preferred-name></author><affiliation afid="60031106" country="svn" dptid="104580817"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za Računalništvo in Informatiko</organization><address-part>Tržaška 25</address-part><city-group>1000 Ljubljana</city-group><affiliation-id afid="60031106" dptid="104580817"/><country>Slovenia</country></affiliation></author-group><correspondence><person> <ce:initials>T.</ce:initials> <ce:indexed-name>Dobravec T.</ce:indexed-name> <ce:surname>Dobravec</ce:surname> </person><affiliation country="svn"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za Računalništvo in Informatiko</organization><address-part>Tržaška 25</address-part><city-group>1000 Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"> <ce:para>Extended abstract. This paper presents a hardware/software view of CUDA. A Graphical Processing Unit (GPU) has up to 480 streaming processors (SP), organized as up to 30 streaming multiprocessors (SM). Each SM has 8 or 16 SPs [2, 3]. SM is an independent processing unit. SM consists of a Fetch/Issue unit (Figure 1) and an execution unit (Figure 2). SM uses two different clock domains for the Fetch/Issue unit and the execution unit, respectively. SM manages and executes threads in groups of 32 parallel threads called warps. An issued warp executes in the execution unit as a set of 32 threads over four processor cycles (Figure 3). Besides a shared memory and a register set for each SM, each SM has a uniform access to a global memory. The global memory (up to 4GB DDR3 SDRAM) is connected to SMs via a 384-bit wide bus and is used for comunication between CPU and GPU and for sharing data among threads in different SMs. From the programmers point of view, CUDA is a device which can be used through a parallel programming model. The code is split into the so called kernels which are executed in up to several thousand parallel and/or asynchronous threads. Parallel threads can cooperate using synchronization mechanisms and shared memory, while asynchronous threads can only use a common global memory.</ce:para> </abstract></abstracts><source country="svn" srcid="16651" type="j"><sourcetitle>Elektrotehniski Vestnik/Electrotechnical Review</sourcetitle><sourcetitle-abbrev>Elektroteh Vestn Electrotech Rev</sourcetitle-abbrev><issn type="print">00135852</issn><codencode>ELVEA</codencode><volisspag> <voliss issue="5" volume="77"/> <pagerange first="267" last="272"/> </volisspag><publicationyear first="2010"/><publicationdate> <year>2010</year> <date-text xfab-added="true">2010</date-text></publicationdate></source><enhancement><classificationgroup><classifications type="CPXCLASS"> <classification> <classification-code>722</classification-code> <classification-description>Computer Hardware</classification-description> </classification> <classification> <classification-code>723.1</classification-code> <classification-description>Computer Programming</classification-description> </classification> <classification> <classification-code>741</classification-code> <classification-description>Light, Optics and Optical Devices</classification-description> </classification> <classification> <classification-code>819.5</classification-code> <classification-description>Textile Products and Processing</classification-description> </classification> </classifications><classifications type="GEOCLASS"> <classification> <classification-code>Related Topics</classification-code> </classification> </classifications><classifications type="ASJC"> <classification>2208</classification> </classifications><classifications type="SUBJABBR"><classification>ENGI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="6"> <reference id="1"> <ref-info> <ref-title> <ref-titletext>Parallel computing experiences with CUDA</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">53749092570</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>M.</ce:initials> <ce:indexed-name>Garland M.</ce:indexed-name> <ce:surname>Garland</ce:surname> </author> <author seq="2"> <ce:initials>S.</ce:initials> <ce:indexed-name>Le Grand S.</ce:indexed-name> <ce:surname>Le Grand</ce:surname> </author> <author seq="3"> <ce:initials>J.</ce:initials> <ce:indexed-name>Nickolls J.</ce:indexed-name> <ce:surname>Nickolls</ce:surname> </author> <author seq="4"> <ce:initials>J.</ce:initials> <ce:indexed-name>Anderson J.</ce:indexed-name> <ce:surname>Anderson</ce:surname> </author> <author seq="5"> <ce:initials>J.</ce:initials> <ce:indexed-name>Hardwick J.</ce:indexed-name> <ce:surname>Hardwick</ce:surname> </author> <author seq="6"> <ce:initials>S.</ce:initials> <ce:indexed-name>Morton S.</ce:indexed-name> <ce:surname>Morton</ce:surname> </author> <author seq="7"> <ce:initials>E.</ce:initials> <ce:indexed-name>Phillips E.</ce:indexed-name> <ce:surname>Phillips</ce:surname> </author> <author seq="8"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Zhang Y.</ce:indexed-name> <ce:surname>Zhang</ce:surname> </author> <author seq="9"> <ce:initials>V.</ce:initials> <ce:indexed-name>Volkov V.</ce:indexed-name> <ce:surname>Volkov</ce:surname> </author> </ref-authors> <ref-sourcetitle>IEEE Computer</ref-sourcetitle> <ref-publicationyear first="2008"/> <ref-volisspag> <voliss issue="4" volume="28"/> <pagerange first="13" last="27"/> </ref-volisspag> <ref-text>September</ref-text> </ref-info> <ref-fulltext>M. Garland, S. Le Grand, J. Nickolls, J. Anderson, J. Hardwick, S. Morton, E. Phillips, Yao Zhang, V. Volkov, Parallel Computing Experiences with CUDA, IEEE Computer, Vol. 28, No. 4, September 2008, pp. 13-27.</ref-fulltext> </reference> <reference id="2"> <ref-info> <ref-title> <ref-titletext>Parallel processing with CUDA</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">49449105103</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>T.R.</ce:initials> <ce:indexed-name>Halfhill T.R.</ce:indexed-name> <ce:surname>Halfhill</ce:surname> </author> </ref-authors> <ref-sourcetitle>Microprocessor Report</ref-sourcetitle> <ref-publicationyear first="2008"/> <ref-volisspag> <pagerange first="1" last="8"/> </ref-volisspag> <ref-text>January 28</ref-text> </ref-info> <ref-fulltext>T. R. Halfhill, Parallel Processing with CUDA, Microprocessor Report, January 28, 2008, pp. 1-8.</ref-fulltext> </reference> <reference id="3"> <ref-info> <ref-title> <ref-titletext>NVIDIA TESLA: A unified graphics and computing architecture</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">44849137198</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>E.</ce:initials> <ce:indexed-name>Lindholm E.</ce:indexed-name> <ce:surname>Lindholm</ce:surname> </author> <author seq="2"> <ce:initials>J.</ce:initials> <ce:indexed-name>Nickolls J.</ce:indexed-name> <ce:surname>Nickolls</ce:surname> </author> <author seq="3"> <ce:initials>S.</ce:initials> <ce:indexed-name>Oberman S.</ce:indexed-name> <ce:surname>Oberman</ce:surname> </author> <author seq="4"> <ce:initials>J.</ce:initials> <ce:indexed-name>Montrym J.</ce:indexed-name> <ce:surname>Montrym</ce:surname> </author> </ref-authors> <ref-sourcetitle>IEEE Micro</ref-sourcetitle> <ref-publicationyear first="2008"/> <ref-volisspag> <voliss issue="2" volume="28"/> <pagerange first="39" last="55"/> </ref-volisspag> <ref-text>March-April</ref-text> </ref-info> <ref-fulltext>E. Lindholm, J. Nickolls, S. Oberman, J. Montrym, NVIDIA TESLA: A Unified Graphics and Computing Architecture, IEEE Micro, Vol. 28, No. 2, March-April 2008, pp. 39-55.</ref-fulltext> </reference> <reference id="4"> <ref-info> <ref-title> <ref-titletext>The GPU computing era</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">77951154340</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>J.</ce:initials> <ce:indexed-name>Nickolls J.</ce:indexed-name> <ce:surname>Nickolls</ce:surname> </author> <author seq="2"> <ce:initials>W.J.</ce:initials> <ce:indexed-name>Dally W.J.</ce:indexed-name> <ce:surname>Dally</ce:surname> </author> </ref-authors> <ref-sourcetitle>IEEE Micro</ref-sourcetitle> <ref-publicationyear first="2010"/> <ref-volisspag> <voliss issue="2" volume="30"/> <pagerange first="56" last="69"/> </ref-volisspag> </ref-info> <ref-fulltext>J. Nickolls, W J. Dally, The GPU Computing Era, IEEE Micro, Vol. 30, No. 2, 2010, pp. 56-69.</ref-fulltext> </reference> <reference id="5"> <ref-info> <ref-title> <ref-titletext>GPU computing</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">49049088756</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>J.D.</ce:initials> <ce:indexed-name>Owens J.D.</ce:indexed-name> <ce:surname>Owens</ce:surname> </author> <author seq="2"> <ce:initials>M.</ce:initials> <ce:indexed-name>Houston M.</ce:indexed-name> <ce:surname>Houston</ce:surname> </author> <author seq="3"> <ce:initials>D.</ce:initials> <ce:indexed-name>Luebke D.</ce:indexed-name> <ce:surname>Luebke</ce:surname> </author> <author seq="4"> <ce:initials>S.</ce:initials> <ce:indexed-name>Green S.</ce:indexed-name> <ce:surname>Green</ce:surname> </author> <author seq="5"> <ce:initials>E.</ce:initials> <ce:indexed-name>John E.</ce:indexed-name> <ce:surname>John</ce:surname> </author> <author seq="6"> <ce:initials>S.J.C.</ce:initials> <ce:indexed-name>Phillips S.J.C.</ce:indexed-name> <ce:surname>Phillips</ce:surname> </author> </ref-authors> <ref-sourcetitle>Proceedings of the IEEE</ref-sourcetitle> <ref-publicationyear first="2008"/> <ref-volisspag> <voliss issue="5" volume="96"/> <pagerange first="879" last="899"/> </ref-volisspag> <ref-text>May</ref-text> </ref-info> <ref-fulltext>J. D. Owens, M. Houston, D. Luebke, S. Green, John E. Stone, J. C. Phillips, GPU Computing, Proceedings of the IEEE, Vol. 96, No. 5, May 2008, pp. 879-899.</ref-fulltext> </reference> <reference id="6"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">69449101594</itemid> </refd-itemidlist> <ref-sourcetitle>Programming Guide</ref-sourcetitle> <ref-website> <ce:e-address type="url">http://developer.nvidia.com,20February2010</ce:e-address> </ref-website> <ref-text>NVIDIA. (version 3.0)</ref-text> </ref-info> <ref-fulltext>NVIDIA. Programming guide (version 3.0), http://developer.nvidia.com, 20 February 2010.</ref-fulltext> </reference> </bibliography></tail></bibrecord></item></abstracts-retrieval-response>