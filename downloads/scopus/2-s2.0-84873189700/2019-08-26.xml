<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84873189700</prism:url><dc:identifier>SCOPUS_ID:84873189700</dc:identifier><eid>2-s2.0-84873189700</eid><prism:doi>10.1109/ICDMW.2012.165</prism:doi><article-number>6406414</article-number><dc:title>Model selection with combining valid and optimal prediction intervals</dc:title><prism:aggregationType>Conference Proceeding</prism:aggregationType><srctype>p</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>1</citedby-count><prism:publicationName>Proceedings - 12th IEEE International Conference on Data Mining Workshops, ICDMW 2012</prism:publicationName><source-id>21100228065</source-id><prism:isbn>9780769549255</prism:isbn><prism:startingPage>653</prism:startingPage><prism:endingPage>658</prism:endingPage><prism:pageRange>653-658</prism:pageRange><prism:coverDate>2012-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="37102202800"><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/37102202800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>In this paper we explore the possibility of automatic model selection in the supervised learning framework with the use of prediction intervals. First we compare two families of non-parametric approaches of constructing prediction intervals for arbitrary regression models. The first family of approaches is based on the idea of explaining the total prediction error as a sum of the model's error and the error caused by noise inherent to the data - the two are estimated independently. The second family assumes local similarity of the data and these approaches estimate the prediction intervals with use of the sample's nearest neighbors. The comparison shows that the first family strives to produce valid prediction intervals whereas the second family strives for optimality. We propose a statistic for model selection where we compare the discrepancy between valid and optimal prediction intervals. Experiments performed on a set of artificial datasets strongly support the hypothesis that for the correct model, this discrepancy is minimal among competing models. Â© 2012 IEEE.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84873189700" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84873189700&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84873189700&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="37102202800"><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/37102202800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="57188535146"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57188535146</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Estimation error</author-keyword><author-keyword>Machine Learning</author-keyword><author-keyword>Predictive models</author-keyword><author-keyword>Regression analysis</author-keyword><author-keyword>Supervised learning</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Artificial datasets</mainterm><mainterm weight="a" candidate="n">Automatic model selection</mainterm><mainterm weight="a" candidate="n">Competing models</mainterm><mainterm weight="a" candidate="n">Estimation errors</mainterm><mainterm weight="a" candidate="n">Local similarity</mainterm><mainterm weight="a" candidate="n">Model Selection</mainterm><mainterm weight="a" candidate="n">Nearest neighbors</mainterm><mainterm weight="a" candidate="n">Nonparametric approaches</mainterm><mainterm weight="a" candidate="n">Optimality</mainterm><mainterm weight="a" candidate="n">Prediction errors</mainterm><mainterm weight="a" candidate="n">Prediction interval</mainterm><mainterm weight="a" candidate="n">Predictive models</mainterm><mainterm weight="a" candidate="n">Regression model</mainterm></idxterms><subject-areas><subject-area code="1712" abbrev="COMP">Software</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2017" month="11" day="18" timestamp="2017-11-18T15:54:55.000055-05:00"/><ait:date-sort year="2012" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2013 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1109/ICDMW.2012.165</ce:doi><itemid idtype="PUI">368238171</itemid><itemid idtype="CPX">20130615994755</itemid><itemid idtype="SCP">84873189700</itemid><itemid idtype="SGR">84873189700</itemid></itemidlist><history><date-created year="2013" month="02" day="06"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword>Estimation error</author-keyword><author-keyword>Machine Learning</author-keyword><author-keyword>Predictive models</author-keyword><author-keyword>Regression analysis</author-keyword><author-keyword>Supervised learning</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Model selection with combining valid and optimal prediction intervals</titletext></citation-title><author-group><author auid="37102202800" seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name></preferred-name></author><author auid="57188535146" seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><city-group>Ljubljana</city-group><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname></person><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><city-group>Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>In this paper we explore the possibility of automatic model selection in the supervised learning framework with the use of prediction intervals. First we compare two families of non-parametric approaches of constructing prediction intervals for arbitrary regression models. The first family of approaches is based on the idea of explaining the total prediction error as a sum of the model's error and the error caused by noise inherent to the data - the two are estimated independently. The second family assumes local similarity of the data and these approaches estimate the prediction intervals with use of the sample's nearest neighbors. The comparison shows that the first family strives to produce valid prediction intervals whereas the second family strives for optimality. We propose a statistic for model selection where we compare the discrepancy between valid and optimal prediction intervals. Experiments performed on a set of artificial datasets strongly support the hypothesis that for the correct model, this discrepancy is minimal among competing models. Â© 2012 IEEE.</ce:para></abstract></abstracts><source srcid="21100228065" type="p" country="usa"><sourcetitle>Proceedings - 12th IEEE International Conference on Data Mining Workshops, ICDMW 2012</sourcetitle><sourcetitle-abbrev>Proc. - IEEE Int. Conf. Data Min. Workshops, ICDMW</sourcetitle-abbrev><issuetitle>Proceedings - 12th IEEE International Conference on Data Mining Workshops, ICDMW 2012</issuetitle><isbn length="13">9780769549255</isbn><volisspag><pagerange first="653" last="658"/></volisspag><article-number>6406414</article-number><publicationyear first="2012"/><publicationdate><year>2012</year><date-text xfab-added="true">2012</date-text></publicationdate><additional-srcinfo><conferenceinfo><confevent><confname>12th IEEE International Conference on Data Mining Workshops, ICDMW 2012</confname><conflocation country="bel"><city-group>Brussels</city-group></conflocation><confdate><startdate year="2012" month="12" day="10"/><enddate year="2012" month="12" day="10"/></confdate><confcatnumber>E4925</confcatnumber><confcode>95285</confcode></confevent><confpublication><procpagerange>var.pagings</procpagerange></confpublication></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="ASJC"><classification>1712</classification></classifications><classifications type="CPXCLASS"><classification> <classification-code>723.3</classification-code> <classification-description>Database Systems</classification-description> </classification><classification> <classification-code>731.5</classification-code> <classification-description>Robotics</classification-description> </classification><classification> <classification-code>921.5</classification-code> <classification-description>Optimization Techniques</classification-description> </classification><classification> <classification-code>922.2</classification-code> <classification-description>Mathematical Statistics</classification-description> </classification></classifications><classifications type="GEOCLASS"><classification> <classification-code>Related Topics</classification-code> </classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="11"><reference id="1"><ref-info><ref-title><ref-titletext>Machine learning approaches for estimation of prediction interval for the model output</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33645987256</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.L.</ce:initials><ce:indexed-name>Shrestha D.L.</ce:indexed-name><ce:surname>Shrestha</ce:surname></author><author seq="2"><ce:initials>D.P.</ce:initials><ce:indexed-name>Solomatine D.P.</ce:indexed-name><ce:surname>Solomatine</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="19" issue="2"/><pagerange first="225" last="235"/></ref-volisspag></ref-info><ref-fulltext>D. L. Shrestha and D. P. Solomatine, "Machine learning approaches for estimation of prediction interval for the model output," Neural Networks, vol. 19, no. 2, pp. 225-235, 2006.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Outline of a theory of statistical estimation based on the classical theory of probability</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000679487</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Neyman J.</ce:indexed-name><ce:surname>Neyman</ce:surname></author></ref-authors><ref-sourcetitle>Royal Society of London Philosophical Transactions Series A</ref-sourcetitle><ref-publicationyear first="1937"/><ref-volisspag><voliss volume="236"/><pagerange first="333" last="380"/></ref-volisspag><ref-text>Aug</ref-text></ref-info><ref-fulltext>J. Neyman, "Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability," Royal Society of London Philosophical Transactions Series A, vol. 236, pp. 333-380, Aug. 1937.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Estimating the mean and variance of the target probability distribution</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0028739205</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Nix D.</ce:indexed-name><ce:surname>Nix</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Weigend A.</ce:indexed-name><ce:surname>Weigend</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks, 1994 IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="1"/><pagerange first="55" last="60"/></ref-volisspag><ref-text>Jun-2 Jul vol.1</ref-text></ref-info><ref-fulltext>D. Nix and A. Weigend, "Estimating the mean and variance of the target probability distribution," in Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on, vol. 1, jun-2 jul 1994, pp. 55 -60 vol.1.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>A comparison of some error estimates for neural network models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001572432</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Tibshirani R.</ce:indexed-name><ce:surname>Tibshirani</ce:surname></author></ref-authors><ref-sourcetitle>Neural Computation</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><voliss volume="8" issue="1"/><pagerange first="152" last="163"/></ref-volisspag><ref-text>Jan</ref-text></ref-info><ref-fulltext>R. Tibshirani, "A comparison of some error estimates for neural network models," Neural Computation, vol. 8, no. 1, pp. 152-163, Jan. 1996.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Reliable prediction intervals with regression neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80051799000</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Papadopoulos H.</ce:indexed-name><ce:surname>Papadopoulos</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Haralambous H.</ce:indexed-name><ce:surname>Haralambous</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="24" issue="8"/><pagerange first="842" last="851"/></ref-volisspag><ref-text>artificial Neural Networks: Selected Papers from ICANN 2010</ref-text></ref-info><ref-fulltext>H. Papadopoulos and H. Haralambous, "Reliable prediction intervals with regression neural networks," Neural Networks, vol. 24, no. 8, pp. 842 - 851, 2011, artificial Neural Networks: Selected Papers from ICANN 2010.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Comparison of approaches for estimating reliability of individual regression predictions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">54349094489</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>BosniÄ</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Data Knowl. Eng.</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="67" issue="3"/><pagerange first="504" last="516"/></ref-volisspag></ref-info><ref-fulltext>Z. BosniÄ and I. Kononenko, "Comparison of approaches for estimating reliability of individual regression predictions," Data Knowl. Eng., vol. 67, no. 3, pp. 504-516, 2008.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Practical confidence and prediction intervals</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898947879</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Heskes T.</ce:indexed-name><ce:surname>Heskes</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="9"/><pagerange first="176" last="182"/></ref-volisspag><ref-text>MIT press</ref-text></ref-info><ref-fulltext>T. Heskes, "Practical confidence and prediction intervals," in Advances in Neural Information Processing Systems 9. MIT press, 1997, pp. 176-182.</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Prediction intervals for neural network models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">22944467367</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Zapranis A.</ce:indexed-name><ce:surname>Zapranis</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Livanis E.</ce:indexed-name><ce:surname>Livanis</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 9th WSEAS International Conference on Computers. Stevens Point, Wisconsin, USA: World Scientific and Engineering Academy and Society (WSEAS)</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><pagerange first="761" last="767"/></ref-volisspag></ref-info><ref-fulltext>A. Zapranis and E. Livanis, "Prediction intervals for neural network models," in Proceedings of the 9th WSEAS International Conference on Computers. Stevens Point, Wisconsin, USA: World Scientific and Engineering Academy and Society (WSEAS), 2005, pp. 76:1-76:7.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>A general method for visualizing and explaining black-box regression models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">79955077667</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.</ce:initials><ce:indexed-name>Strumbelj E.</ce:indexed-name><ce:surname>Å trumbelj</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 10th International Conference on Adaptive and Natural Computing Algorithms - Volume Part II, Ser. ICANNGA'11</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><pagerange first="21" last="30"/></ref-volisspag><ref-text>Berlin, Heidelberg: Springer-Verlag</ref-text></ref-info><ref-fulltext>E. Å trumbelj and I. Kononenko, "A general method for visualizing and explaining black-box regression models," in Proceedings of the 10th international conference on Adaptive and natural computing algorithms - Volume Part II, ser. ICANNGA'11. Berlin, Heidelberg: Springer-Verlag, 2011, pp. 21-30.</ref-fulltext></reference><reference id="10"><ref-info><refd-itemidlist><itemid idtype="SGR">33748324384</itemid></refd-itemidlist><ref-sourcetitle>R: A Language and Environment for Statistical Computing</ref-sourcetitle><ref-publicationyear first="2006"/><ref-website><ce:e-address type="url">http://www.R-project.org</ce:e-address></ref-website><ref-text>R Development Core Team, R Foundation for Statistical Computing Vienna Austria, ISBN 3-900051-07-0</ref-text></ref-info><ref-fulltext>R Development Core Team, R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing, Vienna, Austria, 2006, ISBN 3-900051-07-0. [Online]. Available: http://www.R-project.org</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Explaining instance classifications with interactions of subsets of feature values</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">69249209926</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.</ce:initials><ce:indexed-name>Strumbelj E.</ce:indexed-name><ce:surname>Å trumbelj</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Robnik-Sikonja M.</ce:indexed-name><ce:surname>Robnik-Å ikonja</ce:surname></author></ref-authors><ref-sourcetitle>Data Knowl. Eng.</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="68" issue="10"/><pagerange first="886" last="904"/></ref-volisspag></ref-info><ref-fulltext>E. Å trumbelj, I. Kononenko, and M. Robnik-Å ikonja, "Explaining instance classifications with interactions of subsets of feature values," Data Knowl. Eng., vol. 68, no. 10, pp. 886-904, 2009.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>