<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84873431799</prism:url><dc:identifier>SCOPUS_ID:84873431799</dc:identifier><eid>2-s2.0-84873431799</eid><dc:title>Finding repeating stanzas in folk songs</dc:title><prism:aggregationType>Conference Proceeding</prism:aggregationType><srctype>p</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>1</citedby-count><prism:publicationName>Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012</prism:publicationName><source-id>21100228507</source-id><prism:isbn>9789727521449</prism:isbn><prism:startingPage>451</prism:startingPage><prism:endingPage>456</prism:endingPage><prism:pageRange>451-456</prism:pageRange><prism:coverDate>2012-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="55582372100"><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name><preferred-name><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/55582372100</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>Folk songs are typically composed of repeating parts - stanzas. To find such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of specific issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several methods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method consists of several steps. In the first step breathing (vocal) pauses are detected, which represent the candidate beginnings of individual stanzas. Next, a similarity measure is calculated between the first and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate beginnings represent the actual boundaries between stanzas, a scoring function is defined based on the calculated similarities between stanzas. A peak picking method is used in combination with global thresholding for the final selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive. © 2012 International Society for Music Information Retrieval.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84873431799" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84873431799&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84873431799&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="55582372100"><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name><preferred-name><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/55582372100</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="6603601816"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603601816</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords/><idxterms><mainterm weight="a" candidate="n">Audio segmentation</mainterm><mainterm weight="a" candidate="n">Folk songs</mainterm><mainterm weight="a" candidate="n">Global thresholding</mainterm><mainterm weight="a" candidate="n">Music recording</mainterm><mainterm weight="a" candidate="n">Peak-picking method</mainterm><mainterm weight="a" candidate="n">Pitch changes</mainterm><mainterm weight="a" candidate="n">Scoring functions</mainterm><mainterm weight="a" candidate="n">Segmentation methods</mainterm><mainterm weight="a" candidate="n">Similarity measure</mainterm></idxterms><subject-areas><subject-area code="1210" abbrev="ARTS">Music</subject-area><subject-area code="1710" abbrev="COMP">Information Systems</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2017" month="08" day="04" timestamp="2017-08-04T04:04:32.000032-04:00"/><ait:date-sort year="2012" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2013 Elsevier B.V., All rights reserved.</copyright><itemidlist><itemid idtype="PUI">368282356</itemid><itemid idtype="CPX">20130716015398</itemid><itemid idtype="SCP">84873431799</itemid><itemid idtype="SGR">84873431799</itemid></itemidlist><history><date-created year="2013" month="02" day="12"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Finding repeating stanzas in folk songs</titletext></citation-title><author-group><author auid="55582372100" seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name><preferred-name><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name></preferred-name></author><author auid="6603601816" seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>University of Ljubljana</organization><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname></person><affiliation country="svn"><organization>University of Ljubljana</organization><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>Folk songs are typically composed of repeating parts - stanzas. To find such parts in audio recordings of folk songs, segmentation methods can be used that split a recording into separate parts according to different criteria. Most audio segmentation methods were developed for popular and classical music, however these do not perform well on folk music recordings. This is mainly because folk song recordings contain a number of specific issues that are not considered by these methods, such as inaccurate singing of performers, variable tempo throughout the song and the presence of noise. In recent years several methods for segmentation of folk songs were developed. In this paper we present a novel method for segmentation of folk songs into repeating stanzas that does not rely on additional information about an individual stanza. The method consists of several steps. In the first step breathing (vocal) pauses are detected, which represent the candidate beginnings of individual stanzas. Next, a similarity measure is calculated between the first and all other candidate stanzas, which takes into account pitch changes between stanzas and tempo variations. To evaluate which candidate beginnings represent the actual boundaries between stanzas, a scoring function is defined based on the calculated similarities between stanzas. A peak picking method is used in combination with global thresholding for the final selection of stanza boundaries. The presented method was tested and evaluated on a collection of Slovenian folk songs from EthnoMuse archive. © 2012 International Society for Music Information Retrieval.</ce:para></abstract></abstracts><source srcid="21100228507" type="p" country="prt"><sourcetitle>Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012</sourcetitle><sourcetitle-abbrev>Proc. Int. Soc. Music Inf. Retr. Conf., ISMIR</sourcetitle-abbrev><issuetitle>Proceedings of the 13th International Society for Music Information Retrieval Conference, ISMIR 2012</issuetitle><isbn length="13">9789727521449</isbn><volisspag><pagerange first="451" last="456"/></volisspag><publicationyear first="2012"/><publicationdate><year>2012</year><date-text xfab-added="true">2012</date-text></publicationdate><additional-srcinfo><conferenceinfo><confevent><confname>13th International Society for Music Information Retrieval Conference, ISMIR 2012</confname><conflocation country="prt"><city-group>Porto</city-group></conflocation><confdate><startdate year="2012" month="10" day="08"/><enddate year="2012" month="10" day="12"/></confdate><confcode>95398</confcode></confevent><confpublication><procpagerange>var.pagings</procpagerange></confpublication></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="ASJC"><classification>1210</classification><classification>1710</classification></classifications><classifications type="CPXCLASS"><classification> <classification-code>723.5</classification-code> <classification-description>Computer Applications</classification-description> </classification><classification> <classification-code>752.3</classification-code> <classification-description>Sound Reproduction</classification-description> </classification></classifications><classifications type="GEOCLASS"><classification> <classification-code>Related Topics</classification-code> </classification></classifications><classifications type="SUBJABBR"><classification>ARTS</classification><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="14"><reference id="1"><ref-info><ref-title><ref-titletext>To catch a chorus: Using chroma-based representations for audio thumbnailing</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0035685514</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.A.</ce:initials><ce:indexed-name>Bartsch M.A.</ce:indexed-name><ce:surname>Bartsch</ce:surname></author><author seq="2"><ce:initials>G.H.</ce:initials><ce:indexed-name>Wakefield G.H.</ce:indexed-name><ce:surname>Wakefield</ce:surname></author></ref-authors><ref-sourcetitle>Applications of Signal Processing to Audio and Acoustics</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><pagerange first="15" last="19"/></ref-volisspag><ref-text>New Platz, NY, USA, October</ref-text></ref-info><ref-fulltext>Mark A. Bartsch and Gregory H. Wakefield. To catch a chorus: Using chroma-based representations for audio thumbnailing. In Applications of Signal Processing to Audio and Acoustics, pages 15-19, New Platz, NY, USA, October 2001.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Automatic music summarization via similarity analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038166992</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.L.</ce:initials><ce:indexed-name>Cooper M.L.</ce:indexed-name><ce:surname>Cooper</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Foote J.</ce:indexed-name><ce:surname>Foote</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 3rd International Conference on Music Information Retrieval (ISMIR 2002)</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><pagerange first="81" last="85"/></ref-volisspag><ref-text>Paris, France, October</ref-text></ref-info><ref-fulltext>Matthew L. Cooper and Jonathan Foote. Automatic music summarization via similarity analysis. In Proceedings of the 3rd International Conference on Music Information Retrieval (ISMIR 2002), pages 81-85, Paris, France, October 2002.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>YIN, a fundamental frequency estimator for speech and music</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036214787</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>De Cheveigne A.</ce:indexed-name><ce:surname>De Cheveigné</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Kawahara H.</ce:indexed-name><ce:surname>Kawahara</ce:surname></author></ref-authors><ref-sourcetitle>The Journal of the Acoustical Society of America</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="111" issue="4"/><pagerange first="1917" last="1930"/></ref-volisspag></ref-info><ref-fulltext>Alain de Cheveigné and Hideki Kawahara. YIN, a fundamental frequency estimator for speech and music. The Journal of the Acoustical Society of America, 111(4):1917-1930, 2002.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Visualizing music and audio using self-similarity</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0033279123</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.T.</ce:initials><ce:indexed-name>Foote J.T.</ce:indexed-name><ce:surname>Foote</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 7th ACM International Conference on Multimedia</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="77" last="80"/></ref-volisspag></ref-info><ref-fulltext>Jonathan T. Foote. Visualizing music and audio using self-similarity. In Proceedings of the 7th ACM international conference on Multimedia, pages 77-80, 1999.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Media segmentation using self-similarity decomposition</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038056191</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.T.</ce:initials><ce:indexed-name>Foote J.T.</ce:indexed-name><ce:surname>Foote</ce:surname></author><author seq="2"><ce:initials>M.L.</ce:initials><ce:indexed-name>Cooper M.L.</ce:indexed-name><ce:surname>Cooper</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of SPIE Storage and Retrieval for Multimedia Databases</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><pagerange first="167" last="175"/></ref-volisspag></ref-info><ref-fulltext>Jonathan T. Foote and Matthew L. Cooper. Media segmentation using self-similarity decomposition. In Proceedings of SPIE Storage and Retrieval for Multimedia Databases, pages 167-175, 2003.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>A chorus section detection method for musical audio signals and its application to a music listening station</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">34147158903</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Goto M.</ce:indexed-name><ce:surname>Goto</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Audio, Speech &amp; Language Processing</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="14" issue="5"/><pagerange first="1783" last="1794"/></ref-volisspag></ref-info><ref-fulltext>Masataka Goto. A chorus section detection method for musical audio signals and its application to a music listening station. IEEE Transactions on Audio, Speech &amp; Language Processing, 14(5):1783-94, 2006.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Probabilistic segmentation and labeling of ethnomusicological field recordings</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873426490</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR 2009)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="75" last="80"/></ref-volisspag><ref-text>Kobe, Japan, October</ref-text></ref-info><ref-fulltext>Matija Marolt. Probabilistic segmentation and labeling of ethnomusicological field recordings. In Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR 2009), pages 75-80, Kobe, Japan, October 2009.</ref-fulltext></reference><reference id="8"><ref-info><refd-itemidlist><itemid idtype="SGR">84892355208</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Muller M.</ce:indexed-name><ce:surname>Müller</ce:surname></author></ref-authors><ref-sourcetitle>Information Retrieval for Music and Motion</ref-sourcetitle><ref-publicationyear first="2007"/><ref-text>Springer Verlag</ref-text></ref-info><ref-fulltext>Meinard Müller. Information Retrieval for Music and Motion. Springer Verlag, 2007.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>A segment-based fitness measure for capturing repetitive structures of music recordings</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873602790</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Muller M.</ce:indexed-name><ce:surname>Müller</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Grosche P.</ce:indexed-name><ce:surname>Grosche</ce:surname></author><author seq="3"><ce:initials>N.</ce:initials><ce:indexed-name>Jiang N.</ce:indexed-name><ce:surname>Jiang</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2011"/></ref-info><ref-fulltext>Meinard Müller, Peter Grosche, and Nanzhu Jiang. A segment-based fitness measure for capturing repetitive structures of music recordings. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), 2011.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Robust segmentation and annotation of folk song recordings</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84864694464</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Muller M.</ce:indexed-name><ce:surname>Müller</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Grosche P.</ce:indexed-name><ce:surname>Grosche</ce:surname></author><author seq="3"><ce:initials>F.</ce:initials><ce:indexed-name>Wiering F.</ce:indexed-name><ce:surname>Wiering</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR 2009)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="735" last="740"/></ref-volisspag><ref-text>Kobe, Japan, October</ref-text></ref-info><ref-fulltext>Meinard Müller, Peter Grosche, and Frans Wiering. Robust segmentation and annotation of folk song recordings. In Proceedings of the 10th International Society for Music Information Retrieval Conference (ISMIR 2009), pages 735-740, Kobe, Japan, October 2009.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Audio-based music structure analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84863552197</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Paulus J.</ce:indexed-name><ce:surname>Paulus</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Muller M.</ce:indexed-name><ce:surname>Müller</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Klapuri A.</ce:indexed-name><ce:surname>Klapuri</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 11th International Conference on Music Information Retrieval (ISMIR 2010)</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><pagerange first="625" last="636"/></ref-volisspag><ref-text>Utrecht, Netherlands, August</ref-text></ref-info><ref-fulltext>Jouni Paulus, Meinard Müller, and Anssi Klapuri. Audio-based music structure analysis. In Proceedings of the 11th International Conference on Music Information Retrieval (ISMIR 2010), pages 625-636, Utrecht, Netherlands, August 2010.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Toward automatic music audio summary generation from signal analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">2342543489</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Peeters G.</ce:indexed-name><ce:surname>Peeters</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 3rd International Conference on Music Information Retrieval (ISMIR 2002)</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><pagerange first="94" last="100"/></ref-volisspag><ref-text>Paris, France, October</ref-text></ref-info><ref-fulltext>Geoffroy Peeters. Toward automatic music audio summary generation from signal analysis. In Proceedings of the 3rd International Conference on Music Information Retrieval (ISMIR 2002), pages 94-100, Paris, France, October 2002.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>The ethnomuse digital library: Conceptual representation and annotation of ethnomusicological materials</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84865826953</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname></author></ref-authors><ref-sourcetitle>International Journal on Digital Libraries</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><pagerange first="1" last="15"/></ref-volisspag><ref-text>10.1007/s00799-012-0086-z</ref-text></ref-info><ref-fulltext>Gregor Strle and Matija Marolt. The ethnomuse digital library: conceptual representation and annotation of ethnomusicological materials. International Journal on Digital Libraries, pages 1-15, 2011. 10.1007/s00799-012-0086-z.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>A computational approach to the modeling and employment of cognitive units of folk song melodies using audio recordings</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873458957</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Van Kranenburg P.</ce:indexed-name><ce:surname>Van Kranenburg</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Tzanetakis G.</ce:indexed-name><ce:surname>Tzanetakis</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 11th International Conference on Music Perception and Cognition</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><pagerange first="794" last="797"/></ref-volisspag></ref-info><ref-fulltext>Peter van Kranenburg and George Tzanetakis. A computational approach to the modeling and employment of cognitive units of folk song melodies using audio recordings. In Proceedings of the 11th International Conference on Music Perception and Cognition, pages 794-797, 2010.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>