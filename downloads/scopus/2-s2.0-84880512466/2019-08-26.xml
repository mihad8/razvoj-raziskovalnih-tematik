<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84880512466</prism:url><dc:identifier>SCOPUS_ID:84880512466</dc:identifier><eid>2-s2.0-84880512466</eid><prism:doi>10.1007/s11063-012-9259-4</prism:doi><dc:title>Selective recurrent neural network</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>6</citedby-count><prism:publicationName>Neural Processing Letters</prism:publicationName><source-id>24806</source-id><prism:issn>13704621 1573773X</prism:issn><prism:volume>38</prism:volume><prism:issueIdentifier>1</prism:issueIdentifier><prism:startingPage>1</prism:startingPage><prism:endingPage>15</prism:endingPage><prism:pageRange>1-15</prism:pageRange><prism:coverDate>2013-08-01</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="6507593861"><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Šter</ce:surname><ce:given-name>Branko</ce:given-name><preferred-name><ce:initials>B.</ce:initials><ce:indexed-name>Šter B.</ce:indexed-name><ce:surname>Šter</ce:surname><ce:given-name>Branko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6507593861</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>It is known that recurrent neural networks may have difficulties remembering data over long time lags. To overcome this problem, we propose an extended architecture of recurrent neural networks, which is able to deal with long time lags between relevant input signals. A register of latches at the input layer of the network is applied to bypass irrelevant input information and to propagate relevant inputs. The latches are implemented with differentiable multiplexers, thus enabling the derivatives to be propagated through the network. The relevance of input vectors is learned concurrently with the weights of the network using a gradient-based algorithm. © 2012 Springer Science+Business Media New York.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84880512466" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84880512466&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84880512466&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="6507593861"><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Šter</ce:surname><ce:given-name>Branko</ce:given-name><preferred-name><ce:initials>B.</ce:initials><ce:indexed-name>Šter B.</ce:indexed-name><ce:surname>Šter</ce:surname><ce:given-name>Branko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6507593861</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Finite state automata</author-keyword><author-keyword>Latch</author-keyword><author-keyword>Long-term dependencies</author-keyword><author-keyword>Multiplexer</author-keyword><author-keyword>Recurrent neural networks</author-keyword><author-keyword>Temporal processing</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Gradient based algorithm</mainterm><mainterm weight="a" candidate="n">Input layers</mainterm><mainterm weight="a" candidate="n">Input vector</mainterm><mainterm weight="a" candidate="n">Latch</mainterm><mainterm weight="a" candidate="n">Long-term dependencies</mainterm><mainterm weight="a" candidate="n">Multiplexer</mainterm><mainterm weight="a" candidate="n">Temporal processing</mainterm><mainterm weight="a" candidate="n">Time lag</mainterm></idxterms><subject-areas><subject-area code="1712" abbrev="COMP">Software</subject-area><subject-area code="2800" abbrev="NEUR">Neuroscience (all)</subject-area><subject-area code="1705" abbrev="COMP">Computer Networks and Communications</subject-area><subject-area code="1702" abbrev="COMP">Artificial Intelligence</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2017-11-08T15:03:46.795Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/nlp</xocs:funding-addon-type></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered year="2019" month="06" day="03" timestamp="2019-06-03T22:46:59.000059-04:00"/><ait:date-sort year="2013" month="08" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2013 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1007/s11063-012-9259-4</ce:doi><itemid idtype="PUI">52317918</itemid><itemid idtype="CPX">20133016544203</itemid><itemid idtype="SCP">84880512466</itemid><itemid idtype="SGR">84880512466</itemid></itemidlist><history><date-created year="2012" month="11" day="26"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">Finite state automata</author-keyword><author-keyword xml:lang="eng">Latch</author-keyword><author-keyword xml:lang="eng">Long-term dependencies</author-keyword><author-keyword xml:lang="eng">Multiplexer</author-keyword><author-keyword xml:lang="eng">Recurrent neural networks</author-keyword><author-keyword xml:lang="eng">Temporal processing</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Selective recurrent neural network</titletext></citation-title><author-group><author auid="6507593861" seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Šter</ce:surname><ce:given-name>Branko</ce:given-name><preferred-name><ce:initials>B.</ce:initials><ce:indexed-name>Šter B.</ce:indexed-name><ce:surname>Šter</ce:surname><ce:given-name>Branko</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city-group>1000 Ljubljana</city-group><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Šter</ce:surname></person><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city-group>1000 Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>It is known that recurrent neural networks may have difficulties remembering data over long time lags. To overcome this problem, we propose an extended architecture of recurrent neural networks, which is able to deal with long time lags between relevant input signals. A register of latches at the input layer of the network is applied to bypass irrelevant input information and to propagate relevant inputs. The latches are implemented with differentiable multiplexers, thus enabling the derivatives to be propagated through the network. The relevance of input vectors is learned concurrently with the weights of the network using a gradient-based algorithm. © 2012 Springer Science+Business Media New York.</ce:para></abstract></abstracts><source srcid="24806" type="j" country="nld"><sourcetitle>Neural Processing Letters</sourcetitle><sourcetitle-abbrev>Neural Process Letters</sourcetitle-abbrev><issn type="print">13704621</issn><issn type="electronic">1573773X</issn><codencode>NPLEF</codencode><volisspag><voliss volume="38" issue="1"/><pagerange first="1" last="15"/></volisspag><publicationyear first="2013"/><publicationdate><year>2013</year><month>08</month><date-text xfab-added="true">August 2013</date-text></publicationdate></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification> <classification-code>716</classification-code> <classification-description>Electronic Equipment, Radar, Radio and Television</classification-description> </classification><classification> <classification-code>717</classification-code> <classification-description>Electro-Optical Communication</classification-description> </classification><classification> <classification-code>718</classification-code> <classification-description>Telephone and Other Line Communications</classification-description> </classification><classification> <classification-code>721.1</classification-code> <classification-description>Computer Theory (Includes Formal Logic, Automata Theory, Switching Theory and Programming Theory)</classification-description> </classification><classification> <classification-code>723</classification-code> <classification-description>Computer Software, Data Handling and Applications</classification-description> </classification><classification> <classification-code>723.4</classification-code> <classification-description>Artificial Intelligence</classification-description> </classification></classifications><classifications type="GEOCLASS"><classification> <classification-code>Related Topics</classification-code> </classification></classifications><classifications type="ASJC"><classification>1712</classification><classification>2800</classification><classification>1705</classification><classification>1702</classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification><classification>NEUR</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="18"><reference id="1"><ref-info><ref-title><ref-titletext>A learning algorithm for continually running fully recurrent neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001202594</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.J.</ce:initials><ce:indexed-name>Williams R.J.</ce:indexed-name><ce:surname>Williams</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Zipser D.</ce:indexed-name><ce:surname>Zipser</ce:surname></author></ref-authors><ref-sourcetitle>Neural Comput</ref-sourcetitle><ref-publicationyear first="1989"/><ref-volisspag><voliss volume="1" issue="2"/><pagerange first="270" last="280"/></ref-volisspag><ref-text>10.1162/neco.1989.1.2.270</ref-text></ref-info><ref-fulltext>Williams RJ, Zipser D (1989) A learning algorithm for continually running fully recurrent neural networks. Neural Comput 1(2): 270-280</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Finite state automata and simple recurrent networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000111307</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Cleeremans A.</ce:indexed-name><ce:surname>Cleeremans</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Servan-Schreiber D.</ce:indexed-name><ce:surname>Servan-Schreiber</ce:surname></author><author seq="3"><ce:initials>J.L.</ce:initials><ce:indexed-name>McClelland J.L.</ce:indexed-name><ce:surname>McClelland</ce:surname></author></ref-authors><ref-sourcetitle>Neural Comput</ref-sourcetitle><ref-publicationyear first="1989"/><ref-volisspag><voliss volume="1" issue="3"/><pagerange first="372" last="381"/></ref-volisspag><ref-text>10.1162/neco.1989.1.3.372</ref-text></ref-info><ref-fulltext>Cleeremans A, Servan-Schreiber D, McClelland JL (1989) Finite state automata and simple recurrent networks. Neural Comput 1(3): 372-381</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>On-line identification and reconstruction of finite automata with generalized recurrent neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0037254330</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>I.</ce:initials><ce:indexed-name>Gabrijel I.</ce:indexed-name><ce:surname>Gabrijel</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Dobnikar A.</ce:indexed-name><ce:surname>Dobnikar</ce:surname></author></ref-authors><ref-sourcetitle>Neural Netw</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><voliss volume="16" issue="1"/><pagerange first="101" last="120"/></ref-volisspag><ref-text>10.1016/S0893-6080(02)00221-6</ref-text></ref-info><ref-fulltext>Gabrijel I, Dobnikar A (2003) On-line identification and reconstruction of finite automata with generalized recurrent neural networks. Neural Netw 16(1): 101-120</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Grammatical inference using an adaptive recurrent neural network</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">26444445179</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.H.</ce:initials><ce:indexed-name>Chen L.H.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="2"><ce:initials>H.C.</ce:initials><ce:indexed-name>Chua H.C.</ce:indexed-name><ce:surname>Chua</ce:surname></author><author seq="3"><ce:initials>P.B.</ce:initials><ce:indexed-name>Tan P.B.</ce:indexed-name><ce:surname>Tan</ce:surname></author></ref-authors><ref-sourcetitle>Neural Process Lett</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><voliss volume="8"/><pagerange first="211" last="219"/></ref-volisspag><ref-text>10.1023/A:1009673616664</ref-text></ref-info><ref-fulltext>Chen LH, Chua HC, Tan PB (1998) Grammatical inference using an adaptive recurrent neural network. Neural Process Lett 8: 211-219</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Backpropagation through time: What it does and how to do it</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0025503558</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.J.</ce:initials><ce:indexed-name>Werbos P.J.</ce:indexed-name><ce:surname>Werbos</ce:surname></author></ref-authors><ref-sourcetitle>Proc IEEE</ref-sourcetitle><ref-publicationyear first="1990"/><ref-volisspag><voliss volume="78" issue="10"/><pagerange first="1550" last="1560"/></ref-volisspag><ref-text>10.1109/5.58337</ref-text></ref-info><ref-fulltext>Werbos PJ (1990) Backpropagation through time: what it does and how to do it. Proc IEEE 78(10): 1550-1560</ref-fulltext></reference><reference id="6"><ref-info><refd-itemidlist><itemid idtype="SGR">0012347694</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Haykin S.</ce:indexed-name><ce:surname>Haykin</ce:surname></author><author seq="2"><ce:initials>G.V.</ce:initials><ce:indexed-name>Puskorius G.V.</ce:indexed-name><ce:surname>Puskorius</ce:surname></author><author seq="3"><ce:initials>L.A.</ce:initials><ce:indexed-name>Feldkamp L.A.</ce:indexed-name><ce:surname>Feldkamp</ce:surname></author><author seq="4"><ce:initials>G.S.</ce:initials><ce:indexed-name>Patel G.S.</ce:indexed-name><ce:surname>Patel</ce:surname></author><author seq="5"><ce:initials>S.</ce:initials><ce:indexed-name>Becker S.</ce:indexed-name><ce:surname>Becker</ce:surname></author><author seq="6"><ce:initials>R.</ce:initials><ce:indexed-name>Racine R.</ce:indexed-name><ce:surname>Racine</ce:surname></author><author seq="7"><ce:initials>E.A.</ce:initials><ce:indexed-name>Wan E.A.</ce:indexed-name><ce:surname>Wan</ce:surname></author><author seq="8"><ce:initials>A.T.</ce:initials><ce:indexed-name>Nelson A.T.</ce:indexed-name><ce:surname>Nelson</ce:surname></author><author seq="9"><ce:initials>S.T.</ce:initials><ce:indexed-name>Rowels S.T.</ce:indexed-name><ce:surname>Rowels</ce:surname></author><author seq="10"><ce:initials>Z.</ce:initials><ce:indexed-name>Ghahramani Z.</ce:indexed-name><ce:surname>Ghahramani</ce:surname></author><author seq="11"><ce:initials>R.</ce:initials><ce:indexed-name>Van Der Merwe R.</ce:indexed-name><ce:surname>Van Der Merwe</ce:surname></author></ref-authors><ref-sourcetitle>Kalman Filtering and Neural Networks</ref-sourcetitle><ref-publicationyear first="2002"/><ref-text>Wiley New York</ref-text></ref-info><ref-fulltext>Haykin S, Puskorius GV, Feldkamp LA, Patel GS, Becker S, Racine R, Wan EA, Nelson AT, Rowels ST, Ghahramani Z, van der Merwe R (2002) Kalman filtering and neural networks. Wiley, New York</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Learning long-term dependencies with gradient descent is difficult</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0028392483</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Simard P.</ce:indexed-name><ce:surname>Simard</ce:surname></author><author seq="3"><ce:initials>P.</ce:initials><ce:indexed-name>Frasconi P.</ce:indexed-name><ce:surname>Frasconi</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans Neural Netw</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="5" issue="2"/><pagerange first="157" last="166"/></ref-volisspag><ref-text>10.1109/72.279181</ref-text></ref-info><ref-fulltext>Bengio Y, Simard P, Frasconi P (1994) Learning long-term dependencies with gradient descent is difficult. IEEE Trans Neural Netw 5(2): 157-166</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Learning long term dependencies with recurrent neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33749866722</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.M.</ce:initials><ce:indexed-name>Schaefer A.M.</ce:indexed-name><ce:surname>Schaefer</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Udluft S.</ce:indexed-name><ce:surname>Udluft</ce:surname></author><author seq="3"><ce:initials>H.G.</ce:initials><ce:indexed-name>Zimmermann H.G.</ce:indexed-name><ce:surname>Zimmermann</ce:surname></author></ref-authors><ref-sourcetitle>ICANN 2006</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="4131"/><pagerange first="71" last="80"/></ref-volisspag><ref-text>Lecture Notes in Computer Science 2006</ref-text></ref-info><ref-fulltext>Schaefer AM, Udluft S, Zimmermann HG (2006) Learning long term dependencies with recurrent neural networks. ICANN 2006, Lecture Notes in Computer Science, Volume 4131/2006, 71-80</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Learning recurrent neural networks with Hessian-free optimization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80053451847</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Martens J.</ce:indexed-name><ce:surname>Martens</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Sutskever I.</ce:indexed-name><ce:surname>Sutskever</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 28th International Conference on Machine Learning (ICML)</ref-sourcetitle><ref-publicationyear first="2011"/></ref-info><ref-fulltext>Martens J, Sutskever I (2011) Learning recurrent neural networks with Hessian-free optimization. In: Proceedings of the 28th international conference on machine learning (ICML)</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Learning complex, extended sequences using the principle of history compression</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001033889</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Schmidhuber J.</ce:indexed-name><ce:surname>Schmidhuber</ce:surname></author></ref-authors><ref-sourcetitle>Neural Comput</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="4" issue="2"/><pagerange first="234" last="242"/></ref-volisspag><ref-text>10.1162/neco.1992.4.2.234</ref-text></ref-info><ref-fulltext>Schmidhuber J (1992) Learning complex, extended sequences using the principle of history compression. Neural Comput 4(2): 234-242</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Long short-term memory</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031573117</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Hochreiter S.</ce:indexed-name><ce:surname>Hochreiter</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Schmidhuber J.</ce:indexed-name><ce:surname>Schmidhuber</ce:surname></author></ref-authors><ref-sourcetitle>Neural Comput</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="9" issue="8"/><pagerange first="1735" last="1780"/></ref-volisspag><ref-text>10.1162/neco.1997.9.8.1735</ref-text></ref-info><ref-fulltext>Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8): 1735-1780</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Learning to forget: Continual prediction with LSTM</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0034293152</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.A.</ce:initials><ce:indexed-name>Gers F.A.</ce:indexed-name><ce:surname>Gers</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Schmidhuber J.</ce:indexed-name><ce:surname>Schmidhuber</ce:surname></author><author seq="3"><ce:initials>F.</ce:initials><ce:indexed-name>Cummins F.</ce:indexed-name><ce:surname>Cummins</ce:surname></author></ref-authors><ref-sourcetitle>Neural Comput</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="12" issue="10"/><pagerange first="2451" last="2471"/></ref-volisspag><ref-text>10.1162/089976600300015015</ref-text></ref-info><ref-fulltext>Gers FA, Schmidhuber J, Cummins F (2000) Learning to forget: continual prediction with LSTM. Neural Comput 12(10): 2451-2471</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Hierarchical recurrent neural networks for long-term dependencies</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0003331189</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>El Hihi S.</ce:indexed-name><ce:surname>El Hihi</ce:surname></author></ref-authors><ref-sourcetitle>NIPS 8</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><pagerange first="493" last="499"/></ref-volisspag><ref-text>The MIT Press, Cambridge</ref-text></ref-info><ref-fulltext>Bengio Y, el Hihi S (1996) Hierarchical recurrent neural networks for long-term dependencies. NIPS 8, The MIT Press, Cambridge, pp 493-499</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Learning long-term dependencies by the selective addition of time-delayed connections to recurrent neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036825531</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Bone R.</ce:indexed-name><ce:surname>Bone</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Crucianu M.</ce:indexed-name><ce:surname>Crucianu</ce:surname></author><author seq="3"><ce:initials>J.P.</ce:initials><ce:indexed-name>Asselinde Beauville J.P.</ce:indexed-name><ce:surname>Asselinde Beauville</ce:surname></author></ref-authors><ref-sourcetitle>Neurocomputing</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="48" issue="1"/><pagerange first="251" last="266"/></ref-volisspag><ref-text>1006.68801 10.1016/S0925-2312(01)00654-3</ref-text></ref-info><ref-fulltext>Bone R, Crucianu M, Asselinde Beauville JP (2002) Learning long-term dependencies by the selective addition of time-delayed connections to recurrent neural networks. Neurocomputing 48(1): 251-266</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>The problem of learning long-term dependencies in recurrent networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84943223999</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Frasconi P.</ce:indexed-name><ce:surname>Frasconi</ce:surname></author><author seq="3"><ce:initials>P.</ce:initials><ce:indexed-name>Simard P.</ce:indexed-name><ce:surname>Simard</ce:surname></author></ref-authors><ref-sourcetitle>IEEE International Conference on Neural Networks</ref-sourcetitle><ref-publicationyear first="1993"/><ref-volisspag><pagerange first="1183" last="1195"/></ref-volisspag><ref-text>IEEE Press</ref-text></ref-info><ref-fulltext>Bengio Y, Frasconi P, Simard P (1993) The problem of learning long-term dependencies in recurrent networks. In: IEEE International Conference on Neural Networks, IEEE Press, pp 1183-1195</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Learning long-term dependencies in NARX recurrent neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33646241633</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Lin T.</ce:indexed-name><ce:surname>Lin</ce:surname></author><author seq="2"><ce:initials>B.G.</ce:initials><ce:indexed-name>Horne B.G.</ce:indexed-name><ce:surname>Horne</ce:surname></author><author seq="3"><ce:initials>P.</ce:initials><ce:indexed-name>Tino P.</ce:indexed-name><ce:surname>Tino</ce:surname></author><author seq="4"><ce:initials>C.L.</ce:initials><ce:indexed-name>Giles C.L.</ce:indexed-name><ce:surname>Giles</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans Neural Netw</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><voliss volume="7" issue="6"/><pagerange first="1329" last="1338"/></ref-volisspag><ref-text>10.1109/72.548162</ref-text></ref-info><ref-fulltext>Lin T, Horne BG, Tino P, Giles CL (1996) Learning long-term dependencies in NARX recurrent neural networks. IEEE Trans Neural Netw 7(6): 1329-1338</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>Latched recurrent neural network</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038241188</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Ster B.</ce:indexed-name><ce:surname>Ster</ce:surname></author></ref-authors><ref-sourcetitle>Elektrotehniški Vestnik</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><voliss volume="70" issue="1-2"/><pagerange first="46" last="51"/></ref-volisspag></ref-info><ref-fulltext>Ster B (2003) Latched recurrent neural network. Elektrotehniški vestnik 70(1-2): 46-51</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Dynamic recurrent neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0003464265</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.A.</ce:initials><ce:indexed-name>Pearlmutter B.A.</ce:indexed-name><ce:surname>Pearlmutter</ce:surname></author></ref-authors><ref-sourcetitle>Technical Report CMU-CS-90-196, Carnegie Mellon University</ref-sourcetitle><ref-publicationyear first="1990"/></ref-info><ref-fulltext>Pearlmutter BA (1990) Dynamic recurrent neural networks. Technical Report CMU-CS-90-196, Carnegie Mellon University</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>