<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84880728502</prism:url><dc:identifier>SCOPUS_ID:84880728502</dc:identifier><eid>2-s2.0-84880728502</eid><dc:title>Combining learning constraints and numerical regression</dc:title><prism:aggregationType>Conference Proceeding</prism:aggregationType><srctype>p</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>IJCAI International Joint Conference on Artificial Intelligence</prism:publicationName><source-id>19400157504</source-id><prism:issn>10450823</prism:issn><prism:startingPage>596</prism:startingPage><prism:endingPage>602</prism:endingPage><prism:pageRange>596-602</prism:pageRange><prism:coverDate>2005-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="6602558523"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sǔc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6602558523</author-url><affiliation id="60108057" href="https://api.elsevier.com/content/affiliation/affiliation_id/60108057"/><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>Usual numerical learning methods are primarily concerned with finding a good numerical fit to data and often make predictions that do not correspond to qualitative laws in the domain of modelling or expert intuition. In contrast, the idea of Q<sup>2</sup> learning is to induce qualitative constraints from training data, and use the constraints to guide numerical regression. The resulting numerical predictions are consistent with a learned qualitative model which is beneficial in terms of explanation of phenomena in the modelled domain, and can also improve numerical accuracy. This paper proposes a method for combining the learning of qualitative constraints with an arbitrary numerical learner and explores the accuracy and explanation benefits of learning monotonic qualitative constraints in a number of domains. We show that Q<sup>2</sup> learning can correct for errors caused by the bias of the learning algorithm and discuss the potentials of similar hierarchical learning schemes.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84880728502" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84880728502&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84880728502&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60108057" href="https://api.elsevier.com/content/affiliation/affiliation_id/60108057"><affilname>CSIRO Data61</affilname><affiliation-city>Sydney</affiliation-city><affiliation-country>Australia</affiliation-country></affiliation><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="6602558523"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sǔc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6602558523</author-url><affiliation id="60108057" href="https://api.elsevier.com/content/affiliation/affiliation_id/60108057"/><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="7003286588"><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname><ce:given-name>Ivan</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname><ce:given-name>Ivan</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7003286588</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords/><idxterms><mainterm weight="a" candidate="n">Hierarchical learning</mainterm><mainterm weight="a" candidate="n">Learning methods</mainterm><mainterm weight="a" candidate="n">Numerical accuracy</mainterm><mainterm weight="a" candidate="n">Numerical predictions</mainterm><mainterm weight="a" candidate="n">Qualitative constraints</mainterm><mainterm weight="a" candidate="n">Qualitative model</mainterm><mainterm weight="a" candidate="n">Training data</mainterm></idxterms><subject-areas><subject-area code="1702" abbrev="COMP">Artificial Intelligence</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2019-02-21T15:31:39.968Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/car</xocs:funding-addon-type></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered year="2019" month="08" day="04" timestamp="2019-08-04T15:02:28.000028-04:00"/><ait:date-sort year="2005" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2013 Elsevier B.V., All rights reserved.</copyright><itemidlist><itemid idtype="PUI">369417292</itemid><itemid idtype="CPX">20133116561595</itemid><itemid idtype="SCP">84880728502</itemid><itemid idtype="SGR">84880728502</itemid></itemidlist><history><date-created year="2013" month="08" day="01"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Combining learning constraints and numerical regression</titletext></citation-title><author-group><author auid="6602558523" seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sǔc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name></preferred-name></author><affiliation afid="60108057" country="aus"><organization>National ICT Australia</organization><organization>Sydney Laboratory at UNSW</organization><city-group>NSW 2052</city-group><affiliation-id afid="60108057"/><affiliation-id afid="60028333"/><country>Australia</country></affiliation></author-group><author-group><author auid="6602558523" seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sǔc D.</ce:indexed-name><ce:surname>Sǔc</ce:surname><ce:given-name>Dorian</ce:given-name></preferred-name></author><author auid="7003286588" seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname><ce:given-name>Ivan</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname><ce:given-name>Ivan</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city-group>1000 Ljubljana</city-group><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><affiliation country="aus"><organization>National ICT Australia</organization><organization>Sydney Laboratory at UNSW</organization><city-group>NSW 2052</city-group><country>Australia</country></affiliation></correspondence><grantlist complete="y"><grant><grant-acronym>ARC</grant-acronym><grant-agency>Australian Research Council</grant-agency></grant></grantlist><abstracts><abstract original="y" xml:lang="eng"><ce:para>Usual numerical learning methods are primarily concerned with finding a good numerical fit to data and often make predictions that do not correspond to qualitative laws in the domain of modelling or expert intuition. In contrast, the idea of Q<sup>2</sup> learning is to induce qualitative constraints from training data, and use the constraints to guide numerical regression. The resulting numerical predictions are consistent with a learned qualitative model which is beneficial in terms of explanation of phenomena in the modelled domain, and can also improve numerical accuracy. This paper proposes a method for combining the learning of qualitative constraints with an arbitrary numerical learner and explores the accuracy and explanation benefits of learning monotonic qualitative constraints in a number of domains. We show that Q<sup>2</sup> learning can correct for errors caused by the bias of the learning algorithm and discuss the potentials of similar hierarchical learning schemes.</ce:para></abstract></abstracts><source srcid="19400157504" type="p" country="usa"><sourcetitle>IJCAI International Joint Conference on Artificial Intelligence</sourcetitle><sourcetitle-abbrev>IJCAI Int. Joint Conf. Artif. Intell.</sourcetitle-abbrev><issuetitle>19th International Joint Conference on Artificial Intelligence, IJCAI 2005</issuetitle><issn type="print">10450823</issn><volisspag><pagerange first="596" last="602"/></volisspag><publicationyear first="2005"/><publicationdate><year>2005</year><date-text xfab-added="true">2005</date-text></publicationdate><additional-srcinfo><conferenceinfo><confevent><confname>19th International Joint Conference on Artificial Intelligence, IJCAI 2005</confname><conflocation country="gbr"><city-group>Edinburgh</city-group></conflocation><confdate><startdate year="2005" month="07" day="30"/><enddate year="2005" month="08" day="05"/></confdate><confcode>97872</confcode><confsponsors complete="n"><confsponsor>BCS</confsponsor><confsponsor>Scottish Enterprise</confsponsor><confsponsor>QinetiQ</confsponsor><confsponsor>BT</confsponsor><confsponsor>Foresight</confsponsor></confsponsors></confevent><confpublication><procpagerange>var.pagings</procpagerange></confpublication></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="ASJC"><classification>1702</classification></classifications><classifications type="CPXCLASS"><classification> <classification-code>723.4</classification-code> <classification-description>Artificial Intelligence</classification-description> </classification><classification> <classification-code>921</classification-code> <classification-description>Applied Mathematics</classification-description> </classification></classifications><classifications type="GEOCLASS"><classification> <classification-code>Related Topics</classification-code> </classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="17"><reference id="1"><ref-info><ref-title><ref-titletext>Locally Weighted Learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031074521</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.G.</ce:initials><ce:indexed-name>Atkeson C.G.</ce:indexed-name><ce:surname>Atkeson</ce:surname></author><author seq="2"><ce:initials>A.W.</ce:initials><ce:indexed-name>Moore A.W.</ce:indexed-name><ce:surname>Moore</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Schaal S.</ce:indexed-name><ce:surname>Schaal</ce:surname></author></ref-authors><ref-sourcetitle>Artificial Intelligence Review</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="11" issue="1-5"/><pagerange first="11" last="73"/></ref-volisspag></ref-info><ref-fulltext>C.G Atkeson, A.W Moore, and S.A. Schaal. Locally weighted learning. Artificial Intelligence Review, 11:11-73, 1997. (Pubitemid 127508233)</ref-fulltext></reference><reference id="2"><ref-info><refd-itemidlist><itemid idtype="SGR">0003408496</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.L.</ce:initials><ce:indexed-name>Blake C.L.</ce:indexed-name><ce:surname>Blake</ce:surname></author><author seq="2"><ce:initials>C.J.</ce:initials><ce:indexed-name>Merz C.J.</ce:indexed-name><ce:surname>Merz</ce:surname></author></ref-authors><ref-sourcetitle>UCI Repository of Machine Learning Databases</ref-sourcetitle><ref-publicationyear first="1998"/></ref-info><ref-fulltext>C.L. Blake and C.J. Merz. UCI repository of machine learning databases, 1998.</ref-fulltext></reference><reference id="3"><ref-info><refd-itemidlist><itemid idtype="SGR">0003802343</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author><author seq="2"><ce:initials>J.H.</ce:initials><ce:indexed-name>Friedman J.H.</ce:indexed-name><ce:surname>Friedman</ce:surname></author><author seq="3"><ce:initials>R.A.</ce:initials><ce:indexed-name>Olshen R.A.</ce:indexed-name><ce:surname>Olshen</ce:surname></author><author seq="4"><ce:initials>C.J.</ce:initials><ce:indexed-name>Stone C.J.</ce:indexed-name><ce:surname>Stone</ce:surname></author></ref-authors><ref-sourcetitle>Classification and Regression Trees</ref-sourcetitle><ref-publicationyear first="1984"/><ref-text>Wadsworth, Belmont, California</ref-text></ref-info><ref-fulltext>L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. Classification and Regression Trees. Wadsworth, Belmont, California, 1984.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Machine-learning research: Four current directions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031361611</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.G.</ce:initials><ce:indexed-name>Dietterich T.G.</ce:indexed-name><ce:surname>Dietterich</ce:surname></author></ref-authors><ref-sourcetitle>The AI Magazine</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><voliss volume="18" issue="4"/><pagerange first="97" last="136"/></ref-volisspag></ref-info><ref-fulltext>T.G. Dietterich. Machine-learning research: Four current directions. The AI Magazine, 18(4):97-136, 1998.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>A unified bias-variance decomposition and its applications</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0012937288</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Domingos P.</ce:indexed-name><ce:surname>Domingos</ce:surname></author></ref-authors><ref-sourcetitle>Proc. 17th International Conf. on Machine Learning</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><pagerange first="231" last="238"/></ref-volisspag><ref-text>Morgan Kaufmann, San Francisco, CA</ref-text></ref-info><ref-fulltext>Pedro Domingos. A unified bias-variance decomposition and its applications. In Proc. 17th International Conf. on Machine Learning, pages 231-238. Morgan Kaufmann, San Francisco, CA, 2000.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Cascade generalization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0034541162</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Gama J.</ce:indexed-name><ce:surname>Gama</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Brazdil P.</ce:indexed-name><ce:surname>Brazdil</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="41" issue="3"/><pagerange first="315" last="343"/></ref-volisspag></ref-info><ref-fulltext>J. Gama and P. Brazdil. Cascade generalization. Machine Learning, 41(3):315-343, 2000.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Neural networks and the bias/variance dilemma</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001942829</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Geman S.</ce:indexed-name><ce:surname>Geman</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Bienenstock E.</ce:indexed-name><ce:surname>Bienenstock</ce:surname></author><author seq="3"><ce:initials>R.</ce:initials><ce:indexed-name>Doursat R.</ce:indexed-name><ce:surname>Doursat</ce:surname></author></ref-authors><ref-sourcetitle>Neural Computation</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="4" issue="1"/><pagerange first="1" last="58"/></ref-volisspag></ref-info><ref-fulltext>S. Geman, E. Bienenstock, and R. Doursat. Neural networks and the bias/variance dilemma. Neural Computation, 4(1):1-58, 1992.</ref-fulltext></reference><reference id="8"><ref-info><refd-itemidlist><itemid idtype="SGR">0004135367</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Kuipers B.</ce:indexed-name><ce:surname>Kuipers</ce:surname></author></ref-authors><ref-sourcetitle>Qualitative Reasoning: Modeling and Simulation with Incomplete Knowledge</ref-sourcetitle><ref-publicationyear first="1994"/><ref-text>MIT Press, Cambridge, Massachusetts</ref-text></ref-info><ref-fulltext>B. Kuipers. Qualitative Reasoning: Modeling and Simulation with Incomplete Knowledge. MIT Press, Cambridge, Massachusetts, 1994.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Combining instance-based and model-based learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002418869</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.R.</ce:initials><ce:indexed-name>Quinlan J.R.</ce:indexed-name><ce:surname>Quinlan</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 12th International Conf. on Machine Learning</ref-sourcetitle><ref-publicationyear first="1993"/><ref-volisspag><pagerange first="236" last="243"/></ref-volisspag><ref-text>San Mateo, CA, Morgan Kaufmann</ref-text></ref-info><ref-fulltext>J.R. Quinlan. Combining instance-based and model-based learning. In Proceedings of the 12th International Conf. on Machine Learning, pages 236-243, San Mateo, CA, 1993. Morgan Kaufmann.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Layered learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84974678409</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Stone P.</ce:indexed-name><ce:surname>Stone</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Veloso M.</ce:indexed-name><ce:surname>Veloso</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning: ECML 2000 (Proceedings of the 11th European Conference on Machine Learning)</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><pagerange first="369" last="381"/></ref-volisspag><ref-text>Springer Verlag</ref-text></ref-info><ref-fulltext>P. Stone and M. Veloso. Layered learning. In Machine Learning: ECML 2000 (Proceedings of the 11th European Conference on Machine Learning), pages 369-381. Springer Verlag, 2000.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Combining classifiers with meta decision trees</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0037365188</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Todorovski L.</ce:indexed-name><ce:surname>Todorovski</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Dzeroski S.</ce:indexed-name><ce:surname>Džeroski</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><voliss volume="50" issue="3"/><pagerange first="223" last="249"/></ref-volisspag></ref-info><ref-fulltext>L. Todorovski and S. Džeroski. Combining classifiers with meta decision trees. Machine Learning, 50(3):223-249, 2003.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Qualitative reverse engineering</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">1942450283</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Šuc</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 19th International Conf. on Machine Learning</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><pagerange first="610" last="617"/></ref-volisspag><ref-text>Morgan Kaufmann</ref-text></ref-info><ref-fulltext>D. Šuc and I. Bratko. Qualitative reverse engineering. In Proceedings of the 19th International Conf. on Machine Learning, pages 610-617. Morgan Kaufmann, 2002.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Improving numerical accuracy with qualitative constraints</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">9444242738</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Šuc</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 14th European Conference on Machine Learning</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><pagerange first="385" last="396"/></ref-volisspag><ref-text>Springer</ref-text></ref-info><ref-fulltext>D. Šuc and I. Bratko. Improving numerical accuracy with qualitative constraints. In Proceedings of the 14th European Conference on Machine Learning, pages 385-396. Springer, 2003.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Qualitatively faithful quantitative prediction</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">4344686215</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Šuc</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Vladusic D.</ce:indexed-name><ce:surname>Vladušič</ce:surname></author><author seq="3"><ce:initials>I.</ce:initials><ce:indexed-name>Bratko I.</ce:indexed-name><ce:surname>Bratko</ce:surname></author></ref-authors><ref-sourcetitle>Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="158" issue="2"/><pagerange first="190" last="219"/></ref-volisspag></ref-info><ref-fulltext>D. Šuc, D. Vladušič, and I. Bratko. Qualitatively faithful quantitative prediction. Artificial Intelligence, 158(2):190-219, 2004.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Machine Reconstruction of Human Control Strategies</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">4344607387</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Suc D.</ce:indexed-name><ce:surname>Šuc</ce:surname></author></ref-authors><ref-sourcetitle>Frontiers in Artificial Intelligence and Applications</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><voliss volume="99"/></ref-volisspag><ref-text>IOS Press, Amsterdam, The Netherlands</ref-text></ref-info><ref-fulltext>D. Šuc. Machine Reconstruction of Human Control Strategies, volume 99 of Frontiers in Artificial Intelligence and Applications. IOS Press, Amsterdam, The Netherlands, 2003.</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Induction of model trees for predicting continuous classes</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84880077216</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>I.H.</ce:initials><ce:indexed-name>Witten I.H.</ce:indexed-name><ce:surname>Witten</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Wang Y.</ce:indexed-name><ce:surname>Wang</ce:surname></author></ref-authors><ref-sourcetitle>Proc. Poster Papers Europ. Conf. Machine Learning, 1997</ref-sourcetitle></ref-info><ref-fulltext>I.H. Witten and Y. Wang. Induction of model trees for predicting continuous classes. In Proc. Poster Papers Europ. Conf. Machine Learning, 1997.</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>Stacked generalization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0026692226</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.H.</ce:initials><ce:indexed-name>Wolpert D.H.</ce:indexed-name><ce:surname>Wolpert</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="5" issue="2"/><pagerange first="241" last="259"/></ref-volisspag></ref-info><ref-fulltext>D.H. Wolpert. Stacked generalization. Neural Networks, 5(2):241-259, 1992.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>