<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84893469578</prism:url><dc:identifier>SCOPUS_ID:84893469578</dc:identifier><eid>2-s2.0-84893469578</eid><prism:doi>10.1007/978-3-642-37213-1-20</prism:doi><dc:title>Generalized information-theoretic measures for feature selection</dc:title><prism:aggregationType>Book Series</prism:aggregationType><srctype>k</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>2</citedby-count><prism:publicationName>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</prism:publicationName><source-id>25674</source-id><prism:isbn>9783642372124</prism:isbn><prism:issn>03029743 16113349</prism:issn><prism:volume>7824 LNCS</prism:volume><prism:startingPage>189</prism:startingPage><prism:endingPage>197</prism:endingPage><prism:pageRange>189-197</prism:pageRange><prism:coverDate>2013-12-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="54988061800"><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/54988061800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>Information-theoretic measures are frequently employed to select the most relevant subset of features from datasets. This paper focuses on the analysis of continuous-valued features. We compare the common approach with discretization of features prior the analysis, to the direct usage of exact values. Due to the overwhelming costs of computing continuous information-theoretic measures based on Shannon entropy the Renyi and Tsallis generalized measures are considered. To enable computation with continuous Tsallis measures a novel modification of the information potential is introduced. The quality of the analysed measures was assessed indirectly through the classification accuracy in conjuction with the greedy feature selection process. The experiments on datasets from UCI repository show considerable improvements of the results when using both generalized continuous measures. © 2013 Springer-Verlag Berlin Heidelberg.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84893469578" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84893469578&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84893469578&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="54988061800"><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/54988061800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="6506205187"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotric</ce:surname><ce:given-name>Uros</ce:given-name><preferred-name><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotric</ce:surname><ce:given-name>Uros</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6506205187</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Feature selection</author-keyword><author-keyword>Information theory</author-keyword><author-keyword>Renyi entropy</author-keyword><author-keyword>Tsallis entropy</author-keyword></authkeywords><idxterms><mainterm weight="a" candidate="n">Classification accuracy</mainterm><mainterm weight="a" candidate="n">Discretizations</mainterm><mainterm weight="a" candidate="n">Information potential</mainterm><mainterm weight="a" candidate="n">Renyi entropy</mainterm><mainterm weight="a" candidate="n">Shannon entropy</mainterm><mainterm weight="a" candidate="n">Tsallis entropies</mainterm><mainterm weight="a" candidate="n">UCI repository</mainterm></idxterms><subject-areas><subject-area code="2614" abbrev="MATH">Theoretical Computer Science</subject-area><subject-area code="1700" abbrev="COMP">Computer Science (all)</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2017" month="06" day="02" timestamp="2017-06-02T11:45:29.000029+01:00"/><ait:date-sort year="2013" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2014 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1007/978-3-642-37213-1-20</ce:doi><itemid idtype="PUI">372133740</itemid><itemid idtype="CPX">20140417226601</itemid><itemid idtype="SCP">84893469578</itemid><itemid idtype="SGR">84893469578</itemid></itemidlist><history><date-created year="2014" month="01" day="22"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword>Feature selection</author-keyword><author-keyword>Information theory</author-keyword><author-keyword>Renyi entropy</author-keyword><author-keyword>Tsallis entropy</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Generalized information-theoretic measures for feature selection</titletext></citation-title><author-group><author auid="54988061800" seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name></preferred-name></author><author auid="6506205187" seq="2"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotric</ce:surname><ce:given-name>Uros</ce:given-name><preferred-name><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotric</ce:surname><ce:given-name>Uros</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Trzaska 25</address-part><city-group>1000 Ljubljana</city-group><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Trzaska 25</address-part><city-group>1000 Ljubljana</city-group><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>Information-theoretic measures are frequently employed to select the most relevant subset of features from datasets. This paper focuses on the analysis of continuous-valued features. We compare the common approach with discretization of features prior the analysis, to the direct usage of exact values. Due to the overwhelming costs of computing continuous information-theoretic measures based on Shannon entropy the Renyi and Tsallis generalized measures are considered. To enable computation with continuous Tsallis measures a novel modification of the information potential is introduced. The quality of the analysed measures was assessed indirectly through the classification accuracy in conjuction with the greedy feature selection process. The experiments on datasets from UCI repository show considerable improvements of the results when using both generalized continuous measures. © 2013 Springer-Verlag Berlin Heidelberg.</ce:para></abstract></abstracts><source srcid="25674" type="k" country="deu"><sourcetitle>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</sourcetitle><sourcetitle-abbrev>Lect. Notes Comput. Sci.</sourcetitle-abbrev><issuetitle>Adaptive and Natural Computing Algorithms - 11th International Conference, ICANNGA 2013, Proceedings</issuetitle><issn type="print">03029743</issn><issn type="electronic">16113349</issn><isbn length="13" level="volume">9783642372124</isbn><volisspag><voliss volume="7824 LNCS"/><pagerange first="189" last="197"/></volisspag><publicationyear first="2013"/><publicationdate><year>2013</year><date-text xfab-added="true">2013</date-text></publicationdate><additional-srcinfo><conferenceinfo><confevent><confname>11th International Conference on Adaptive and Natural Computing Algorithms, ICANNGA 2013</confname><conflocation country="che"><city-group>Lausanne</city-group></conflocation><confdate><startdate year="2013" month="04" day="04"/><enddate year="2013" month="04" day="06"/></confdate><confcode>102029</confcode></confevent><confpublication><procpagerange>var.pagings</procpagerange></confpublication></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification> <classification-code>716</classification-code> <classification-description>Electronic Equipment, Radar, Radio and Television</classification-description> </classification><classification> <classification-code>716.1</classification-code> <classification-description>Information and Communication Theory</classification-description> </classification><classification> <classification-code>723</classification-code> <classification-description>Computer Software, Data Handling and Applications</classification-description> </classification><classification> <classification-code>921</classification-code> <classification-description>Applied Mathematics</classification-description> </classification></classifications><classifications type="GEOCLASS"><classification> <classification-code>Related Topics</classification-code> </classification></classifications><classifications type="ASJC"><classification>2614</classification><classification>1700</classification></classifications><classifications type="SUBJABBR"><classification>MATH</classification><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="18"><reference id="1"><ref-info><ref-title><ref-titletext>Feature selection for classification</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0013326060</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Dash M.</ce:indexed-name><ce:surname>Dash</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Liu H.</ce:indexed-name><ce:surname>Liu</ce:surname></author></ref-authors><ref-sourcetitle>Intelligent Data Analysis</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="1" issue="1-4"/><pagerange first="131" last="156"/></ref-volisspag></ref-info><ref-fulltext>Dash, M., Liu, H.: Feature Selection for Classification. Intelligent Data Analysis 1(1-4), 131-156 (1997)</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Selection of relevant features and examples in machine Learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031334221</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Blum A.</ce:indexed-name><ce:surname>Blum</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Langley P.</ce:indexed-name><ce:surname>Langley</ce:surname></author></ref-authors><ref-sourcetitle>Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="97" issue="1-2"/><pagerange first="245" last="271"/></ref-volisspag></ref-info><ref-fulltext>Blum, A., Langley, P.: Selection of relevant features and examples in machine Learning. Artificial Intelligence 97(1-2), 245-271 (1997)</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Feature selection and feature extraction for text categorization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002312061</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Lewis D.</ce:indexed-name><ce:surname>Lewis</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of Speech and Natural Language Workshop</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><pagerange first="212" last="217"/></ref-volisspag><ref-text>Morgan Kaufmann, San Francisco</ref-text></ref-info><ref-fulltext>Lewis, D.: Feature selection and feature extraction for text categorization. In: Proceedings of Speech and Natural Language Workshop, pp. 212-217. Morgan Kaufmann, San Francisco (1992)</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Feature selection with dynamic mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">62349118015</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Liu H.</ce:indexed-name><ce:surname>Liu</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Sun J.</ce:indexed-name><ce:surname>Sun</ce:surname></author><author seq="3"><ce:initials>L.</ce:initials><ce:indexed-name>Liu L.</ce:indexed-name><ce:surname>Liu</ce:surname></author><author seq="4"><ce:initials>H.</ce:initials><ce:indexed-name>Zhang H.</ce:indexed-name><ce:surname>Zhang</ce:surname></author></ref-authors><ref-sourcetitle>Pattern Recognition</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="42" issue="7"/><pagerange first="1330" last="1339"/></ref-volisspag></ref-info><ref-fulltext>Liu, H., Sun, J., Liu, L., Zhang, H.: Feature selection with dynamic mutual information. Pattern Recognition 42(7), 1330-1339 (2009)</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Normalized mutual information feature selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">60849097547</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.A.</ce:initials><ce:indexed-name>Estevez P.A.</ce:indexed-name><ce:surname>Estevez</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Tesmer M.</ce:indexed-name><ce:surname>Tesmer</ce:surname></author><author seq="3"><ce:initials>C.A.</ce:initials><ce:indexed-name>Perez C.A.</ce:indexed-name><ce:surname>Perez</ce:surname></author><author seq="4"><ce:initials>J.M.</ce:initials><ce:indexed-name>Zurada J.M.</ce:indexed-name><ce:surname>Zurada</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Neural Networks</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="20" issue="2"/><pagerange first="189" last="201"/></ref-volisspag></ref-info><ref-fulltext>Estevez, P.A., Tesmer, M., Perez, C.A., Zurada, J.M.: Normalized Mutual Information Feature Selection. IEEE Transactions on Neural Networks 20(2), 189-201 (2009)</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Feature selection enviroment for genomic applications</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">60849092543</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.M.</ce:initials><ce:indexed-name>Lopes F.M.</ce:indexed-name><ce:surname>Lopes</ce:surname></author><author seq="2"><ce:initials>D.C.</ce:initials><ce:indexed-name>Martins D.C.</ce:indexed-name><ce:surname>Martins</ce:surname></author><author seq="3"><ce:initials>R.M.</ce:initials><ce:indexed-name>Cesar R.M.</ce:indexed-name><ce:surname>Cesar</ce:surname></author></ref-authors><ref-sourcetitle>BMC Bioinformatics</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="9" issue="1"/><pagerange first="451" last="458"/></ref-volisspag></ref-info><ref-fulltext>Lopes, F.M., Martins, D.C., Cesar, R.M.: Feature selection enviroment for genomic applications. BMC Bioinformatics 9(1), 451-458 (2008)</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Fast binary feature selection with conditional mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33645690579</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Fleuret F.</ce:indexed-name><ce:surname>Fleuret</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Machine Learning Research</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="5"/><pagerange first="1531" last="1555"/></ref-volisspag></ref-info><ref-fulltext>Fleuret, F.: Fast binary feature selection with conditional mutual information. Journal of Machine Learning Research (5), 1531-1555 (2004)</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Feature extraction using information-theoretic learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33746475260</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.E.</ce:initials><ce:indexed-name>Hild Il K.E.</ce:indexed-name><ce:surname>Hild Il</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Erdogmus D.</ce:indexed-name><ce:surname>Erdogmus</ce:surname></author><author seq="3"><ce:initials>K.</ce:initials><ce:indexed-name>Torkkola K.</ce:indexed-name><ce:surname>Torkkola</ce:surname></author><author seq="4"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Principe</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="28" issue="9"/><pagerange first="1385" last="1392"/></ref-volisspag></ref-info><ref-fulltext>Hild Il, K.E., Erdogmus, D., Torkkola, K., Principe, J.C.: Feature Extraction Using Information-Theoretic Learning. IEEE Transactions on Pattern Analysis and Machine Intelligence 28(9), 1385-1392 (2006)</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Information theoretical properties of tsallis entropies</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33644630453</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Furuichi S.</ce:indexed-name><ce:surname>Furuichi</ce:surname></author></ref-authors><ref-sourcetitle>J. of Mathematical Physics</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="47" issue="2"/></ref-volisspag></ref-info><ref-fulltext>Furuichi, S.: Information theoretical properties of Tsallis entropies. J. of Mathematical Physics 47(2) (2006)</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Two simple and effective feature selection methods for continuous attributes with discrete multi-class</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">38349031209</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Mejia-Lavalle M.</ce:indexed-name><ce:surname>Mejía-Lavalle</ce:surname></author><author seq="2"><ce:initials>E.F.</ce:initials><ce:indexed-name>Morales E.F.</ce:indexed-name><ce:surname>Morales</ce:surname></author><author seq="3"><ce:initials>G.</ce:initials><ce:indexed-name>Arroyo G.</ce:indexed-name><ce:surname>Arroyo</ce:surname></author></ref-authors><ref-sourcetitle>MICAI 2007. LNCS (LNAI)</ref-sourcetitle><ref-publicationyear first="2007"/><ref-volisspag><voliss volume="4827"/><pagerange first="452" last="461"/></ref-volisspag><ref-text>Gelbukh, A., Kuri Morales, A.F. (eds.) Springer, Heidelberg</ref-text></ref-info><ref-fulltext>Mejía-Lavalle, M., Morales, E.F., Arroyo, G.: Two Simple and Effective Feature Selection Methods for Continuous Attributes with Discrete Multi-class. In: Gelbukh, A., Kuri Morales, A.F. (eds.) MICAI 2007. LNCS (LNAI), vol. 4827, pp. 452-461. Springer, Heidelberg (2007)</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Tsallis mutual information for document classification</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80053585312</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Vila M.</ce:indexed-name><ce:surname>Vila</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Bardera A.</ce:indexed-name><ce:surname>Bardera</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Feixas M.</ce:indexed-name><ce:surname>Feixas</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Sbert M.</ce:indexed-name><ce:surname>Sbert</ce:surname></author></ref-authors><ref-sourcetitle>Entropy</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="13"/><pagerange first="1694" last="1707"/></ref-volisspag></ref-info><ref-fulltext>Vila, M., Bardera, A., Feixas, M., Sbert, M.: Tsallis Mutual Information for Document Classification. Entropy (13), 1694-1707 (2011)</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Hybrid genetic algorithms for feature selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">12844260916</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.I.-S.</ce:initials><ce:indexed-name>Oh O.I.-S.</ce:indexed-name><ce:surname>Oh</ce:surname></author><author seq="2"><ce:initials>J.-S.</ce:initials><ce:indexed-name>Lee J.-S.</ce:indexed-name><ce:surname>Lee</ce:surname></author><author seq="3"><ce:initials>B.-R.</ce:initials><ce:indexed-name>Moon B.-R.</ce:indexed-name><ce:surname>Moon</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="26" issue="11"/><pagerange first="1424" last="1437"/></ref-volisspag></ref-info><ref-fulltext>Oh, O.I.-S., Lee, J.-S., Moon, B.-R.: Hybrid genetic algorithms for feature selection. IEEE Transactions on Pattern Analysis and Machine Intelligence 26(11), 1424-1437 (2004)</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Performance of feature-selection methods in the classification of high-dimension data</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">54549099006</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Jianping H.</ce:indexed-name><ce:surname>Jianping</ce:surname></author><author seq="2"><ce:initials>T.D.</ce:initials><ce:indexed-name>Waibhav T.D.</ce:indexed-name><ce:surname>Waibhav</ce:surname></author><author seq="3"><ce:initials>E.R.</ce:initials><ce:indexed-name>Dougherty E.R.</ce:indexed-name><ce:surname>Dougherty</ce:surname></author></ref-authors><ref-sourcetitle>Pattern Recognition</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="42" issue="3"/><pagerange first="409" last="442"/></ref-volisspag></ref-info><ref-fulltext>Jianping, H., Waibhav, T.D., Dougherty, E.R.: Performance of feature-selection methods in the classification of high-dimension data. Pattern Recognition 42(3), 409-442 (2009)</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>A simulated-annealing-based approach for simultaneous parameter optimization and featureselection of back-propagation networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">36148950145</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.-W.</ce:initials><ce:indexed-name>Lin S.-W.</ce:indexed-name><ce:surname>Lin</ce:surname></author><author seq="2"><ce:initials>T.-Y.</ce:initials><ce:indexed-name>Tseng T.-Y.</ce:indexed-name><ce:surname>Tseng</ce:surname></author><author seq="3"><ce:initials>S.-Y.</ce:initials><ce:indexed-name>Chou S.-Y.</ce:indexed-name><ce:surname>Chou</ce:surname></author><author seq="4"><ce:initials>S.-C.</ce:initials><ce:indexed-name>Chen S.-C.</ce:indexed-name><ce:surname>Chen</ce:surname></author></ref-authors><ref-sourcetitle>Expert Systems with Application</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="34" issue="2"/></ref-volisspag></ref-info><ref-fulltext>Lin, S.-W., Tseng, T.-Y., Chou, S.-Y., Chen, S.-C: A simulated-annealing- based approach for simultaneous parameter optimization and featureselection of back-propagation networks. Expert Systems with Application 34(2) (2008)</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Fast branch &amp; bound algorithms for optimal feature selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">3042527351</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Somol P.</ce:indexed-name><ce:surname>Somol</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Pudil P.</ce:indexed-name><ce:surname>Pudil</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Kittler J.</ce:indexed-name><ce:surname>Kittler</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="26" issue="7"/><pagerange first="900" last="912"/></ref-volisspag></ref-info><ref-fulltext>Somol, P., Pudil, P., Kittler, J.: Fast branch &amp; bound algorithms for optimal feature selection. IEEE Transactions on Pattern Analysis and Machine Intelligence 26(7), 900-912 (2004)</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Gene selection algorithms for microarray data based on least square support vector machine</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33645157313</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.-K.</ce:initials><ce:indexed-name>Tang E.-K.</ce:indexed-name><ce:surname>Tang</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Suganthan P.</ce:indexed-name><ce:surname>Suganthan</ce:surname></author><author seq="3"><ce:initials>X.</ce:initials><ce:indexed-name>Yao X.</ce:indexed-name><ce:surname>Yao</ce:surname></author></ref-authors><ref-sourcetitle>BMC Bioinformatics</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="7"/><pagerange first="95"/></ref-volisspag></ref-info><ref-fulltext>Tang, E.-K., Suganthan, P., Yao, X.: Gene selection algorithms for microarray data based on least square support vector machine. BMC Bioinformatics 7, 95 (2006)</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>The weka data mining software: An update</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">76749092270</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Hall M.</ce:indexed-name><ce:surname>Hall</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Frank E.</ce:indexed-name><ce:surname>Frank</ce:surname></author><author seq="3"><ce:initials>G.</ce:initials><ce:indexed-name>Holmes G.</ce:indexed-name><ce:surname>Holmes</ce:surname></author><author seq="4"><ce:initials>B.</ce:initials><ce:indexed-name>Pfahringer B.</ce:indexed-name><ce:surname>Pfahringer</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Reutemann P.</ce:indexed-name><ce:surname>Reutemann</ce:surname></author><author seq="6"><ce:initials>I.H.</ce:initials><ce:indexed-name>Witten I.H.</ce:indexed-name><ce:surname>Witten</ce:surname></author></ref-authors><ref-sourcetitle>SIGKDD Explorations</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="11" issue="1"/></ref-volisspag></ref-info><ref-fulltext>Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: The WEKA Data Mining Software: An Update. SIGKDD Explorations 11(1) (2009)</ref-fulltext></reference><reference id="18"><ref-info><refd-itemidlist><itemid idtype="SGR">78649934709</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Frank A.</ce:indexed-name><ce:surname>Frank</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Asuncion A.</ce:indexed-name><ce:surname>Asuncion</ce:surname></author></ref-authors><ref-sourcetitle>UCI Machine Learning Repository</ref-sourcetitle><ref-publicationyear first="2012"/><ref-website><ce:e-address type="url">http://archive.ics.uci.edu/ml</ce:e-address></ref-website><ref-text>University of California, School of Information and Computer Science, Irvine, CA</ref-text></ref-info><ref-fulltext>Frank, A., Asuncion, A.: UCI Machine Learning Repository. University of California, School of Information and Computer Science, Irvine, CA (2012), http://archive.ics.uci.edu/ml</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>