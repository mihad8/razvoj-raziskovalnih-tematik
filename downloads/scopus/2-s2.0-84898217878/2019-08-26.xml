<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84898217878</prism:url><dc:identifier>SCOPUS_ID:84898217878</dc:identifier><eid>2-s2.0-84898217878</eid><prism:doi>10.4018/978-1-4666-1806-0.ch003</prism:doi><dc:title>Individual prediction reliability estimates in classification and regression</dc:title><prism:aggregationType>Book</prism:aggregationType><srctype>b</srctype><subtype>ch</subtype><subtypeDescription>Chapter</subtypeDescription><citedby-count>1</citedby-count><prism:publicationName>Intelligent Data Analysis for Real-Life Applications: Theory and Practice</prism:publicationName><dc:publisher>IGI Global</dc:publisher><source-id>21100302268</source-id><prism:isbn>9781466618060</prism:isbn><prism:startingPage>35</prism:startingPage><prism:endingPage>56</prism:endingPage><prism:pageRange>35-56</prism:pageRange><prism:coverDate>2012-12-01</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="37102202800"><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/37102202800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>Current machine learning algorithms perform well in many problem domains, but in risk-sensitive decision making - for example, in medicine and finance - experts do not rely on common evaluation methods that provide overall assessments of models because such techniques do not provide any information about single predictions. This chapter summarizes the research areas that have motivated the development of various approaches to individual prediction reliability. Based on these motivations, the authors describe six approaches to reliability estimation: inverse transduction, local sensitivity analysis, bagging variance, local cross-validation, local error modelling, and density-based estimation. Empirical evaluation of the benchmark datasets provides promising results, especially for use with decision and regression trees. The testing results also reveal that the reliability estimators exhibit different performance levels when used with different models and in different domains. The authors show the usefulness of individual prediction reliability estimates in attempts to predict breast cancer recurrence. In this context, estimating prediction reliability for individual predictions is of crucial importance for physicians seeking to validate predictions derived using classification and regression models. © 2012, IGI Global.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84898217878" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84898217878&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84898217878&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="37102202800"><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/37102202800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="23566763400"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/23566763400</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="57188535146"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57188535146</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords/><idxterms/><subject-areas><subject-area code="1700" abbrev="COMP">Computer Science (all)</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2019" month="08" day="15" timestamp="2019-08-15T04:06:10.000010-04:00"/><ait:date-sort year="2012" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2014 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.4018/978-1-4666-1806-0.ch003</ce:doi><itemid idtype="PUI">255350036</itemid><itemid idtype="SNBOOK">2014119264</itemid><itemid idtype="SCP">84898217878</itemid><itemid idtype="SGR">84898217878</itemid></itemidlist><history><date-created year="2014" month="04" day="17"/></history><dbcollection>SNBOOK</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ch"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Individual prediction reliability estimates in classification and regression</titletext></citation-title><author-group><author auid="37102202800" seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Pevec D.</ce:indexed-name><ce:surname>Pevec</ce:surname><ce:given-name>Darko</ce:given-name></preferred-name></author><author auid="23566763400" seq="2"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnić Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname><ce:given-name>Zoran</ce:given-name></preferred-name></author><author auid="57188535146" seq="3"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name><preferred-name><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname><ce:given-name>Igor</ce:given-name></preferred-name></author><affiliation afid="60031106" dptid="105764893" country="svn"><organization>Laboratory for Cognitive Modeling</organization><organization>University of Ljubljana</organization><affiliation-id afid="60031106" dptid="105764893"/><country>Slovenia</country></affiliation></author-group><correspondence><affiliation country="svn"><organization>Laboratory for Cognitive Modeling</organization><organization>University of Ljubljana</organization><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><ce:para>Current machine learning algorithms perform well in many problem domains, but in risk-sensitive decision making - for example, in medicine and finance - experts do not rely on common evaluation methods that provide overall assessments of models because such techniques do not provide any information about single predictions. This chapter summarizes the research areas that have motivated the development of various approaches to individual prediction reliability. Based on these motivations, the authors describe six approaches to reliability estimation: inverse transduction, local sensitivity analysis, bagging variance, local cross-validation, local error modelling, and density-based estimation. Empirical evaluation of the benchmark datasets provides promising results, especially for use with decision and regression trees. The testing results also reveal that the reliability estimators exhibit different performance levels when used with different models and in different domains. The authors show the usefulness of individual prediction reliability estimates in attempts to predict breast cancer recurrence. In this context, estimating prediction reliability for individual predictions is of crucial importance for physicians seeking to validate predictions derived using classification and regression models. © 2012, IGI Global.</ce:para></abstract></abstracts><source srcid="21100302268" type="b"><sourcetitle>Intelligent Data Analysis for Real-Life Applications: Theory and Practice</sourcetitle><sourcetitle-abbrev>Intelligent Data Anal. for Real-Life Applic.: Theor. and Pract.</sourcetitle-abbrev><isbn length="13">9781466618060</isbn><volisspag><pagerange first="35" last="56"/></volisspag><publicationyear first="2012"/><publicationdate><year>2012</year><date-text xfab-added="true">2012</date-text></publicationdate><website><ce:e-address type="url">http://www.igi-global.com/book/intelligent-data-analysis-real-life/62622</ce:e-address></website><publisher><publishername>IGI Global</publishername></publisher></source><enhancement><classificationgroup><classifications type="ASJC"><classification>1700</classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="47"><reference id="1"><ref-info><refd-itemidlist><itemid idtype="SGR">36948999941</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Asunction A.</ce:indexed-name><ce:surname>Asunction</ce:surname></author><author seq="2"><ce:initials>D.J.</ce:initials><ce:indexed-name>Newman D.J.</ce:indexed-name><ce:surname>Newman</ce:surname></author></ref-authors><ref-sourcetitle>UCI Machine Learning Repository</ref-sourcetitle><ref-publicationyear first="2007"/></ref-info><ref-fulltext>Asunction, A., &amp; Newman, D. J. (2007). UCI Machine Learning repository.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Probabilistic modeling for face orientation discrimination: Learning from labelled and unlabelled data</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898963451</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Baluja S.</ce:indexed-name><ce:surname>Baluja</ce:surname></author></ref-authors><ref-sourcetitle>In Neural Information Procesing Systems (NIPS '98)</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><pagerange first="854" last="860"/></ref-volisspag></ref-info><ref-fulltext>Baluja, S. (1998). Probabilistic modeling for face orientation discrimination: Learning from labelled and unlabelled data. In Neural Information Procesing systems (NIPS '98), (pp. 854-860).</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Combining labeled and unlabelled data with co-training</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031620208</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Blum A.</ce:indexed-name><ce:surname>Blum</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Mitchell T.</ce:indexed-name><ce:surname>Mitchell</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><pagerange first="92" last="100"/></ref-volisspag></ref-info><ref-fulltext>Blum, A., &amp; Mitchell, T. (1998). Combining labeled and unlabelled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, (pp. 92-100).</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Estimation of individual prediction reliability using the local sensitivity analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">54249164497</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Applied Intelligence</ref-sourcetitle><ref-publicationyear first="2007"/><ref-volisspag><voliss volume="29" issue="3"/><pagerange first="187" last="203"/></ref-volisspag><ref-text>doi:10.1007/s10489-007-0084-9</ref-text></ref-info><ref-fulltext>Bosnić, Z., &amp; Kononenko, I. (2007). Estimation of individual prediction reliability using the local sensitivity analysis. Applied Intelligence, 29(3), 187-203. doi:10.1007/s10489-007-0084-9</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Comparison of approaches for estimating reliability of individual regression predictions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">54349094489</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Data &amp; Knowledge Engineering</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="67" issue="3"/><pagerange first="504" last="516"/></ref-volisspag><ref-text>doi:10.1016/j.datak.2008.08.001</ref-text></ref-info><ref-fulltext>Bosnić, Z., &amp; Kononenko, I. (2008). Comparison of approaches for estimating reliability of individual regression predictions. Data &amp; Knowledge Engineering, 67(3), 504-516. doi:10.1016/j.datak.2008.08.001</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Evaluation of prediction reliability in regression using the transduction principle</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">62249148128</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Bosnic Z.</ce:indexed-name><ce:surname>Bosnić</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Robnik-Sikonja M.</ce:indexed-name><ce:surname>Robnik-Šikonja</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Kukar M.</ce:indexed-name><ce:surname>Kukar</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of Eurocon 2003</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><pagerange first="99" last="103"/></ref-volisspag><ref-text>Zajc, B., &amp; Tkalčič, M. (Eds.), doi:10.1109/EURCON.2003.1248158</ref-text></ref-info><ref-fulltext>Bosnić, Z., Kononenko, I., Robnik-Šikonja, M., &amp; Kukar, M. (2003). Evaluation of prediction reliability in regression using the transduction principle. In Zajc, B., &amp; Tkalčič, M. (Eds.), Proceedings of Eurocon 2003 (pp. 99-103). doi:10.1109/EURCON.2003.1248158</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Algorithmic stability and generalization performance</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0012296113</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Elisseeff A.</ce:indexed-name><ce:surname>Elisseeff</ce:surname></author></ref-authors><ref-sourcetitle>In Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><pagerange first="196" last="202"/></ref-volisspag></ref-info><ref-fulltext>Bousquet, O., &amp; Elisseeff, A. (2000). Algorithmic stability and generalization performance. In Neural Information Processing Systems, (pp. 196-202).</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Stability and generalization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038368335</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Elisseeff A.</ce:indexed-name><ce:surname>Elisseeff</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Machine Learning Research</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="2"/><pagerange first="499" last="526"/></ref-volisspag></ref-info><ref-fulltext>Bousquet, O., &amp; Elisseeff, A. (2002). Stability and generalization. Journal of Machine Learning Research, 2, 499-526.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Leave-oneout error and stability of learning algorithms with applications</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">54349123122</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Bousquet O.</ce:indexed-name><ce:surname>Bousquet</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Pontil M.</ce:indexed-name><ce:surname>Pontil</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Learning Theory: Methods, Models and Applications</ref-sourcetitle><ref-publicationyear first="2003"/><ref-text>Suykens, J. (Eds.), IOS Press</ref-text></ref-info><ref-fulltext>Bousquet, O., &amp; Pontil, M. (2003). Leave-oneout error and stability of learning algorithms with applications. In Suykens, J. (Eds.), Advances in learning theory: Methods, models and applications. IOS Press.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>An introduction to sensitivity analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84859775486</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breierova L.</ce:indexed-name><ce:surname>Breierova</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Choudhari M.</ce:indexed-name><ce:surname>Choudhari</ce:surname></author></ref-authors><ref-sourcetitle>MIT System Dynamics in Education Project</ref-sourcetitle><ref-publicationyear first="1996"/></ref-info><ref-fulltext>Breierova, L., &amp; Choudhari, M. (1996). An introduction to sensitivity analysis. MIT System Dynamics in Education Project.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Bagging predictors</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0030211964</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="1996"/><ref-volisspag><voliss volume="24" issue="2"/><pagerange first="123" last="140"/></ref-volisspag><ref-text>doi:10.1007/BF00058655</ref-text></ref-info><ref-fulltext>Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123-140. doi:10.1007/BF00058655</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Randomforests</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0035478854</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author></ref-authors><ref-sourcetitle>Machine Learning</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><voliss volume="45" issue="1"/><pagerange first="5" last="32"/></ref-volisspag><ref-text>doi:10.1023/A:1010933404324</ref-text></ref-info><ref-fulltext>Breiman, L. (2001). Randomforests. Machine Learning, 45(1), 5-32. doi:10.1023/A:1010933404324</ref-fulltext></reference><reference id="13"><ref-info><refd-itemidlist><itemid idtype="SGR">0003802343</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Breiman L.</ce:indexed-name><ce:surname>Breiman</ce:surname></author><author seq="2"><ce:initials>J.H.</ce:initials><ce:indexed-name>Friedman J.H.</ce:indexed-name><ce:surname>Friedman</ce:surname></author><author seq="3"><ce:initials>R.A.</ce:initials><ce:indexed-name>Olshen R.A.</ce:indexed-name><ce:surname>Olshen</ce:surname></author><author seq="4"><ce:initials>C.J.</ce:initials><ce:indexed-name>Stone C.J.</ce:indexed-name><ce:surname>Stone</ce:surname></author></ref-authors><ref-sourcetitle>Classification and Regression Trees</ref-sourcetitle><ref-publicationyear first="1984"/><ref-text>Belmont, CA, Wadsworth International Group</ref-text></ref-info><ref-fulltext>Breiman, L., Friedman, J. H., Olshen, R. A., &amp; Stone, C. J. (1984). Classification and regression trees. Belmont, CA: Wadsworth International Group.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Confidence and prediction intervals for neural network ensembles</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0033351401</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Carney J.</ce:indexed-name><ce:surname>Carney</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Cunningham P.</ce:indexed-name><ce:surname>Cunningham</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of IJCNN'99, the International Joint Conference on Neural Networks, Washington, USA</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="1215" last="1218"/></ref-volisspag></ref-info><ref-fulltext>Carney, J., &amp; Cunningham, P. (1999). Confidence and prediction intervals for neural network ensembles. In Proceedings of IJCNN'99, The International Joint Conference on Neural Networks, Washington, USA, (pp. 1215-1218).</ref-fulltext></reference><reference id="15"><ref-info><refd-itemidlist><itemid idtype="SGR">0003710380</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Chang C.</ce:indexed-name><ce:surname>Chang</ce:surname></author><author seq="2"><ce:initials>C.</ce:initials><ce:indexed-name>Lin C.</ce:indexed-name><ce:surname>Lin</ce:surname></author></ref-authors><ref-sourcetitle>LIBSVM: A Library for Support Vector Machines</ref-sourcetitle><ref-publicationyear first="2001"/><ref-website><ce:e-address type="url">http://www.csie.ntu.edu.tw/~cjlin/libsvm/</ce:e-address></ref-website><ref-text>Retrieved from</ref-text></ref-info><ref-fulltext>Chang, C., &amp; Lin, C. (2001). LIBSVM: a library for support vector machines. Retrieved from http://www.csie.ntu.edu.tw/~cjlin/libsvm/.</ref-fulltext></reference><reference id="16"><ref-info><refd-itemidlist><itemid idtype="SGR">0003798635</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.</ce:initials><ce:indexed-name>Christiannini N.</ce:indexed-name><ce:surname>Christiannini</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Shawe-Taylor J.</ce:indexed-name><ce:surname>Shawe-Taylor</ce:surname></author></ref-authors><ref-sourcetitle>Support Vector Machines and Other Kernel-based Learning Methods</ref-sourcetitle><ref-publicationyear first="2000"/><ref-text>Cambridge University Press</ref-text></ref-info><ref-fulltext>Christiannini, N., &amp; Shawe-Taylor, J. (2000). Support vector machines and other kernel-based learning methods. Cambridge University Press.</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>Learning classification with unlabelled data</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0005986550</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>de Sa V.</ce:indexed-name><ce:surname>de Sa</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of NIPS'93, Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1993"/><ref-volisspag><pagerange first="112" last="119"/></ref-volisspag><ref-text>J. D. Cowan, G. Tesauro &amp; J. Alspector, (Eds.), San Francisco, CA: Morgan Kaufmann Publishers</ref-text></ref-info><ref-fulltext>de Sa, V. (1993). Learning classification with unlabelled data. In J. D. Cowan, G. Tesauro &amp; J. Alspector, (Eds.), Proceedings of NIPS'93, Neural Information Processing Systems, (pp. 112-119). San Francisco, CA: Morgan Kaufmann Publishers.</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Maximum likelihood from incomplete data via the EM algorithm</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002629270</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.P.</ce:initials><ce:indexed-name>Dempster A.P.</ce:indexed-name><ce:surname>Dempster</ce:surname></author><author seq="2"><ce:initials>N.M.</ce:initials><ce:indexed-name>Laird N.M.</ce:indexed-name><ce:surname>Laird</ce:surname></author><author seq="3"><ce:initials>D.B.</ce:initials><ce:indexed-name>Rubin D.B.</ce:indexed-name><ce:surname>Rubin</ce:surname></author></ref-authors><ref-sourcetitle>Journal of the Royal Statistical Society. Series B. Methodological</ref-sourcetitle><ref-publicationyear first="1977"/><ref-volisspag><voliss volume="39" issue="1"/><pagerange first="1" last="38"/></ref-volisspag></ref-info><ref-fulltext>Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B. Methodological, 39(1), 1-38.</ref-fulltext></reference><reference id="19"><ref-info><refd-itemidlist><itemid idtype="SGR">39349111351</itemid></refd-itemidlist><ref-sourcetitle>Statlib - Data, Software and News from the Statistics Community</ref-sourcetitle><ref-publicationyear first="2005"/><ref-website><ce:e-address type="url">http://lib.stat.cmu.edu/</ce:e-address></ref-website><ref-text>Department of Statistics at Carnegie Mellon University, Retrieved from</ref-text></ref-info><ref-fulltext>Department of Statistics at Carnegie Mellon University. (2005). Statlib - Data, software and news from the statistics community. Retrieved from http://lib.stat.cmu.edu/</ref-fulltext></reference><reference id="20"><ref-info><ref-title><ref-titletext>Improving regressors using boosting techniques</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000201141</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Drucker H.</ce:indexed-name><ce:surname>Drucker</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of 14th International Conference on Machine Learning</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><pagerange first="107" last="115"/></ref-volisspag><ref-text>Morgan Kaufmann</ref-text></ref-info><ref-fulltext>Drucker, H. (1997). Improving regressors using boosting techniques. In Proceedings of 14th International Conference on Machine Learning, (pp. 107-115). Morgan Kaufmann.</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>A decisiontheoretic generalization of on-line learning and an application to boosting</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031211090</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Freund Y.</ce:indexed-name><ce:surname>Freund</ce:surname></author><author seq="2"><ce:initials>R.</ce:initials><ce:indexed-name>Schapire R.</ce:indexed-name><ce:surname>Schapire</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Computer and System Sciences</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="55" issue="1"/><pagerange first="119" last="139"/></ref-volisspag><ref-text>doi:10.1006/jcss.1997.1504</ref-text></ref-info><ref-fulltext>Freund, Y., &amp; Schapire, R. (1997). A decisiontheoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1), 119-139. doi:10.1006/jcss.1997.1504</ref-fulltext></reference><reference id="22"><ref-info><ref-title><ref-titletext>Learning by transduction</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002947383</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Gammerman A.</ce:indexed-name><ce:surname>Gammerman</ce:surname></author><author seq="2"><ce:initials>V.</ce:initials><ce:indexed-name>Vovk V.</ce:indexed-name><ce:surname>Vovk</ce:surname></author><author seq="3"><ce:initials>V.</ce:initials><ce:indexed-name>Vapnik V.</ce:indexed-name><ce:surname>Vapnik</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="1998"/><ref-volisspag><pagerange first="148" last="155"/></ref-volisspag><ref-text>Madison, Wisconsin</ref-text></ref-info><ref-fulltext>Gammerman, A., Vovk, V., &amp; Vapnik, V. (1998). Learning by transduction. In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence, (pp. 148-155). Madison, Wisconsin.</ref-fulltext></reference><reference id="23"><ref-info><ref-title><ref-titletext>Dual perturb and combine algorithm</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0006769278</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Geurts P.</ce:indexed-name><ce:surname>Geurts</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><pagerange first="196" last="201"/></ref-volisspag></ref-info><ref-fulltext>Geurts, P. (2001). Dual perturb and combine algorithm. In Proceedings of the Eighth International Workshop on Artificial Intelligence and Statistics, (pp. 196-201).</ref-fulltext></reference><reference id="24"><ref-info><ref-title><ref-titletext>Supervised learning from incomplete data via an EM approach</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001551844</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Ghahramani Z.</ce:indexed-name><ce:surname>Ghahramani</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Jordan M.</ce:indexed-name><ce:surname>Jordan</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="6"/><pagerange first="120" last="127"/></ref-volisspag></ref-info><ref-fulltext>Ghahramani, Z., &amp; Jordan, M. (1994). Supervised learning from incomplete data via an EM approach. Advances in Neural Information Processing Systems, 6, 120-127.</ref-fulltext></reference><reference id="25"><ref-info><ref-title><ref-titletext>Sensitivity analysis for feedforward artificial neural networks with differentiable activation functions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000847245</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Hashem H.</ce:indexed-name><ce:surname>Hashem</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of 1992 International Joint Conference on Neural Networks IJCNN92</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="1"/><pagerange first="419" last="424"/></ref-volisspag><ref-text>Vol</ref-text></ref-info><ref-fulltext>Hashem, H. (1992). Sensitivity analysis for feedforward artificial neural networks with differentiable activation functions. In Proceedings of 1992 International Joint Conference on Neural Networks IJCNN92, Vol. 1, (pp. 419-424).</ref-fulltext></reference><reference id="26"><ref-info><refd-itemidlist><itemid idtype="SGR">0003598526</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Hastie T.</ce:indexed-name><ce:surname>Hastie</ce:surname></author><author seq="2"><ce:initials>R.</ce:initials><ce:indexed-name>Tibshirani R.</ce:indexed-name><ce:surname>Tibshirani</ce:surname></author></ref-authors><ref-sourcetitle>Generalized Additive Models</ref-sourcetitle><ref-publicationyear first="1990"/><ref-text>London, UK, Chapman and Hall</ref-text></ref-info><ref-fulltext>Hastie, T., &amp; Tibshirani, R. (1990). Generalized additive models. London, UK: Chapman and Hall.</ref-fulltext></reference><reference id="27"><ref-info><ref-title><ref-titletext>Practical confidence and prediction intervals</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898947879</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Heskes T.</ce:indexed-name><ce:surname>Heskes</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="9"/><pagerange first="176" last="182"/></ref-volisspag><ref-text>M. C. Mozer, M. I. Jordan, &amp; T. Petsche (Eds.), The MIT Press</ref-text></ref-info><ref-fulltext>Heskes, T. (1997). Practical confidence and prediction intervals. In M. C. Mozer, M. I. Jordan, &amp; T. Petsche (Eds.), Advances in Neural Information Processing Systems, 9, 176-182. The MIT Press.</ref-fulltext></reference><reference id="28"><ref-info><ref-title><ref-titletext>Algorithmic stability and sanity-check bounds for leave- oneout cross-validation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0030654389</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.J.</ce:initials><ce:indexed-name>Kearns M.J.</ce:indexed-name><ce:surname>Kearns</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Ron D.</ce:indexed-name><ce:surname>Ron</ce:surname></author></ref-authors><ref-sourcetitle>In Computational Learning Theory</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><pagerange first="152" last="162"/></ref-volisspag></ref-info><ref-fulltext>Kearns, M. J., &amp; Ron, D. (1997). Algorithmic stability and sanity-check bounds for leave- oneout cross-validation. In Computational Learning Theory, (pp. 152-162).</ref-fulltext></reference><reference id="29"><ref-info><ref-title><ref-titletext>Bootstrapping cluster analysis: Assessing the reliability of conclusions from microarray experiments</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0035979259</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Kerr M.</ce:indexed-name><ce:surname>Kerr</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Churchill G.</ce:indexed-name><ce:surname>Churchill</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the National Academy of Sciences of the United States of America</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="96"/><pagerange first="8961" last="8965"/></ref-volisspag></ref-info><ref-fulltext>Kerr, M., &amp; Churchill, G. (2000). Bootstrapping cluster analysis: assessing the reliability of conclusions from microarray experiments. Proceedings of the National Academy of Sciences of the United States of America, 96, 8961-8965.</ref-fulltext></reference><reference id="30"><ref-info><ref-title><ref-titletext>Making sensitivity analysis computationally efficient</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0003272309</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>U.</ce:initials><ce:indexed-name>Kjaerulff U.</ce:indexed-name><ce:surname>Kjaerulff</ce:surname></author><author seq="2"><ce:initials>L.C.</ce:initials><ce:indexed-name>van der Gaag L.C.</ce:indexed-name><ce:surname>van der Gaag</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><pagerange first="317" last="325"/></ref-volisspag><ref-text>San Francisco, CA: Morgan Kaufmann</ref-text></ref-info><ref-fulltext>Kjaerulff, U., &amp; van der Gaag, L. C. (2000). Making sensitivity analysis computationally efficient. In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence, (pp. 317-325). San Francisco, CA: Morgan Kaufmann.</ref-fulltext></reference><reference id="31"><ref-info><ref-title><ref-titletext>Experimental designs for sensitivity analysis of simulation models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84901462987</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Kleijnen J.</ce:indexed-name><ce:surname>Kleijnen</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of EUROSIM 2001</ref-sourcetitle><ref-publicationyear first="2001"/></ref-info><ref-fulltext>Kleijnen, J. (2001). Experimental designs for sensitivity analysis of simulation models. In Proceedings of EUROSIM 2001.</ref-fulltext></reference><reference id="32"><ref-info><refd-itemidlist><itemid idtype="SGR">84898367700</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Kukar M.</ce:indexed-name><ce:surname>Kukar</ce:surname></author></ref-authors><ref-sourcetitle>Estimating classifications' reliability and cost-sensitive combination of machine learning methods</ref-sourcetitle><ref-publicationyear first="2001"/><ref-text>Unpublished Doctoral Dissertation, University of Ljubljana, 2001</ref-text></ref-info><ref-fulltext>Kukar, M. (2001). Estimating classifications' reliability and cost-sensitive combination of machine learning methods. Unpublished doctoral dissertation, University of Ljubljana, 2001.</ref-fulltext></reference><reference id="33"><ref-info><ref-title><ref-titletext>Reliable classifications with machine learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84945287811</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Kukar M.</ce:indexed-name><ce:surname>Kukar</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Kononenko I.</ce:indexed-name><ce:surname>Kononenko</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of Machine Learning: ECML-2002</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><pagerange first="219" last="231"/></ref-volisspag><ref-text>Elomaa, T., Manilla, H., &amp; Toivonen, H. (Eds.), Helsinki, Finland: Springer Verlag. doi:10.1007/3-540-36755-1_19</ref-text></ref-info><ref-fulltext>Kukar, M., &amp; Kononenko, I. (2002). Reliable classifications with machine learning. In Elomaa, T., Manilla, H., &amp; Toivonen, H. (Eds.), Proceedings of Machine Learning: ECML-2002 (pp. 219-231). Helsinki, Finland: Springer Verlag. doi:10.1007/3-540-36755-1_19</ref-fulltext></reference><reference id="34"><ref-info><ref-title><ref-titletext>Open set face recognition using transduction</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">28044457202</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Li F.</ce:indexed-name><ce:surname>Li</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Wechsler H.</ce:indexed-name><ce:surname>Wechsler</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="27" issue="11"/><pagerange first="1686" last="1697"/></ref-volisspag><ref-text>doi:10.1109/TPAMI.2005.224</ref-text></ref-info><ref-fulltext>Li, F., &amp; Wechsler, H. (2005). Open set face recognition using transduction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(11), 1686-1697. doi:10.1109/TPAMI.2005.224</ref-fulltext></reference><reference id="35"><ref-info><ref-title><ref-titletext>The role of unlabelled data in supervised learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0005320050</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Mitchell T.</ce:indexed-name><ce:surname>Mitchell</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of the 6th International Colloquium of Cognitive Science</ref-sourcetitle><ref-publicationyear first="1999"/><ref-text>San Sebastian, Spain</ref-text></ref-info><ref-fulltext>Mitchell, T. (1999). The role of unlabelled data in supervised learning. In Proceedings of the 6th International Colloquium of Cognitive Science, San Sebastian, Spain.</ref-fulltext></reference><reference id="36"><ref-info><ref-title><ref-titletext>Boosting methodology for regression problems</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002311782</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Ridgeway G.</ce:indexed-name><ce:surname>Ridgeway</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Madigan D.</ce:indexed-name><ce:surname>Madigan</ce:surname></author><author seq="3"><ce:initials>T.</ce:initials><ce:indexed-name>Richardson T.</ce:indexed-name><ce:surname>Richardson</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of Artificial Intelligence and Statistics</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="152" last="161"/></ref-volisspag><ref-text>Heckerman, D., &amp; Whittaker, J. (Eds.)</ref-text></ref-info><ref-fulltext>Ridgeway, G., Madigan, D., &amp; Richardson, T. (1999). Boosting methodology for regression problems. In Heckerman, D., &amp; Whittaker, J. (Eds.), Proceedings of Artificial Intelligence and Statistics (pp. 152-161).</ref-fulltext></reference><reference id="37"><ref-info><refd-itemidlist><itemid idtype="SGR">0003444648</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Rumelhart D.</ce:indexed-name><ce:surname>Rumelhart</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Hinton G.</ce:indexed-name><ce:surname>Hinton</ce:surname></author><author seq="3"><ce:initials>R.</ce:initials><ce:indexed-name>Williams R.</ce:indexed-name><ce:surname>Williams</ce:surname></author></ref-authors><ref-sourcetitle>Learning Internal Representations by Error Propagation</ref-sourcetitle><ref-publicationyear first="1986"/><ref-volisspag><pagerange first="318" last="362"/></ref-volisspag><ref-text>Cambridge, MA, MIT Press</ref-text></ref-info><ref-fulltext>Rumelhart, D., Hinton, G., &amp; Williams, R. (1986). Learning internal representations by error propagation (pp. 318-362). Cambridge, MA: MIT Press.</ref-fulltext></reference><reference id="38"><ref-info><ref-title><ref-titletext>Sensitivity analysis for chemical models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">22944490817</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Saltelli A.</ce:indexed-name><ce:surname>Saltelli</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Ratto M.</ce:indexed-name><ce:surname>Ratto</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Tarantola S.</ce:indexed-name><ce:surname>Tarantola</ce:surname></author><author seq="4"><ce:initials>F.</ce:initials><ce:indexed-name>Campolongo F.</ce:indexed-name><ce:surname>Campolongo</ce:surname></author></ref-authors><ref-sourcetitle>Chemical Reviews</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="105" issue="7"/><pagerange first="2811" last="2828"/></ref-volisspag><ref-text>doi:10.1021/cr040659d</ref-text></ref-info><ref-fulltext>Saltelli, A., Ratto, M., Tarantola, S., &amp; Campolongo, F. (2005). Sensitivity analysis for chemical models. Chemical Reviews, 105(7), 2811-2828. doi:10.1021/cr040659d</ref-fulltext></reference><reference id="39"><ref-info><refd-itemidlist><itemid idtype="SGR">5444223864</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Saltelli A.</ce:indexed-name><ce:surname>Saltelli</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Tarantola S.</ce:indexed-name><ce:surname>Tarantola</ce:surname></author><author seq="3"><ce:initials>F.</ce:initials><ce:indexed-name>Campolongo F.</ce:indexed-name><ce:surname>Campolongo</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Ratto M.</ce:indexed-name><ce:surname>Ratto</ce:surname></author></ref-authors><ref-sourcetitle>Sensitivity Analysis in Practice: A Guide to Assessing Scientific Models</ref-sourcetitle><ref-publicationyear first="2003"/><ref-text>London, UK, John Wiley &amp; Sons Ltd</ref-text></ref-info><ref-fulltext>Saltelli, A., Tarantola, S., Campolongo, F., &amp; Ratto, M. (2003). Sensitivity analysis in practice: A guide to assessing scientific models. London, UK: John Wiley &amp; Sons Ltd.</ref-fulltext></reference><reference id="40"><ref-info><ref-title><ref-titletext>Transduction with confidence and credibility</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84880657197</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Saunders C.</ce:indexed-name><ce:surname>Saunders</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Gammerman A.</ce:indexed-name><ce:surname>Gammerman</ce:surname></author><author seq="3"><ce:initials>V.</ce:initials><ce:indexed-name>Vovk V.</ce:indexed-name><ce:surname>Vovk</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of IJCAI'99</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="2"/><pagerange first="722" last="726"/></ref-volisspag><ref-text>Vol</ref-text></ref-info><ref-fulltext>Saunders, C., Gammerman, A., &amp; Vovk, V. (1999). Transduction with confidence and credibility. In Proceedings of IJCAI'99, Vol. 2, (pp. 722-726).</ref-fulltext></reference><reference id="41"><ref-info><ref-title><ref-titletext>A brief introduction to boosting</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84880692052</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.E.</ce:initials><ce:indexed-name>Schapire R.E.</ce:indexed-name><ce:surname>Schapire</ce:surname></author></ref-authors><ref-sourcetitle>In International Joint Conferences on Aritifical Intelligence</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="1401" last="1406"/></ref-volisspag></ref-info><ref-fulltext>Schapire, R. E. (1999). A brief introduction to boosting. In International Joint Conferences on Aritifical Intelligence, (pp. 1401-1406).</ref-fulltext></reference><reference id="42"><ref-info><refd-itemidlist><itemid idtype="SGR">0005977840</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Seeger M.</ce:indexed-name><ce:surname>Seeger</ce:surname></author></ref-authors><ref-sourcetitle>Learning with Labeled and Unlabelled Data</ref-sourcetitle><ref-publicationyear first="2000"/><ref-website><ce:e-address type="url">http://www.dai.ed.ac.uk/~seeger/papers.html</ce:e-address></ref-website><ref-text>Technical report. Retrieved from</ref-text></ref-info><ref-fulltext>Seeger, M. (2000). Learning with labeled and unlabelled data. Technical report. Retrieved from http://www.dai.ed.ac.uk/~seeger/papers.html</ref-fulltext></reference><reference id="43"><ref-info><refd-itemidlist><itemid idtype="SGR">0003401675</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.J.</ce:initials><ce:indexed-name>Smola A.J.</ce:indexed-name><ce:surname>Smola</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Scholkopf B.</ce:indexed-name><ce:surname>Schölkopf</ce:surname></author></ref-authors><ref-sourcetitle>A Tutorial on Support Vector Regression</ref-sourcetitle><ref-publicationyear first="1998"/><ref-text>NeuroCOLT2 Technical Report NC2-TR-1998-030</ref-text></ref-info><ref-fulltext>Smola, A. J., &amp; Schölkopf, B. (1998). A tutorial on support vector regression. NeuroCOLT2 Technical Report NC2-TR-1998-030.</ref-fulltext></reference><reference id="44"><ref-info><ref-title><ref-titletext>Model search and inference by bootstrap bumping</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0033266602</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Tibshirani R.</ce:indexed-name><ce:surname>Tibshirani</ce:surname></author><author seq="2"><ce:initials>K.</ce:initials><ce:indexed-name>Knight K.</ce:indexed-name><ce:surname>Knight</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Computational and Graphical Statistics</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="8"/><pagerange first="671" last="686"/></ref-volisspag></ref-info><ref-fulltext>Tibshirani, R., &amp; Knight, K. (1999). Model search and inference by bootstrap bumping. Journal of Computational and Graphical Statistics, 8, 671-686.</ref-fulltext></reference><reference id="45"><ref-info><refd-itemidlist><itemid idtype="SGR">0003450542</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>Vapnik V.</ce:indexed-name><ce:surname>Vapnik</ce:surname></author></ref-authors><ref-sourcetitle>The Nature of Statistical Learning Theory</ref-sourcetitle><ref-publicationyear first="1995"/><ref-text>Springer</ref-text></ref-info><ref-fulltext>Vapnik, V. (1995). The nature of statistical learning theory. Springer.</ref-fulltext></reference><reference id="46"><ref-info><ref-title><ref-titletext>Stacked generalization</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0026692226</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.H.</ce:initials><ce:indexed-name>Wolpert D.H.</ce:indexed-name><ce:surname>Wolpert</ce:surname></author></ref-authors><ref-sourcetitle>Neural Networks</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="5"/><pagerange first="241" last="259"/></ref-volisspag><ref-text>doi:10.1016/S0893-6080(05)80023-1</ref-text></ref-info><ref-fulltext>Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5, 241-259. doi:10.1016/S0893-6080(05)80023-1</ref-fulltext></reference><reference id="47"><ref-info><refd-itemidlist><itemid idtype="SGR">33745837600</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.N.</ce:initials><ce:indexed-name>Wood S.N.</ce:indexed-name><ce:surname>Wood</ce:surname></author></ref-authors><ref-sourcetitle>Generalized Additive Models: An Introduction with R. Chapman &amp; Hall/CRC</ref-sourcetitle><ref-publicationyear first="2006"/></ref-info><ref-fulltext>Wood, S. N. (2006). Generalized additive models: An introduction with R. Chapman &amp; Hall/CRC.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>