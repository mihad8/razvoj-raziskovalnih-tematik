<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84925251633</prism:url><dc:identifier>SCOPUS_ID:84925251633</dc:identifier><eid>2-s2.0-84925251633</eid><dc:title>Gathering a dataset of multi-modal mood-dependent perceptual responses to music</dc:title><prism:aggregationType>Conference Proceeding</prism:aggregationType><srctype>p</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>3</citedby-count><prism:publicationName>CEUR Workshop Proceedings</prism:publicationName><dc:publisher>CEUR-WS</dc:publisher><source-id>21100218356</source-id><prism:issn>16130073</prism:issn><prism:volume>1181</prism:volume><prism:startingPage>11</prism:startingPage><prism:endingPage>22</prism:endingPage><prism:pageRange>11-22</prism:pageRange><prism:coverDate>2014-01-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="56258907000"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56258907000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>The paper presents a new dataset that captures the effect of mood on visual and auditory perception of music. With an online survey, we have collected a dataset of over 6600 responses capturing users' mood, emotions evoked and expressed by music and the perception of color with regard to emotions and music. We describe the methodology of gathering the responses and present two new models for capturing users' emotional states: the MoodGraph and MoodStripe. Also, general research questions and goals, as well as possible future applications of the collected dataset, are being discussed.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84925251633" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84925251633&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84925251633&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><affiliation id="60030129" href="https://api.elsevier.com/content/affiliation/affiliation_id/60030129"><affilname>Znanstvenoraziskovalni center Slovenske akademije znanosti in umetnosti</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="56258907000"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56258907000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="56377091000"><ce:initials>P.</ce:initials><ce:indexed-name>Godec P.</ce:indexed-name><ce:surname>Godec</ce:surname><ce:given-name>Primož</ce:given-name><preferred-name><ce:initials>P.</ce:initials><ce:indexed-name>Godec P.</ce:indexed-name><ce:surname>Godec</ce:surname><ce:given-name>Primož</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56377091000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="56376897200"><ce:initials>M.</ce:initials><ce:indexed-name>Poredos M.</ce:indexed-name><ce:surname>Poredoš</ce:surname><ce:given-name>Mojca</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Poredoš M.</ce:indexed-name><ce:surname>Poredoš</ce:surname><ce:given-name>Mojca</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56376897200</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="4" auid="35175418100"><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name><preferred-name><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/35175418100</author-url><affiliation id="60030129" href="https://api.elsevier.com/content/affiliation/affiliation_id/60030129"/></author><author seq="5" auid="17345882300"><ce:initials>J.</ce:initials><ce:indexed-name>Guna J.</ce:indexed-name><ce:surname>Guna</ce:surname><ce:given-name>Jože</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Guna J.</ce:indexed-name><ce:surname>Guna</ce:surname><ce:given-name>Jože</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/17345882300</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="6" auid="54407530200"><ce:initials>E.</ce:initials><ce:indexed-name>Stojmenova E.</ce:indexed-name><ce:surname>Stojmenova</ce:surname><ce:given-name>Emilija</ce:given-name><preferred-name><ce:initials>E.</ce:initials><ce:indexed-name>Stojmenova E.</ce:indexed-name><ce:surname>Stojmenova</ce:surname><ce:given-name>Emilija</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/54407530200</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="7" auid="6603825626"><ce:initials>M.</ce:initials><ce:indexed-name>Pogacnik M.</ce:indexed-name><ce:surname>Pogačnik</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pogačnik M.</ce:indexed-name><ce:surname>Pogačnik</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603825626</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="8" auid="6603601816"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603601816</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Color perception</author-keyword><author-keyword>Human computer interaction</author-keyword><author-keyword>Mood estimation</author-keyword><author-keyword>Music information retrieval</author-keyword></authkeywords><idxterms><mainterm weight="b" candidate="n">Auditory perception</mainterm><mainterm weight="b" candidate="n">Color perception</mainterm><mainterm weight="b" candidate="n">Emotional state</mainterm><mainterm weight="b" candidate="n">General research questions</mainterm><mainterm weight="b" candidate="n">Multi-modal</mainterm><mainterm weight="b" candidate="n">Music information retrieval</mainterm><mainterm weight="b" candidate="n">Online surveys</mainterm><mainterm weight="b" candidate="n">Possible futures</mainterm></idxterms><subject-areas><subject-area code="1700" abbrev="COMP">Computer Science (all)</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2019" month="08" day="02" timestamp="2019-08-02T00:55:37.000037-04:00"/><ait:date-sort year="2014" month="01" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2015 Elsevier B.V., All rights reserved.</copyright><itemidlist><itemid idtype="PUI">603202803</itemid><itemid idtype="CAR-ID">634107123</itemid><itemid idtype="CPX">20151200671080</itemid><itemid idtype="SCP">84925251633</itemid><itemid idtype="SGR">84925251633</itemid></itemidlist><history><date-created year="2015" month="03" day="25"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">Color perception</author-keyword><author-keyword xml:lang="eng">Human computer interaction</author-keyword><author-keyword xml:lang="eng">Mood estimation</author-keyword><author-keyword xml:lang="eng">Music information retrieval</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Gathering a dataset of multi-modal mood-dependent perceptual responses to music</titletext></citation-title><author-group><author auid="56258907000" seq="1" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name></author><author auid="56377091000" seq="2" type="auth"><ce:initials>P.</ce:initials><ce:indexed-name>Godec P.</ce:indexed-name><ce:surname>Godec</ce:surname><ce:given-name>Primož</ce:given-name><preferred-name><ce:initials>P.</ce:initials><ce:indexed-name>Godec P.</ce:indexed-name><ce:surname>Godec</ce:surname><ce:given-name>Primož</ce:given-name></preferred-name></author><author auid="56376897200" seq="3" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Poredos M.</ce:indexed-name><ce:surname>Poredoš</ce:surname><ce:given-name>Mojca</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Poredoš M.</ce:indexed-name><ce:surname>Poredoš</ce:surname><ce:given-name>Mojca</ce:given-name></preferred-name></author><author auid="6603601816" seq="8" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>University of Ljubljana, Faculty of Computer and Information Science</organization><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="35175418100" seq="4" type="auth"><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name><preferred-name><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name></preferred-name></author><affiliation afid="60030129" dptid="116209068" country="svn"><organization>Scientfic Research Centre of the Slovenian Academy of Sciences and Arts, Institute of Ethnomusicology</organization><affiliation-id afid="60030129" dptid="116209068"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="17345882300" seq="5" type="auth"><ce:initials>J.</ce:initials><ce:indexed-name>Guna J.</ce:indexed-name><ce:surname>Guna</ce:surname><ce:given-name>Jože</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Guna J.</ce:indexed-name><ce:surname>Guna</ce:surname><ce:given-name>Jože</ce:given-name></preferred-name></author><author auid="54407530200" seq="6" type="auth"><ce:initials>E.</ce:initials><ce:indexed-name>Stojmenova E.</ce:indexed-name><ce:surname>Stojmenova</ce:surname><ce:given-name>Emilija</ce:given-name><preferred-name><ce:initials>E.</ce:initials><ce:indexed-name>Stojmenova E.</ce:indexed-name><ce:surname>Stojmenova</ce:surname><ce:given-name>Emilija</ce:given-name></preferred-name></author><author auid="6603825626" seq="7" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Pogacnik M.</ce:indexed-name><ce:surname>Pogačnik</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pogačnik M.</ce:indexed-name><ce:surname>Pogačnik</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name></author><affiliation afid="60031106" dptid="104580922" country="svn"><organization>University of Ljubljana, Faculty of Electrotechnics</organization><affiliation-id afid="60031106" dptid="104580922"/><country>Slovenia</country></affiliation></author-group><abstracts><abstract original="y" xml:lang="eng"><ce:para>The paper presents a new dataset that captures the effect of mood on visual and auditory perception of music. With an online survey, we have collected a dataset of over 6600 responses capturing users' mood, emotions evoked and expressed by music and the perception of color with regard to emotions and music. We describe the methodology of gathering the responses and present two new models for capturing users' emotional states: the MoodGraph and MoodStripe. Also, general research questions and goals, as well as possible future applications of the collected dataset, are being discussed.</ce:para></abstract></abstracts><source srcid="21100218356" type="p" country="usa"><sourcetitle>CEUR Workshop Proceedings</sourcetitle><sourcetitle-abbrev>CEUR Workshop Proc.</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">CEUR Workshop Proceedings</translated-sourcetitle><volumetitle>UMAP 2014 Extended Proceedings - Posters, Demos, Late-Breaking Results and Workshop Proceedings of the 22nd Conference on User Modeling, Adaptation, and Personalization, Co-located with the 22nd Conference on User Modeling, Adaptation, and Personalization, UMAP 2014</volumetitle><issn type="print">16130073</issn><volisspag><voliss volume="1181"/><pagerange first="11" last="22"/></volisspag><publicationyear first="2014"/><publicationdate><year>2014</year><date-text xfab-added="true">2014</date-text></publicationdate><website><ce:e-address type="email">http://ceur-ws.org/</ce:e-address></website><contributor-group><contributor role="edit" seq="1"><ce:initials>I.</ce:initials><ce:indexed-name>Cantador I.</ce:indexed-name><ce:surname>Cantador</ce:surname><ce:given-name>Ivan</ce:given-name></contributor></contributor-group><contributor-group><contributor role="edit" seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Farzan R.</ce:indexed-name><ce:surname>Farzan</ce:surname><ce:given-name>Rosta</ce:given-name></contributor></contributor-group><contributor-group><contributor role="edit" seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Jaschke R.</ce:indexed-name><ce:surname>Jaschke</ce:surname><ce:given-name>Robert</ce:given-name></contributor></contributor-group><contributor-group><contributor role="edit" seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Chi M.</ce:indexed-name><ce:surname>Chi</ce:surname><ce:given-name>Min</ce:given-name></contributor></contributor-group><publisher><publishername>CEUR-WS</publishername></publisher><additional-srcinfo><conferenceinfo><confevent><confname>Workshop of the 22nd Conference on User Modeling, Adaptation, and Personalization, UMAP 2014 - Co-located with the 22nd Conference on User Modeling, Adaptation, and Personalization, UMAP 2014</confname><confnumber>22</confnumber><confseriestitle>Conference on User Modeling, Adaptation, and Personalization</confseriestitle><conflocation country="dnk"><city>Aalborg</city></conflocation><confdate><startdate year="2014" month="07" day="07"/><enddate year="2014" month="07" day="11"/></confdate><confcode>111522</confcode><confsponsors complete="n"><confsponsor/></confsponsors></confevent><confpublication><procpartno>1 of 1</procpartno></confpublication></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification> <classification-code>461.4</classification-code> <classification-description>Human Engineering</classification-description> </classification><classification> <classification-code>741.2</classification-code> <classification-description>Vision</classification-description> </classification><classification> <classification-code>971</classification-code> <classification-description>Social Sciences</classification-description> </classification></classifications><classifications type="FLXCLASS"><classification> <classification-code>902</classification-code> <classification-description>FLUIDEX; Related Topics</classification-description> </classification></classifications><classifications type="ASJC"><classification>1700</classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="18"><reference id="1"><ref-info><ref-title><ref-titletext>Measuring the user experience: Collecting, analyzing, and presenting usability metrics (Google eBook)</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84919392366</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>W.</ce:initials><ce:indexed-name>Albert W.</ce:indexed-name><ce:surname>Albert</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Tullis T.</ce:indexed-name><ce:surname>Tullis</ce:surname></author></ref-authors><ref-sourcetitle>Newnes</ref-sourcetitle><ref-publicationyear first="2013"/></ref-info><ref-fulltext>William Albert and Thomas Tullis. Measuring the User Experience: Collecting, Analyzing, and Presenting Usability Metrics (Google eBook). Newnes, 2013.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Neural systems subserving valence and arousal during the experience of induced emotions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">77953253878</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Colibazzi T.</ce:indexed-name><ce:surname>Colibazzi</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Posner J.</ce:indexed-name><ce:surname>Posner</ce:surname></author><author seq="3"><ce:initials>Z.</ce:initials><ce:indexed-name>Wang Z.</ce:indexed-name><ce:surname>Wang</ce:surname></author><author seq="4"><ce:initials>D.</ce:initials><ce:indexed-name>Gorman D.</ce:indexed-name><ce:surname>Gorman</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Gerber A.</ce:indexed-name><ce:surname>Gerber</ce:surname></author><author seq="6"><ce:initials>S.</ce:initials><ce:indexed-name>Yu S.</ce:indexed-name><ce:surname>Yu</ce:surname></author><author seq="7"><ce:initials>H.</ce:initials><ce:indexed-name>Zhu H.</ce:indexed-name><ce:surname>Zhu</ce:surname></author><author seq="8"><ce:initials>A.</ce:initials><ce:indexed-name>Kangarlu A.</ce:indexed-name><ce:surname>Kangarlu</ce:surname></author><author seq="9"><ce:initials>Y.</ce:initials><ce:indexed-name>Duan Y.</ce:indexed-name><ce:surname>Duan</ce:surname></author><author seq="10"><ce:initials>J.A.</ce:initials><ce:indexed-name>Russell J.A.</ce:indexed-name><ce:surname>Russell</ce:surname></author><author seq="11"><ce:initials>B.S.</ce:initials><ce:indexed-name>Peterson B.S.</ce:indexed-name><ce:surname>Peterson</ce:surname></author></ref-authors><ref-sourcetitle>Emotion (Washington, D.C.)</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><voliss volume="10" issue="3"/><pagerange first="377" last="389"/></ref-volisspag><ref-text>June</ref-text></ref-info><ref-fulltext>Tiziano Colibazzi, Jonathan Posner, Zhishun Wang, Daniel Gorman, Andrew Gerber, Shan Yu, Hongtu Zhu, Alayar Kangarlu, Yunsuo Duan, James A Russell, and Bradley S Peterson. Neural systems subserving valence and arousal during the experience of induced emotions. Emotion (Washington, D.C.), 10(3):377-89, June 2010.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Using visualizations for music discovery</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80053120539</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Donaldson J.</ce:indexed-name><ce:surname>Donaldson</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Lamere P.</ce:indexed-name><ce:surname>Lamere</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-text>page Tutorial</ref-text></ref-info><ref-fulltext>Justin Donaldson and Paul Lamere. Using Visualizations for Music Discovery. In Proceedings of the International Conference on Music Information Retrieval (ISMIR), page Tutorial, 2009.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>A comparison of the discrete and dimensional models of emotion in music</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">78650825957</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Eerola T.</ce:indexed-name><ce:surname>Eerola</ce:surname></author><author seq="2"><ce:initials>J.K.</ce:initials><ce:indexed-name>Vuoskoski J.K.</ce:indexed-name><ce:surname>Vuoskoski</ce:surname></author></ref-authors><ref-sourcetitle>Psychology of Music</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><voliss volume="39" issue="1"/><pagerange first="18" last="49"/></ref-volisspag><ref-text>August</ref-text></ref-info><ref-fulltext>T. Eerola and J. K. Vuoskoski. A comparison of the discrete and dimensional models of emotion in music. Psychology of Music, 39(1):18-49, August 2010.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>An argument for basic emotions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84889960454</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Ekman P.</ce:indexed-name><ce:surname>Ekman</ce:surname></author></ref-authors><ref-sourcetitle>Cognition and Emotion</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="169" issue="6"/></ref-volisspag><ref-text>200</ref-text></ref-info><ref-fulltext>Paul Ekman. An argument for basic emotions. Cognition and Emotion, 169-200(6), 1992.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Emotion perceived and emotion felt: Same or different?</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33745759392</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Gabrielsson A.</ce:indexed-name><ce:surname>Gabrielsson</ce:surname></author></ref-authors><ref-sourcetitle>Musicae Scientiae</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="5" issue="1"/><pagerange first="123" last="147"/></ref-volisspag><ref-text>September</ref-text></ref-info><ref-fulltext>Alf Gabrielsson. Emotion Perceived and Emotion Felt: Same or Different? Musicae Scientiae, 5(1 suppl):123-147, September 2002.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Music emotion recognition: A state of the art review</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873591302</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.E.</ce:initials><ce:indexed-name>Kim Y.E.</ce:indexed-name><ce:surname>Kim</ce:surname></author><author seq="2"><ce:initials>E.M.</ce:initials><ce:indexed-name>Schmidt E.M.</ce:indexed-name><ce:surname>Schmidt</ce:surname></author><author seq="3"><ce:initials>R.</ce:initials><ce:indexed-name>Migneco R.</ce:indexed-name><ce:surname>Migneco</ce:surname></author><author seq="4"><ce:initials>B.G.</ce:initials><ce:indexed-name>Morton B.G.</ce:indexed-name><ce:surname>Morton</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Richardson P.</ce:indexed-name><ce:surname>Richardson</ce:surname></author><author seq="6"><ce:initials>J.</ce:initials><ce:indexed-name>Scott J.</ce:indexed-name><ce:surname>Scott</ce:surname></author><author seq="7"><ce:initials>J.A.</ce:initials><ce:indexed-name>Speck J.A.</ce:indexed-name><ce:surname>Speck</ce:surname></author><author seq="8"><ce:initials>D.</ce:initials><ce:indexed-name>Turnbull D.</ce:indexed-name><ce:surname>Turnbull</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><pagerange first="255" last="266"/></ref-volisspag><ref-text>Utrecht</ref-text></ref-info><ref-fulltext>Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton, P. Richardson, J. Scott, J. A. Speck, and D. Turnbull. Music emotion recognition: A state of the art review. In Proceedings of the International Conference on Music Information Retrieval (ISMIR), pages 255-266, Utrecht, 2010.</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Indexing music by mood: Design and integration of an automatic content-based annotator</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">77950188142</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Laurier C.</ce:indexed-name><ce:surname>Laurier</ce:surname></author><author seq="2"><ce:initials>O.</ce:initials><ce:indexed-name>Meyers O.</ce:indexed-name><ce:surname>Meyers</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Serra J.</ce:indexed-name><ce:surname>Serrà</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Blech M.</ce:indexed-name><ce:surname>Blech</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Herrera P.</ce:indexed-name><ce:surname>Herrera</ce:surname></author><author seq="6"><ce:initials>X.</ce:initials><ce:indexed-name>Serra X.</ce:indexed-name><ce:surname>Serra</ce:surname></author></ref-authors><ref-sourcetitle>Multimedia Tools and Applications</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="48" issue="1"/><pagerange first="161" last="184"/></ref-volisspag><ref-text>October</ref-text></ref-info><ref-fulltext>Cyril Laurier, Owen Meyers, Joan Serrà, Martin Blech, Perfecto Herrera, and Xavier Serra. Indexing music by mood: design and integration of an automatic content-based annotator. Multimedia Tools and Applications, 48(1):161-184, October 2009.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Music retrieval: A tutorial and review</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33846152329</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.</ce:initials><ce:indexed-name>Orio N.</ce:indexed-name><ce:surname>Orio</ce:surname></author></ref-authors><ref-sourcetitle>Foundations and Trends&amp;reg; in Information Retrieval</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="1" issue="1"/><pagerange first="1" last="90"/></ref-volisspag></ref-info><ref-fulltext>Nicola Orio. Music Retrieval: A Tutorial and Review. Foundations and Trends&amp;reg; in Information Retrieval, 1(1):1-90, 2006.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Reexamining the circumplex model of affect</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0034241517</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.A.</ce:initials><ce:indexed-name>Remmington N.A.</ce:indexed-name><ce:surname>Remmington</ce:surname></author><author seq="2"><ce:initials>L.R.</ce:initials><ce:indexed-name>Fabrigar L.R.</ce:indexed-name><ce:surname>Fabrigar</ce:surname></author><author seq="3"><ce:initials>P.S.</ce:initials><ce:indexed-name>Visser P.S.</ce:indexed-name><ce:surname>Visser</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Personality and Social Psychology</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><voliss volume="79" issue="2"/><pagerange first="286" last="300"/></ref-volisspag><ref-text>August</ref-text></ref-info><ref-fulltext>N A Remmington, L R Fabrigar, and P S Visser. Reexamining the circumplex model of affect. Journal of personality and social psychology, 79(2):286-300, August 2000.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Specmurt anasylis: A piano-rollvisualization of polyphonic music signal by deconvolution of log-frequency spectrum</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84904679841</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Sagayama S.</ce:indexed-name><ce:surname>Sagayama</ce:surname></author><author seq="2"><ce:initials>K.</ce:initials><ce:indexed-name>Takahashi K.</ce:indexed-name><ce:surname>Takahashi</ce:surname></author></ref-authors><ref-sourcetitle>ISCA Tutorial and Research Workshop on Statistical and Perceptual Audio Processing</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>Jeju, Korea</ref-text></ref-info><ref-fulltext>Shigeki Sagayama and Keigo Takahashi. Specmurt anasylis: A piano-rollvisualization of polyphonic music signal by deconvolution of log-frequency spectrum. In ISCA Tutorial and Research Workshop on Statistical and Perceptual Audio Processing, Jeju, Korea, 2004.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>The neglected user in music information retrieval research</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84888379546</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Schedl M.</ce:indexed-name><ce:surname>Schedl</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Flexer A.</ce:indexed-name><ce:surname>Flexer</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Urbano J.</ce:indexed-name><ce:surname>Urbano</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Information Systems</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="41" issue="3"/><pagerange first="523" last="539"/></ref-volisspag><ref-text>July</ref-text></ref-info><ref-fulltext>Markus Schedl, Arthur Flexer, and Julián Urbano. The neglected user in music information retrieval research. Journal of Intelligent Information Systems, 41(3):523-539, July 2013.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Emotional effects of music: Production rules</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038410367</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.R.</ce:initials><ce:indexed-name>Scherer K.R.</ce:indexed-name><ce:surname>Scherer</ce:surname></author><author seq="2"><ce:initials>M.R.</ce:initials><ce:indexed-name>Zentner M.R.</ce:indexed-name><ce:surname>Zentner</ce:surname></author></ref-authors><ref-sourcetitle>Music and Emotion</ref-sourcetitle><ref-publicationyear first="2001"/><ref-text>P. N. Juslin and J. A Sloboda, editors, Oxford University Press, New York</ref-text></ref-info><ref-fulltext>K. R. Scherer and M. R. Zentner. Emotional effects of music: production rules. In P. N. Juslin and J. A Sloboda, editors, Music and emotion. Oxford University Press, New York, 2001.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>A survey of music recommendation systems and future perspectives</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84897069484</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Song Y.</ce:indexed-name><ce:surname>Song</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Dixon S.</ce:indexed-name><ce:surname>Dixon</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Pearce M.</ce:indexed-name><ce:surname>Pearce</ce:surname></author></ref-authors><ref-sourcetitle>Proc. 9th Int. Symp. Computer Music Modelling and Retrieval (CMMR</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="395" last="410"/></ref-volisspag><ref-text>London</ref-text></ref-info><ref-fulltext>Y. Song, S. Dixon, and M. Pearce. A survey of music recommendation systems and future perspectives. In Proc. 9th Int. Symp. Computer Music Modelling and Retrieval (CMMR), pages 395-410, London, 2012.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Affective labeling in a content-based recommender system for images</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84872741048</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Tkalcic M.</ce:indexed-name><ce:surname>Tkalcic</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Odic A.</ce:indexed-name><ce:surname>Odic</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Kosir A.</ce:indexed-name><ce:surname>Kosir</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Tasic J.</ce:indexed-name><ce:surname>Tasic</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Transactions on Multimedia</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="15" issue="2"/><pagerange first="391" last="400"/></ref-volisspag><ref-text>February</ref-text></ref-info><ref-fulltext>Marko Tkalcic, Ante Odic, Andrej Kosir, and Jurij Tasic. Affective Labeling in a Content-Based Recommender System for Images. IEEE Transactions on Multimedia, 15(2):391-400, February 2013.</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>The role of music in the lives of homeless young people</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873422734</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Palzkill Woelfer J.</ce:indexed-name><ce:surname>Palzkill Woelfer</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="367" last="372"/></ref-volisspag><ref-text>Porto</ref-text></ref-info><ref-fulltext>Jill Palzkill Woelfer. The role of music in the lives of homeless young people. In Proceedings of the International Conference on Music Information Retrieval (ISMIR), pages 367-372, Porto, 2012.</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>Spectral correlates in emotion labeling of sustained musical instrument tones</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85055724292</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Wu B.</ce:indexed-name><ce:surname>Wu</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Wun S.</ce:indexed-name><ce:surname>Wun</ce:surname></author><author seq="3"><ce:initials>C.</ce:initials><ce:indexed-name>Lee C.</ce:indexed-name><ce:surname>Lee</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Horner A.</ce:indexed-name><ce:surname>Horner</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><pagerange first="415" last="421"/></ref-volisspag></ref-info><ref-fulltext>B. Wu, S. Wun, C. Lee, and A. Horner. Spectral correlates in emotion labeling of sustained musical instrument tones. In Proceedings of the International Conference on Music Information Retrieval (ISMIR), pages 415-421, 2013.</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Audio-based music visualization for music structure analysis</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84925241395</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Wu H.</ce:indexed-name><ce:surname>Wu</ce:surname></author><author seq="2"><ce:initials>J.P.</ce:initials><ce:indexed-name>Bello J.P.</ce:indexed-name><ce:surname>Bello</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2010"/><ref-text>Barcelona</ref-text></ref-info><ref-fulltext>Ho-Hsiang Wu and Juan P. Bello. Audio-based music visualization for music structure analysis. In Proceedings of the International Conference on Music Information Retrieval (ISMIR), Barcelona, 2010.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>