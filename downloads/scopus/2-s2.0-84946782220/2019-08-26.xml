<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84946782220</prism:url><dc:identifier>SCOPUS_ID:84946782220</dc:identifier><eid>2-s2.0-84946782220</eid><dc:title>User interfaces and methodology for gathering multimodal data about music Uporabniški vmesniki in metodologija pridobivanja večmodalnih podatkov o glasbi</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Elektrotehniski Vestnik/Electrotechnical Review</prism:publicationName><dc:publisher>Electrotechnical Society of Slovenia</dc:publisher><source-id>16651</source-id><prism:issn>22323228 00135852</prism:issn><prism:volume>82</prism:volume><prism:issueIdentifier>3</prism:issueIdentifier><prism:startingPage>93</prism:startingPage><prism:endingPage>101</prism:endingPage><prism:pageRange>93-101</prism:pageRange><prism:coverDate>2015-01-01</prism:coverDate><openaccess/><openaccessFlag/><dc:creator><author seq="1" auid="56258907000"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56258907000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><ce:para>Several studies dealing with music recommendation and visualization base their approaches on datasets gathered with user surveys. However, the gathering procedure is seldom the focus of music research, even though the user interfaces and methodology are an important part of gathering the music data and evaluation of the music information retrieval algorithms. The paper presents the main elements of gathering the Moodo dataset that combines the demographic data, the users' mood and perception of emotions with the users' emotional and color responses to music. For this purpose, two novel user interfaces were developed, i.e. the MoodStripe and MoodGraph, which have several advantages over the existing classical models, both in terms of intuitiveness and functionality. The proposed interfaces are also applicable to other domains dealing with the user data.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84946782220" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84946782220&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84946782220&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><affiliation id="100530254" href="https://api.elsevier.com/content/affiliation/affiliation_id/100530254"><affilname>Znanstvenoraziskovalni Center</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="56258907000"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56258907000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="35175418100"><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name><preferred-name><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/35175418100</author-url><affiliation id="100530254" href="https://api.elsevier.com/content/affiliation/affiliation_id/100530254"/></author><author seq="3" auid="6603601816"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603601816</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="slv"/><authkeywords/><idxterms><mainterm weight="b" candidate="n">Classical model</mainterm><mainterm weight="b" candidate="n">Demographic data</mainterm><mainterm weight="b" candidate="n">Multi-modal data</mainterm><mainterm weight="b" candidate="n">Music data</mainterm><mainterm weight="b" candidate="n">Music information retrieval</mainterm><mainterm weight="b" candidate="n">Music recommendation</mainterm><mainterm weight="b" candidate="n">User data</mainterm><mainterm weight="b" candidate="n">User surveys</mainterm></idxterms><subject-areas><subject-area code="2208" abbrev="ENGI">Electrical and Electronic Engineering</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered day="02" month="08" timestamp="2019-08-02T23:30:10.000010-04:00" year="2019"/><ait:date-sort day="01" month="01" year="2015"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2015 Elsevier B.V., All rights reserved.</copyright><itemidlist><itemid idtype="PUI">606885792</itemid><itemid idtype="CAR-ID">642094933</itemid><itemid idtype="CPX">20154601554793</itemid><itemid idtype="SCP">84946782220</itemid><itemid idtype="SGR">84946782220</itemid></itemidlist><history><date-created day="18" month="11" timestamp="BST 03:02:28" year="2015"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="slv" language="Slovenian"/><abstract-language xml:lang="eng" language="English"/><abstract-language xml:lang="slv" language="Slovenian"/></citation-info><citation-title><titletext original="n" xml:lang="eng" language="English">User interfaces and methodology for gathering multimodal data about music</titletext><titletext original="y" xml:lang="slv" language="Slovenian">Uporabniški vmesniki in metodologija pridobivanja večmodalnih podatkov o glasbi</titletext></citation-title><author-group><author auid="56258907000" seq="1" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name></author><author auid="6603601816" seq="3" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn" dptid="104580817"><organization>Univerza v Ljubljani</organization><organization>Fakulteta za Racunalnistvo in Informatiko</organization><affiliation-id afid="60031106" dptid="104580817"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="35175418100" seq="2" type="auth"><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name><preferred-name><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname><ce:given-name>Gregor</ce:given-name></preferred-name></author><affiliation afid="100530254" country="svn"><organization>Znanstvenoraziskovalni Center</organization><organization>Slovenske Akademije Znanosti in Umetnosti</organization><affiliation-id afid="100530254"/><country>Slovenia</country></affiliation></author-group><abstracts><abstract original="y" xml:lang="eng"><ce:para>Several studies dealing with music recommendation and visualization base their approaches on datasets gathered with user surveys. However, the gathering procedure is seldom the focus of music research, even though the user interfaces and methodology are an important part of gathering the music data and evaluation of the music information retrieval algorithms. The paper presents the main elements of gathering the Moodo dataset that combines the demographic data, the users' mood and perception of emotions with the users' emotional and color responses to music. For this purpose, two novel user interfaces were developed, i.e. the MoodStripe and MoodGraph, which have several advantages over the existing classical models, both in terms of intuitiveness and functionality. The proposed interfaces are also applicable to other domains dealing with the user data.</ce:para></abstract></abstracts><source country="svn" srcid="16651" type="j"><sourcetitle>Elektrotehniski Vestnik/Electrotechnical Review</sourcetitle><sourcetitle-abbrev>Elektroteh Vestn Electrotech Rev</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">Elektrotehniski Vestnik/Electrotechnical Review</translated-sourcetitle><issn type="electronic">22323228</issn><issn type="print">00135852</issn><codencode>ELVEA</codencode><volisspag><voliss issue="3" volume="82"/><pagerange first="93" last="101"/></volisspag><publicationyear first="2015"/><publicationdate><year>2015</year><date-text xfab-added="true">2015</date-text></publicationdate><website><ce:e-address type="email">http://ev.fe.uni-lj.si/3-2015/Pesek.pdf</ce:e-address></website><publisher><publishername>Electrotechnical Society of Slovenia</publishername></publisher></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification> <classification-code>722.2</classification-code> <classification-description>Computer Peripheral Equipment</classification-description> </classification><classification> <classification-code>971</classification-code> <classification-description>Social Sciences</classification-description> </classification></classifications><classifications type="FLXCLASS"><classification> <classification-code>902</classification-code> <classification-description>FLUIDEX; Related Topics</classification-description> </classification></classifications><classifications type="ASJC"><classification>2208</classification></classifications><classifications type="SUBJABBR"><classification>ENGI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="31"><reference id="1"><ref-info><ref-title><ref-titletext>Emotional responses to music: The need to consider underlying mechanisms</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">49949087918</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.N.</ce:initials><ce:indexed-name>Juslin P.N.</ce:indexed-name><ce:surname>Juslin</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Vastfjall D.</ce:indexed-name><ce:surname>Västfjäll</ce:surname></author></ref-authors><ref-sourcetitle>Behavioral and Brain Sciences</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss issue="5" volume="31"/><pagerange first="559" last="575"/></ref-volisspag></ref-info><ref-fulltext>P. N. Juslin and D. Västfjäll, "Emotional responses to music: The need to consider underlying mechanisms," Behavioral and brain sciences, Vol. 31, no. 5, pp. 559-575, 2008.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>A survey of music recommendation systems and future perspectives</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84897069484</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Song Y.</ce:indexed-name><ce:surname>Song</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Dixon S.</ce:indexed-name><ce:surname>Dixon</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Pearce M.</ce:indexed-name><ce:surname>Pearce</ce:surname></author></ref-authors><ref-sourcetitle>Proc. 9th Int. Symp. Computer Music Modelling and Retrieval (CMMR)</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="395" last="410"/></ref-volisspag><ref-text>London</ref-text></ref-info><ref-fulltext>Y. Song, S. Dixon, and M. Pearce, "A survey of music recommendation systems and future perspectives," in Proc. 9th Int. Symp. Computer Music Modelling and Retrieval (CMMR), London, 2012, pp. 395-410.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Music emotion recognition: A state of the art review</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873591302</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.E.</ce:initials><ce:indexed-name>Kim Y.E.</ce:indexed-name><ce:surname>Kim</ce:surname></author><author seq="2"><ce:initials>E.M.</ce:initials><ce:indexed-name>Schmidt E.M.</ce:indexed-name><ce:surname>Schmidt</ce:surname></author><author seq="3"><ce:initials>R.</ce:initials><ce:indexed-name>Migneco R.</ce:indexed-name><ce:surname>Migneco</ce:surname></author><author seq="4"><ce:initials>B.G.</ce:initials><ce:indexed-name>Morton B.G.</ce:indexed-name><ce:surname>Morton</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Richardson P.</ce:indexed-name><ce:surname>Richardson</ce:surname></author><author seq="6"><ce:initials>J.</ce:initials><ce:indexed-name>Scott J.</ce:indexed-name><ce:surname>Scott</ce:surname></author><author seq="7"><ce:initials>J.A.</ce:initials><ce:indexed-name>Speck J.A.</ce:indexed-name><ce:surname>Speck</ce:surname></author><author seq="8"><ce:initials>D.</ce:initials><ce:indexed-name>Turnbull D.</ce:indexed-name><ce:surname>Turnbull</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><pagerange first="255" last="266"/></ref-volisspag><ref-text>Utrecht</ref-text></ref-info><ref-fulltext>Y. E. Kim, E. M. Schmidt, R. Migneco, B. G. Morton, P. Richardson, J. Scott, J. A. Speck, and D. Turnbull, "Music emotion recognition: A state of the art review," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Utrecht, 2010, pp. 255-266.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>A circumplex model of affect</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">4644280844</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.A.</ce:initials><ce:indexed-name>Russell J.A.</ce:indexed-name><ce:surname>Russell</ce:surname></author></ref-authors><ref-sourcetitle>1Journal of Personality and Social Psychology</ref-sourcetitle><ref-publicationyear first="1980"/><ref-volisspag><voliss issue="6" volume="39"/><pagerange first="1161" last="1178"/></ref-volisspag></ref-info><ref-fulltext>J. A. Russell, "A circumplex model of affect," 1Journal of personality and social psychology, Vol. 39, no. 6, pp. 1161-1178, 1980.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>A comparison of the discrete and dimensional models of emotion in music</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">78650825957</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Eerola T.</ce:indexed-name><ce:surname>Eerola</ce:surname></author><author seq="2"><ce:initials>J.K.</ce:initials><ce:indexed-name>Vuoskoski J.K.</ce:indexed-name><ce:surname>Vuoskoski</ce:surname></author></ref-authors><ref-sourcetitle>Psychology of Music</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><voliss issue="1" volume="39"/><pagerange first="18" last="49"/></ref-volisspag><ref-text>Aug.</ref-text></ref-info><ref-fulltext>T. Eerola and J. K. Vuoskoski, "A comparison of the discrete and dimensional models of emotion in music," Psychology of Music, Vol. 39, no. 1, pp. 18-49, Aug. 2010.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Mining the correlation between lyrical and audio features and the emergence of mood</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84872703021</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Mcvicar M.</ce:indexed-name><ce:surname>Mcvicar</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Freeman T.</ce:indexed-name><ce:surname>Freeman</ce:surname></author><author seq="3"><ce:initials>T.</ce:initials><ce:indexed-name>De Bie T.</ce:indexed-name><ce:surname>De Bie</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><pagerange first="783" last="788"/></ref-volisspag><ref-text>Miami</ref-text></ref-info><ref-fulltext>M. Mcvicar, T. Freeman, and T. De Bie, "Mining the Correlation Between Lyrical and Audio Features and the Emergence of Mood," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Miami, 2011, pp. 783-788.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Cross-cultural music mood classification: A comparison on English and Chinese songs</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873451540</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Yang Y.</ce:indexed-name><ce:surname>Yang</ce:surname></author><author seq="2"><ce:initials>X.</ce:initials><ce:indexed-name>Hu X.</ce:indexed-name><ce:surname>Hu</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2012"/><ref-text>Porto</ref-text></ref-info><ref-fulltext>Y. Yang and X. Hu, "Cross-cultural Music Mood Classification: A Comparison on English and Chinese Songs," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Porto, 2012.</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Music mood representations from social tags</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873632528</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Laurier C.</ce:indexed-name><ce:surname>Laurier</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Sordo M.</ce:indexed-name><ce:surname>Sordo</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Serra J.</ce:indexed-name><ce:surname>Serrà</ce:surname></author><author seq="4"><ce:initials>P.</ce:initials><ce:indexed-name>Herrera P.</ce:indexed-name><ce:surname>Herrera</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="381" last="386"/></ref-volisspag></ref-info><ref-fulltext>C. Laurier, M. Sordo, J. Serrà, and P. Herrera, "Music Mood Representations from Social Tags," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), 2009, pp. 381-386.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Design and evaluation of semantic mood models for music recommendation using editorial tags</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84946808361</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Barthet M.</ce:indexed-name><ce:surname>Barthet</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Marston D.</ce:indexed-name><ce:surname>Marston</ce:surname></author><author seq="3"><ce:initials>C.</ce:initials><ce:indexed-name>Baume C.</ce:indexed-name><ce:surname>Baume</ce:surname></author><author seq="4"><ce:initials>G.</ce:initials><ce:indexed-name>Fazekas G.</ce:indexed-name><ce:surname>Fazekas</ce:surname></author><author seq="5"><ce:initials>M.</ce:initials><ce:indexed-name>Sandler M.</ce:indexed-name><ce:surname>Sandler</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2013"/><ref-text>Curitiba</ref-text></ref-info><ref-fulltext>M. Barthet, D. Marston, C. Baume, G. Fazekas, and M. Sandler, "Design and evaluation of semantic mood models for music recommendation using editorial tags," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Curitiba, 2013.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Modeling musical emotion dynamics with conditional random fields</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84872700353</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.M.</ce:initials><ce:indexed-name>Schmidt E.M.</ce:indexed-name><ce:surname>Schmidt</ce:surname></author><author seq="2"><ce:initials>Y.E.</ce:initials><ce:indexed-name>Kim Y.E.</ce:indexed-name><ce:surname>Kim</ce:surname></author></ref-authors><ref-sourcetitle>ISMIR</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><pagerange first="777" last="782"/></ref-volisspag></ref-info><ref-fulltext>E. M. Schmidt and Y. E. Kim, "Modeling Musical Emotion Dynamics with Conditional Random Fields." in ISMIR, 2011, pp. 777-782.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>'Mister DJ, cheer me up!': Musical and textual features for automatic mood classification</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">77952100523</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Schuller B.</ce:indexed-name><ce:surname>Schuller</ce:surname></author><author seq="2"><ce:initials>C.</ce:initials><ce:indexed-name>Hage C.</ce:indexed-name><ce:surname>Hage</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Schuller D.</ce:indexed-name><ce:surname>Schuller</ce:surname></author><author seq="4"><ce:initials>G.</ce:initials><ce:indexed-name>Rigoll G.</ce:indexed-name><ce:surname>Rigoll</ce:surname></author></ref-authors><ref-sourcetitle>Journal of New Music Research</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><voliss issue="1" volume="39"/><pagerange first="13" last="34"/></ref-volisspag></ref-info><ref-fulltext>B. Schuller, C. Hage, D. Schuller, and G. Rigoll, "'Mister DJ, Cheer Me Up!': Musical and textual features for automatic mood classification," Journal of New Music Research, Vol. 39, no. 1, pp. 13-34, 2010.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Emotions evoked by the sound of music: Characterization, classification, and measurement</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">50849112055</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Zentner M.</ce:indexed-name><ce:surname>Zentner</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Grandjean D.</ce:indexed-name><ce:surname>Grandjean</ce:surname></author><author seq="3"><ce:initials>K.R.</ce:initials><ce:indexed-name>Scherer K.R.</ce:indexed-name><ce:surname>Scherer</ce:surname></author></ref-authors><ref-sourcetitle>Emotion</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss issue="4" volume="8"/><pagerange first="494"/></ref-volisspag></ref-info><ref-fulltext>M. Zentner, D. Grandjean, and K. R. Scherer, "Emotions evoked by the sound of music: characterization, classification, and measurement," Emotion, Vol. 8, no. 4, p. 494, 2008.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Exploring mood metadata: Relationships with genre, artist and usage metadata</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873596722</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>X.</ce:initials><ce:indexed-name>Hu X.</ce:indexed-name><ce:surname>Hu</ce:surname></author><author seq="2"><ce:initials>J.S.</ce:initials><ce:indexed-name>Downie J.S.</ce:indexed-name><ce:surname>Downie</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2007"/><ref-text>Vienna</ref-text></ref-info><ref-fulltext>X. Hu and J. S. Downie, "Exploring Mood Metadata: Relationships with Genre, Artist and Usage Metadata," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Vienna, 2007.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Multidisciplinary perspectives on music emotion recognition: Implications for content and context-based models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84884563198</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Barthet M.</ce:indexed-name><ce:surname>Barthet</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Fazekas G.</ce:indexed-name><ce:surname>Fazekas</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Sandler M.</ce:indexed-name><ce:surname>Sandler</ce:surname></author></ref-authors><ref-sourcetitle>Proc. CMMR</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="492" last="507"/></ref-volisspag><ref-text>London</ref-text></ref-info><ref-fulltext>M. Barthet, G. Fazekas, and M. Sandler, "Multidisciplinary perspectives on music emotion recognition: Implications for content and context-based models," in Proc. CMMR, London, 2012, pp. 492-507.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>The neglected user in music information retrieval research</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84888379546</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Schedl M.</ce:indexed-name><ce:surname>Schedl</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Flexer A.</ce:indexed-name><ce:surname>Flexer</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Urbano J.</ce:indexed-name><ce:surname>Urbano</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Information Systems</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss issue="3" volume="41"/><pagerange first="523" last="539"/></ref-volisspag><ref-text>Jul.</ref-text></ref-info><ref-fulltext>M. Schedl, A. Flexer, and J. Urbano, "The neglected user in music information retrieval research," Journal of Intelligent Information Systems, Vol. 41, no. 3, pp. 523-539, Jul. 2013.</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Using visualizations for music discovery</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80053120539</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Donaldson J.</ce:indexed-name><ce:surname>Donaldson</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Lamere P.</ce:indexed-name><ce:surname>Lamere</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-text>Tutorial</ref-text></ref-info><ref-fulltext>J. Donaldson and P. Lamere, "Using Visualizations for Music Discovery," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), 2009, p. Tutorial.</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>What you see is what you get: On visualizing music</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">77952690386</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.</ce:initials><ce:indexed-name>Isaacson E.</ce:indexed-name><ce:surname>Isaacson</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><pagerange first="389" last="395"/></ref-volisspag><ref-text>London</ref-text></ref-info><ref-fulltext>E. Isaacson, "What You See Is What You Get: On Visualizing Music," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), London, 2005, pp. 389-395.</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Visualizing music: Tonal progressions and distributions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84867912258</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Mardirossian A.</ce:indexed-name><ce:surname>Mardirossian</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Chew E.</ce:indexed-name><ce:surname>Chew</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2007"/><ref-volisspag><pagerange first="189" last="194"/></ref-volisspag><ref-text>Vienna</ref-text></ref-info><ref-fulltext>A. Mardirossian and E. Chew, "Visualizing Music: Tonal Progressions and Distributions," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Vienna, 2007, pp. 189-194.</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>On-verting path structures into block structures using eigenvalue decompositions of self-similarity matrices</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84905219825</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Grohganz H.</ce:indexed-name><ce:surname>Grohganz</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Clausen M.</ce:indexed-name><ce:surname>Clausen</ce:surname></author><author seq="3"><ce:initials>N.</ce:initials><ce:indexed-name>Jiang N.</ce:indexed-name><ce:surname>Jiang</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Mueller M.</ce:indexed-name><ce:surname>Mueller</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2013"/><ref-text>Curitiba</ref-text></ref-info><ref-fulltext>H. Grohganz, M. Clausen, N. Jiang, and M. Mueller, "on-verting path structures into block structures using eigenvalue decompositions of self-similarity matrices," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Curitiba, 2013.</ref-fulltext></reference><reference id="20"><ref-info><ref-title><ref-titletext>Automated methods for analyzing music recordings in sonata form</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85063429818</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.</ce:initials><ce:indexed-name>Jiang N.</ce:indexed-name><ce:surname>Jiang</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Mueller M.</ce:indexed-name><ce:surname>Mueller</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2013"/><ref-text>Curitiba</ref-text></ref-info><ref-fulltext>N. Jiang and M. Mueller, "Automated methods for analyzing music recordings in sonata form," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Curitiba, 2013.</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>Visualizing and exploring personal music libraries</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33744968648</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Torrens M.</ce:indexed-name><ce:surname>Torrens</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Hertzog P.</ce:indexed-name><ce:surname>Hertzog</ce:surname></author><author seq="3"><ce:initials>J.L.</ce:initials><ce:indexed-name>Arcos J.L.</ce:indexed-name><ce:surname>Arcos</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>Barcelona</ref-text></ref-info><ref-fulltext>M. Torrens, P. Hertzog, and J. L. Arcos, "Visualizing and Exploring Personal Music Libraries," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Barcelona, 2004.</ref-fulltext></reference><reference id="22"><ref-info><ref-title><ref-titletext>Mapping music in the palm of your hand, explore and discover your collection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33744987157</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Van Gulik R.</ce:indexed-name><ce:surname>Van Gulik</ce:surname></author><author seq="2"><ce:initials>F.</ce:initials><ce:indexed-name>Vignoli F.</ce:indexed-name><ce:surname>Vignoli</ce:surname></author><author seq="3"><ce:initials>H.</ce:initials><ce:indexed-name>Van De Wetering H.</ce:indexed-name><ce:surname>Van De Wetering</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>Barcelona</ref-text></ref-info><ref-fulltext>R. Van Gulik, F. Vignoli, and H. Van de Wetering, "Mapping Music In The Palm Of Your Hand, Explore And Discover Your Collection," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Barcelona, 2004.</ref-fulltext></reference><reference id="23"><ref-info><ref-title><ref-titletext>SongExplorer: A tabletop application for exploring large collections of songs</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80053102744</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.F.</ce:initials><ce:indexed-name>Julia C.F.</ce:indexed-name><ce:surname>Julia</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Jorda S.</ce:indexed-name><ce:surname>Jorda</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="675" last="680"/></ref-volisspag><ref-text>Kobe</ref-text></ref-info><ref-fulltext>C. F. Julia and S. Jorda, "SongExplorer: A Tabletop Application for Exploring Large Collections of Songs," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Kobe, 2009, pp. 675-680.</ref-fulltext></reference><reference id="24"><ref-info><ref-title><ref-titletext>Music thumbnailer: Visualizing musical pieces in thumbnail images based on acoustic features</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873448962</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Yoshii K.</ce:indexed-name><ce:surname>Yoshii</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Goto M.</ce:indexed-name><ce:surname>Goto</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><pagerange first="211" last="216"/></ref-volisspag><ref-text>Philadelphia</ref-text></ref-info><ref-fulltext>K. Yoshii and M. Goto, "Music Thumbnailer: Visualizing Musical Pieces in Thumbnail Images Based on Acoustic Features," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Philadelphia, 2008, pp. 211-216.</ref-fulltext></reference><reference id="25"><ref-info><ref-title><ref-titletext>Human computer interaction: Analysis and journey through eras</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84925681262</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.T.</ce:initials><ce:indexed-name>Jose P.T.</ce:indexed-name><ce:surname>Jose</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Miglani S.</ce:indexed-name><ce:surname>Miglani</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Yadav S.</ce:indexed-name><ce:surname>Yadav</ce:surname></author></ref-authors><ref-sourcetitle>International Journal of Computer Science and Mobile Computing</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><voliss issue="4" volume="3"/><pagerange first="653" last="650"/></ref-volisspag></ref-info><ref-fulltext>P. T. Jose, S. Miglani, and S. Yadav, "Human Computer Interaction: Analysis and Journey through Eras," International Journal of Computer Science and Mobile Computing, Vol. 3, no. 4, pp. 653-650, 2014.</ref-fulltext></reference><reference id="26"><ref-info><refd-itemidlist><itemid idtype="SGR">0003959340</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.W.</ce:initials><ce:indexed-name>Picard R.W.</ce:indexed-name><ce:surname>Picard</ce:surname></author></ref-authors><ref-sourcetitle>Affective Computing</ref-sourcetitle><ref-publicationyear first="2000"/><ref-text>MIT Press</ref-text></ref-info><ref-fulltext>R. W. Picard, Affective Computing. MIT Press, 2000.</ref-fulltext></reference><reference id="27"><ref-info><refd-itemidlist><itemid idtype="SGR">84946808364</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Tao J.</ce:indexed-name><ce:surname>Tao</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Tan T.</ce:indexed-name><ce:surname>Tan</ce:surname></author></ref-authors><ref-sourcetitle>Affective Computing: A Review</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss issue="1" volume="1"/></ref-volisspag><ref-text>Springer Berlin Heidelberg</ref-text></ref-info><ref-fulltext>J. Tao and T. Tan, Affective computing: A review. Springer Berlin Heidelberg, 2005, vol. 1, no. 1.</ref-fulltext></reference><reference id="28"><ref-info><ref-title><ref-titletext>An argument for basic emotions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84889960454</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Ekman P.</ce:indexed-name><ce:surname>Ekman</ce:surname></author></ref-authors><ref-sourcetitle>Cognition and Emotion</ref-sourcetitle><ref-publicationyear first="1992"/><ref-volisspag><voliss volume="6"/><pagerange first="169" last="200"/></ref-volisspag></ref-info><ref-fulltext>P. Ekman, "An argument for basic emotions," Cognition and Emotion, Vol. 6, pp. 169-200, 1992.</ref-fulltext></reference><reference id="29"><ref-info><ref-title><ref-titletext>Introducing A dataset of emotional and color responses to music</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84946808365</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Godec P.</ce:indexed-name><ce:surname>Godec</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Poredos M.</ce:indexed-name><ce:surname>Poredos</ce:surname></author><author seq="4"><ce:initials>G.</ce:initials><ce:indexed-name>Strle G.</ce:indexed-name><ce:surname>Strle</ce:surname></author><author seq="5"><ce:initials>J.</ce:initials><ce:indexed-name>Guna J.</ce:indexed-name><ce:surname>Guna</ce:surname></author><author seq="6"><ce:initials>E.</ce:initials><ce:indexed-name>Stojmenova E.</ce:indexed-name><ce:surname>Stojmenova</ce:surname></author><author seq="7"><ce:initials>M.</ce:initials><ce:indexed-name>Pogacnik M.</ce:indexed-name><ce:surname>Pogacnik</ce:surname></author><author seq="8"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Conference on Music Information Retrieval (ISMIR)</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><pagerange first="355" last="360"/></ref-volisspag><ref-text>Taipei</ref-text></ref-info><ref-fulltext>M. Pesek, P. Godec, M. Poredos, G. Strle, J. Guna, E. Stojmenova, M. Pogacnik, and M. Marolt, "Introducing A Dataset Of Emotional And Color Responses To Music," in Proceedings of the International Conference on Music Information Retrieval (ISMIR), Taipei, 2014, pp. 355-360.</ref-fulltext></reference><reference id="30"><ref-info><refd-itemidlist><itemid idtype="SGR">84902038845</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>W.</ce:initials><ce:indexed-name>Albert W.</ce:indexed-name><ce:surname>Albert</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Tullis T.</ce:indexed-name><ce:surname>Tullis</ce:surname></author></ref-authors><ref-sourcetitle>Measuring the User Experience: Collecting, Analyzing, and Presenting Usability Metrics</ref-sourcetitle><ref-publicationyear first="2013"/><ref-text>(Google eBook). Newnes</ref-text></ref-info><ref-fulltext>W. Albert and T. Tullis, Measuring the User Experience: Collecting, Analyzing, and Presenting Usability Metrics (Google eBook). Newnes, 2013.</ref-fulltext></reference><reference id="31"><ref-info><ref-title><ref-titletext>Nasa-task load index (NASA-TLX); 20 years later</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">44349088702</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.G.</ce:initials><ce:indexed-name>Hart S.G.</ce:indexed-name><ce:surname>Hart</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss issue="9" volume="50"/><pagerange first="904" last="908"/></ref-volisspag><ref-text>Oct.</ref-text></ref-info><ref-fulltext>S. G. Hart, "Nasa-Task Load Index (NASA-TLX); 20 Years Later," Proceedings of the Human Factors and Ergonomics Society Annual Meeting, Vol. 50, no. 9, pp. 904-908, Oct. 2006.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>