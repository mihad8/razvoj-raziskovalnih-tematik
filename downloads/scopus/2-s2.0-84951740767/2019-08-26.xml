<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84951740767</prism:url><dc:identifier>SCOPUS_ID:84951740767</dc:identifier><eid>2-s2.0-84951740767</eid><prism:doi>10.1007/978-3-319-24033-6_40</prism:doi><dc:title>Development and evaluation of the emotional Slovenian speech database - EmoLUKS</dc:title><prism:aggregationType>Book Series</prism:aggregationType><srctype>k</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</prism:publicationName><dc:publisher>Springer Verlagservice@springer.de</dc:publisher><source-id>25674</source-id><prism:isbn>9783319240329</prism:isbn><prism:issn>16113349 03029743</prism:issn><prism:volume>9302</prism:volume><prism:startingPage>351</prism:startingPage><prism:endingPage>359</prism:endingPage><prism:pageRange>351-359</prism:pageRange><prism:coverDate>2015-01-01</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="36462551200"><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname><ce:given-name>Tadej</ce:given-name><preferred-name><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname><ce:given-name>Tadej</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/36462551200</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><publishercopyright>© Springer International Publishing Switzerland 2015.</publishercopyright><ce:para>This paper describes a speech database built from 17 Slovenian radio dramas. The dramas were obtained from the national radio-and-television station (RTV Slovenia) and were given at the universities disposal with an academic license for processing and annotating the audio material. The utterances of one male and one female speaker were transcribed, segmented and then annotated with emotional states of the speakers. The annotation of the emotional states was conducted in two stages with our own web-based application for crowd sourcing. The final (emotional) speech database consists of 1385 recordings of one male (975 recordings) and one female (410 recordings) speaker and contains labeled emotional speech with a total duration of around 1 hour and 15 minutes. The paper presents the two-stage annotation process used to label the data and demonstrates the usefulness of the employed annotation methodology. Baseline emotion recognition experiments are also presented. The reported results are presented with the un-weighted as well as weighted average recalls and precisions for 2-class and 7-class recognition experiments.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84951740767" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84951740767&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84951740767&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><affiliation id="60006286" href="https://api.elsevier.com/content/affiliation/affiliation_id/60006286"><affilname>University of Primorska</affilname><affiliation-city>Koper</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="36462551200"><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname><ce:given-name>Tadej</ce:given-name><preferred-name><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname><ce:given-name>Tadej</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/36462551200</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="17347474600"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Štruc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/17347474600</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="6507417859"><ce:initials>J.</ce:initials><ce:indexed-name>Zibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Žibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6507417859</author-url><affiliation id="60006286" href="https://api.elsevier.com/content/affiliation/affiliation_id/60006286"/></author><author seq="4" auid="6604057186"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name><preferred-name><ce:initials>F.</ce:initials><ce:indexed-name>Mihelič F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6604057186</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Database development</author-keyword><author-keyword>Emotion recognition</author-keyword><author-keyword>Emotional speech database</author-keyword></authkeywords><idxterms><mainterm weight="b" candidate="n">Crowd sourcing</mainterm><mainterm weight="b" candidate="n">Database development</mainterm><mainterm weight="b" candidate="n">Emotion recognition</mainterm><mainterm weight="b" candidate="n">Emotional speech</mainterm><mainterm weight="b" candidate="n">Emotional state</mainterm><mainterm weight="b" candidate="n">Speech database</mainterm><mainterm weight="b" candidate="n">Web-based applications</mainterm><mainterm weight="b" candidate="n">Weighted averages</mainterm></idxterms><subject-areas><subject-area code="2614" abbrev="MATH">Theoretical Computer Science</subject-area><subject-area code="1700" abbrev="COMP">Computer Science (all)</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered year="2017" month="09" day="02" timestamp="2017-09-02T15:10:43.000043-04:00"/><ait:date-sort year="2015" month="01" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2016 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1007/978-3-319-24033-6_40</ce:doi><itemid idtype="PUI">607418702</itemid><itemid idtype="CAR-ID">643555797</itemid><itemid idtype="CPX">20155301741224</itemid><itemid idtype="SCP">84951740767</itemid><itemid idtype="SGR">84951740767</itemid></itemidlist><history><date-created year="2015" month="12" day="31" timestamp="BST 08:07:13"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword>Database development</author-keyword><author-keyword>Emotion recognition</author-keyword><author-keyword>Emotional speech database</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Development and evaluation of the emotional Slovenian speech database - EmoLUKS</titletext></citation-title><author-group><author auid="36462551200" seq="1" type="auth"><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname><ce:given-name>Tadej</ce:given-name><preferred-name><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname><ce:given-name>Tadej</ce:given-name></preferred-name></author><author auid="17347474600" seq="2" type="auth"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Štruc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name></preferred-name></author><author auid="6604057186" seq="4" type="auth"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name><preferred-name><ce:initials>F.</ce:initials><ce:indexed-name>Mihelič F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city>Ljubljana</city><postal-code>1000</postal-code><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="6507417859" seq="3" type="auth"><ce:initials>J.</ce:initials><ce:indexed-name>Zibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Žibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname><ce:given-name>Janez</ce:given-name></preferred-name></author><affiliation afid="60006286" dptid="104943018" country="svn"><organization>Natural Sciences and Information Technologies</organization><organization>University of Primorska</organization><address-part>Glagoljaška 8</address-part><city>Koper</city><postal-code>6000</postal-code><affiliation-id afid="60006286" dptid="104943018"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname><ce:given-name>France</ce:given-name></person><affiliation country="svn"><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city>Ljubljana</city><postal-code>1000</postal-code><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><publishercopyright>© Springer International Publishing Switzerland 2015.</publishercopyright><ce:para>This paper describes a speech database built from 17 Slovenian radio dramas. The dramas were obtained from the national radio-and-television station (RTV Slovenia) and were given at the universities disposal with an academic license for processing and annotating the audio material. The utterances of one male and one female speaker were transcribed, segmented and then annotated with emotional states of the speakers. The annotation of the emotional states was conducted in two stages with our own web-based application for crowd sourcing. The final (emotional) speech database consists of 1385 recordings of one male (975 recordings) and one female (410 recordings) speaker and contains labeled emotional speech with a total duration of around 1 hour and 15 minutes. The paper presents the two-stage annotation process used to label the data and demonstrates the usefulness of the employed annotation methodology. Baseline emotion recognition experiments are also presented. The reported results are presented with the un-weighted as well as weighted average recalls and precisions for 2-class and 7-class recognition experiments.</ce:para></abstract></abstracts><source srcid="25674" type="k" country="deu"><sourcetitle>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</sourcetitle><sourcetitle-abbrev>Lect. Notes Comput. Sci.</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</translated-sourcetitle><issuetitle>Text, Speech, and Dialogue - 18th International Conference, TSD 2015, Proceedings</issuetitle><issn type="electronic">16113349</issn><issn type="print">03029743</issn><isbn type="print" length="13" level="volume">9783319240329</isbn><volisspag><voliss volume="9302"/><pagerange first="351" last="359"/></volisspag><publicationyear first="2015"/><publicationdate><year>2015</year><date-text xfab-added="true">2015</date-text></publicationdate><website><ce:e-address type="email">http://springerlink.com/content/0302-9743/copyright/2005/</ce:e-address></website><contributor-group><contributor role="edit" seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>Matousek V.</ce:indexed-name><ce:surname>Matoušek</ce:surname><ce:given-name>Václav</ce:given-name></contributor></contributor-group><contributor-group><contributor role="edit" seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Kral P.</ce:indexed-name><ce:surname>Král</ce:surname><ce:given-name>Pavel</ce:given-name></contributor></contributor-group><publisher><publishername>Springer Verlag</publishername><ce:e-address type="email">service@springer.de</ce:e-address></publisher><additional-srcinfo><conferenceinfo><confevent><confname>18th International Conference on Text, Speech and Dialogue, TSD 2015</confname><confnumber>18th</confnumber><confseriestitle>International Conference on Text, Speech and Dialogue</confseriestitle><conflocation country="cze"><city>Pilsen</city></conflocation><confdate><startdate year="2015" month="09" day="14"/><enddate year="2015" month="09" day="17"/></confdate><confcode>141409</confcode><confsponsors complete="n"><confsponsor>Czech Society for Cybernetics and Informatics (CSKI)</confsponsor><confsponsor>International Speech Communication Association (ISCA)</confsponsor><confsponsor>Kerio Technologies</confsponsor></confsponsors></confevent></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification> <classification-code>723.3</classification-code> <classification-description>Database Systems</classification-description> </classification><classification> <classification-code>751.5</classification-code> <classification-description>Speech</classification-description> </classification><classification> <classification-code>752.3</classification-code> <classification-description>Sound Reproduction</classification-description> </classification></classifications><classifications type="FLXCLASS"><classification> <classification-code>902</classification-code> <classification-description>FLUIDEX; Related Topics</classification-description> </classification></classifications><classifications type="ASJC"><classification>2614</classification><classification>1700</classification></classifications><classifications type="SUBJABBR"><classification>MATH</classification><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="18"><reference id="1"><ref-info><ref-title><ref-titletext>Multi-modal emotional database: AvID</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">64249151844</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname></author><author seq="2"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname></author><author seq="3"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Podlesek A.</ce:indexed-name><ce:surname>Podlesek</ce:surname></author><author seq="5"><ce:initials>L.</ce:initials><ce:indexed-name>Komidar L.</ce:indexed-name><ce:surname>Komidar</ce:surname></author><author seq="6"><ce:initials>G.</ce:initials><ce:indexed-name>Socan G.</ce:indexed-name><ce:surname>Sočan</ce:surname></author><author seq="7"><ce:initials>B.</ce:initials><ce:indexed-name>Bajec B.</ce:indexed-name><ce:surname>Bajec</ce:surname></author></ref-authors><ref-sourcetitle>Informatica (Slovenia)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="33" issue="1"/><pagerange first="101" last="106"/></ref-volisspag></ref-info><ref-fulltext>Gajšek, R., Štruc, V., Mihelič, F., Podlesek, A., Komidar, L., Sočan, G., Bajec, B.: Multi-modal emotional database: AvID. Informatica (Slovenia) 33(1), 101–106 (2009)</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>The prosody of pet robot directed speech: Evidence from children</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">38749149760</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Batliner A.</ce:indexed-name><ce:surname>Batliner</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Biersack S.</ce:indexed-name><ce:surname>Biersack</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Steidl S.</ce:indexed-name><ce:surname>Steidl</ce:surname></author></ref-authors><ref-sourcetitle>Proc. of Speech Prosody</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><pagerange first="1" last="4"/></ref-volisspag></ref-info><ref-fulltext>Batliner, A., Biersack, S., Steidl, S.: The prosody of pet robot directed speech: evidence from children. In: Proc. of Speech Prosody, pp. 1–4 (2006)</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Emotion recognition from speech: A review</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84864692637</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Koolagudi S.</ce:indexed-name><ce:surname>Koolagudi</ce:surname></author><author seq="2"><ce:initials>K.</ce:initials><ce:indexed-name>Rao K.</ce:indexed-name><ce:surname>Rao</ce:surname></author></ref-authors><ref-sourcetitle>International Journal of Speech Technology</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><voliss volume="15" issue="2"/><pagerange first="99" last="117"/></ref-volisspag></ref-info><ref-fulltext>Koolagudi, S., Rao, K.: Emotion recognition from speech: a review. International Journal of Speech Technology 15(2), 99–117 (2012)</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Emotion recognition using linear transformations in combination with video</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">70450163582</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname></author><author seq="2"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Dobrisek S.</ce:indexed-name><ce:surname>Dobrišek</ce:surname></author><author seq="4"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname></author></ref-authors><ref-sourcetitle>10Th INTERSPEECH</ref-sourcetitle><ref-publicationyear first="2009"/></ref-info><ref-fulltext>Gajšek, R., Štruc, V., Dobrišek, S., Mihelič, F.: Emotion recognition using linear transformations in combination with video. In: 10th INTERSPEECH (2009)</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Gender and affect recognition based on GMM and GMM-UBM modeling with relevance MAP estimation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">79959823933</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Zibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname></author><author seq="3"><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname></author><author seq="4"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname></author><author seq="5"><ce:initials>B.</ce:initials><ce:indexed-name>Vesnicer B.</ce:indexed-name><ce:surname>Vesnicer</ce:surname></author><author seq="6"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelič</ce:surname></author></ref-authors><ref-sourcetitle>11Th INTERSPEECH</ref-sourcetitle><ref-publicationyear first="2010"/></ref-info><ref-fulltext>Gajšek, R., Žibert, J., Justin, T., Štruc, V., Vesnicer, B., Mihelič, F.: Gender and affect recognition based on GMM and GMM-UBM modeling with relevance MAP estimation. In: 11th INTERSPEECH (2010)</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Towards efficient multimodal emotion recognition</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84872968777</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Dobrisek S.</ce:indexed-name><ce:surname>Dobrišek</ce:surname></author><author seq="2"><ce:initials>R.</ce:initials><ce:indexed-name>Gajsek R.</ce:indexed-name><ce:surname>Gajšek</ce:surname></author><author seq="3"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelic</ce:surname></author><author seq="4"><ce:initials>N.</ce:initials><ce:indexed-name>Pavesic N.</ce:indexed-name><ce:surname>Pavešič</ce:surname></author><author seq="5"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname></author></ref-authors><ref-sourcetitle>International Journal of Advanced Robotic Systems</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="10" issue="53"/><pagerange first="1" last="10"/></ref-volisspag></ref-info><ref-fulltext>Dobrišek, S., Gajšek, R., Mihelic, F., Pavešič, N., Štruc, V.: Towards efficient multimodal emotion recognition. International Journal of Advanced Robotic Systems 10(53), 1–10 (2013)</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Describing the emotional states that are expressed in speech</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0037382510</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Cowie R.</ce:indexed-name><ce:surname>Cowie</ce:surname></author><author seq="2"><ce:initials>R.R.</ce:initials><ce:indexed-name>Cornelius R.R.</ce:indexed-name><ce:surname>Cornelius</ce:surname></author></ref-authors><ref-sourcetitle>Speech Communication</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><voliss volume="40" issue="1-2"/><pagerange first="5" last="32"/></ref-volisspag></ref-info><ref-fulltext>Cowie, R., Cornelius, R.R.: Describing the emotional states that are expressed in speech. Speech Communication 40(1–2), 5–32 (2003)</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Paralinguistics in speech and language – state-of-the-art and the challenge</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84867332081</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Schuller B.</ce:indexed-name><ce:surname>Schuller</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Steidl S.</ce:indexed-name><ce:surname>Steidl</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Batliner A.</ce:indexed-name><ce:surname>Batliner</ce:surname></author><author seq="4"><ce:initials>F.</ce:initials><ce:indexed-name>Burkhardt F.</ce:indexed-name><ce:surname>Burkhardt</ce:surname></author><author seq="5"><ce:initials>L.</ce:initials><ce:indexed-name>Devillers L.</ce:indexed-name><ce:surname>Devillers</ce:surname></author><author seq="6"><ce:initials>C.</ce:initials><ce:indexed-name>Muller C.</ce:indexed-name><ce:surname>Müller</ce:surname></author><author seq="7"><ce:initials>S.</ce:initials><ce:indexed-name>Narayanan S.</ce:indexed-name><ce:surname>Narayanan</ce:surname></author></ref-authors><ref-sourcetitle>Computer Speech &amp; Language</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="27" issue="1"/><pagerange first="4" last="39"/></ref-volisspag></ref-info><ref-fulltext>Schuller, B., Steidl, S., Batliner, A., Burkhardt, F., Devillers, L., Müller, C., Narayanan, S.: Paralinguistics in speech and language – state-of-the-art and the challenge. Computer Speech &amp; Language 27(1), 4–39 (2013)</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>A review of paralinguistic information processing for natural speech communication</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84875895016</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Yamashita Y.</ce:indexed-name><ce:surname>Yamashita</ce:surname></author></ref-authors><ref-sourcetitle>Acoustical Science and Technology</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="34" issue="2"/><pagerange first="73" last="79"/></ref-volisspag></ref-info><ref-fulltext>Yamashita, Y.: A review of paralinguistic information processing for natural speech communication. Acoustical Science and Technology 34(2), 73–79 (2013)</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Transcriber: Development and use of a tool for assisting speech corpora production</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0035148809</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Barras C.</ce:indexed-name><ce:surname>Barras</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Geoffrois E.</ce:indexed-name><ce:surname>Geoffrois</ce:surname></author><author seq="3"><ce:initials>Z.</ce:initials><ce:indexed-name>Wu Z.</ce:indexed-name><ce:surname>Wu</ce:surname></author><author seq="4"><ce:initials>M.</ce:initials><ce:indexed-name>Liberman M.</ce:indexed-name><ce:surname>Liberman</ce:surname></author></ref-authors><ref-sourcetitle>Speech Communication</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><voliss volume="33" issue="12"/><pagerange first="5" last="22"/></ref-volisspag></ref-info><ref-fulltext>Barras, C., Geoffrois, E., Wu, Z., Liberman, M.: Transcriber: Development and use of a tool for assisting speech corpora production. Speech Communication 33(1–2), 5–22 (2001)</ref-fulltext></reference><reference id="11"><ref-info><refd-itemidlist><itemid idtype="SGR">0003735139</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.R.</ce:initials><ce:indexed-name>Cornelius R.R.</ce:indexed-name><ce:surname>Cornelius</ce:surname></author></ref-authors><ref-sourcetitle>The Science of Emotion: Research and Tradition in the Psychology of Emotions</ref-sourcetitle><ref-publicationyear first="1996"/><ref-text>Prentice-Hall, Inc</ref-text></ref-info><ref-fulltext>Cornelius, R.R.: The science of emotion: Research and tradition in the psychology of emotions. Prentice-Hall, Inc. (1996)</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Theoretical approaches to emotion</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0038674461</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.R.</ce:initials><ce:indexed-name>Cornelius R.R.</ce:indexed-name><ce:surname>Cornelius</ce:surname></author></ref-authors><ref-sourcetitle>ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion</ref-sourcetitle><ref-publicationyear first="2000"/></ref-info><ref-fulltext>Cornelius, R.R.: Theoretical approaches to emotion. In: ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion (2000)</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>The rise of crowdsourcing</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33847246935</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Howe J.</ce:indexed-name><ce:surname>Howe</ce:surname></author></ref-authors><ref-sourcetitle>Wired Magazine</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="14" issue="6"/><pagerange first="1" last="4"/></ref-volisspag></ref-info><ref-fulltext>Howe, J.: The rise of crowdsourcing. Wired Magazine 14(6), 1–4 (2006)</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>Development of emotional Slovenian speech database based on radio drama – EmoLUKS</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84951834656</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Justin T.</ce:indexed-name><ce:surname>Justin</ce:surname></author><author seq="2"><ce:initials>F.</ce:initials><ce:indexed-name>Mihelic F.</ce:indexed-name><ce:surname>Mihelic</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Zibert J.</ce:indexed-name><ce:surname>Žibert</ce:surname></author></ref-authors><ref-sourcetitle>Language Technologies. Proceedings of the 17Th International Multiconference INFORMATION SOCIETY - IS 2014, Vol. G, Institut “Jožef Stefan” Ljubljana</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><pagerange first="157" last="162"/></ref-volisspag></ref-info><ref-fulltext>Justin, T., Mihelic, F., Žibert, J.: Development of emotional Slovenian speech database based on radio drama – EmoLUKS. In: Language Technologies. Proceedings of the 17th International Multiconference INFORMATION SOCIETY - IS 2014, vol. G, Institut “Jožef Stefan” Ljubljana, pp. 157–162 (2014)</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Recent developments in openSMILE, the Munich open-source multimedia feature extractor</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84887494391</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Eyben F.</ce:indexed-name><ce:surname>Eyben</ce:surname></author><author seq="2"><ce:initials>F.</ce:initials><ce:indexed-name>Weninger F.</ce:indexed-name><ce:surname>Weninger</ce:surname></author><author seq="3"><ce:initials>F.</ce:initials><ce:indexed-name>Gros F.</ce:indexed-name><ce:surname>Gros</ce:surname></author><author seq="4"><ce:initials>B.</ce:initials><ce:indexed-name>Schuller B.</ce:indexed-name><ce:surname>Schuller</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 21St ACM International Conference on Multimedia</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><pagerange first="835" last="838"/></ref-volisspag><ref-text>ACM</ref-text></ref-info><ref-fulltext>Eyben, F., Weninger, F., Gros, F., Schuller, B.: Recent developments in openSMILE, the Munich open-source multimedia feature extractor. In: Proceedings of the 21st ACM International Conference on Multimedia, pp. 835–838. ACM (2013)</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>The Interspeech 2009 emotion challenge</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">70450206416</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Schuller B.</ce:indexed-name><ce:surname>Schuller</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Steidl S.</ce:indexed-name><ce:surname>Steidl</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Batliner A.</ce:indexed-name><ce:surname>Batliner</ce:surname></author></ref-authors><ref-sourcetitle>10Th INTERSPEECH</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="312" last="315"/></ref-volisspag></ref-info><ref-fulltext>Schuller, B., Steidl, S., Batliner, A.: The Interspeech 2009 emotion challenge. In: 10th INTERSPEECH, pp. 312–315 (2009)</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>Improvements to Platt’s SMO algorithm for SVM classifier design</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000545946</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Keerthi S.</ce:indexed-name><ce:surname>Keerthi</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Shevade S.</ce:indexed-name><ce:surname>Shevade</ce:surname></author><author seq="3"><ce:initials>C.</ce:initials><ce:indexed-name>Bhattacharyya C.</ce:indexed-name><ce:surname>Bhattacharyya</ce:surname></author><author seq="4"><ce:initials>K.</ce:initials><ce:indexed-name>Murthy K.</ce:indexed-name><ce:surname>Murthy</ce:surname></author></ref-authors><ref-sourcetitle>Neural Computation</ref-sourcetitle><ref-publicationyear first="2001"/><ref-volisspag><voliss volume="13" issue="3"/><pagerange first="637" last="649"/></ref-volisspag></ref-info><ref-fulltext>Keerthi, S., Shevade, S., Bhattacharyya, C., Murthy, K.: Improvements to Platt’s SMO algorithm for SVM classifier design. Neural Computation 13(3), 637–649 (2001)</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>The WEKA data mining software: An update</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">76749092270</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Hall M.</ce:indexed-name><ce:surname>Hall</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Frank E.</ce:indexed-name><ce:surname>Frank</ce:surname></author><author seq="3"><ce:initials>G.</ce:initials><ce:indexed-name>Holmes G.</ce:indexed-name><ce:surname>Holmes</ce:surname></author><author seq="4"><ce:initials>B.</ce:initials><ce:indexed-name>Pfahringer B.</ce:indexed-name><ce:surname>Pfahringer</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Reutemann P.</ce:indexed-name><ce:surname>Reutemann</ce:surname></author><author seq="6"><ce:initials>I.H.</ce:initials><ce:indexed-name>Witten I.H.</ce:indexed-name><ce:surname>Witten</ce:surname></author></ref-authors><ref-sourcetitle>ACM SIGKDD Explorations Newsletter</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="11" issue="1"/><pagerange first="10" last="18"/></ref-volisspag></ref-info><ref-fulltext>Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: The WEKA data mining software: an update. ACM SIGKDD Explorations Newsletter 11(1), 10–18 (2009)</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>