<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/84994098432</prism:url><dc:identifier>SCOPUS_ID:84994098432</dc:identifier><eid>2-s2.0-84994098432</eid><pii>S1077314216301187</pii><prism:doi>10.1016/j.cviu.2016.08.010</prism:doi><dc:title>A local-global coupled-layer puppet model for robust online human pose tracking</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>4</citedby-count><prism:publicationName>Computer Vision and Image Understanding</prism:publicationName><dc:publisher>Academic Press Inc.apjcs@harcourt.com</dc:publisher><source-id>24161</source-id><prism:issn>1090235X 10773142</prism:issn><prism:volume>153</prism:volume><prism:startingPage>163</prism:startingPage><prism:endingPage>178</prism:endingPage><prism:pageRange>163-178</prism:pageRange><prism:coverDate>2016-12-01</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="56285857400"><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56285857400</author-url><affiliation id="60031031" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031031"/><affiliation id="60019702" href="https://api.elsevier.com/content/affiliation/affiliation_id/60019702"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><publishercopyright>© 2016 Elsevier Inc.</publishercopyright><ce:para>This paper addresses the problem of online tracking of articulated human body poses in dynamic environments. Many previous approaches perform poorly in realistic applications: often future frames or entire sequences are used anticausally to mutually refine the poses in each individual frame, making online tracking impossible; tracking often relies on strong assumptions about e.g. clothing styles, body-part colours and constraints on body-part motion ranges, limiting such algorithms to a particular dataset; the use of holistic feature models limits the ability of optimisation-based matching to distinguish between pose errors of different body parts. We overcome these problems by proposing a coupled-layer framework, which uses the previous notions of deformable structure (DS) puppet models. The underlying idea is to decompose the global pose candidate in any particular frame into several local parts to obtain a refined pose. We introduce an adaptive penalty with our model to improve the searching scope for a local part pose, and also to overcome the problem of using fixed constraints. Since the pose is computed using only current and previous frames, our method is suitable for online sequential tracking. We have carried out empirical experiments using three different public benchmark datasets, comparing two variants of our algorithm against four recent state-of-the-art (SOA) methods from the literature. The results suggest comparatively strong performance of our method, regardless of weaker constraints and fewer assumptions about the scene, and despite the fact that our algorithm is performing online sequential tracking, whereas the comparison methods perform mutual optimisation backwards and forwards over all frames of the entire video sequence.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/84994098432" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=84994098432&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=84994098432&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60108932" href="https://api.elsevier.com/content/affiliation/affiliation_id/60108932"><affilname>KUKA Aktiengesellschaft, Europe</affilname><affiliation-city>Augsburg</affiliation-city><affiliation-country>Germany</affiliation-country></affiliation><affiliation id="60031031" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031031"><affilname>Shandong University</affilname><affiliation-city>Jinan</affiliation-city><affiliation-country>China</affiliation-country></affiliation><affiliation id="60019702" href="https://api.elsevier.com/content/affiliation/affiliation_id/60019702"><affilname>University of Birmingham</affilname><affiliation-city>Birmingham</affiliation-city><affiliation-country>United Kingdom</affiliation-country></affiliation><authors><author seq="1" auid="56285857400"><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56285857400</author-url><affiliation id="60031031" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031031"/><affiliation id="60019702" href="https://api.elsevier.com/content/affiliation/affiliation_id/60019702"/></author><author seq="2" auid="55178519000"><ce:initials>N.</ce:initials><ce:indexed-name>Marturi N.</ce:indexed-name><ce:surname>Marturi</ce:surname><ce:given-name>Naresh</ce:given-name><preferred-name><ce:initials>N.</ce:initials><ce:indexed-name>Marturi N.</ce:indexed-name><ce:surname>Marturi</ce:surname><ce:given-name>Naresh</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/55178519000</author-url><affiliation id="60019702" href="https://api.elsevier.com/content/affiliation/affiliation_id/60019702"/><affiliation id="60108932" href="https://api.elsevier.com/content/affiliation/affiliation_id/60108932"/></author><author seq="3" auid="55907838900"><ce:initials>Y.</ce:initials><ce:indexed-name>Li Y.</ce:indexed-name><ce:surname>Li</ce:surname><ce:given-name>Yibin</ce:given-name><preferred-name><ce:initials>Y.</ce:initials><ce:indexed-name>Li Y.</ce:indexed-name><ce:surname>Li</ce:surname><ce:given-name>Yibin</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/55907838900</author-url><affiliation id="60031031" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031031"/></author><author seq="4" auid="16025500800"><ce:initials>R.</ce:initials><ce:indexed-name>Stolkin R.</ce:indexed-name><ce:surname>Stolkin</ce:surname><ce:given-name>Rustam</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Stolkin R.</ce:indexed-name><ce:surname>Stolkin</ce:surname><ce:given-name>Rustam</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/16025500800</author-url><affiliation id="60019702" href="https://api.elsevier.com/content/affiliation/affiliation_id/60019702"/></author><author seq="5" auid="7003317327"><ce:initials>A.</ce:initials><ce:indexed-name>Leonardis A.</ce:indexed-name><ce:surname>Leonardis</ce:surname><ce:given-name>Ales</ce:given-name><preferred-name><ce:initials>A.</ce:initials><ce:indexed-name>Leonardis A.</ce:indexed-name><ce:surname>Leonardis</ce:surname><ce:given-name>Ales</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7003317327</author-url><affiliation id="60019702" href="https://api.elsevier.com/content/affiliation/affiliation_id/60019702"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Coupled-layer model</author-keyword><author-keyword>Human pose tracking</author-keyword><author-keyword>Human tracking</author-keyword><author-keyword>Pose estimation</author-keyword><author-keyword>Video tracking</author-keyword></authkeywords><idxterms><mainterm weight="b" candidate="n">Human pose tracking</mainterm><mainterm weight="b" candidate="n">Human Tracking</mainterm><mainterm weight="b" candidate="n">Layer model</mainterm><mainterm weight="b" candidate="n">Pose estimation</mainterm><mainterm weight="b" candidate="n">Video tracking</mainterm></idxterms><subject-areas><subject-area code="1712" abbrev="COMP">Software</subject-area><subject-area code="1711" abbrev="COMP">Signal Processing</subject-area><subject-area code="1707" abbrev="COMP">Computer Vision and Pattern Recognition</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2017-12-20T17:18:23.331Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/car-curated</xocs:funding-addon-type><xocs:funding><xocs:funding-agency-matched-string>Engineering and Physical Sciences Research Council</xocs:funding-agency-matched-string><xocs:funding-id>EP/N019415/1</xocs:funding-id><xocs:funding-agency-acronym>EPSRC</xocs:funding-agency-acronym><xocs:funding-agency>Engineering and Physical Sciences Research Council</xocs:funding-agency><xocs:funding-agency-id>http://data.elsevier.com/vocabulary/SciValFunders/501100000266</xocs:funding-agency-id><xocs:funding-agency-country>http://sws.geonames.org/2635167/</xocs:funding-agency-country></xocs:funding></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered year="2019" month="08" day="18" timestamp="2019-08-18T06:31:49.000049-04:00"/><ait:date-sort year="2016" month="12" day="01"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2017 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:pii>S1077314216301187</ce:pii><ce:doi>10.1016/j.cviu.2016.08.010</ce:doi><itemid idtype="PUI">613089430</itemid><itemid idtype="CAR-ID">657807194</itemid><itemid idtype="CPX">20164502985342</itemid><itemid idtype="SCP">84994098432</itemid><itemid idtype="SGR">84994098432</itemid></itemidlist><history><date-created year="2016" month="11" day="08" timestamp="BST 04:22:02"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">Coupled-layer model</author-keyword><author-keyword xml:lang="eng">Human pose tracking</author-keyword><author-keyword xml:lang="eng">Human tracking</author-keyword><author-keyword xml:lang="eng">Pose estimation</author-keyword><author-keyword xml:lang="eng">Video tracking</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">A local-global coupled-layer puppet model for robust online human pose tracking</titletext></citation-title><author-group><author auid="56285857400" seq="1" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name></preferred-name></author><author auid="55907838900" seq="3" type="auth"><ce:initials>Y.</ce:initials><ce:indexed-name>Li Y.</ce:indexed-name><ce:surname>Li</ce:surname><ce:given-name>Yibin</ce:given-name><preferred-name><ce:initials>Y.</ce:initials><ce:indexed-name>Li Y.</ce:indexed-name><ce:surname>Li</ce:surname><ce:given-name>Yibin</ce:given-name></preferred-name></author><affiliation afid="60031031" country="chn"><organization>Shandong University</organization><address-part>Jinan, Shandong, 250061</address-part><affiliation-id afid="60031031"/><country>China</country></affiliation></author-group><author-group><author auid="56285857400" seq="1" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name></preferred-name></author><author auid="55178519000" seq="2" type="auth"><ce:initials>N.</ce:initials><ce:indexed-name>Marturi N.</ce:indexed-name><ce:surname>Marturi</ce:surname><ce:given-name>Naresh</ce:given-name><preferred-name><ce:initials>N.</ce:initials><ce:indexed-name>Marturi N.</ce:indexed-name><ce:surname>Marturi</ce:surname><ce:given-name>Naresh</ce:given-name></preferred-name></author><author auid="16025500800" seq="4" type="auth"><ce:initials>R.</ce:initials><ce:indexed-name>Stolkin R.</ce:indexed-name><ce:surname>Stolkin</ce:surname><ce:given-name>Rustam</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Stolkin R.</ce:indexed-name><ce:surname>Stolkin</ce:surname><ce:given-name>Rustam</ce:given-name></preferred-name></author><author auid="7003317327" seq="5" type="auth" date-locked="2017-04-05T02:15:44.929"><ce:initials>A.</ce:initials><ce:indexed-name>Leonardis A.</ce:indexed-name><ce:surname>Leonardis</ce:surname><ce:given-name>Ales</ce:given-name><preferred-name><ce:initials>A.</ce:initials><ce:indexed-name>Leonardis A.</ce:indexed-name><ce:surname>Leonardis</ce:surname><ce:given-name>Ales</ce:given-name></preferred-name></author><affiliation afid="60019702" country="gbr"><organization>University of Birmingham</organization><address-part>Edgbaston, Birmingham, B15 2TT</address-part><affiliation-id afid="60019702"/><country>United Kingdom</country></affiliation></author-group><author-group><author auid="55178519000" seq="2" type="auth"><ce:initials>N.</ce:initials><ce:indexed-name>Marturi N.</ce:indexed-name><ce:surname>Marturi</ce:surname><ce:given-name>Naresh</ce:given-name><preferred-name><ce:initials>N.</ce:initials><ce:indexed-name>Marturi N.</ce:indexed-name><ce:surname>Marturi</ce:surname><ce:given-name>Naresh</ce:given-name></preferred-name></author><affiliation afid="60108932" country="gbr"><organization>KUKA robotics UK Ltd.</organization><address-part>Wednesbury Great Western Street, WS10 7LL</address-part><affiliation-id afid="60108932"/><country>United Kingdom</country></affiliation></author-group><correspondence><person><ce:initials>M.</ce:initials><ce:indexed-name>Ma M.</ce:indexed-name><ce:surname>Ma</ce:surname><ce:given-name>Miao</ce:given-name></person><affiliation country="chn"><organization>Shandong University</organization><address-part>Jinan, Shandong, 250061</address-part><country>China</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><publishercopyright>© 2016 Elsevier Inc.</publishercopyright><ce:para>This paper addresses the problem of online tracking of articulated human body poses in dynamic environments. Many previous approaches perform poorly in realistic applications: often future frames or entire sequences are used anticausally to mutually refine the poses in each individual frame, making online tracking impossible; tracking often relies on strong assumptions about e.g. clothing styles, body-part colours and constraints on body-part motion ranges, limiting such algorithms to a particular dataset; the use of holistic feature models limits the ability of optimisation-based matching to distinguish between pose errors of different body parts. We overcome these problems by proposing a coupled-layer framework, which uses the previous notions of deformable structure (DS) puppet models. The underlying idea is to decompose the global pose candidate in any particular frame into several local parts to obtain a refined pose. We introduce an adaptive penalty with our model to improve the searching scope for a local part pose, and also to overcome the problem of using fixed constraints. Since the pose is computed using only current and previous frames, our method is suitable for online sequential tracking. We have carried out empirical experiments using three different public benchmark datasets, comparing two variants of our algorithm against four recent state-of-the-art (SOA) methods from the literature. The results suggest comparatively strong performance of our method, regardless of weaker constraints and fewer assumptions about the scene, and despite the fact that our algorithm is performing online sequential tracking, whereas the comparison methods perform mutual optimisation backwards and forwards over all frames of the entire video sequence.</ce:para></abstract></abstracts><source srcid="24161" type="j" country="usa"><sourcetitle>Computer Vision and Image Understanding</sourcetitle><sourcetitle-abbrev>Comput Vision Image Understanding</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">Computer Vision and Image Understanding</translated-sourcetitle><issn type="electronic">1090235X</issn><issn type="print">10773142</issn><codencode>CVIUF</codencode><volisspag><voliss volume="153"/><pagerange first="163" last="178"/></volisspag><publicationyear first="2016"/><publicationdate><year>2016</year><month>12</month><day>01</day><date-text xfab-added="true">1 December 2016</date-text></publicationdate><website><ce:e-address type="email">http://www.elsevier.com/inca/publications/store/6/2/2/8/0/9/index.htt</ce:e-address></website><publisher><publishername>Academic Press Inc.</publishername><ce:e-address type="email">apjcs@harcourt.com</ce:e-address></publisher></source><enhancement><classificationgroup><classifications type="ASJC"><classification>1712</classification><classification>1711</classification><classification>1707</classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="36"><reference id="1"><ref-info><ref-title><ref-titletext>Estimating human dynamics on-the-fly using monocular video for pose estimation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84884929019</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Agarwal P.</ce:indexed-name><ce:surname>Agarwal</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Kumar S.</ce:indexed-name><ce:surname>Kumar</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Ryde J.</ce:indexed-name><ce:surname>Ryde</ce:surname></author><author seq="4"><ce:initials>J.J.</ce:initials><ce:indexed-name>Corso J.J.</ce:indexed-name><ce:surname>Corso</ce:surname></author><author seq="5"><ce:initials>V.N.</ce:initials><ce:indexed-name>Krovi V.N.</ce:indexed-name><ce:surname>Krovi</ce:surname></author></ref-authors><ref-sourcetitle>Robotics: Science and Systems</ref-sourcetitle><ref-publicationyear first="2012"/><ref-text>Citeseer</ref-text></ref-info><ref-fulltext>Agarwal, P., Kumar, S., Ryde, J., Corso, J.J., Krovi, V.N., Estimating human dynamics on-the-fly using monocular video for pose estimation. Robotics: Science and Systems, 2012, Citeseer.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>SCAPE: shape completion and animation of people</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33646057375</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Anguelov D.</ce:indexed-name><ce:surname>Anguelov</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Srinivasan P.</ce:indexed-name><ce:surname>Srinivasan</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Koller D.</ce:indexed-name><ce:surname>Koller</ce:surname></author><author seq="4"><ce:initials>S.</ce:initials><ce:indexed-name>Thrun S.</ce:indexed-name><ce:surname>Thrun</ce:surname></author><author seq="5"><ce:initials>J.</ce:initials><ce:indexed-name>Rodgers J.</ce:indexed-name><ce:surname>Rodgers</ce:surname></author><author seq="6"><ce:initials>J.</ce:initials><ce:indexed-name>Davis J.</ce:indexed-name><ce:surname>Davis</ce:surname></author></ref-authors><ref-sourcetitle>ACM Transactions on Graphics (TOG)</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="24"/><pagerange first="408" last="416"/></ref-volisspag><ref-text>ACM</ref-text></ref-info><ref-fulltext>Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J., SCAPE: shape completion and animation of people. ACM Transactions on Graphics (TOG), Vol. 24, 2005, ACM, 408–416.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Upper body detection and tracking in extended signing sequences</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80052651120</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Buehler P.</ce:indexed-name><ce:surname>Buehler</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Everingham M.</ce:indexed-name><ce:surname>Everingham</ce:surname></author><author seq="3"><ce:initials>D.P.</ce:initials><ce:indexed-name>Huttenlocher D.P.</ce:indexed-name><ce:surname>Huttenlocher</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Zisserman A.</ce:indexed-name><ce:surname>Zisserman</ce:surname></author></ref-authors><ref-sourcetitle>Int. J. Comput. Vis. (IJCV).</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="95" issue="2"/><pagerange first="180" last="197"/></ref-volisspag></ref-info><ref-fulltext>Buehler, P., Everingham, M., Huttenlocher, D.P., Zisserman, A., Upper body detection and tracking in extended signing sequences. Int. J. Comput. Vis. (IJCV). 95:2 (2011), 180–197.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Merging pose estimates across space and time</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898451885</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>X.</ce:initials><ce:indexed-name>Burgos-Artizzu X.</ce:indexed-name><ce:surname>Burgos-Artizzu</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Hall D.</ce:indexed-name><ce:surname>Hall</ce:surname></author><author seq="3"><ce:initials>P.</ce:initials><ce:indexed-name>Perona P.</ce:indexed-name><ce:surname>Perona</ce:surname></author><author seq="4"><ce:initials>P.</ce:initials><ce:indexed-name>Dollar P.</ce:indexed-name><ce:surname>Dollár</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the British Machine Vision Conference. BMVC Press</ref-sourcetitle><ref-publicationyear first="2013"/><ref-text>Citeseer</ref-text></ref-info><ref-fulltext>Burgos-Artizzu, X., Hall, D., Perona, P., Dollár, P., Merging pose estimates across space and time. Proceedings of the British Machine Vision Conference. BMVC Press, 2013, Citeseer.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Social behavior recognition in continuous video</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84866698755</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>X.P.</ce:initials><ce:indexed-name>Burgos-Artizzu X.P.</ce:indexed-name><ce:surname>Burgos-Artizzu</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Dollar P.</ce:indexed-name><ce:surname>Dollár</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Lin D.</ce:indexed-name><ce:surname>Lin</ce:surname></author><author seq="4"><ce:initials>D.J.</ce:initials><ce:indexed-name>Anderson D.J.</ce:indexed-name><ce:surname>Anderson</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Perona P.</ce:indexed-name><ce:surname>Perona</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="1322" last="1329"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Burgos-Artizzu, X.P., Dollár, P., Lin, D., Anderson, D.J., Perona, P., Social behavior recognition in continuous video. Conference on Computer Vision and Pattern Recognition (CVPR), 2012, IEEE, 1322–1329.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Robust visual tracking using an adaptive coupled-layer visual model</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84874519372</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Cehovin L.</ce:indexed-name><ce:surname>Cehovin</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Kristan M.</ce:indexed-name><ce:surname>Kristan</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Leonardis A.</ce:indexed-name><ce:surname>Leonardis</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI).</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="35" issue="4"/><pagerange first="941" last="953"/></ref-volisspag></ref-info><ref-fulltext>Cehovin, L., Kristan, M., Leonardis, A., Robust visual tracking using an adaptive coupled-layer visual model. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI). 35:4 (2013), 941–953.</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Domain adaptation for upper body pose tracking in signed tv broadcasts</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898465319</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Charles J.</ce:indexed-name><ce:surname>Charles</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Pfister T.</ce:indexed-name><ce:surname>Pfister</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Magee D.</ce:indexed-name><ce:surname>Magee</ce:surname></author><author seq="4"><ce:initials>D.</ce:initials><ce:indexed-name>Hogg D.</ce:indexed-name><ce:surname>Hogg</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Zisserman A.</ce:indexed-name><ce:surname>Zisserman</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the British Machine Vision Conference</ref-sourcetitle><ref-publicationyear first="2013"/><ref-text>BMVC Press</ref-text></ref-info><ref-fulltext>Charles, J., Pfister, T., Magee, D., Hogg, D., Zisserman, A., Domain adaptation for upper body pose tracking in signed tv broadcasts. Proceedings of the British Machine Vision Conference, 2013, BMVC Press.</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Articulated pose estimation by a graphical model with image dependent pairwise relations</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84937873698</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>X.</ce:initials><ce:indexed-name>Chen X.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="2"><ce:initials>A.L.</ce:initials><ce:indexed-name>Yuille A.L.</ce:indexed-name><ce:surname>Yuille</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems (NIPS)</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><pagerange first="1736" last="1744"/></ref-volisspag></ref-info><ref-fulltext>Chen, X., Yuille, A.L., Articulated pose estimation by a graphical model with image dependent pairwise relations. Advances in Neural Information Processing Systems (NIPS), 2014, 1736–1744.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Mixing body-part sequences for human pose estimation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84911446929</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Cherian A.</ce:indexed-name><ce:surname>Cherian</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Mairal J.</ce:indexed-name><ce:surname>Mairal</ce:surname></author><author seq="3"><ce:initials>K.</ce:initials><ce:indexed-name>Alahari K.</ce:indexed-name><ce:surname>Alahari</ce:surname></author><author seq="4"><ce:initials>C.</ce:initials><ce:indexed-name>Schmid C.</ce:indexed-name><ce:surname>Schmid</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><pagerange first="2361" last="2368"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Cherian, A., Mairal, J., Alahari, K., Schmid, C., Mixing body-part sequences for human pose estimation. Conference on Computer Vision and Pattern Recognition (CVPR), 2014, IEEE, 2361–2368.</ref-fulltext></reference><reference id="10"><ref-info><ref-title><ref-titletext>Adaptive occlusion state estimation for human pose tracking under self-occlusions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84870249775</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.G.</ce:initials><ce:indexed-name>Cho N.G.</ce:indexed-name><ce:surname>Cho</ce:surname></author><author seq="2"><ce:initials>A.L.</ce:initials><ce:indexed-name>Yuille A.L.</ce:indexed-name><ce:surname>Yuille</ce:surname></author><author seq="3"><ce:initials>S.W.</ce:initials><ce:indexed-name>Lee S.W.</ce:indexed-name><ce:surname>Lee</ce:surname></author></ref-authors><ref-sourcetitle>Pattern Recognit.</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="46" issue="3"/><pagerange first="649" last="661"/></ref-volisspag></ref-info><ref-fulltext>Cho, N.G., Yuille, A.L., Lee, S.W., Adaptive occlusion state estimation for human pose tracking under self-occlusions. Pattern Recognit. 46:3 (2013), 649–661.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Human pose estimation using body parts dependent joint regressors</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84887344431</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Dantone M.</ce:indexed-name><ce:surname>Dantone</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Gall J.</ce:indexed-name><ce:surname>Gall</ce:surname></author><author seq="3"><ce:initials>C.</ce:initials><ce:indexed-name>Leistner C.</ce:indexed-name><ce:surname>Leistner</ce:surname></author><author seq="4"><ce:initials>L.</ce:initials><ce:indexed-name>Van Gool L.</ce:indexed-name><ce:surname>Van Gool</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><pagerange first="3041" last="3048"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Dantone, M., Gall, J., Leistner, C., Van Gool, L., Human pose estimation using body parts dependent joint regressors. Conference on Computer Vision and Pattern Recognition (CVPR), 2013, IEEE, 3041–3048.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>2d articulated human pose estimation and retrieval in (almost) unconstrained still images</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84863625140</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Eichner M.</ce:indexed-name><ce:surname>Eichner</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Marin-Jimenez M.</ce:indexed-name><ce:surname>Marin-Jimenez</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Zisserman A.</ce:indexed-name><ce:surname>Zisserman</ce:surname></author><author seq="4"><ce:initials>V.</ce:initials><ce:indexed-name>Ferrari V.</ce:indexed-name><ce:surname>Ferrari</ce:surname></author></ref-authors><ref-sourcetitle>Int. J. Comput. Vis. (IJCV).</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><voliss volume="99" issue="2"/><pagerange first="190" last="214"/></ref-volisspag></ref-info><ref-fulltext>Eichner, M., Marin-Jimenez, M., Zisserman, A., Ferrari, V., 2d articulated human pose estimation and retrieval in (almost) unconstrained still images. Int. J. Comput. Vis. (IJCV). 99:2 (2012), 190–214.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Pictorial structures for object recognition</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">4644354464</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.F.</ce:initials><ce:indexed-name>Felzenszwalb P.F.</ce:indexed-name><ce:surname>Felzenszwalb</ce:surname></author><author seq="2"><ce:initials>D.P.</ce:initials><ce:indexed-name>Huttenlocher D.P.</ce:indexed-name><ce:surname>Huttenlocher</ce:surname></author></ref-authors><ref-sourcetitle>Int. J. Comput. Vis. (IJCV).</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="61" issue="1"/><pagerange first="55" last="79"/></ref-volisspag></ref-info><ref-fulltext>Felzenszwalb, P.F., Huttenlocher, D.P., Pictorial structures for object recognition. Int. J. Comput. Vis. (IJCV). 61:1 (2005), 55–79.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>2D human pose estimation in tv shows</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84872232274</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>Ferrari V.</ce:indexed-name><ce:surname>Ferrari</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Marin-Jimenez M.</ce:indexed-name><ce:surname>Marín-Jiménez</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Zisserman A.</ce:indexed-name><ce:surname>Zisserman</ce:surname></author></ref-authors><ref-sourcetitle>Statistical and Geometrical Approaches to Visual Motion Analysis</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="128" last="147"/></ref-volisspag><ref-text>Springer</ref-text></ref-info><ref-fulltext>Ferrari, V., Marín-Jiménez, M., Zisserman, A., 2D human pose estimation in tv shows. Statistical and Geometrical Approaches to Visual Motion Analysis, 2009, Springer, 128–147.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>The representation and matching of pictorial structures</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0015567825</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.A.</ce:initials><ce:indexed-name>Fischler M.A.</ce:indexed-name><ce:surname>Fischler</ce:surname></author><author seq="2"><ce:initials>R.A.</ce:initials><ce:indexed-name>Elschlager R.A.</ce:indexed-name><ce:surname>Elschlager</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Comput.</ref-sourcetitle><ref-publicationyear first="1973"/><ref-volisspag><voliss volume="22" issue="1"/><pagerange first="67" last="92"/></ref-volisspag></ref-info><ref-fulltext>Fischler, M.A., Elschlager, R.A., The representation and matching of pictorial structures. IEEE Trans. Comput. 22:1 (1973), 67–92.</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Pose from flow and flow from pose</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84887325363</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Fragkiadaki K.</ce:indexed-name><ce:surname>Fragkiadaki</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Hu H.</ce:indexed-name><ce:surname>Hu</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Shi J.</ce:indexed-name><ce:surname>Shi</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><pagerange first="2059" last="2066"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Fragkiadaki, K., Hu, H., Shi, J., Pose from flow and flow from pose. Conference on Computer Vision and Pattern Recognition (CVPR), 2013, IEEE, 2059–2066.</ref-fulltext></reference><reference id="17"><ref-info><ref-title><ref-titletext>Contour people: a parameterized model of 2D articulated human shape</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">77956004470</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Freifeld O.</ce:indexed-name><ce:surname>Freifeld</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Weiss A.</ce:indexed-name><ce:surname>Weiss</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Zuffi S.</ce:indexed-name><ce:surname>Zuffi</ce:surname></author><author seq="4"><ce:initials>M.J.</ce:initials><ce:indexed-name>Black M.J.</ce:indexed-name><ce:surname>Black</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><pagerange first="639" last="646"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Freifeld, O., Weiss, A., Zuffi, S., Black, M.J., Contour people: a parameterized model of 2D articulated human shape. Conference on Computer Vision and Pattern Recognition (CVPR), 2010, IEEE, 639–646.</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Model-based vision: a program to see a walking person</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0020968792</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Hogg D.</ce:indexed-name><ce:surname>Hogg</ce:surname></author></ref-authors><ref-sourcetitle>Image Vis. Computi.</ref-sourcetitle><ref-publicationyear first="1983"/><ref-volisspag><voliss volume="1" issue="1"/><pagerange first="5" last="20"/></ref-volisspag></ref-info><ref-fulltext>Hogg, D., Model-based vision: a program to see a walking person. Image Vis. Computi. 1:1 (1983), 5–20.</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>Human pose tracking using multi-level structured models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33745824280</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.W.</ce:initials><ce:indexed-name>Lee M.W.</ce:indexed-name><ce:surname>Lee</ce:surname></author><author seq="2"><ce:initials>R.</ce:initials><ce:indexed-name>Nevatia R.</ce:indexed-name><ce:surname>Nevatia</ce:surname></author></ref-authors><ref-sourcetitle>European Conference on Computer Vision (ECCV)</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><pagerange first="368" last="381"/></ref-volisspag><ref-text>Springer Berlin Heidelberg</ref-text></ref-info><ref-fulltext>Lee, M.W., Nevatia, R., Human pose tracking using multi-level structured models. European Conference on Computer Vision (ECCV), 2006, Springer, Berlin Heidelberg, 368–381.</ref-fulltext></reference><reference id="20"><ref-info><refd-itemidlist><itemid idtype="SGR">77953616632</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Liu C.</ce:indexed-name><ce:surname>Liu</ce:surname></author></ref-authors><ref-sourcetitle>Beyond Pixels: Exploring New Representations and Applications for Motion Analysis</ref-sourcetitle><ref-publicationyear first="2009"/><ref-text>Ph.D. thesis Massachusetts Institute of Technology</ref-text></ref-info><ref-fulltext>Liu, C., Beyond Pixels: Exploring New Representations and Applications for Motion Analysis, 2009, Ph.D. thesis Massachusetts Institute of Technology.</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>Tracking people's hands and feet using mixed network and/or search</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84875477100</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.I.</ce:initials><ce:indexed-name>Morariu V.I.</ce:indexed-name><ce:surname>Morariu</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Harwood D.</ce:indexed-name><ce:surname>Harwood</ce:surname></author><author seq="3"><ce:initials>L.S.</ce:initials><ce:indexed-name>Davis L.S.</ce:indexed-name><ce:surname>Davis</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI).</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="35" issue="5"/><pagerange first="1248" last="1262"/></ref-volisspag></ref-info><ref-fulltext>Morariu, V.I., Harwood, D., Davis, L.S., Tracking people's hands and feet using mixed network and/or search. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI). 35:5 (2013), 1248–1262.</ref-fulltext></reference><reference id="22"><ref-info><ref-title><ref-titletext>A survey on transfer learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">77956031473</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.J.</ce:initials><ce:indexed-name>Pan S.J.</ce:indexed-name><ce:surname>Pan</ce:surname></author><author seq="2"><ce:initials>Q.</ce:initials><ce:indexed-name>Yang Q.</ce:indexed-name><ce:surname>Yang</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Knowl. Data Eng.</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><voliss volume="22" issue="10"/><pagerange first="1345" last="1359"/></ref-volisspag></ref-info><ref-fulltext>Pan, S.J., Yang, Q., A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 22:10 (2010), 1345–1359.</ref-fulltext></reference><reference id="23"><ref-info><ref-title><ref-titletext>N-best maximal decoders for part models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84856682999</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Park D.</ce:indexed-name><ce:surname>Park</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Ramanan D.</ce:indexed-name><ce:surname>Ramanan</ce:surname></author></ref-authors><ref-sourcetitle>International Conference on Computer Vision (ICCV)</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><pagerange first="2627" last="2634"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Park, D., Ramanan, D., N-best maximal decoders for part models. International Conference on Computer Vision (ICCV), 2011, IEEE, 2627–2634.</ref-fulltext></reference><reference id="24"><ref-info><ref-title><ref-titletext>Integration of bottom-up/top-down approaches for 2D pose estimation using probabilistic gaussian modelling</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">78751645910</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Paul K.</ce:indexed-name><ce:surname>Paul</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Dimitrios M.</ce:indexed-name><ce:surname>Dimitrios</ce:surname></author><author seq="3"><ce:initials>N.</ce:initials><ce:indexed-name>Jean-Christophe N.</ce:indexed-name><ce:surname>Jean-Christophe</ce:surname></author></ref-authors><ref-sourcetitle>Comput. Vis. Image Underst. (CVIU).</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="115" issue="2"/><pagerange first="242" last="255"/></ref-volisspag></ref-info><ref-fulltext>Paul, K., Dimitrios, M., Jean-Christophe, N., Integration of bottom-up/top-down approaches for 2D pose estimation using probabilistic gaussian modelling. Comput. Vis. Image Underst. (CVIU). 115:2 (2011), 242–255.</ref-fulltext></reference><reference id="25"><ref-info><ref-title><ref-titletext>Articulated people detection and pose estimation: reshaping the future</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84866713740</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Pishchulin L.</ce:indexed-name><ce:surname>Pishchulin</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Jain A.</ce:indexed-name><ce:surname>Jain</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Andriluka M.</ce:indexed-name><ce:surname>Andriluka</ce:surname></author><author seq="4"><ce:initials>T.</ce:initials><ce:indexed-name>Thormahlen T.</ce:indexed-name><ce:surname>Thormahlen</ce:surname></author><author seq="5"><ce:initials>B.</ce:initials><ce:indexed-name>Schiele B.</ce:indexed-name><ce:surname>Schiele</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="3178" last="3185"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Pishchulin, L., Jain, A., Andriluka, M., Thormahlen, T., Schiele, B., Articulated people detection and pose estimation: reshaping the future. Conference on Computer Vision and Pattern Recognition (CVPR), 2012, IEEE, 3178–3185.</ref-fulltext></reference><reference id="26"><ref-info><ref-title><ref-titletext>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0003243224</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Platt J.</ce:indexed-name><ce:surname>Platt</ce:surname></author></ref-authors><ref-sourcetitle>Adv. Large Margin Classifiers</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="10" issue="3"/><pagerange first="61" last="74"/></ref-volisspag></ref-info><ref-fulltext>Platt, J., Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Adv. Large Margin Classifiers 10:3 (1999), 61–74.</ref-fulltext></reference><reference id="27"><ref-info><ref-title><ref-titletext>Strike a pose: tracking people by finding stylized poses</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">24644504137</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Ramanan D.</ce:indexed-name><ce:surname>Ramanan</ce:surname></author><author seq="2"><ce:initials>D.A.</ce:initials><ce:indexed-name>Forsyth D.A.</ce:indexed-name><ce:surname>Forsyth</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Zisserman A.</ce:indexed-name><ce:surname>Zisserman</ce:surname></author></ref-authors><ref-sourcetitle>Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="1"/><pagerange first="271" last="278"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Ramanan, D., Forsyth, D.A., Zisserman, A., Strike a pose: tracking people by finding stylized poses. Computer Vision and Pattern Recognition (CVPR), Vol. 1, 2005, IEEE, 271–278.</ref-fulltext></reference><reference id="28"><ref-info><ref-title><ref-titletext>Parsing human motion with stretchable models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80052890828</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Sapp B.</ce:indexed-name><ce:surname>Sapp</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Weiss D.</ce:indexed-name><ce:surname>Weiss</ce:surname></author><author seq="3"><ce:initials>B.</ce:initials><ce:indexed-name>Taskar B.</ce:indexed-name><ce:surname>Taskar</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><pagerange first="1281" last="1288"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Sapp, B., Weiss, D., Taskar, B., Parsing human motion with stretchable models. Conference on Computer Vision and Pattern Recognition (CVPR), 2011, IEEE, 1281–1288.</ref-fulltext></reference><reference id="29"><ref-info><ref-title><ref-titletext>Measure locally, reason globally: occlusion-sensitive articulated pose estimation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33845575116</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Sigal L.</ce:indexed-name><ce:surname>Sigal</ce:surname></author><author seq="2"><ce:initials>M.J.</ce:initials><ce:indexed-name>Black M.J.</ce:indexed-name><ce:surname>Black</ce:surname></author></ref-authors><ref-sourcetitle>Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><voliss volume="2"/><pagerange first="2041" last="2048"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Sigal, L., Black, M.J., Measure locally, reason globally: occlusion-sensitive articulated pose estimation. Computer Vision and Pattern Recognition (CVPR), Vol. 2, 2006, IEEE, 2041–2048.</ref-fulltext></reference><reference id="30"><ref-info><ref-title><ref-titletext>Predicting 3D people from 2D pictures</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33746588668</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.</ce:initials><ce:indexed-name>Sigal L.</ce:indexed-name><ce:surname>Sigal</ce:surname></author><author seq="2"><ce:initials>M.J.</ce:initials><ce:indexed-name>Black M.J.</ce:indexed-name><ce:surname>Black</ce:surname></author></ref-authors><ref-sourcetitle>Articulated Motion and Deformable Objects</ref-sourcetitle><ref-publicationyear first="2006"/><ref-volisspag><pagerange first="185" last="195"/></ref-volisspag><ref-text>Springer Berlin Heidelberg</ref-text></ref-info><ref-fulltext>Sigal, L., Black, M.J., Predicting 3D people from 2D pictures. Articulated Motion and Deformable Objects, 2006, Springer Berlin Heidelberg, 185–195.</ref-fulltext></reference><reference id="31"><ref-info><ref-title><ref-titletext>Capturing articulated human hand motion: a divide-and-conquer approach</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0033283694</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Wu Y.</ce:indexed-name><ce:surname>Wu</ce:surname></author><author seq="2"><ce:initials>T.S.</ce:initials><ce:indexed-name>Huang T.S.</ce:indexed-name><ce:surname>Huang</ce:surname></author></ref-authors><ref-sourcetitle>International Conference on Computer Vision (ICCV)</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><voliss volume="Vol. 1"/><pagerange first="606" last="611"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Wu, Y., Huang, T.S., Capturing articulated human hand motion: a divide-and-conquer approach. International Conference on Computer Vision (ICCV), Vol. 1, 1999, IEEE, 606–611.</ref-fulltext></reference><reference id="32"><ref-info><ref-title><ref-titletext>Articulated pose estimation with flexible mixtures-of-parts</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80052895150</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Yang Y.</ce:indexed-name><ce:surname>Yang</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Ramanan D.</ce:indexed-name><ce:surname>Ramanan</ce:surname></author></ref-authors><ref-sourcetitle>Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><pagerange first="1385" last="1392"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Yang, Y., Ramanan, D., Articulated pose estimation with flexible mixtures-of-parts. Computer Vision and Pattern Recognition (CVPR), 2011, IEEE, 1385–1392.</ref-fulltext></reference><reference id="33"><ref-info><ref-title><ref-titletext>Articulated human detection with flexible mixtures of parts</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84887598018</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Yang Y.</ce:indexed-name><ce:surname>Yang</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Ramanan D.</ce:indexed-name><ce:surname>Ramanan</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Pattern Anal. Mach. Intell. (PAMI).</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="35" issue="12"/><pagerange first="2878" last="2890"/></ref-volisspag></ref-info><ref-fulltext>Yang, Y., Ramanan, D., Articulated human detection with flexible mixtures of parts. IEEE Trans. Pattern Anal. Mach. Intell. (PAMI). 35:12 (2013), 2878–2890.</ref-fulltext></reference><reference id="34"><ref-info><ref-title><ref-titletext>Does human action recognition benefit from pose estimation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898476963</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Yao A.</ce:indexed-name><ce:surname>Yao</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Gall J.</ce:indexed-name><ce:surname>Gall</ce:surname></author><author seq="3"><ce:initials>G.</ce:initials><ce:indexed-name>Fanelli G.</ce:indexed-name><ce:surname>Fanelli</ce:surname></author><author seq="4"><ce:initials>L.J.</ce:initials><ce:indexed-name>Van Gool L.J.</ce:indexed-name><ce:surname>Van Gool</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the British Machine Vision Conference</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="3"/><pagerange first="6"/></ref-volisspag><ref-text>BMVC Press</ref-text></ref-info><ref-fulltext>Yao, A., Gall, J., Fanelli, G., Van Gool, L.J., Does human action recognition benefit from pose estimation. Proceedings of the British Machine Vision Conference, Vol. 3, 2011, BMVC Press, 6.</ref-fulltext></reference><reference id="35"><ref-info><ref-title><ref-titletext>From pictorial structures to deformable structures</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84866682756</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Zuffi S.</ce:indexed-name><ce:surname>Zuffi</ce:surname></author><author seq="2"><ce:initials>O.</ce:initials><ce:indexed-name>Freifeld O.</ce:indexed-name><ce:surname>Freifeld</ce:surname></author><author seq="3"><ce:initials>M.J.</ce:initials><ce:indexed-name>Black M.J.</ce:indexed-name><ce:surname>Black</ce:surname></author></ref-authors><ref-sourcetitle>Conference on Computer Vision and Pattern Recognition (CVPR)</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="3546" last="3553"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Zuffi, S., Freifeld, O., Black, M.J., From pictorial structures to deformable structures. Conference on Computer Vision and Pattern Recognition (CVPR), 2012, IEEE, 3546–3553.</ref-fulltext></reference><reference id="36"><ref-info><ref-title><ref-titletext>Estimating human pose with flowing puppets</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84898823761</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Zuffi S.</ce:indexed-name><ce:surname>Zuffi</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Romero J.</ce:indexed-name><ce:surname>Romero</ce:surname></author><author seq="3"><ce:initials>C.</ce:initials><ce:indexed-name>Schmid C.</ce:indexed-name><ce:surname>Schmid</ce:surname></author><author seq="4"><ce:initials>M.J.</ce:initials><ce:indexed-name>Black M.J.</ce:indexed-name><ce:surname>Black</ce:surname></author></ref-authors><ref-sourcetitle>International Conference on Computer Vision (ICCV)</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><pagerange first="3312" last="3319"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Zuffi, S., Romero, J., Schmid, C., Black, M.J., Estimating human pose with flowing puppets. International Conference on Computer Vision (ICCV), 2013, IEEE, 3312–3319.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>