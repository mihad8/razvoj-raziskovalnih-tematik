<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85016792815</prism:url><dc:identifier>SCOPUS_ID:85016792815</dc:identifier><eid>2-s2.0-85016792815</eid><prism:doi>10.3390/e19040157</prism:doi><article-number>157</article-number><dc:title>Quadratic mutual information feature selection</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>4</citedby-count><prism:publicationName>Entropy</prism:publicationName><dc:publisher>MDPI AGPostfachBaselCH-4005</dc:publisher><source-id>13715</source-id><prism:issn>10994300</prism:issn><prism:volume>19</prism:volume><prism:issueIdentifier>4</prism:issueIdentifier><prism:coverDate>2017-01-01</prism:coverDate><openaccess>1</openaccess><openaccessFlag>true</openaccessFlag><dc:creator><author seq="1" auid="54988061800"><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/54988061800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><publishercopyright>© 2017 by the authors.</publishercopyright><ce:para>We propose a novel feature selection method based on quadratic mutual information which has its roots in Cauchy-Schwarz divergence and Renyi entropy. The method uses the direct estimation of quadratic mutual information from data samples using Gaussian kernel functions, and can detect second order non-linear relations. Its main advantages are: (i) unified analysis of discrete and continuous data, excluding any discretization; and (ii) its parameter-free design. The effectiveness of the proposed method is demonstrated through an extensive comparison with mutual information feature selection (MIFS), minimum redundancy maximum relevance (MRMR), and joint mutual information (JMI) on classification and regression problem domains. The experiments show that proposed method performs comparably to the other methods when applied to classification problems, except it is considerably faster. In the case of regression, it compares favourably to the others, but is slower.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85016792815" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85016792815&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85016792815&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="54988061800"><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/54988061800</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="6506205187"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name><preferred-name><ce:initials>U.</ce:initials><ce:indexed-name>Lotrič U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6506205187</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Cauchy-Schwarz divergence</author-keyword><author-keyword>Feature selection</author-keyword><author-keyword>Information-theoretic measures</author-keyword><author-keyword>Quadratic mutual information</author-keyword></authkeywords><idxterms/><subject-areas><subject-area code="3100" abbrev="PHYS">Physics and Astronomy (all)</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2019-03-15T23:01:25.844Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/nlp</xocs:funding-addon-type><xocs:funding><xocs:funding-agency-matched-string>Slovenian Research Agency</xocs:funding-agency-matched-string><xocs:funding-id>P2-0241</xocs:funding-id><xocs:funding-agency-acronym>ARRS</xocs:funding-agency-acronym><xocs:funding-agency>Javna Agencija za Raziskovalno Dejavnost RS</xocs:funding-agency><xocs:funding-agency-id>http://data.elsevier.com/vocabulary/SciValFunders/501100004329</xocs:funding-agency-id><xocs:funding-agency-country>http://sws.geonames.org/3190538/</xocs:funding-agency-country></xocs:funding><xocs:funding-text>This research was supported by Slovenian Research Agency under grant P2-0241 (National research program Synergetics of complex systems and processes).</xocs:funding-text></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered day="02" month="07" timestamp="2019-07-02T14:13:22.000022-04:00" year="2019"/><ait:date-sort day="01" month="01" year="2017"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2017 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.3390/e19040157</ce:doi><itemid idtype="PUI">615232468</itemid><itemid idtype="CAR-ID">663029659</itemid><itemid idtype="REAXYSCAR">20170632905</itemid><itemid idtype="SNCPX">2017039170</itemid><itemid idtype="SCP">85016792815</itemid><itemid idtype="SGR">85016792815</itemid></itemidlist><history><date-created day="10" month="04" timestamp="BST 15:27:58" year="2017"/></history><dbcollection>REAXYSCAR</dbcollection><dbcollection>SNCPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">Cauchy-Schwarz divergence</author-keyword><author-keyword xml:lang="eng">Feature selection</author-keyword><author-keyword xml:lang="eng">Information-theoretic measures</author-keyword><author-keyword xml:lang="eng">Quadratic mutual information</author-keyword></author-keywords></citation-info><citation-title><titletext original="y" xml:lang="eng" language="English">Quadratic mutual information feature selection</titletext></citation-title><author-group><author auid="54988061800" seq="1" type="auth"><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name></preferred-name></author><author auid="6506205187" seq="2" type="auth"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name><preferred-name><ce:initials>U.</ce:initials><ce:indexed-name>Lotrič U.</ce:indexed-name><ce:surname>Lotrič</ce:surname><ce:given-name>Uroš</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>University of Ljubljana</organization><organization>Faculty of Computer and Information Science</organization><city>Ljubljana</city><postal-code>1000</postal-code><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname><ce:given-name>Davor</ce:given-name></person><affiliation country="svn"><organization>University of Ljubljana</organization><organization>Faculty of Computer and Information Science</organization><city>Ljubljana</city><postal-code>1000</postal-code><country>Slovenia</country></affiliation></correspondence><grantlist complete="y"><grant><grant-id>P2-0241</grant-id><grant-acronym>ARRS</grant-acronym><grant-agency>Javna Agencija za Raziskovalno Dejavnost RS</grant-agency></grant><grant-text xml:lang="eng">This research was supported by Slovenian Research Agency under grant P2-0241 (National research program Synergetics of complex systems and processes).</grant-text></grantlist><abstracts><abstract original="y" xml:lang="eng"><publishercopyright>© 2017 by the authors.</publishercopyright><ce:para>We propose a novel feature selection method based on quadratic mutual information which has its roots in Cauchy-Schwarz divergence and Renyi entropy. The method uses the direct estimation of quadratic mutual information from data samples using Gaussian kernel functions, and can detect second order non-linear relations. Its main advantages are: (i) unified analysis of discrete and continuous data, excluding any discretization; and (ii) its parameter-free design. The effectiveness of the proposed method is demonstrated through an extensive comparison with mutual information feature selection (MIFS), minimum redundancy maximum relevance (MRMR), and joint mutual information (JMI) on classification and regression problem domains. The experiments show that proposed method performs comparably to the other methods when applied to classification problems, except it is considerably faster. In the case of regression, it compares favourably to the others, but is slower.</ce:para></abstract></abstracts><source country="che" srcid="13715" type="j"><sourcetitle>Entropy</sourcetitle><sourcetitle-abbrev>Entropy</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">Entropy</translated-sourcetitle><issn type="electronic">10994300</issn><volisspag><voliss issue="4" volume="19"/></volisspag><article-number>157</article-number><publicationyear first="2017"/><publicationdate><year>2017</year><date-text xfab-added="true">2017</date-text></publicationdate><website><ce:e-address type="email">http://www.mdpi.com/1099-4300/19/4/157/pdf</ce:e-address></website><publisher><publishername>MDPI AG</publishername><affiliation country="che"><address-part>Postfach</address-part><city>Basel</city><postal-code>CH-4005</postal-code></affiliation></publisher></source><enhancement><classificationgroup><classifications type="ASJC"><classification>3100</classification></classifications><classifications type="SUBJABBR"><classification>PHYS</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="35"><reference id="1"><ref-info><ref-title><ref-titletext>An introduction to variable and feature selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33745561205</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>I.</ce:initials><ce:indexed-name>Guyon I.</ce:indexed-name><ce:surname>Guyon</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Elisseeff A.</ce:indexed-name><ce:surname>Elisseeff</ce:surname></author></ref-authors><ref-sourcetitle>J. Mach. Learn. Res</ref-sourcetitle><ref-publicationyear first="2003"/><ref-volisspag><voliss volume="3"/><pagerange first="1157" last="1182"/></ref-volisspag></ref-info><ref-fulltext>Guyon, I.; Elisseeff, A. An introduction to variable and feature selection. J. Mach. Learn. Res. 2003, 3, 1157-1182.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>A review of feature selection methods based on mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84891840571</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.R.</ce:initials><ce:indexed-name>Vergara J.R.</ce:indexed-name><ce:surname>Vergara</ce:surname></author><author seq="2"><ce:initials>P.A.</ce:initials><ce:indexed-name>Estevez P.A.</ce:indexed-name><ce:surname>Estévez</ce:surname></author></ref-authors><ref-sourcetitle>Neural Comput. Appl</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><voliss volume="24"/><pagerange first="175" last="186"/></ref-volisspag></ref-info><ref-fulltext>Vergara, J.R.; Estévez, P.A. A review of feature selection methods based on mutual information. Neural Comput. Appl. 2014, 24, 175-186.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Wrappers for feature subset selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0031381525</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Kohavi R.</ce:indexed-name><ce:surname>Kohavi</ce:surname></author><author seq="2"><ce:initials>G.H.</ce:initials><ce:indexed-name>John G.H.</ce:indexed-name><ce:surname>John</ce:surname></author></ref-authors><ref-sourcetitle>Artif. Intell</ref-sourcetitle><ref-publicationyear first="1997"/><ref-volisspag><voliss volume="97"/><pagerange first="273" last="324"/></ref-volisspag></ref-info><ref-fulltext>Kohavi, R.; John, G.H. Wrappers for feature subset selection. Artif. Intell. 1997, 97, 273-324.</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Correlation-based feature selection of discrete and numeric class machine learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000772708</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.A.</ce:initials><ce:indexed-name>Hall M.A.</ce:indexed-name><ce:surname>Hall</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Seventeenth International Conference on Machine Learning</ref-sourcetitle><ref-publicationyear first="2000"/><ref-volisspag><pagerange first="359" last="366"/></ref-volisspag><ref-text>Stanford, CA, USA, 29 June-2 July</ref-text></ref-info><ref-fulltext>Hall, M.A. Correlation-based feature selection of discrete and numeric class machine learning. In Proceedings of the Seventeenth International Conference on Machine Learning, Stanford, CA, USA, 29 June-2 July 2000; pp. 359-366.</ref-fulltext></reference><reference id="5"><ref-info><ref-title><ref-titletext>Fast binary feature selection with conditional mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">33645690579</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Fleuret F.</ce:indexed-name><ce:surname>Fleuret</ce:surname></author></ref-authors><ref-sourcetitle>J. Mach. Learn. Res</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="5"/><pagerange first="1531" last="1555"/></ref-volisspag></ref-info><ref-fulltext>Fleuret, F. Fast binary feature selection with conditional mutual information. J. Mach. Learn. Res. 2004, 5, 1531-1555.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>A survey on feature selection methods</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84894903349</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Chandrashekar G.</ce:indexed-name><ce:surname>Chandrashekar</ce:surname></author><author seq="2"><ce:initials>F.</ce:initials><ce:indexed-name>Sahin F.</ce:indexed-name><ce:surname>Sahin</ce:surname></author></ref-authors><ref-sourcetitle>Comput. Electr. Eng</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><voliss volume="40"/><pagerange first="16" last="28"/></ref-volisspag></ref-info><ref-fulltext>Chandrashekar, G.; Sahin, F. A survey on feature selection methods. Comput. Electr. Eng. 2014, 40, 16-28.</ref-fulltext></reference><reference id="7"><ref-info><refd-itemidlist><itemid idtype="SGR">33845299971</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Principe</ce:surname></author></ref-authors><ref-sourcetitle>Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives</ref-sourcetitle><ref-publicationyear first="2010"/><ref-text>Springer Science &amp; Business Media: New York, NY, USA</ref-text></ref-info><ref-fulltext>Principe, J.C. Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives; Springer Science &amp; Business Media: New York, NY, USA, 2010.</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>A new perspective for information theoretic feature selection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">83455217064</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Brown G.</ce:indexed-name><ce:surname>Brown</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS-09)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="49" last="56"/></ref-volisspag><ref-text>Clearwater Beach, FL, USA, 16-18 April</ref-text></ref-info><ref-fulltext>Brown, G. A new perspective for information theoretic feature selection. In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS-09), Clearwater Beach, FL, USA, 16-18 April 2009; pp. 49-56.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Rényi entropy and Cauchy-Schwarz mutual information applied to mifs-u variable selection algorithm: A comparative study</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">80855140399</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.B.</ce:initials><ce:indexed-name>Goncalves L.B.</ce:indexed-name><ce:surname>Gonçalves</ce:surname></author><author seq="2"><ce:initials>J.L.R.</ce:initials><ce:indexed-name>Macrini J.L.R.</ce:indexed-name><ce:surname>Macrini</ce:surname></author></ref-authors><ref-sourcetitle>Pesqui. Oper</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="31"/><pagerange first="499" last="519"/></ref-volisspag></ref-info><ref-fulltext>Gonçalves, L.B.; Macrini, J.L.R. Rényi entropy and Cauchy-Schwarz mutual information applied to mifs-u variable selection algorithm: A comparative study. Pesqui. Oper. 2011, 31, 499-519.</ref-fulltext></reference><reference id="10"><ref-info><refd-itemidlist><itemid idtype="SGR">84893469578</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Sluga D.</ce:indexed-name><ce:surname>Sluga</ce:surname></author><author seq="2"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotric</ce:surname></author></ref-authors><ref-sourcetitle>Generalized information-theoretic measures for feature selection. In Proceedings of the International Conference on Adaptive and Natural Computing Algorithms</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><pagerange first="189" last="197"/></ref-volisspag><ref-text>Lausanne, Switzerland, 4-6 April 2013. Springer: Berlin/Heidelberg, Germany</ref-text></ref-info><ref-fulltext>Sluga, D.; Lotric, U. Generalized information-theoretic measures for feature selection. In Proceedings of the International Conference on Adaptive and Natural Computing Algorithms, Lausanne, Switzerland, 4-6 April 2013; Springer: Berlin/Heidelberg, Germany, 2013; pp. 189-197.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Estimating optimal feature subsets using efficient estimation of high-dimensional mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">13844298045</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.W.</ce:initials><ce:indexed-name>Chow T.W.</ce:indexed-name><ce:surname>Chow</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Huang D.</ce:indexed-name><ce:surname>Huang</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Neural Netw</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="16"/><pagerange first="213" last="224"/></ref-volisspag></ref-info><ref-fulltext>Chow, T.W.; Huang, D. Estimating optimal feature subsets using efficient estimation of high-dimensional mutual information. IEEE Trans. Neural Netw. 2005, 16, 213-224.</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>A survey of discretization techniques: Taxonomy and empirical analysis in supervised learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84874613998</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Garcia S.</ce:indexed-name><ce:surname>Garcia</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Luengo J.</ce:indexed-name><ce:surname>Luengo</ce:surname></author><author seq="3"><ce:initials>J.A.</ce:initials><ce:indexed-name>Saez J.A.</ce:indexed-name><ce:surname>Sáez</ce:surname></author><author seq="4"><ce:initials>V.</ce:initials><ce:indexed-name>Lopez V.</ce:indexed-name><ce:surname>Lopez</ce:surname></author><author seq="5"><ce:initials>F.</ce:initials><ce:indexed-name>Herrera F.</ce:indexed-name><ce:surname>Herrera</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Knowl. Data Eng</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="25"/><pagerange first="734" last="750"/></ref-volisspag></ref-info><ref-fulltext>Garcia, S.; Luengo, J.; Sáez, J.A.; Lopez, V.; Herrera, F. A survey of discretization techniques: Taxonomy and empirical analysis in supervised learning. IEEE Trans. Knowl. Data Eng. 2013, 25, 734-750.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Multi-interval discretization of continuous-valued attributes for classification learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002593344</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.B.</ce:initials><ce:indexed-name>Irani K.B.</ce:indexed-name><ce:surname>Irani</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 13th International Joint Conference on Artificial Intelligence</ref-sourcetitle><ref-publicationyear first="1993"/><ref-volisspag><pagerange first="1022" last="1029"/></ref-volisspag><ref-text>Chambery, France, 28 August-3 September</ref-text></ref-info><ref-fulltext>Irani, K.B. Multi-interval discretization of continuous-valued attributes for classification learning. In Proceedings of the 13th International Joint Conference on Artificial Intelligence, Chambery, France, 28 August-3 September 1993; pp. 1022-1029.</ref-fulltext></reference><reference id="14"><ref-info><ref-title><ref-titletext>On estimation of a probability density function and mode</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0001473437</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.</ce:initials><ce:indexed-name>Parzen E.</ce:indexed-name><ce:surname>Parzen</ce:surname></author></ref-authors><ref-sourcetitle>Ann. Math. Stat</ref-sourcetitle><ref-publicationyear first="1962"/><ref-volisspag><voliss volume="33"/><pagerange first="1065" last="1076"/></ref-volisspag></ref-info><ref-fulltext>Parzen, E. On estimation of a probability density function and mode. Ann. Math. Stat. 1962, 33, 1065-1076.</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Kernel density estimationwith adaptive varyingwindowsize</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036885193</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>Katkovnik V.</ce:indexed-name><ce:surname>Katkovnik</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Shmulevich I.</ce:indexed-name><ce:surname>Shmulevich</ce:surname></author></ref-authors><ref-sourcetitle>Pattern Recognit. Lett</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="23"/><pagerange first="1641" last="1648"/></ref-volisspag></ref-info><ref-fulltext>Katkovnik, V.; Shmulevich, I. Kernel density estimationwith adaptive varyingwindowsize. Pattern Recognit. Lett. 2002, 23, 1641-1648.</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Estimating mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="ARTNUM">066138</itemid><itemid idtype="SGR">39749164774</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Kraskov A.</ce:indexed-name><ce:surname>Kraskov</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Stogbauer H.</ce:indexed-name><ce:surname>Stögbauer</ce:surname></author><author seq="3"><ce:initials>P.</ce:initials><ce:indexed-name>Grassberger P.</ce:indexed-name><ce:surname>Grassberger</ce:surname></author></ref-authors><ref-sourcetitle>Phys. Rev. E</ref-sourcetitle><ref-publicationyear first="2004"/><ref-volisspag><voliss volume="69"/></ref-volisspag></ref-info><ref-fulltext>Kraskov, A.; Stögbauer, H.; Grassberger, P. Estimating mutual information. Phys. Rev. E 2004, 69, 066138.</ref-fulltext></reference><reference id="17"><ref-info><refd-itemidlist><itemid idtype="SGR">84937717371</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Walters-Williams J.</ce:indexed-name><ce:surname>Walters-Williams</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Li Y.</ce:indexed-name><ce:surname>Li</ce:surname></author></ref-authors><ref-sourcetitle>Estimation of mutual information: A survey. In Proceedings of the International Conference on Rough Sets and Knowledge Technology</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="389" last="396"/></ref-volisspag><ref-text>Gold Coast, QLD, Australia, 14-16 July 2009. Springer: Berlin/Heidelberg, Germany</ref-text></ref-info><ref-fulltext>Walters-Williams, J.; Li, Y. Estimation of mutual information: A survey. In Proceedings of the International Conference on Rough Sets and Knowledge Technology, Gold Coast, QLD, Australia, 14-16 July 2009; Springer: Berlin/Heidelberg, Germany, 2009; pp. 389-396.</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Machine learning with squared-loss mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873194326</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Sugiyama M.</ce:indexed-name><ce:surname>Sugiyama</ce:surname></author></ref-authors><ref-sourcetitle>Entropy</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><voliss volume="15"/><pagerange first="80" last="112"/></ref-volisspag></ref-info><ref-fulltext>Sugiyama, M. Machine learning with squared-loss mutual information. Entropy 2012, 15, 80-112.</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>Generalised information and entropy measures in physics</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">67650242997</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Beck C.</ce:indexed-name><ce:surname>Beck</ce:surname></author></ref-authors><ref-sourcetitle>Contemp. Phys</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="50"/><pagerange first="495" last="510"/></ref-volisspag></ref-info><ref-fulltext>Beck, C. Generalised information and entropy measures in physics. Contemp. Phys. 2009, 50, 495-510.</ref-fulltext></reference><reference id="20"><ref-info><ref-title><ref-titletext>On measures of entropy and information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0002408684</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Renyi A.</ce:indexed-name><ce:surname>Renyi</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability</ref-sourcetitle><ref-publicationyear first="1960"/><ref-volisspag><pagerange first="547" last="561"/></ref-volisspag><ref-text>Berkeley, CA, USA, 20 June-30 July</ref-text></ref-info><ref-fulltext>Renyi, A. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, CA, USA, 20 June-30 July 1960; pp. 547-561.</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>Generalized information potential criterion for adaptive system training</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036737108</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Erdogmus D.</ce:indexed-name><ce:surname>Erdogmus</ce:surname></author><author seq="2"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Principe</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Neural Netw</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="13"/><pagerange first="1035" last="1044"/></ref-volisspag></ref-info><ref-fulltext>Erdogmus, D.; Principe, J.C. Generalized information potential criterion for adaptive system training. IEEE Trans. Neural Netw. 2002, 13, 1035-1044.</ref-fulltext></reference><reference id="22"><ref-info><refd-itemidlist><itemid idtype="SGR">0001204475</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Renyi A.</ce:indexed-name><ce:surname>Renyi</ce:surname></author></ref-authors><ref-sourcetitle>Some Fundamental Questions About Information Theory</ref-sourcetitle><ref-publicationyear first="1976"/><ref-volisspag><voliss volume="2"/></ref-volisspag><ref-text>Akademia Kiado: Budapest, Hungary</ref-text></ref-info><ref-fulltext>Renyi, A. Some Fundamental Questions About Information Theory; Akademia Kiado: Budapest, Hungary, 1976; Volume 2.</ref-fulltext></reference><reference id="23"><ref-info><ref-title><ref-titletext>Using mutual information for selecting features in supervised neural net learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0028468293</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.</ce:initials><ce:indexed-name>Battiti R.</ce:indexed-name><ce:surname>Battiti</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Neural Netw</ref-sourcetitle><ref-publicationyear first="1994"/><ref-volisspag><voliss volume="5"/><pagerange first="537" last="550"/></ref-volisspag></ref-info><ref-fulltext>Battiti, R. Using mutual information for selecting features in supervised neural net learning. IEEE Trans. Neural Netw. 1994, 5, 537-550.</ref-fulltext></reference><reference id="24"><ref-info><ref-title><ref-titletext>Input feature selection for classification problems</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0036127473</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.</ce:initials><ce:indexed-name>Kwak N.</ce:indexed-name><ce:surname>Kwak</ce:surname></author><author seq="2"><ce:initials>C.H.</ce:initials><ce:indexed-name>Choi C.H.</ce:indexed-name><ce:surname>Choi</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Neural Netw</ref-sourcetitle><ref-publicationyear first="2002"/><ref-volisspag><voliss volume="13"/><pagerange first="143" last="159"/></ref-volisspag></ref-info><ref-fulltext>Kwak, N.; Choi, C.H. Input feature selection for classification problems. IEEE Trans. Neural Netw. 2002, 13, 143-159.</ref-fulltext></reference><reference id="25"><ref-info><ref-title><ref-titletext>Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">24344458137</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Peng H.</ce:indexed-name><ce:surname>Peng</ce:surname></author><author seq="2"><ce:initials>F.</ce:initials><ce:indexed-name>Long F.</ce:indexed-name><ce:surname>Long</ce:surname></author><author seq="3"><ce:initials>C.</ce:initials><ce:indexed-name>Ding C.</ce:indexed-name><ce:surname>Ding</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Pattern Anal. Mach. Intell</ref-sourcetitle><ref-publicationyear first="2005"/><ref-volisspag><voliss volume="27"/><pagerange first="1226" last="1238"/></ref-volisspag></ref-info><ref-fulltext>Peng, H.; Long, F.; Ding, C. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE Trans. Pattern Anal. Mach. Intell. 2005, 27, 1226-1238.</ref-fulltext></reference><reference id="26"><ref-info><ref-title><ref-titletext>Feature selection based on joint mutual information</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0012613608</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Yang H.</ce:indexed-name><ce:surname>Yang</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Moody J.</ce:indexed-name><ce:surname>Moody</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International ICSC Symposium on Advances in Intelligent Data Analysis</ref-sourcetitle><ref-publicationyear first="1999"/><ref-volisspag><pagerange first="22" last="25"/></ref-volisspag><ref-text>Rochester, NY, USA, 22-25 June</ref-text></ref-info><ref-fulltext>Yang, H.; Moody, J. Feature selection based on joint mutual information. In Proceedings of the International ICSC Symposium on Advances in Intelligent Data Analysis, Rochester, NY, USA, 22-25 June 1999; pp. 22-25.</ref-fulltext></reference><reference id="27"><ref-info><ref-title><ref-titletext>Maximally informative "stimulus energies" in the analysis of neural responses to natural signals</ref-titletext></ref-title><refd-itemidlist><itemid idtype="ARTNUM">e71959</itemid><itemid idtype="SGR">84892651115</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Rajan K.</ce:indexed-name><ce:surname>Rajan</ce:surname></author><author seq="2"><ce:initials>W.</ce:initials><ce:indexed-name>Bialek W.</ce:indexed-name><ce:surname>Bialek</ce:surname></author></ref-authors><ref-sourcetitle>PLoS ONE</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="8"/></ref-volisspag></ref-info><ref-fulltext>Rajan, K.; Bialek, W. Maximally informative "stimulus energies" in the analysis of neural responses to natural signals. PLoS ONE 2013, 8, e71959.</ref-fulltext></reference><reference id="28"><ref-info><ref-title><ref-titletext>Second order dimensionality reduction using minimum and maximum mutual information models</ref-titletext></ref-title><refd-itemidlist><itemid idtype="ARTNUM">e1002249</itemid><itemid idtype="SGR">80055086683</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.D.</ce:initials><ce:indexed-name>Fitzgerald J.D.</ce:indexed-name><ce:surname>Fitzgerald</ce:surname></author><author seq="2"><ce:initials>R.J.</ce:initials><ce:indexed-name>Rowekamp R.J.</ce:indexed-name><ce:surname>Rowekamp</ce:surname></author><author seq="3"><ce:initials>L.C.</ce:initials><ce:indexed-name>Sincich L.C.</ce:indexed-name><ce:surname>Sincich</ce:surname></author><author seq="4"><ce:initials>T.O.</ce:initials><ce:indexed-name>Sharpee T.O.</ce:indexed-name><ce:surname>Sharpee</ce:surname></author></ref-authors><ref-sourcetitle>PLoS Comput. Biol</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="7"/></ref-volisspag></ref-info><ref-fulltext>Fitzgerald, J.D.; Rowekamp, R.J.; Sincich, L.C.; Sharpee, T.O. Second order dimensionality reduction using minimum and maximum mutual information models. PLoS Comput. Biol. 2011, 7, e1002249.</ref-fulltext></reference><reference id="29"><ref-info><ref-title><ref-titletext>Analyzing multicomponent receptive fields from neural responses to natural stimuli</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">82855160917</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>R.J.</ce:initials><ce:indexed-name>Rowekamp R.J.</ce:indexed-name><ce:surname>Rowekamp</ce:surname></author><author seq="2"><ce:initials>T.O.</ce:initials><ce:indexed-name>Sharpee T.O.</ce:indexed-name><ce:surname>Sharpee</ce:surname></author></ref-authors><ref-sourcetitle>Netw. Comput. Neural Syst</ref-sourcetitle><ref-publicationyear first="2011"/><ref-volisspag><voliss volume="22"/><pagerange first="45" last="73"/></ref-volisspag></ref-info><ref-fulltext>Rowekamp, R.J.; Sharpee, T.O. Analyzing multicomponent receptive fields from neural responses to natural stimuli. Netw. Comput. Neural Syst. 2011, 22, 45-73.</ref-fulltext></reference><reference id="30"><ref-info><refd-itemidlist><itemid idtype="SGR">38449087399</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.</ce:initials><ce:indexed-name>Sanchez-Marono N.</ce:indexed-name><ce:surname>Sánchez-Maroño</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Alonso-Betanzos A.</ce:indexed-name><ce:surname>Alonso-Betanzos</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Tombilla-Sanroman M.</ce:indexed-name><ce:surname>Tombilla-Sanromán</ce:surname></author></ref-authors><ref-sourcetitle>Filter methods for feature selection-A comparative study. In Proceedings of the International Conference on Intelligent Data Engineering and Automated Learning</ref-sourcetitle><ref-publicationyear first="2007"/><ref-volisspag><pagerange first="178" last="187"/></ref-volisspag><ref-text>Birmingham, UK, 16-19 December 2007; Springer: Berlin/Heidelberg, Germany</ref-text></ref-info><ref-fulltext>Sánchez-Maroño, N.; Alonso-Betanzos, A.; Tombilla-Sanromán, M. Filter methods for feature selection-A comparative study. In Proceedings of the International Conference on Intelligent Data Engineering and Automated Learning, Birmingham, UK, 16-19 December 2007; Springer: Berlin/Heidelberg, Germany, 2007; pp. 178-187.</ref-fulltext></reference><reference id="31"><ref-info><ref-title><ref-titletext>Is mutual information adequate for feature selection in regression?</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84880930604</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Frenay B.</ce:indexed-name><ce:surname>Frénay</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Doquire G.</ce:indexed-name><ce:surname>Doquire</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Verleysen M.</ce:indexed-name><ce:surname>Verleysen</ce:surname></author></ref-authors><ref-sourcetitle>Neural Netw</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="48"/><pagerange first="1" last="7"/></ref-volisspag></ref-info><ref-fulltext>Frénay, B.; Doquire, G.; Verleysen, M. Is mutual information adequate for feature selection in regression? Neural Netw. 2013, 48, 1-7.</ref-fulltext></reference><reference id="32"><ref-info><refd-itemidlist><itemid idtype="SGR">85050992891</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.W.</ce:initials><ce:indexed-name>Silverman B.W.</ce:indexed-name><ce:surname>Silverman</ce:surname></author></ref-authors><ref-sourcetitle>Density Estimation for Statistics and Data Analysis</ref-sourcetitle><ref-publicationyear first="1986"/><ref-volisspag><voliss volume="26"/></ref-volisspag><ref-text>CRC Press: Boca Raton, FL, USA</ref-text></ref-info><ref-fulltext>Silverman, B.W. Density Estimation for Statistics and Data Analysis; CRC Press: Boca Raton, FL, USA, 1986; Volume 26.</ref-fulltext></reference><reference id="33"><ref-info><ref-title><ref-titletext>On speeding up computation in information theoretic learning</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">70449395271</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Seth S.</ce:indexed-name><ce:surname>Seth</ce:surname></author><author seq="2"><ce:initials>J.C.</ce:initials><ce:indexed-name>Principe J.C.</ce:indexed-name><ce:surname>Príncipe</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Joint Conference on Neural Networks (IJCNN 2009)</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="2883" last="2887"/></ref-volisspag><ref-text>Atlanta, GA, USA, 14-19 June</ref-text></ref-info><ref-fulltext>Seth, S.; Príncipe, J.C. On speeding up computation in information theoretic learning. In Proceedings of the International Joint Conference on Neural Networks (IJCNN 2009), Atlanta, GA, USA, 14-19 June 2009; pp. 2883-2887.</ref-fulltext></reference><reference id="34"><ref-info><refd-itemidlist><itemid idtype="SGR">84886567160</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Lichman M.</ce:indexed-name><ce:surname>Lichman</ce:surname></author></ref-authors><ref-sourcetitle>UCI Machine Learning Repository</ref-sourcetitle><ref-website><ce:e-address type="email">http://archive.ics.uci.edu/ml</ce:e-address></ref-website><ref-text>(accessed on 1 December 2016)</ref-text></ref-info><ref-fulltext>Lichman, M. UCI Machine Learning Repository. Available online: http://archive.ics.uci.edu/ml (accessed on 1 December 2016).</ref-fulltext></reference><reference id="35"><ref-info><ref-title><ref-titletext>The WEKA data mining software: An update</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">76749092270</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Hall M.</ce:indexed-name><ce:surname>Hall</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Frank E.</ce:indexed-name><ce:surname>Frank</ce:surname></author><author seq="3"><ce:initials>G.</ce:initials><ce:indexed-name>Holmes G.</ce:indexed-name><ce:surname>Holmes</ce:surname></author><author seq="4"><ce:initials>B.</ce:initials><ce:indexed-name>Pfahringer B.</ce:indexed-name><ce:surname>Pfahringer</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Reutemann P.</ce:indexed-name><ce:surname>Reutemann</ce:surname></author><author seq="6"><ce:initials>I.H.</ce:initials><ce:indexed-name>Witten I.H.</ce:indexed-name><ce:surname>Witten</ce:surname></author></ref-authors><ref-sourcetitle>ACM SIGKDD Explor. Newsl</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><voliss volume="11"/><pagerange first="10" last="18"/></ref-volisspag></ref-info><ref-fulltext>Hall, M.; Frank, E.; Holmes, G.; Pfahringer, B.; Reutemann, P.; Witten, I.H. The WEKA data mining software: An update. ACM SIGKDD Explor. Newsl. 2009, 11, 10-18.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>