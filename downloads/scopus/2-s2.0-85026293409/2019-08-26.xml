<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85026293409</prism:url><dc:identifier>SCOPUS_ID:85026293409</dc:identifier><eid>2-s2.0-85026293409</eid><prism:doi>10.1109/FG.2017.123</prism:doi><article-number>7961853</article-number><dc:title>Training Convolutional Neural Networks with Limited Training Data for Ear Recognition in the Wild</dc:title><prism:aggregationType>Conference Proceeding</prism:aggregationType><srctype>p</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>16</citedby-count><prism:publicationName>Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017</prism:publicationName><dc:publisher> Institute of Electrical and Electronics Engineers Inc. </dc:publisher><source-id>21100825137</source-id><prism:isbn>9781509040230</prism:isbn><prism:startingPage>987</prism:startingPage><prism:endingPage>994</prism:endingPage><prism:pageRange>987-994</prism:pageRange><prism:coverDate>2017-06-28</prism:coverDate><openaccess>2</openaccess><openaccessFlag/><dc:creator><author seq="1" auid="56097253100"><ce:initials>Z.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emersic</ce:surname><ce:given-name>Ziga</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emersic</ce:surname><ce:given-name>Ziga</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56097253100</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"> <publishercopyright>© 2017 IEEE.</publishercopyright> <ce:para>Identity recognition from ear images is an active field of research within the biometric community. The ability to capture ear images from a distance and in a covert manner makes ear recognition technology an appealing choice for surveillance and security applications as well as related application domains. In contrast to other biometric modalities, where large datasets captured in uncontrolled settings are readily available, datasets of ear images are still limited in size and mostly of laboratory-like quality. As a consequence, ear recognition technology has not benefited yet from advances in deep learning and convolutionalneural networks (CNNs) and is still lacking behind other modalities that experienced significant performance gains owing to deep recognition technology. In this paper we address this problem and aim at building a CNNbased ear recognition model. We explore different strategies towards model training with limited amounts of training data and show that by selecting an appropriate model architecture, using aggressive data augmentation and selective learning on existing (pre-trained) models, we are able to learn an effective CNN·based model using a little more than 1300training images. The result of our work is the first CNN·based approach to ear recognition that is also made publicly available to the research community. With our model we are able to improve on the rank one recognition rate of the previous state-of-the-art by more than 25% on a challenging dataset of ear images captured from the web (a.k.a, in the wild).</ce:para> </abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85026293409" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85026293409&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85026293409&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="56097253100"><ce:initials>Z.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emersic</ce:surname><ce:given-name>Ziga</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emersic</ce:surname><ce:given-name>Ziga</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56097253100</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="57195223615"><ce:initials>D.</ce:initials><ce:indexed-name>Stepec D.</ce:indexed-name><ce:surname>Stepec</ce:surname><ce:given-name>Dejan</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Stepec D.</ce:indexed-name><ce:surname>Stepec</ce:surname><ce:given-name>Dejan</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57195223615</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="17347474600"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Struc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Struc</ce:surname><ce:given-name>Vitomir</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/17347474600</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="4" auid="7003277146"><ce:initials>P.</ce:initials><ce:indexed-name>Peer P.</ce:indexed-name><ce:surname>Peer</ce:surname><ce:given-name>Peter</ce:given-name><preferred-name><ce:initials>P.</ce:initials><ce:indexed-name>Peer P.</ce:indexed-name><ce:surname>Peer</ce:surname><ce:given-name>Peter</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7003277146</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords/><idxterms><mainterm weight="b" candidate="n">Appropriate models</mainterm><mainterm weight="b" candidate="n">Convolutional neural network</mainterm><mainterm weight="b" candidate="n">Data augmentation</mainterm><mainterm weight="b" candidate="n">Identity recognition</mainterm><mainterm weight="b" candidate="n">Limited training data</mainterm><mainterm weight="b" candidate="n">Research communities</mainterm><mainterm weight="b" candidate="n">Security application</mainterm><mainterm weight="b" candidate="n">Selective learning</mainterm></idxterms><subject-areas><subject-area code="2214" abbrev="ENGI">Media Technology</subject-area><subject-area code="1702" abbrev="COMP">Artificial Intelligence</subject-area><subject-area code="1707" abbrev="COMP">Computer Vision and Pattern Recognition</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered day="06" month="10" timestamp="2017-10-06T07:12:47.000047-04:00" year="2017"/><ait:date-sort day="28" month="06" year="2017"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2017 Elsevier B.V., All rights reserved.</copyright><itemidlist> <ce:doi>10.1109/FG.2017.123</ce:doi> <itemid idtype="PUI">617550512</itemid> <itemid idtype="CAR-ID">666933230</itemid> <itemid idtype="CPX">20173104000637</itemid> <itemid idtype="SCP">85026293409</itemid> <itemid idtype="SGR">85026293409</itemid> </itemidlist><history> <date-created day="01" month="08" timestamp="BST 06:14:37" year="2017"/> </history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/></citation-info><citation-title><titletext original="y" xml:lang="eng" language="English">Training Convolutional Neural Networks with Limited Training Data for Ear Recognition in the Wild</titletext></citation-title><author-group><author auid="56097253100" seq="1" type="auth"><ce:initials>Z.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emersic</ce:surname><ce:given-name>Ziga</ce:given-name><preferred-name> <ce:initials>Z.</ce:initials> <ce:indexed-name>Emersic Z.</ce:indexed-name> <ce:surname>Emersic</ce:surname> <ce:given-name>Ziga</ce:given-name> </preferred-name></author><author auid="57195223615" seq="2" type="auth"><ce:initials>D.</ce:initials><ce:indexed-name>Stepec D.</ce:indexed-name><ce:surname>Stepec</ce:surname><ce:given-name>Dejan</ce:given-name><preferred-name> <ce:initials>D.</ce:initials> <ce:indexed-name>Stepec D.</ce:indexed-name> <ce:surname>Stepec</ce:surname> <ce:given-name>Dejan</ce:given-name> </preferred-name></author><author auid="7003277146" seq="4" type="auth"><ce:initials>P.</ce:initials><ce:indexed-name>Peer P.</ce:indexed-name><ce:surname>Peer</ce:surname><ce:given-name>Peter</ce:given-name><preferred-name> <ce:initials>P.</ce:initials> <ce:indexed-name>Peer P.</ce:indexed-name> <ce:surname>Peer</ce:surname> <ce:given-name>Peter</ce:given-name> </preferred-name></author><affiliation afid="60031106" country="svn" dptid="104580834"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><affiliation-id afid="60031106" dptid="104580834"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="17347474600" seq="3" type="auth"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Struc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name> <ce:initials>V.</ce:initials> <ce:indexed-name>Struc V.</ce:indexed-name> <ce:surname>Struc</ce:surname> <ce:given-name>Vitomir</ce:given-name> </preferred-name></author><affiliation afid="60031106" country="svn" dptid="112085966"><organization>Faculty of Electrical Engineering</organization><organization>University of Ljubljana</organization><affiliation-id afid="60031106" dptid="112085966"/><country>Slovenia</country></affiliation></author-group><abstracts><abstract original="y" xml:lang="eng"> <publishercopyright>© 2017 IEEE.</publishercopyright> <ce:para>Identity recognition from ear images is an active field of research within the biometric community. The ability to capture ear images from a distance and in a covert manner makes ear recognition technology an appealing choice for surveillance and security applications as well as related application domains. In contrast to other biometric modalities, where large datasets captured in uncontrolled settings are readily available, datasets of ear images are still limited in size and mostly of laboratory-like quality. As a consequence, ear recognition technology has not benefited yet from advances in deep learning and convolutionalneural networks (CNNs) and is still lacking behind other modalities that experienced significant performance gains owing to deep recognition technology. In this paper we address this problem and aim at building a CNNbased ear recognition model. We explore different strategies towards model training with limited amounts of training data and show that by selecting an appropriate model architecture, using aggressive data augmentation and selective learning on existing (pre-trained) models, we are able to learn an effective CNN·based model using a little more than 1300training images. The result of our work is the first CNN·based approach to ear recognition that is also made publicly available to the research community. With our model we are able to improve on the rank one recognition rate of the previous state-of-the-art by more than 25% on a challenging dataset of ear images captured from the web (a.k.a, in the wild).</ce:para> </abstract></abstracts><source country="usa" srcid="21100825137" type="p"><sourcetitle>Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017</sourcetitle><sourcetitle-abbrev>Proc. - IEEE Int. Conf. Auto. Face Gesture Recognit., FG - Int. Workshop Adapt. Shot Learn. Gesture Underst. Prod., ASL4GUP, Biom. Wild, Bwild, Heterog. Face Recognit., HFR, Jt. Chall. Domin. Complement. Emot. Recognit. Micro Emot. Featur. Head-Pose Estim., DCER HPE Facial Expr. Recognit. Anal. Chall., FERA</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017</translated-sourcetitle><issuetitle>Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heterogeneous Face Recognition, HFR 2017, Joint Challenge on Dominant and Complementary Emotion Recognition Using Micro Emotion Features and Head-Pose Estimation, DCER and HPE 2017 and 3rd Facial Expression Recognition and Analysis Challenge, FERA 2017</issuetitle><isbn length="13" level="volume" type="electronic">9781509040230</isbn><volisspag> <pagerange first="987" last="994"/> </volisspag><article-number>7961853</article-number><publicationyear first="2017"/><publicationdate> <year>2017</year> <month>06</month> <day>28</day> <date-text xfab-added="true">28 June 2017</date-text></publicationdate><publisher> <publishername>Institute of Electrical and Electronics Engineers Inc.</publishername> </publisher><additional-srcinfo> <conferenceinfo> <confevent> <confname>12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017</confname> <confnumber>12</confnumber> <confseriestitle>IEEE International Conference on Automatic Face and Gesture Recognition</confseriestitle> <conflocation country="usa"> <city>Washington</city> <state>DC</state> </conflocation> <confdate> <startdate day="30" month="05" year="2017"/> <enddate day="03" month="06" year="2017"/> </confdate> <confcatnumber>E6188</confcatnumber> <confcode>128713</confcode> <confsponsors complete="n"> <confsponsor>3dMD</confsponsor> <confsponsor>Baidu</confsponsor> <confsponsor>DI4D</confsponsor> <confsponsor>et al.</confsponsor> <confsponsor>Mitsubishi Electric Research Laboratories, Inc</confsponsor> <confsponsor>NSF</confsponsor> </confsponsors> </confevent> <confpublication> <procpartno>1 of 1</procpartno> </confpublication> </conferenceinfo> </additional-srcinfo></source><enhancement><classificationgroup><classifications type="ASJC"> <classification>2214</classification> <classification>1702</classification> <classification>1707</classification> </classifications><classifications type="CPXCLASS"> <classification> <classification-code>461</classification-code> <classification-description>Bioengineering</classification-description> </classification> <classification> <classification-code>901.2</classification-code> <classification-description>Education</classification-description> </classification> </classifications><classifications type="FLXCLASS"> <classification> <classification-code>902</classification-code> <classification-description>FLUIDEX; Related Topics</classification-description> </classification> </classifications><classifications type="SUBJABBR"><classification>ENGI</classification><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="41"> <reference id="1"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84956678436</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>L.-C.</ce:initials> <ce:indexed-name>Chen L.-C.</ce:indexed-name> <ce:surname>Chen</ce:surname> </author> <author seq="2"> <ce:initials>G.</ce:initials> <ce:indexed-name>Papandreou G.</ce:indexed-name> <ce:surname>Papandreou</ce:surname> </author> <author seq="3"> <ce:initials>I.</ce:initials> <ce:indexed-name>Kokkinos I.</ce:indexed-name> <ce:surname>Kokkinos</ce:surname> </author> <author seq="4"> <ce:initials>K.</ce:initials> <ce:indexed-name>Murphy K.</ce:indexed-name> <ce:surname>Murphy</ce:surname> </author> <author seq="5"> <ce:initials>A.L.</ce:initials> <ce:indexed-name>Yuille A.L.</ce:indexed-name> <ce:surname>Yuille</ce:surname> </author> </ref-authors> <ref-sourcetitle>Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected Crfs</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-text>arXiv preprint arXiv:1412. 7062</ref-text> </ref-info> <ref-fulltext>L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, "Semantic image segmentation with deep convolutional nets and fully connected crfs, " arXiv preprint arXiv:1412. 7062, 2014.</ref-fulltext> </reference> <reference id="2"> <ref-info> <ref-title> <ref-titletext>Decaf: A deep convolutional activation feature for generic visual recognition</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84906332834</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>J.</ce:initials> <ce:indexed-name>Donahue J.</ce:indexed-name> <ce:surname>Donahue</ce:surname> </author> <author seq="2"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Jia Y.</ce:indexed-name> <ce:surname>Jia</ce:surname> </author> <author seq="3"> <ce:initials>O.</ce:initials> <ce:indexed-name>Vinyals O.</ce:indexed-name> <ce:surname>Vinyals</ce:surname> </author> <author seq="4"> <ce:initials>J.</ce:initials> <ce:indexed-name>Hoffman J.</ce:indexed-name> <ce:surname>Hoffman</ce:surname> </author> <author seq="5"> <ce:initials>N.</ce:initials> <ce:indexed-name>Zhang N.</ce:indexed-name> <ce:surname>Zhang</ce:surname> </author> <author seq="6"> <ce:initials>E.</ce:initials> <ce:indexed-name>Tzeng E.</ce:indexed-name> <ce:surname>Tzeng</ce:surname> </author> <author seq="7"> <ce:initials>T.</ce:initials> <ce:indexed-name>Darrell T.</ce:indexed-name> <ce:surname>Darrell</ce:surname> </author> </ref-authors> <ref-sourcetitle>Icml</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-volisspag> <voliss volume="32"/> <pagerange first="647" last="655"/> </ref-volisspag> </ref-info> <ref-fulltext>J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, "Decaf: A deep convolutional activation feature for generic visual recognition. " in Icml, vol. 32, 2014, pp. 647-655.</ref-fulltext> </reference> <reference id="3"> <ref-info> <ref-title> <ref-titletext>You only look once: Unified, real-time object detection</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84986308404</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>J.</ce:initials> <ce:indexed-name>Redmon J.</ce:indexed-name> <ce:surname>Redmon</ce:surname> </author> <author seq="2"> <ce:initials>S.</ce:initials> <ce:indexed-name>Diwala S.</ce:indexed-name> <ce:surname>Diwala</ce:surname> </author> <author seq="3"> <ce:initials>R.</ce:initials> <ce:indexed-name>Girshick R.</ce:indexed-name> <ce:surname>Girshick</ce:surname> </author> <author seq="4"> <ce:initials>A.</ce:initials> <ce:indexed-name>Farhadi A.</ce:indexed-name> <ce:surname>Farhadi</ce:surname> </author> </ref-authors> <ref-sourcetitle>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</ref-sourcetitle> <ref-publicationyear first="2016"/> <ref-volisspag> <pagerange first="779" last="788"/> </ref-volisspag> </ref-info> <ref-fulltext>J. Redmon, S. Diwala, R. Girshick, and A. Farhadi, "You only look once: Unified, real-time object detection, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 779-788.</ref-fulltext> </reference> <reference id="4"> <ref-info> <ref-title> <ref-titletext>Scalable object detection using deep neural networks</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84911443425</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>D.</ce:initials> <ce:indexed-name>Erhan D.</ce:indexed-name> <ce:surname>Erhan</ce:surname> </author> <author seq="2"> <ce:initials>C.</ce:initials> <ce:indexed-name>Szegedy C.</ce:indexed-name> <ce:surname>Szegedy</ce:surname> </author> <author seq="3"> <ce:initials>A.</ce:initials> <ce:indexed-name>Toshev A.</ce:indexed-name> <ce:surname>Toshev</ce:surname> </author> <author seq="4"> <ce:initials>D.</ce:initials> <ce:indexed-name>Anguelov D.</ce:indexed-name> <ce:surname>Anguelov</ce:surname> </author> </ref-authors> <ref-sourcetitle>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-volisspag> <pagerange first="2147" last="2154"/> </ref-volisspag> </ref-info> <ref-fulltext>D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, "Scalable object detection using deep neural networks, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 2147-2154.</ref-fulltext> </reference> <reference id="5"> <ref-info> <ref-title> <ref-titletext>Learning a deep convolutional network for image super-resolution</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84906484697</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>C.</ce:initials> <ce:indexed-name>Dong C.</ce:indexed-name> <ce:surname>Dong</ce:surname> </author> <author seq="2"> <ce:initials>C.C.</ce:initials> <ce:indexed-name>Loy C.C.</ce:indexed-name> <ce:surname>Loy</ce:surname> </author> <author seq="3"> <ce:initials>K.</ce:initials> <ce:indexed-name>He K.</ce:indexed-name> <ce:surname>He</ce:surname> </author> <author seq="4"> <ce:initials>X.</ce:initials> <ce:indexed-name>Tang X.</ce:indexed-name> <ce:surname>Tang</ce:surname> </author> </ref-authors> <ref-sourcetitle>European Conference on Computer Vision</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-volisspag> <pagerange first="184" last="199"/> </ref-volisspag> <ref-text>Springer</ref-text> </ref-info> <ref-fulltext>C. Dong, C. C. Loy, K. He, and X. Tang, "Learning a deep convolutional network for image super-resolution, " in European Conference on Computer Vision. Springer, 2014, pp. 184-199.</ref-fulltext> </reference> <reference id="6"> <ref-info> <ref-title> <ref-titletext>Deep network cascade for image super-resolution</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84906509108</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>Z.</ce:initials> <ce:indexed-name>Cui Z.</ce:indexed-name> <ce:surname>Cui</ce:surname> </author> <author seq="2"> <ce:initials>H.</ce:initials> <ce:indexed-name>Chang H.</ce:indexed-name> <ce:surname>Chang</ce:surname> </author> <author seq="3"> <ce:initials>S.</ce:initials> <ce:indexed-name>Shan S.</ce:indexed-name> <ce:surname>Shan</ce:surname> </author> <author seq="4"> <ce:initials>B.</ce:initials> <ce:indexed-name>Zhong B.</ce:indexed-name> <ce:surname>Zhong</ce:surname> </author> <author seq="5"> <ce:initials>X.</ce:initials> <ce:indexed-name>Chen X.</ce:indexed-name> <ce:surname>Chen</ce:surname> </author> </ref-authors> <ref-sourcetitle>European Conference on Computer Vision</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-volisspag> <pagerange first="49" last="64"/> </ref-volisspag> <ref-text>Springer</ref-text> </ref-info> <ref-fulltext>Z. Cui, H. Chang, S. Shan, B. Zhong, and X. Chen, "Deep network cascade for image super-resolution, " in European Conference on Computer Vision. Springer, 2014, pp. 49-64.</ref-fulltext> </reference> <reference id="7"> <ref-info> <ref-title> <ref-titletext>Image super-resolution using deep convolutional networks</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84962128851</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>C.</ce:initials> <ce:indexed-name>Dong C.</ce:indexed-name> <ce:surname>Dong</ce:surname> </author> <author seq="2"> <ce:initials>C.C.</ce:initials> <ce:indexed-name>Loy C.C.</ce:indexed-name> <ce:surname>Loy</ce:surname> </author> <author seq="3"> <ce:initials>K.</ce:initials> <ce:indexed-name>He K.</ce:indexed-name> <ce:surname>He</ce:surname> </author> <author seq="4"> <ce:initials>X.</ce:initials> <ce:indexed-name>Tang X.</ce:indexed-name> <ce:surname>Tang</ce:surname> </author> </ref-authors> <ref-sourcetitle>IEEE Transactions on Pattern Analysis and Machine Intelligence</ref-sourcetitle> <ref-publicationyear first="2016"/> <ref-volisspag> <voliss issue="2" volume="38"/> <pagerange first="295" last="307"/> </ref-volisspag> </ref-info> <ref-fulltext>C. Dong, C. C. Loy, K. He, and X. Tang, "Image super-resolution using deep convolutional networks, " IEEE transactions on pattern analysis and machine intelligence, vol. 38, no. 2, pp. 295-307, 2016.</ref-fulltext> </reference> <reference id="8"> <ref-info> <ref-title> <ref-titletext>Discriminative deep metric learning for face verification in the wild</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84911459575</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>J.</ce:initials> <ce:indexed-name>Hu J.</ce:indexed-name> <ce:surname>Hu</ce:surname> </author> <author seq="2"> <ce:initials>J.</ce:initials> <ce:indexed-name>Lu J.</ce:indexed-name> <ce:surname>Lu</ce:surname> </author> <author seq="3"> <ce:initials>Y.-R.</ce:initials> <ce:indexed-name>Tan Y.-R.</ce:indexed-name> <ce:surname>Tan</ce:surname> </author> </ref-authors> <ref-sourcetitle>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-volisspag> <pagerange first="1875" last="1882"/> </ref-volisspag> </ref-info> <ref-fulltext>J. Hu, J. Lu, and Y.-R Tan, "Discriminative deep metric learning for face verification in the wild, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1875-1882.</ref-fulltext> </reference> <reference id="9"> <ref-info> <ref-title> <ref-titletext>Deep metric learning using triplet network</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84945953932</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>E.</ce:initials> <ce:indexed-name>Hoffer E.</ce:indexed-name> <ce:surname>Hoffer</ce:surname> </author> <author seq="2"> <ce:initials>N.</ce:initials> <ce:indexed-name>Ailon N.</ce:indexed-name> <ce:surname>Ailon</ce:surname> </author> </ref-authors> <ref-sourcetitle>International Workshop on Similarity-Based Pattern Recognition</ref-sourcetitle> <ref-publicationyear first="2015"/> <ref-volisspag> <pagerange first="84" last="92"/> </ref-volisspag> <ref-text>Springer</ref-text> </ref-info> <ref-fulltext>E. Hoffer and N. Ailon, "Deep metric learning using triplet network, " in International Workshop on Similarity-Based Pattern Recognition. Springer, 2015, pp. 84-92.</ref-fulltext> </reference> <reference id="10"> <ref-info> <ref-title> <ref-titletext>Deep learning for visual understanding: A review</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84957837518</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Guo Y.</ce:indexed-name> <ce:surname>Guo</ce:surname> </author> <author seq="2"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Liu Y.</ce:indexed-name> <ce:surname>Liu</ce:surname> </author> <author seq="3"> <ce:initials>A.</ce:initials> <ce:indexed-name>Oerlemans A.</ce:indexed-name> <ce:surname>Oerlemans</ce:surname> </author> <author seq="4"> <ce:initials>S.</ce:initials> <ce:indexed-name>Lao S.</ce:indexed-name> <ce:surname>Lao</ce:surname> </author> <author seq="5"> <ce:initials>S.</ce:initials> <ce:indexed-name>Wu S.</ce:indexed-name> <ce:surname>Wu</ce:surname> </author> <author seq="6"> <ce:initials>M.S.</ce:initials> <ce:indexed-name>Lew M.S.</ce:indexed-name> <ce:surname>Lew</ce:surname> </author> </ref-authors> <ref-sourcetitle>Neurocomputing</ref-sourcetitle> <ref-publicationyear first="2016"/> <ref-volisspag> <voliss volume="187"/> <pagerange first="278"/> </ref-volisspag> </ref-info> <ref-fulltext>Y. Guo, Y. Liu, A. Oerlemans, S. Lao, S. Wu, and M. S. Lew, "Deep learning for visual understanding: A review, " Neurocomputing, vol. 187, pp. 27-8, 2016.</ref-fulltext> </reference> <reference id="11"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84982835563</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>J.</ce:initials> <ce:indexed-name>Guo J.</ce:indexed-name> <ce:surname>Guo</ce:surname> </author> <author seq="2"> <ce:initials>S.</ce:initials> <ce:indexed-name>Gould S.</ce:indexed-name> <ce:surname>Gould</ce:surname> </author> </ref-authors> <ref-sourcetitle>Deep Cnn Ensemble with Data Augmentation for Object Detection</ref-sourcetitle> <ref-publicationyear first="2015"/> <ref-text>arXiv preprint arXiv:1506. 07224</ref-text> </ref-info> <ref-fulltext>J. Guo and S. Gould, "Deep cnn ensemble with data augmentation for object detection, " arXiv preprint arXiv:1506. 07224, 2015.</ref-fulltext> </reference> <reference id="12"> <ref-info> <ref-title> <ref-titletext>Data augmentation in cnn-based periocular authentication</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">85010452350</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>R.</ce:initials> <ce:indexed-name>Dellana R.</ce:indexed-name> <ce:surname>Dellana</ce:surname> </author> <author seq="2"> <ce:initials>K.</ce:initials> <ce:indexed-name>Roy K.</ce:indexed-name> <ce:surname>Roy</ce:surname> </author> </ref-authors> <ref-sourcetitle>Information Communication and Management (ICICM), International Conference On. IEEE</ref-sourcetitle> <ref-publicationyear first="2016"/> <ref-volisspag> <pagerange first="141" last="145"/> </ref-volisspag> </ref-info> <ref-fulltext>R. Dellana and K. Roy, "Data augmentation in cnn-based periocular authentication, " in Information Communication and Management (ICICM), International Conference on. IEEE, 2016, pp. 141-145.</ref-fulltext> </reference> <reference id="13"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">85026288891</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>G.</ce:initials> <ce:indexed-name>Hu G.</ce:indexed-name> <ce:surname>Hu</ce:surname> </author> <author seq="2"> <ce:initials>X.</ce:initials> <ce:indexed-name>Peng X.</ce:indexed-name> <ce:surname>Peng</ce:surname> </author> <author seq="3"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Yang Y.</ce:indexed-name> <ce:surname>Yang</ce:surname> </author> <author seq="4"> <ce:initials>T.</ce:initials> <ce:indexed-name>Hospedales T.</ce:indexed-name> <ce:surname>Hospedales</ce:surname> </author> <author seq="5"> <ce:initials>J.</ce:initials> <ce:indexed-name>Verbeek J.</ce:indexed-name> <ce:surname>Verbeek</ce:surname> </author> </ref-authors> <ref-sourcetitle>Frankenstein: Learning Deep Face Representations Using Small Data</ref-sourcetitle> <ref-publicationyear first="2016"/> <ref-text>arXiv preprint arXiv:1603. 06470</ref-text> </ref-info> <ref-fulltext>G. Hu, X. Peng, Y. Yang, T. Hospedales, and J. Verbeek, "Frankenstein: Learning deep face representations using small data, " arXiv preprint arXiv:1603. 06470, 2016.</ref-fulltext> </reference> <reference id="14"> <ref-info> <ref-title> <ref-titletext>Deep face recognition for smart surveillance applications</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">85026327096</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>K.</ce:initials> <ce:indexed-name>Grm K.</ce:indexed-name> <ce:surname>Grm</ce:surname> </author> <author seq="2"> <ce:initials>V.</ce:initials> <ce:indexed-name>Struc V.</ce:indexed-name> <ce:surname>Struc</ce:surname> </author> </ref-authors> <ref-sourcetitle>IEEE Intelligent Systems</ref-sourcetitle> <ref-publicationyear first="2017"/> <ref-text>in press</ref-text> </ref-info> <ref-fulltext>K. Grm and V. Struc, "Deep face recognition for smart surveillance applications, " IEEE Intelligent Systems, p. in press, 2017.</ref-fulltext> </reference> <reference id="15"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84913555165</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Jia Y.</ce:indexed-name> <ce:surname>Jia</ce:surname> </author> <author seq="2"> <ce:initials>E.</ce:initials> <ce:indexed-name>Shelhamer E.</ce:indexed-name> <ce:surname>Shelhamer</ce:surname> </author> <author seq="3"> <ce:initials>J.</ce:initials> <ce:indexed-name>Donahue J.</ce:indexed-name> <ce:surname>Donahue</ce:surname> </author> <author seq="4"> <ce:initials>S.</ce:initials> <ce:indexed-name>Karayev S.</ce:indexed-name> <ce:surname>Karayev</ce:surname> </author> <author seq="5"> <ce:initials>J.</ce:initials> <ce:indexed-name>Long J.</ce:indexed-name> <ce:surname>Long</ce:surname> </author> <author seq="6"> <ce:initials>R.</ce:initials> <ce:indexed-name>Girshick R.</ce:indexed-name> <ce:surname>Girshick</ce:surname> </author> <author seq="7"> <ce:initials>S.</ce:initials> <ce:indexed-name>Guadarrama S.</ce:indexed-name> <ce:surname>Guadarrama</ce:surname> </author> <author seq="8"> <ce:initials>T.</ce:initials> <ce:indexed-name>Darrell T.</ce:indexed-name> <ce:surname>Darrell</ce:surname> </author> </ref-authors> <ref-sourcetitle>Caffe: Convolutional Architecture for Fast Feature Embedding</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-text>arXiv preprint arXiv:1408. 5093</ref-text> </ref-info> <ref-fulltext>Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, "Caffe: Convolutional architecture for fast feature embedding, " arXiv preprint arXiv:1408. 5093, 2014.</ref-fulltext> </reference> <reference id="16"> <ref-info> <ref-title> <ref-titletext>Imagenet classification with deep convolutional neural networks</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84876231242</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>A.</ce:initials> <ce:indexed-name>Krizhevsky A.</ce:indexed-name> <ce:surname>Krizhevsky</ce:surname> </author> <author seq="2"> <ce:initials>I.</ce:initials> <ce:indexed-name>Sutskever I.</ce:indexed-name> <ce:surname>Sutskever</ce:surname> </author> <author seq="3"> <ce:initials>G.E.</ce:initials> <ce:indexed-name>Hinton G.E.</ce:indexed-name> <ce:surname>Hinton</ce:surname> </author> </ref-authors> <ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle> <ref-publicationyear first="2012"/> <ref-volisspag> <pagerange first="1097" last="1105"/> </ref-volisspag> </ref-info> <ref-fulltext>A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks, " in Advances in neural information processing systems, 2012, pp. 1097-1105.</ref-fulltext> </reference> <reference id="17"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84925410541</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>K.</ce:initials> <ce:indexed-name>Simonyan K.</ce:indexed-name> <ce:surname>Simonyan</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Zisserman A.</ce:indexed-name> <ce:surname>Zisserman</ce:surname> </author> </ref-authors> <ref-sourcetitle>Very Deep Convolutional Networks for Large-scale Image Recognition</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-text>arXiv preprint arXiv:1409. 1556</ref-text> </ref-info> <ref-fulltext>K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition, " arXiv preprint arXiv:1409. 1556, 2014.</ref-fulltext> </reference> <reference id="18"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84988340112</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>F.N.</ce:initials> <ce:indexed-name>Iandola F.N.</ce:indexed-name> <ce:surname>Iandola</ce:surname> </author> <author seq="2"> <ce:initials>M.W.</ce:initials> <ce:indexed-name>Moskewicz M.W.</ce:indexed-name> <ce:surname>Moskewicz</ce:surname> </author> <author seq="3"> <ce:initials>K.</ce:initials> <ce:indexed-name>Ashraf K.</ce:indexed-name> <ce:surname>Ashraf</ce:surname> </author> <author seq="4"> <ce:initials>S.</ce:initials> <ce:indexed-name>Han S.</ce:indexed-name> <ce:surname>Han</ce:surname> </author> <author seq="5"> <ce:initials>W.J.</ce:initials> <ce:indexed-name>Dally W.J.</ce:indexed-name> <ce:surname>Dally</ce:surname> </author> <author seq="6"> <ce:initials>K.</ce:initials> <ce:indexed-name>Keutzer K.</ce:indexed-name> <ce:surname>Keutzer</ce:surname> </author> </ref-authors> <ref-sourcetitle>Squeezenet: Alexnet-level Accuracy with 50x Fewer Parameters and &lt;Lmb Model Size</ref-sourcetitle> <ref-publicationyear first="2016"/> <ref-text>arXiv:1602. 07360</ref-text> </ref-info> <ref-fulltext>F. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K. Keutzer, "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;lmb model size, " arXiv:1602. 07360, 2016.</ref-fulltext> </reference> <reference id="19"> <ref-info> <ref-title> <ref-titletext>Ear recognition: More than a survey</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">85017361058</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>Z.</ce:initials> <ce:indexed-name>Emersic Z.</ce:indexed-name> <ce:surname>Emersic</ce:surname> </author> <author seq="2"> <ce:initials>V.</ce:initials> <ce:indexed-name>Struc V.</ce:indexed-name> <ce:surname>Struc</ce:surname> </author> <author seq="3"> <ce:initials>P.</ce:initials> <ce:indexed-name>Peer P.</ce:indexed-name> <ce:surname>Peer</ce:surname> </author> </ref-authors> <ref-sourcetitle>Neurocomputing</ref-sourcetitle> <ref-publicationyear first="2017"/> </ref-info> <ref-fulltext>Z. Emersic, V. Struc, and P. Peer, "Ear Recognition: More Than a Survey, " Neurocomputing, 2017.</ref-fulltext> </reference> <reference id="20"> <ref-info> <ref-title> <ref-titletext>Ear biometrics: A survey of detection, feature extraction and recognition methods</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84866878822</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>A.</ce:initials> <ce:indexed-name>Pflug A.</ce:indexed-name> <ce:surname>Pflug</ce:surname> </author> <author seq="2"> <ce:initials>C.</ce:initials> <ce:indexed-name>Busch C.</ce:indexed-name> <ce:surname>Busch</ce:surname> </author> </ref-authors> <ref-sourcetitle>IET Biometrics</ref-sourcetitle> <ref-publicationyear first="2012"/> <ref-volisspag> <voliss issue="2" volume="1"/> <pagerange first="114" last="129"/> </ref-volisspag> </ref-info> <ref-fulltext>A. Pflug and C. Busch, "Ear biometrics: a survey of detection, feature extraction and recognition methods, " IET biometrics, vol. 1, no. 2, pp. 114-129, 2012.</ref-fulltext> </reference> <reference id="21"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">85026308042</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>D.J.</ce:initials> <ce:indexed-name>Hurley D.J.</ce:indexed-name> <ce:surname>Hurley</ce:surname> </author> <author seq="2"> <ce:initials>M.S.</ce:initials> <ce:indexed-name>Nixon M.S.</ce:indexed-name> <ce:surname>Nixon</ce:surname> </author> <author seq="3"> <ce:initials>J.N.</ce:initials> <ce:indexed-name>Carter J.N.</ce:indexed-name> <ce:surname>Carter</ce:surname> </author> </ref-authors> <ref-sourcetitle>Automatic ear recognition by force field transformations</ref-sourcetitle> <ref-publicationyear first="2000"/> </ref-info> <ref-fulltext>D. J. Hurley, M. S. Nixon, and J. N. Carter, "Automatic ear recognition by force field transformations, " 2000.</ref-fulltext> </reference> <reference id="22"> <ref-info> <ref-title> <ref-titletext>Ear recognition using improved non-negative matrix factorization</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">34147163370</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>L.</ce:initials> <ce:indexed-name>Yuan L.</ce:indexed-name> <ce:surname>Yuan</ce:surname> </author> <author seq="2"> <ce:initials>Z.-C.</ce:initials> <ce:indexed-name>Mu Z.-C.</ce:indexed-name> <ce:surname>Mu</ce:surname> </author> <author seq="3"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Zhang Y.</ce:indexed-name> <ce:surname>Zhang</ce:surname> </author> <author seq="4"> <ce:initials>K.</ce:initials> <ce:indexed-name>Liu K.</ce:indexed-name> <ce:surname>Liu</ce:surname> </author> </ref-authors> <ref-sourcetitle>Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</ref-sourcetitle> <ref-publicationyear first="2006"/> <ref-volisspag> <voliss volume="4"/> <pagerange first="501" last="504"/> </ref-volisspag> <ref-text>IEEE</ref-text> </ref-info> <ref-fulltext>L. Yuan, Z.-c. Mu, Y. Zhang, and K. Liu, "Ear recognition using improved non-negative matrix factorization, " in Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, vol. 4. IEEE, 2006, pp. 501-504.</ref-fulltext> </reference> <reference id="23"> <ref-info> <ref-title> <ref-titletext>A novel approach for ear recognition based on ica and rbf network</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">28444488351</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>H.-J.</ce:initials> <ce:indexed-name>Zhang H.-J.</ce:indexed-name> <ce:surname>Zhang</ce:surname> </author> <author seq="2"> <ce:initials>Z.-C.</ce:initials> <ce:indexed-name>Mu Z.-C.</ce:indexed-name> <ce:surname>Mu</ce:surname> </author> <author seq="3"> <ce:initials>W.</ce:initials> <ce:indexed-name>Qu W.</ce:indexed-name> <ce:surname>Qu</ce:surname> </author> <author seq="4"> <ce:initials>L.-M.</ce:initials> <ce:indexed-name>Liu L.-M.</ce:indexed-name> <ce:surname>Liu</ce:surname> </author> <author seq="5"> <ce:initials>C.-Y.</ce:initials> <ce:indexed-name>Zhang C.-Y.</ce:indexed-name> <ce:surname>Zhang</ce:surname> </author> </ref-authors> <ref-sourcetitle>Machine Learning and Cybernetics, 2005. Proceedings of 2005 International Conference on</ref-sourcetitle> <ref-publicationyear first="2005"/> <ref-volisspag> <voliss volume="7"/> <pagerange first="4511" last="4515"/> </ref-volisspag> <ref-text>IEEE</ref-text> </ref-info> <ref-fulltext>H.-J. Zhang, Z.-C. Mu, W. Qu, L.-M. Liu, and C.-Y. Zhang, "A novel approach for ear recognition based on ica and rbf network, " in Machine Learning and Cybernetics, 2005. Proceedings of 2005 International Conference on, vol. 7. IEEE, 2005, pp. 4511-4515.</ref-fulltext> </reference> <reference id="24"> <ref-info> <ref-title> <ref-titletext>Object recognition from local scale-invariant features</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">0033284915</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>D.G.</ce:initials> <ce:indexed-name>Lowe D.G.</ce:indexed-name> <ce:surname>Lowe</ce:surname> </author> </ref-authors> <ref-sourcetitle>Computer Vision, 1999. the Proceedings of the Seventh IEEE International Conference on</ref-sourcetitle> <ref-publicationyear first="1999"/> <ref-volisspag> <voliss volume="2"/> <pagerange first="1150" last="1157"/> </ref-volisspag> <ref-text>Ieee</ref-text> </ref-info> <ref-fulltext>D. G. Lowe, "Object recognition from local scale-invariant features, " in Computer vision, 1999. The proceedings of the seventh IEEE international conference on, vol. 2. Ieee, 1999, pp. 1150-1157.</ref-fulltext> </reference> <reference id="25"> <ref-info> <ref-title> <ref-titletext>Ear recognition using a new local matching approach</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">69949178844</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>Y.</ce:initials> <ce:indexed-name>Guo Y.</ce:indexed-name> <ce:surname>Guo</ce:surname> </author> <author seq="2"> <ce:initials>Z.</ce:initials> <ce:indexed-name>Xu Z.</ce:indexed-name> <ce:surname>Xu</ce:surname> </author> </ref-authors> <ref-sourcetitle>Image Processing, 2008. ICIP 2008. 15th IEEE International Conference On. IEEE</ref-sourcetitle> <ref-publicationyear first="2008"/> <ref-volisspag> <pagerange first="289" last="292"/> </ref-volisspag> </ref-info> <ref-fulltext>Y. Guo and Z. Xu, "Ear recognition using a new local matching approach, " in Image Processing, 2008. ICIP 2008. 15th IEEE International Conference on. IEEE, 2008, pp. 289-292.</ref-fulltext> </reference> <reference id="26"> <ref-info> <ref-title> <ref-titletext>Rotation invariant local phase quantization for blur insensitive texture analysis</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">77957945272</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>V.</ce:initials> <ce:indexed-name>Ojansivu V.</ce:indexed-name> <ce:surname>Ojansivu</ce:surname> </author> <author seq="2"> <ce:initials>E.</ce:initials> <ce:indexed-name>Rahtu E.</ce:indexed-name> <ce:surname>Rahtu</ce:surname> </author> <author seq="3"> <ce:initials>J.</ce:initials> <ce:indexed-name>Heikkila J.</ce:indexed-name> <ce:surname>Heikkila</ce:surname> </author> </ref-authors> <ref-sourcetitle>Pattern Recognition, 2008. ICPR 2008. 19th International Conference On. IEEE</ref-sourcetitle> <ref-publicationyear first="2008"/> <ref-volisspag> <pagerange first="1"/> </ref-volisspag> </ref-info> <ref-fulltext>V. Ojansivu, E. Rahtu, and J. Heikkila, "Rotation invariant local phase quantization for blur insensitive texture analysis, " in Pattern Recognition, 2008. ICPR 2008. 19th International Conference on. IEEE, 2008, pp. 1-1.</ref-fulltext> </reference> <reference id="27"> <ref-info> <ref-title> <ref-titletext>Tace recognition with patterns of oriented edge magnitudes</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">78149286544</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>N.-S.</ce:initials> <ce:indexed-name>Vu N.-S.</ce:indexed-name> <ce:surname>Vu</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Caplier A.</ce:indexed-name> <ce:surname>Caplier</ce:surname> </author> </ref-authors> <ref-sourcetitle>European Conference on Computer Vision</ref-sourcetitle> <ref-publicationyear first="2010"/> <ref-volisspag> <pagerange first="313" last="326"/> </ref-volisspag> <ref-text>Springer</ref-text> </ref-info> <ref-fulltext>N.-S. Vu and A. Caplier, 'Tace recognition with patterns of oriented edge magnitudes, " in European conference on computer vision. Springer, 2010, pp. 313-326.</ref-fulltext> </reference> <reference id="28"> <ref-info> <ref-title> <ref-titletext>A multi-matcher for ear authentication</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">35148850096</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>L.</ce:initials> <ce:indexed-name>Nanni L.</ce:indexed-name> <ce:surname>Nanni</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Lumini A.</ce:indexed-name> <ce:surname>Lumini</ce:surname> </author> </ref-authors> <ref-sourcetitle>Pattern Recognition Letters</ref-sourcetitle> <ref-publicationyear first="2007"/> <ref-volisspag> <voliss issue="16" volume="28"/> <pagerange first="2219" last="2226"/> </ref-volisspag> </ref-info> <ref-fulltext>L. Nanni and A. Lumini, "A multi-matcher for ear authentication, " Pattern Recognition Letters, vol. 28, no. 16, pp. 2219-2226, 2007.</ref-fulltext> </reference> <reference id="29"> <ref-info> <ref-title> <ref-titletext>Ear recognition using texture features - A novel approach</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84911902192</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>L.</ce:initials> <ce:indexed-name>Jacob L.</ce:indexed-name> <ce:surname>Jacob</ce:surname> </author> <author seq="2"> <ce:initials>G.</ce:initials> <ce:indexed-name>Raju G.</ce:indexed-name> <ce:surname>Raju</ce:surname> </author> </ref-authors> <ref-sourcetitle>Advances in Signal Processing and Intelligent Recognition Systems</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-volisspag> <pagerange first="1" last="12"/> </ref-volisspag> <ref-text>Springer</ref-text> </ref-info> <ref-fulltext>L. Jacob and G. Raju, "Ear recognition using texture features - A novel approach, " in Advances in Signal Processing and Intelligent Recognition Systems. Springer, 2014, pp. 1-12.</ref-fulltext> </reference> <reference id="30"> <ref-info> <ref-title> <ref-titletext>Earprint recognition based on an ensemble of global and local features</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84964840268</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>A.</ce:initials> <ce:indexed-name>Morales A.</ce:indexed-name> <ce:surname>Morales</ce:surname> </author> <author seq="2"> <ce:initials>M.</ce:initials> <ce:indexed-name>Diaz M.</ce:indexed-name> <ce:surname>Diaz</ce:surname> </author> <author seq="3"> <ce:initials>G.</ce:initials> <ce:indexed-name>Llinas-Sanchez G.</ce:indexed-name> <ce:surname>Llinas-Sanchez</ce:surname> </author> <author seq="4"> <ce:initials>M.A.</ce:initials> <ce:indexed-name>Ferrer M.A.</ce:indexed-name> <ce:surname>Ferrer</ce:surname> </author> </ref-authors> <ref-sourcetitle>Security Technology (ICCST), 2015 International Carnahan Conference On. IEEE</ref-sourcetitle> <ref-publicationyear first="2015"/> <ref-volisspag> <pagerange first="253" last="258"/> </ref-volisspag> </ref-info> <ref-fulltext>A. Morales, M. Diaz, G. Llinas-Sanchez, and M. A. Ferrer, "Earprint recognition based on an ensemble of global and local features, " in Security Technology (ICCST), 2015 International Carnahan Conference on. IEEE, 2015, pp. 253-258.</ref-fulltext> </reference> <reference id="31"> <ref-info> <ref-title> <ref-titletext>A survey on ear biometrics</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84875180794</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>A.</ce:initials> <ce:indexed-name>Abaza A.</ce:indexed-name> <ce:surname>Abaza</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Ross A.</ce:indexed-name> <ce:surname>Ross</ce:surname> </author> <author seq="3"> <ce:initials>C.</ce:initials> <ce:indexed-name>Hebert C.</ce:indexed-name> <ce:surname>Hebert</ce:surname> </author> <author seq="4"> <ce:initials>M.A.F.</ce:initials> <ce:indexed-name>Harrison M.A.F.</ce:indexed-name> <ce:surname>Harrison</ce:surname> </author> <author seq="5"> <ce:initials>M.S.</ce:initials> <ce:indexed-name>Nixon M.S.</ce:indexed-name> <ce:surname>Nixon</ce:surname> </author> </ref-authors> <ref-sourcetitle>ACM Computing Surveys (CSUR)</ref-sourcetitle> <ref-publicationyear first="2013"/> <ref-volisspag> <voliss issue="2" volume="45"/> <pagerange first="22"/> </ref-volisspag> </ref-info> <ref-fulltext>A. Abaza, A. Ross, C. Hebert, M. A. F. Harrison, and M. S. Nixon, "A survey on ear biometrics, " ACM computing surveys (CSUR), vol. 45, no. 2, p. 22, 2013.</ref-fulltext> </reference> <reference id="32"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84909978410</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>O.</ce:initials> <ce:indexed-name>Russakovsky O.</ce:indexed-name> <ce:surname>Russakovsky</ce:surname> </author> <author seq="2"> <ce:initials>J.</ce:initials> <ce:indexed-name>Deng J.</ce:indexed-name> <ce:surname>Deng</ce:surname> </author> <author seq="3"> <ce:initials>H.</ce:initials> <ce:indexed-name>Su H.</ce:indexed-name> <ce:surname>Su</ce:surname> </author> <author seq="4"> <ce:initials>J.</ce:initials> <ce:indexed-name>Krause J.</ce:indexed-name> <ce:surname>Krause</ce:surname> </author> <author seq="5"> <ce:initials>S.</ce:initials> <ce:indexed-name>Satheesh S.</ce:indexed-name> <ce:surname>Satheesh</ce:surname> </author> <author seq="6"> <ce:initials>S.</ce:initials> <ce:indexed-name>Ma S.</ce:indexed-name> <ce:surname>Ma</ce:surname> </author> <author seq="7"> <ce:initials>Z.</ce:initials> <ce:indexed-name>Huang Z.</ce:indexed-name> <ce:surname>Huang</ce:surname> </author> <author seq="8"> <ce:initials>A.</ce:initials> <ce:indexed-name>Karpathy A.</ce:indexed-name> <ce:surname>Karpathy</ce:surname> </author> <author seq="9"> <ce:initials>A.</ce:initials> <ce:indexed-name>Khosla A.</ce:indexed-name> <ce:surname>Khosla</ce:surname> </author> <author seq="10"> <ce:initials>M.S.</ce:initials> <ce:indexed-name>Bernstein M.S.</ce:indexed-name> <ce:surname>Bernstein</ce:surname> </author> <author seq="11"> <ce:initials>A.C.</ce:initials> <ce:indexed-name>Berg A.C.</ce:indexed-name> <ce:surname>Berg</ce:surname> </author> <author seq="12"> <ce:initials>F.</ce:initials> <ce:indexed-name>Li F.</ce:indexed-name> <ce:surname>Li</ce:surname> </author> </ref-authors> <ref-sourcetitle>Imagenet Large Scale Visual Recognition Challenge</ref-sourcetitle> <ref-publicationyear first="2014"/> <ref-website> <ce:e-address type="email">http://arxiv.org/abs/1409.0575</ce:e-address> </ref-website> <ref-text>CoRR abs/1409. 0575</ref-text> </ref-info> <ref-fulltext>O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and F. Li, "Imagenet large scale visual recognition challenge, " CoRR, vol. abs/1409. 0575, 2014. [Online]. Available: http://arxiv. org/abs/1409. 0575</ref-fulltext> </reference> <reference id="33"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84955283951</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>S.</ce:initials> <ce:indexed-name>Ren S.</ce:indexed-name> <ce:surname>Ren</ce:surname> </author> <author seq="2"> <ce:initials>K.</ce:initials> <ce:indexed-name>He K.</ce:indexed-name> <ce:surname>He</ce:surname> </author> <author seq="3"> <ce:initials>R.B.</ce:initials> <ce:indexed-name>Girshick R.B.</ce:indexed-name> <ce:surname>Girshick</ce:surname> </author> <author seq="4"> <ce:initials>J.</ce:initials> <ce:indexed-name>Sun J.</ce:indexed-name> <ce:surname>Sun</ce:surname> </author> </ref-authors> <ref-sourcetitle>Taster R-CNN: Towards real-time object detection with region proposal networks</ref-sourcetitle> <ref-publicationyear first="2015"/> <ref-website> <ce:e-address type="email">http://arxiv.org/abs/1506.01497</ce:e-address> </ref-website> <ref-text>CoRR abs/1506. 01497</ref-text> </ref-info> <ref-fulltext>S. Ren, K. He, R. B. Girshick, and J. Sun, 'Taster R-CNN: towards real-time object detection with region proposal networks, " CoRR, vol. abs/1506. 01497, 2015. [Online]. Available: http://arxiv. org/abs/1506. 01497</ref-fulltext> </reference> <reference id="34"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84989829253</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>V.</ce:initials> <ce:indexed-name>Badrinarayanan V.</ce:indexed-name> <ce:surname>Badrinarayanan</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Kendall A.</ce:indexed-name> <ce:surname>Kendall</ce:surname> </author> <author seq="3"> <ce:initials>R.</ce:initials> <ce:indexed-name>Cipolla R.</ce:indexed-name> <ce:surname>Cipolla</ce:surname> </author> </ref-authors> <ref-sourcetitle>Segnet: A Deep Convolutional Encoder-decoder Architecture for Image Segmentation</ref-sourcetitle> <ref-publicationyear first="2015"/> <ref-text>arXiv preprint arXiv:1511. 00561</ref-text> </ref-info> <ref-fulltext>V. Badrinarayanan, A. Kendall, and R. Cipolla, "Segnet: A deep convolutional encoder-decoder architecture for image segmentation, " arXiv preprint arXiv:1511. 00561, 2015.</ref-fulltext> </reference> <reference id="35"> <ref-info> <refd-itemidlist> <itemid idtype="SGR">84973893180</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>V.</ce:initials> <ce:indexed-name>Badrinarayanan V.</ce:indexed-name> <ce:surname>Badrinarayanan</ce:surname> </author> <author seq="2"> <ce:initials>A.</ce:initials> <ce:indexed-name>Handa A.</ce:indexed-name> <ce:surname>Handa</ce:surname> </author> <author seq="3"> <ce:initials>R.</ce:initials> <ce:indexed-name>Cipolla R.</ce:indexed-name> <ce:surname>Cipolla</ce:surname> </author> </ref-authors> <ref-sourcetitle>Segnet: A Deep Convolutional Encoder-decoder Architecture for Robust Semantic Pixel-wise Labelling</ref-sourcetitle> <ref-publicationyear first="2015"/> <ref-text>arXiv preprint arXiv:1505. 07293</ref-text> </ref-info> <ref-fulltext>V. Badrinarayanan, A. Handa, and R. Cipolla, "Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling, " arXiv preprint arXiv:1505. 07293, 2015.</ref-fulltext> </reference> <reference id="36"> <ref-info> <ref-title> <ref-titletext>Deep residual learning for image recognition</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84986274465</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>K.</ce:initials> <ce:indexed-name>He K.</ce:indexed-name> <ce:surname>He</ce:surname> </author> <author seq="2"> <ce:initials>X.</ce:initials> <ce:indexed-name>Zhang X.</ce:indexed-name> <ce:surname>Zhang</ce:surname> </author> <author seq="3"> <ce:initials>S.</ce:initials> <ce:indexed-name>Ren S.</ce:indexed-name> <ce:surname>Ren</ce:surname> </author> <author seq="4"> <ce:initials>J.</ce:initials> <ce:indexed-name>Sun J.</ce:indexed-name> <ce:surname>Sun</ce:surname> </author> </ref-authors> <ref-sourcetitle>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</ref-sourcetitle> <ref-publicationyear first="2016"/> <ref-volisspag> <pagerange first="770" last="778"/> </ref-volisspag> </ref-info> <ref-fulltext>K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770-778.</ref-fulltext> </reference> <reference id="37"> <ref-info> <ref-title> <ref-titletext>A brief review of the ear recognition process using deep neural networks</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">85029621289</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>P.L.</ce:initials> <ce:indexed-name>Galdainez P.L.</ce:indexed-name> <ce:surname>Galdainez</ce:surname> </author> <author seq="2"> <ce:initials>W.</ce:initials> <ce:indexed-name>Raveane W.</ce:indexed-name> <ce:surname>Raveane</ce:surname> </author> <author seq="3"> <ce:initials>A.G.</ce:initials> <ce:indexed-name>Arrieta A.G.</ce:indexed-name> <ce:surname>Arrieta</ce:surname> </author> </ref-authors> <ref-sourcetitle>Journal of Applied Logic</ref-sourcetitle> <ref-publicationyear first="2016"/> </ref-info> <ref-fulltext>P. L. Galdainez, W. Raveane, and A. G. Arrieta, "A brief review of the ear recognition process using deep neural networks, " Journal of Applied Logic, 2016.</ref-fulltext> </reference> <reference id="38"> <ref-info> <ref-title> <ref-titletext>Ear recognition using multi-scale histogram of oriented gradients</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84867181304</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>N.</ce:initials> <ce:indexed-name>Darner N.</ce:indexed-name> <ce:surname>Darner</ce:surname> </author> <author seq="2"> <ce:initials>B.</ce:initials> <ce:indexed-name>Fuhrer B.</ce:indexed-name> <ce:surname>Fuhrer</ce:surname> </author> </ref-authors> <ref-sourcetitle>Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), 2012 Eighth International Conference On. IEEE</ref-sourcetitle> <ref-publicationyear first="2012"/> <ref-volisspag> <pagerange first="21" last="24"/> </ref-volisspag> </ref-info> <ref-fulltext>N. Darner and B. Fuhrer, "Ear recognition using multi-scale histogram of oriented gradients, " in Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), 2012 Eighth International Conference on. IEEE, 2012, pp. 21-24.</ref-fulltext> </reference> <reference id="39"> <ref-info> <ref-title> <ref-titletext>Bsif: Binarized statistical image features</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84874563913</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>J.</ce:initials> <ce:indexed-name>Kannala J.</ce:indexed-name> <ce:surname>Kannala</ce:surname> </author> <author seq="2"> <ce:initials>E.</ce:initials> <ce:indexed-name>Rahtu E.</ce:indexed-name> <ce:surname>Rahtu</ce:surname> </author> </ref-authors> <ref-sourcetitle>Pattern Recognition (ICPR), 2012 21st International Conference On. IEEE</ref-sourcetitle> <ref-publicationyear first="2012"/> <ref-volisspag> <pagerange first="1363" last="1366"/> </ref-volisspag> </ref-info> <ref-fulltext>J. Kannala and E. Rahtu, "Bsif: Binarized statistical image features, " in Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 1363-1366.</ref-fulltext> </reference> <reference id="40"> <ref-info> <ref-title> <ref-titletext>Blur insensitive texture classification using local phase quantization</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">49049108892</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>V.</ce:initials> <ce:indexed-name>Ojansivu V.</ce:indexed-name> <ce:surname>Ojansivu</ce:surname> </author> <author seq="2"> <ce:initials>J.</ce:initials> <ce:indexed-name>Heikkila J.</ce:indexed-name> <ce:surname>Heikkila</ce:surname> </author> </ref-authors> <ref-sourcetitle>International Conference on Image and Signal Processing</ref-sourcetitle> <ref-publicationyear first="2008"/> <ref-volisspag> <pagerange first="236" last="243"/> </ref-volisspag> <ref-text>Springer</ref-text> </ref-info> <ref-fulltext>V. Ojansivu and J. Heikkila, "Blur insensitive texture classification using local phase quantization, " in International conference on image and signal processing. Springer, 2008, pp. 236-243.</ref-fulltext> </reference> <reference id="41"> <ref-info> <ref-title> <ref-titletext>Ear photo recognition using scale invariant keypoints</ref-titletext> </ref-title> <refd-itemidlist> <itemid idtype="SGR">84989210479</itemid> </refd-itemidlist> <ref-authors> <author seq="1"> <ce:initials>K.</ce:initials> <ce:indexed-name>Dewi K.</ce:indexed-name> <ce:surname>Dewi</ce:surname> </author> <author seq="2"> <ce:initials>T.</ce:initials> <ce:indexed-name>Yahagi T.</ce:indexed-name> <ce:surname>Yahagi</ce:surname> </author> </ref-authors> <ref-sourcetitle>Computational Intelligence</ref-sourcetitle> <ref-publicationyear first="2006"/> <ref-volisspag> <pagerange first="253" last="258"/> </ref-volisspag> </ref-info> <ref-fulltext>K. Dewi and T. Yahagi, "Ear photo recognition using scale invariant keypoints. " in Computational Intelligence, 2006, pp. 253-258.</ref-fulltext> </reference> </bibliography></tail></bibrecord></item></abstracts-retrieval-response>