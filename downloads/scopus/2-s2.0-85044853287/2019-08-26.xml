<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85044853287</prism:url><dc:identifier>SCOPUS_ID:85044853287</dc:identifier><eid>2-s2.0-85044853287</eid><pubmed-id>29614129</pubmed-id><prism:doi>10.1371/journal.pone.0195297</prism:doi><article-number>e0195297</article-number><dc:title>A Bayesian hierarchical latent trait model for estimating rater bias and reliability in largescale performance assessment</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>1</citedby-count><prism:publicationName>PLoS ONE</prism:publicationName><dc:publisher>
                        Public Library of Science
                        plos@plos.org
                    </dc:publisher><source-id>10600153309</source-id><prism:issn>19326203</prism:issn><prism:volume>13</prism:volume><prism:issueIdentifier>4</prism:issueIdentifier><prism:coverDate>2018-04-01</prism:coverDate><openaccess>1</openaccess><openaccessFlag>true</openaccessFlag><dc:creator><author seq="1" auid="56719663000"><ce:initials>K.</ce:initials><ce:indexed-name>Zupanc K.</ce:indexed-name><ce:surname>Zupanc</ce:surname><ce:given-name>Kaja</ce:given-name><preferred-name><ce:initials>K.</ce:initials><ce:indexed-name>Zupanc K.</ce:indexed-name><ce:surname>Zupanc</ce:surname><ce:given-name>Kaja</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56719663000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng">
                        <publishercopyright>© 2018 Zupanc, Štrumbelj.</publishercopyright>
                        <ce:para>We propose a novel approach to modelling rater effects in scoring-based assessment. The approach is based on a Bayesian hierarchical model and simulations from the posterior distribution. We apply it to large-scale essay assessment data over a period of 5 years. Empirical results suggest that the model provides a good fit for both the total scores and when applied to individual rubrics. We estimate the median impact of rater effects on the final grade to be ± 2 points on a 50 point scale, while 10% of essays would receive a score at least ± 5 different from their actual quality. Most of the impact is due to rater unreliability, not rater bias.</ce:para>
                    </abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85044853287" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85044853287&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85044853287&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="56719663000"><ce:initials>K.</ce:initials><ce:indexed-name>Zupanc K.</ce:indexed-name><ce:surname>Zupanc</ce:surname><ce:given-name>Kaja</ce:given-name><preferred-name><ce:initials>K.</ce:initials><ce:indexed-name>Zupanc K.</ce:indexed-name><ce:surname>Zupanc</ce:surname><ce:given-name>Kaja</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56719663000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="57204664304"><ce:initials>E.</ce:initials><ce:indexed-name>Strumbelj E.</ce:indexed-name><ce:surname>Štrumbelj</ce:surname><ce:given-name>Erik</ce:given-name><preferred-name><ce:initials>E.</ce:initials><ce:indexed-name>Štrumbelj E.</ce:indexed-name><ce:surname>Štrumbelj</ce:surname><ce:given-name>Erik</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57204664304</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords/><idxterms><mainterm weight="b" candidate="n">Academic Performance</mainterm><mainterm weight="b" candidate="n">Bayes Theorem</mainterm><mainterm weight="b" candidate="n">Computer Simulation</mainterm><mainterm weight="b" candidate="n">Humans</mainterm><mainterm weight="b" candidate="n">Judgment</mainterm><mainterm weight="a" candidate="n">Models, Statistical</mainterm><mainterm weight="a" candidate="n">Observer Variation</mainterm><mainterm weight="a" candidate="n">Reproducibility of Results</mainterm><mainterm weight="b" candidate="n">Writing</mainterm></idxterms><subject-areas><subject-area code="1300" abbrev="BIOC">Biochemistry, Genetics and Molecular Biology (all)</subject-area><subject-area code="1100" abbrev="AGRI">Agricultural and Biological Sciences (all)</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2018-04-27T15:44:09.956Z</xocs:funding-addon-generated-timestamp></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered day="22" month="11" timestamp="2018-11-22T14:38:41.000041-05:00" year="2018"/><ait:date-sort day="01" month="04" year="2018"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2018 Elsevier B.V., All rights reserved.</copyright><itemidlist>
                    <ce:doi>10.1371/journal.pone.0195297</ce:doi>
                    <itemid idtype="PUI">621516599</itemid>
                    <itemid idtype="CAR-ID">909884921</itemid>
                    <itemid idtype="EMBASE">20180246449</itemid>
                    <itemid idtype="EMBIO">2017514383</itemid>
                    <itemid idtype="MEDL">29614129</itemid>
                    <itemid idtype="NURSNG">2018120587</itemid>
                    <itemid idtype="REAXYS">2018080992</itemid>
                    <itemid idtype="REAXYSCAR">20180596558</itemid>
                    <itemid idtype="RMC">2018096696</itemid>
                    <itemid idtype="SCOPUS">20181175460</itemid>
                    <itemid idtype="SCP">85044853287</itemid>
                    <itemid idtype="SGR">85044853287</itemid>
                    <itemid idtype="PUIsecondary">623305871</itemid>
                </itemidlist><history>
                    <date-created day="03" month="08" timestamp="BST 14:33:38" year="2018"/>
                </history><dbcollection>EMBASE</dbcollection><dbcollection>EMBIO</dbcollection><dbcollection>MEDL</dbcollection><dbcollection>NURSNG</dbcollection><dbcollection>REAXYS</dbcollection><dbcollection>REAXYSCAR</dbcollection><dbcollection>RMC</dbcollection><dbcollection>SCOPUS</dbcollection><dbcollection>Scopusbase</dbcollection><external-source>MEDLINE</external-source></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/></citation-info><citation-title><titletext original="y" xml:lang="eng" language="English">A Bayesian hierarchical latent trait model for estimating rater bias and reliability in largescale performance assessment</titletext></citation-title><author-group><author auid="56719663000" seq="1" type="auth"><ce:initials>K.</ce:initials><ce:indexed-name>Zupanc K.</ce:indexed-name><ce:surname>Zupanc</ce:surname><ce:given-name>Kaja</ce:given-name><preferred-name>
                            <ce:initials>K.</ce:initials>
                            <ce:indexed-name>Zupanc K.</ce:indexed-name>
                            <ce:surname>Zupanc</ce:surname>
                            <ce:given-name>Kaja</ce:given-name>
                        </preferred-name></author><author auid="57204664304" seq="2" type="auth"><ce:initials>E.</ce:initials><ce:indexed-name>Strumbelj E.</ce:indexed-name><ce:surname>Štrumbelj</ce:surname><ce:given-name>Erik</ce:given-name><preferred-name>
                            <ce:initials>E.</ce:initials>
                            <ce:indexed-name>Štrumbelj E.</ce:indexed-name>
                            <ce:surname>Štrumbelj</ce:surname>
                            <ce:given-name>Erik</ce:given-name>
                        </preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><city>Ljubljana</city><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person>
                        <ce:initials>E.</ce:initials>
                        <ce:indexed-name>Strumbelj E.</ce:indexed-name>
                        <ce:surname>Štrumbelj</ce:surname>
                        <ce:given-name>Erik</ce:given-name>
                    </person><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><city>Ljubljana</city><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng">
                        <publishercopyright>© 2018 Zupanc, Štrumbelj.</publishercopyright>
                        <ce:para>We propose a novel approach to modelling rater effects in scoring-based assessment. The approach is based on a Bayesian hierarchical model and simulations from the posterior distribution. We apply it to large-scale essay assessment data over a period of 5 years. Empirical results suggest that the model provides a good fit for both the total scores and when applied to individual rubrics. We estimate the median impact of rater effects on the final grade to be ± 2 points on a 50 point scale, while 10% of essays would receive a score at least ± 5 different from their actual quality. Most of the impact is due to rater unreliability, not rater bias.</ce:para>
                    </abstract></abstracts><source country="usa" srcid="10600153309" type="j"><sourcetitle>PLoS ONE</sourcetitle><sourcetitle-abbrev>PLoS ONE</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">PLoS ONE</translated-sourcetitle><issn type="electronic">19326203</issn><codencode>POLNC</codencode><volisspag>
                        <voliss issue="4" volume="13"/>
                    </volisspag><article-number>e0195297</article-number><publicationyear first="2018"/><publicationdate>
                        <year>2018</year>
                        <month>04</month>
                        <day>01</day>
                        <date-text>April 2018</date-text>
                    </publicationdate><website>
                        <ce:e-address type="email">http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0195297&amp;type=printable</ce:e-address>
                    </website><publisher>
                        <publishername>Public Library of Science</publishername>
                        <ce:e-address type="email">plos@plos.org</ce:e-address>
                    </publisher></source><enhancement><classificationgroup><classifications type="ASJC">
                            <classification>1300</classification>
                            <classification>1100</classification>
                        </classifications><classifications type="SUBJABBR"><classification>BIOC</classification><classification>AGRI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="32">
                    <reference id="1">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>A many-facet Rasch analysis of the second language group oral discussion task</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1191/0265532203lt245oa</itemid>
                                <itemid idtype="SGR">33646352828</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>W.J.</ce:initials>
                                    <ce:indexed-name>Bonk W.J.</ce:indexed-name>
                                    <ce:surname>Bonk</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>G.J.</ce:initials>
                                    <ce:indexed-name>Ockey G.J.</ce:indexed-name>
                                    <ce:surname>Ockey</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Language Testing.</ref-sourcetitle>
                            <ref-publicationyear first="2003"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="20"/>
                                <pagerange first="89" last="110"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Bonk WJ, Ockey GJ. A many-facet Rasch analysis of the second language group oral discussion task. Language Testing. 2003; 20(1):89-110. https://doi.org/10.1191/0265532203lt245oa</ref-fulltext>
                    </reference>
                    <reference id="2">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>The stability of rater severity in large-scale assessment programs</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3984.2000.tb01081.x</itemid>
                                <itemid idtype="SGR">0034195156</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>P.J.</ce:initials>
                                    <ce:indexed-name>Congdon P.J.</ce:indexed-name>
                                    <ce:surname>Congdon</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>J.</ce:initials>
                                    <ce:indexed-name>MeQueen J.</ce:indexed-name>
                                    <ce:surname>MeQueen</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational Measurement.</ref-sourcetitle>
                            <ref-publicationyear first="2000"/>
                            <ref-volisspag>
                                <voliss issue="2" volume="37"/>
                                <pagerange first="163" last="178"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Congdon PJ, MeQueen J. The Stability of Rater Severity in Large-Scale Assessment Programs. Journal of Educational Measurement. 2000; 37(2):163-178. https://doi.org/10.1111/j.1745-3984.2000. tb01081.x</ref-fulltext>
                    </reference>
                    <reference id="3">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Examining rater effects in testdaf writing and speaking performance assessments: A many-facet rasch analysis</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1207/s15434311laq0203-2</itemid>
                                <itemid idtype="SGR">33745756490</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>T.</ce:initials>
                                    <ce:indexed-name>Eckes T.</ce:indexed-name>
                                    <ce:surname>Eckes</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Language Assessment Quarterly.</ref-sourcetitle>
                            <ref-publicationyear first="2005"/>
                            <ref-volisspag>
                                <voliss issue="3" volume="2"/>
                                <pagerange first="197" last="221"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Eckes T. Examining Rater Effects in TestDaF Writing and Speaking Performance Assessments: A Many-Facet Rasch Analysis. Language Assessment Quarterly. 2005; 2(3):197-21. https://doi.org/10. 1207/s15434311laq0203-2</ref-fulltext>
                    </reference>
                    <reference id="4">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Rater types in writing performance assessments: A classification approach to rater variability</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1177/0265532207086780</itemid>
                                <itemid idtype="SGR">55249090887</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>T.</ce:initials>
                                    <ce:indexed-name>Eckes T.</ce:indexed-name>
                                    <ce:surname>Eckes</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Language Testing.</ref-sourcetitle>
                            <ref-publicationyear first="2008"/>
                            <ref-volisspag>
                                <voliss issue="2" volume="25"/>
                                <pagerange first="155" last="185"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Eckes T. Rater types in writing performance assessments: A classification approach to rater variability. Language Testing. 2008; 25(2):155-185. https://doi.org/10.1177/0265532207086780</ref-fulltext>
                    </reference>
                    <reference id="5">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Evaluating rater responses to an online training program for L2 writing assessment</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1177/0265532207071511</itemid>
                                <itemid idtype="SGR">33847001320</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>C.</ce:initials>
                                    <ce:indexed-name>Elder C.</ce:indexed-name>
                                    <ce:surname>Elder</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>G.</ce:initials>
                                    <ce:indexed-name>Barkhuizen G.</ce:indexed-name>
                                    <ce:surname>Barkhuizen</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>U.</ce:initials>
                                    <ce:indexed-name>Knoch U.</ce:indexed-name>
                                    <ce:surname>Knoch</ce:surname>
                                </author>
                                <author seq="4">
                                    <ce:initials>J.</ce:initials>
                                    <ce:indexed-name>Von Randow J.</ce:indexed-name>
                                    <ce:surname>Von Randow</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Language Testing.</ref-sourcetitle>
                            <ref-publicationyear first="2007"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="24"/>
                                <pagerange first="37" last="64"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Elder C, Barkhuizen G, Knoch U, von Randow J. Evaluating rater responses to an online training program for L2 writing assessment. Language Testing. 2007; 24(1):37-64. https://doi.org/10.1177/ 0265532207071511</ref-fulltext>
                    </reference>
                    <reference id="6">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Real-time feedback on rater drift in constructed-response items: An example from the Golden State Examination</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3984.2001.tb01119.x</itemid>
                                <itemid idtype="SGR">0035536108</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>M.</ce:initials>
                                    <ce:indexed-name>Hoskens M.</ce:indexed-name>
                                    <ce:surname>Hoskens</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>M.</ce:initials>
                                    <ce:indexed-name>Wilson M.</ce:indexed-name>
                                    <ce:surname>Wilson</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational Measurement.</ref-sourcetitle>
                            <ref-publicationyear first="2001"/>
                            <ref-volisspag>
                                <pagerange first="121" last="145"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Hoskens M, Wilson M. Real-time feedback on rater drift in constructed-response items: An example from the Golden State Examination. Journal of Educational Measurement. 2001; p. 121-145. https:// doi.org/10.1111/j.1745-3984.2001.tb01119.x</ref-fulltext>
                    </reference>
                    <reference id="7">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>A FACETS analysis of rater bias in measuring Japanese second language writing performance</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1191/0265532202lt218oa</itemid>
                                <itemid idtype="SGR">84990330622</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>K.</ce:initials>
                                    <ce:indexed-name>Kondo-Brown K.</ce:indexed-name>
                                    <ce:surname>Kondo-Brown</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Language Testing.</ref-sourcetitle>
                            <ref-publicationyear first="2002"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="19"/>
                                <pagerange first="3" last="31"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Kondo-Brown K. A FACETS analysis of rater bias in measuring Japanese second language writing performance. Language Testing. 2002; 19(1):3-31. https://doi.org/10.1191/0265532202lt218oa</ref-fulltext>
                    </reference>
                    <reference id="8">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Rater characteristics and rater bias: Implications for training</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="SGR">84965511141</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>T.</ce:initials>
                                    <ce:indexed-name>Lumley T.</ce:indexed-name>
                                    <ce:surname>Lumley</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>T.F.</ce:initials>
                                    <ce:indexed-name>McNamara T.F.</ce:indexed-name>
                                    <ce:surname>McNamara</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>15th Language Testing Research Colloquium</ref-sourcetitle>
                            <ref-publicationyear first="1995"/>
                            <ref-text>Cambridge, England, UnitedKingdom</ref-text>
                        </ref-info>
                        <ref-fulltext>Lumley T, McNamara TF. Rater Characteristics and Rater Bias: Implications for Training. In: 15th Language Testing Research Colloquium, Cambridge, England, UnitedKingdom; 1995.</ref-fulltext>
                    </reference>
                    <reference id="9">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Monitoring rater performance over time: A framework for detecting differential accuracy and differential scale category use</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3984.2009.00088.x</itemid>
                                <itemid idtype="SGR">71549124344</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>C.M.</ce:initials>
                                    <ce:indexed-name>Myford C.M.</ce:indexed-name>
                                    <ce:surname>Myford</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>E.W.</ce:initials>
                                    <ce:indexed-name>Wolfe E.W.</ce:indexed-name>
                                    <ce:surname>Wolfe</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational Measurement.</ref-sourcetitle>
                            <ref-publicationyear first="2009"/>
                            <ref-volisspag>
                                <voliss issue="4" volume="46"/>
                                <pagerange first="371" last="389"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Myford CM, Wolfe EW. Monitoring Rater Performance Over Time: A Framework for Detecting Differential Accuracy and Differential Scale Category Use. Journal of Educational Measurement. 2009; 46 (4):371-389. https://doi.org/10.1111/j.1745-3984.2009.00088.x</ref-fulltext>
                    </reference>
                    <reference id="10">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Rater bias in assessing Iranian EFL learners' writing performance</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="SGR">85044852577</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>M.</ce:initials>
                                    <ce:indexed-name>Saeidi M.</ce:indexed-name>
                                    <ce:surname>Saeidi</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>M.</ce:initials>
                                    <ce:indexed-name>Yousefi M.</ce:indexed-name>
                                    <ce:surname>Yousefi</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>P.</ce:initials>
                                    <ce:indexed-name>Baghayei P.</ce:indexed-name>
                                    <ce:surname>Baghayei</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Iranian Journal of Applied Linguistics.</ref-sourcetitle>
                            <ref-publicationyear first="2013"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="16"/>
                                <pagerange first="145" last="175"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Saeidi M, Yousefi M, Baghayei P. Rater Bias in Assessing Iranian EFL Learners' Writing Performance. Iranian Journal of Applied Linguistics. 2013; 16(1):145-175.</ref-fulltext>
                    </reference>
                    <reference id="11">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Rater bias patterns in an EFL writing assessment</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1177/0265532208094273</itemid>
                                <itemid idtype="SGR">61549119706</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>E.</ce:initials>
                                    <ce:indexed-name>Schaefer E.</ce:indexed-name>
                                    <ce:surname>Schaefer</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Language Testing.</ref-sourcetitle>
                            <ref-publicationyear first="2008"/>
                            <ref-volisspag>
                                <voliss issue="4" volume="25"/>
                                <pagerange first="465" last="493"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Schaefer E. Rater bias patterns in an EFL writing assessment. Language Testing. 2008; 25(4):465-493. https://doi.org/10.1177/0265532208094273</ref-fulltext>
                    </reference>
                    <reference id="12">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Measuring the impact of rater negotiation in writing performance assessment</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="SGR">84988439590</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>J.</ce:initials>
                                    <ce:indexed-name>Trace J.</ce:indexed-name>
                                    <ce:surname>Trace</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>G.</ce:initials>
                                    <ce:indexed-name>Janssen G.</ce:indexed-name>
                                    <ce:surname>Janssen</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>V.</ce:initials>
                                    <ce:indexed-name>Meier V.</ce:indexed-name>
                                    <ce:surname>Meier</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Language Testing.</ref-sourcetitle>
                            <ref-publicationyear first="2015"/>
                        </ref-info>
                        <ref-fulltext>Trace J, Janssen G, Meier V. Measuring the impact of rater negotiation in writing performance assessment. Language Testing. 2015; p. 0265532215594830.</ref-fulltext>
                    </reference>
                    <reference id="13">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Monitoring reader performance and DRIFT in the AP english literature and composition examination using benchmark essays</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="SGR">85044846504</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>E.W.</ce:initials>
                                    <ce:indexed-name>Wolfe E.W.</ce:indexed-name>
                                    <ce:surname>Wolfe</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>C.M.</ce:initials>
                                    <ce:indexed-name>Myford C.M.</ce:indexed-name>
                                    <ce:surname>Myford</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>G.J.</ce:initials>
                                    <ce:indexed-name>Engelhard G.J.</ce:indexed-name>
                                    <ce:surname>Engelhard</ce:surname>
                                </author>
                                <author seq="4">
                                    <ce:initials>J.R.</ce:initials>
                                    <ce:indexed-name>Manalo J.R.</ce:indexed-name>
                                    <ce:surname>Manalo</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>College Board Research Report</ref-sourcetitle>
                            <ref-publicationyear first="2007"/>
                            <ref-volisspag>
                                <voliss issue="2" volume="2007"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Wolfe EW, Myford CM, Engelhard GJ, Manalo JR. Monitoring Reader Performance and DRIFT in the AP English Literature and Composition Examination Using Benchmark Essays. College Board Research Report. 2007; 2007(2).</ref-fulltext>
                    </reference>
                    <reference id="14">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>An examination of rater drift within a generalizability theory framework</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3984.2009.01068.x</itemid>
                                <itemid idtype="SGR">61349170677</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>P.</ce:initials>
                                    <ce:indexed-name>Harik P.</ce:indexed-name>
                                    <ce:surname>Harik</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>B.E.</ce:initials>
                                    <ce:indexed-name>Clauser B.E.</ce:indexed-name>
                                    <ce:surname>Clauser</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>I.</ce:initials>
                                    <ce:indexed-name>Grabovsky I.</ce:indexed-name>
                                    <ce:surname>Grabovsky</ce:surname>
                                </author>
                                <author seq="4">
                                    <ce:initials>R.J.</ce:initials>
                                    <ce:indexed-name>Nungester R.J.</ce:indexed-name>
                                    <ce:surname>Nungester</ce:surname>
                                </author>
                                <author seq="5">
                                    <ce:initials>D.</ce:initials>
                                    <ce:indexed-name>Swanson D.</ce:indexed-name>
                                    <ce:surname>Swanson</ce:surname>
                                </author>
                                <author seq="6">
                                    <ce:initials>R.</ce:initials>
                                    <ce:indexed-name>Nandakumar R.</ce:indexed-name>
                                    <ce:surname>Nandakumar</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational Measurement.</ref-sourcetitle>
                            <ref-publicationyear first="2009"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="46"/>
                                <pagerange first="43" last="58"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Harik P, Clauser BE, Grabovsky I, Nungester RJ, Swanson D, Nandakumar R. An examination of rater drift within a generalizability theory framework. Journal of Educational Measurement. 2009; 46(1):43-58. https://doi.org/10.1111/j.1745-3984.2009.01068.x</ref-fulltext>
                    </reference>
                    <reference id="15">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Rater effects on essay scoring: A multilevel analysis of severity drift, central tendency, and rater experience</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3984.2011.00152.x</itemid>
                                <itemid idtype="SGR">84055198279</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>G.</ce:initials>
                                    <ce:indexed-name>Leckie G.</ce:indexed-name>
                                    <ce:surname>Leckie</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>J.A.</ce:initials>
                                    <ce:indexed-name>Baird J.A.</ce:indexed-name>
                                    <ce:surname>Baird</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational Measurement.</ref-sourcetitle>
                            <ref-publicationyear first="2011"/>
                            <ref-volisspag>
                                <voliss issue="4" volume="48"/>
                                <pagerange first="399" last="418"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Leckie G, Baird JA. Rater Effects on Essay Scoring: A Multilevel Analysis of Severity Drift, Central Tendency, and Rater Experience. Journal of Educational Measurement. 2011; 48(4):399-418. https://doi. org/10.1111/j.1745-3984.2011.00152.x</ref-fulltext>
                    </reference>
                    <reference id="16">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Marking consistency over time</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.7227/RIE.67.8</itemid>
                                <itemid idtype="SGR">79551558634</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>A.P.</ce:initials>
                                    <ce:indexed-name>De Moira A.P.</ce:indexed-name>
                                    <ce:surname>De Moira</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>C.</ce:initials>
                                    <ce:indexed-name>Massey C.</ce:indexed-name>
                                    <ce:surname>Massey</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>J.A.</ce:initials>
                                    <ce:indexed-name>Baird J.A.</ce:indexed-name>
                                    <ce:surname>Baird</ce:surname>
                                </author>
                                <author seq="4">
                                    <ce:initials>M.</ce:initials>
                                    <ce:indexed-name>Marie M.</ce:indexed-name>
                                    <ce:surname>Marie</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Research in Education.</ref-sourcetitle>
                            <ref-publicationyear first="2002"/>
                            <ref-volisspag>
                                <voliss volume="67"/>
                                <pagerange first="79" last="87"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>de Moira AP, Massey C, Baird JA, Marie M. Marking consistency over time. Research in Education. 2002; 67:79-87. https://doi.org/10.7227/RIE.67.8</ref-fulltext>
                    </reference>
                    <reference id="17">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Examining rater errors in the assessment of written composition with a many-faceted rasch model</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3984.1994.tb00436.x</itemid>
                                <itemid idtype="SGR">84988122960</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>G.J.</ce:initials>
                                    <ce:indexed-name>Engelhard G.J.</ce:indexed-name>
                                    <ce:surname>Engelhard</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational Measurement.</ref-sourcetitle>
                            <ref-publicationyear first="1994"/>
                            <ref-volisspag>
                                <voliss issue="2" volume="31"/>
                                <pagerange first="93" last="112"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Engelhard GJ. Examining Rater Errors in the Assessment of Written Composition With a Many-Faceted Rasch Model. Journal of Educational Measurement. 1994; 31(2):93-112. https://doi.org/10.1111/j. 1745-3984.1994.tb00436.x</ref-fulltext>
                    </reference>
                    <reference id="18">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Monitoring faculty consultant performance in the advanced placement english literature and composition program with a many-faceted rasch model</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="SGR">29144517635</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>G.J.</ce:initials>
                                    <ce:indexed-name>Engelhard G.J.</ce:indexed-name>
                                    <ce:surname>Engelhard</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>C.M.</ce:initials>
                                    <ce:indexed-name>Myford C.M.</ce:indexed-name>
                                    <ce:surname>Myford</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>College Board Research Report.</ref-sourcetitle>
                            <ref-publicationyear first="2003"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="2003"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Engelhard GJ, Myford CM. Monitoring Faculty Consultant Performance in the Advanced Placement English Literature and Composition Program with a Many-Faceted Rasch Model. College Board Research Report. 2003; 2003(1).</ref-fulltext>
                    </reference>
                    <reference id="19">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Biases in marking students' written work: Quality?</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="SGR">85044857429</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>A.R.</ce:initials>
                                    <ce:indexed-name>Fitzpatrick A.R.</ce:indexed-name>
                                    <ce:surname>Fitzpatrick</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>K.</ce:initials>
                                    <ce:indexed-name>Ercikan K.</ce:indexed-name>
                                    <ce:surname>Ercikan</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>W.M.</ce:initials>
                                    <ce:indexed-name>Yen W.M.</ce:indexed-name>
                                    <ce:surname>Yen</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Assessment Matters in Higher Education</ref-sourcetitle>
                            <ref-publicationyear first="1998"/>
                            <ref-volisspag>
                                <voliss issue="2" volume="11"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Fitzpatrick AR, Ercikan K, Yen WM. Biases in Marking Students' Written Work: Quality? Assessment Matters in Higher Education. 1998; 11(2).</ref-fulltext>
                    </reference>
                    <reference id="20">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Application of latent trait models to identifying substantively interesting raters</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3992.2012.00241.x</itemid>
                                <itemid idtype="SGR">84866452272</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>E.W.</ce:initials>
                                    <ce:indexed-name>Wolfe E.W.</ce:indexed-name>
                                    <ce:surname>Wolfe</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>A.</ce:initials>
                                    <ce:indexed-name>McVay A.</ce:indexed-name>
                                    <ce:surname>McVay</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Educational Measurement: Issues and Practice.</ref-sourcetitle>
                            <ref-publicationyear first="2012"/>
                            <ref-volisspag>
                                <voliss issue="3" volume="31"/>
                                <pagerange first="31" last="37"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Wolfe EW, McVay A. Application of Latent Trait Models to Identifying Substantively Interesting Raters. Educational Measurement: Issues and Practice. 2012; 31(3):31-37. https://doi.org/10.1111/j.1745-3992.2012.00241.x</ref-fulltext>
                    </reference>
                    <reference id="21">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Reliability and validity of rubrics for assessment through writing</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1016/j.asw.2010.01.003</itemid>
                                <itemid idtype="SGR">77951252272</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>A.R.</ce:initials>
                                    <ce:indexed-name>Rezaei A.R.</ce:indexed-name>
                                    <ce:surname>Rezaei</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>M.</ce:initials>
                                    <ce:indexed-name>Lovorn M.</ce:indexed-name>
                                    <ce:surname>Lovorn</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Assessing Writing.</ref-sourcetitle>
                            <ref-publicationyear first="2010"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="15"/>
                                <pagerange first="18" last="39"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Rezaei AR, Lovorn M. Reliability and validity of rubrics for assessment through writing. Assessing Writing. 2010; 15(1):18-39. https://doi.org/10.1016/j.asw.2010.01.003</ref-fulltext>
                    </reference>
                    <reference id="22">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>A hierarchical rater model for constructed responses, with a signal detection rater model</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1111/j.1745-3984.2011.00143.x</itemid>
                                <itemid idtype="SGR">80053241573</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>L.T.</ce:initials>
                                    <ce:indexed-name>DeCarlo L.T.</ce:indexed-name>
                                    <ce:surname>DeCarlo</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>Y.</ce:initials>
                                    <ce:indexed-name>Kim Y.</ce:indexed-name>
                                    <ce:surname>Kim</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>M.S.</ce:initials>
                                    <ce:indexed-name>Johnson M.S.</ce:indexed-name>
                                    <ce:surname>Johnson</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational Measurement.</ref-sourcetitle>
                            <ref-publicationyear first="2011"/>
                            <ref-volisspag>
                                <voliss issue="3" volume="48"/>
                                <pagerange first="333" last="356"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>DeCarlo LT, Kim Y, Johnson MS. A hierarchical rater model for constructed responses, with a signal detection rater model. Journal of Educational Measurement. 2011; 48(3):333-356. https://doi.org/10. 1111/j.1745-3984.2011.00143.x</ref-fulltext>
                    </reference>
                    <reference id="23">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>The hierarchical rater model for rated test items and its application to large-scale educational assessment data</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.3102/10769986027004341</itemid>
                                <itemid idtype="SGR">0036960386</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>R.J.</ce:initials>
                                    <ce:indexed-name>Patz R.J.</ce:indexed-name>
                                    <ce:surname>Patz</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>B.W.</ce:initials>
                                    <ce:indexed-name>Junker B.W.</ce:indexed-name>
                                    <ce:surname>Junker</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>M.S.</ce:initials>
                                    <ce:indexed-name>Johnson M.S.</ce:indexed-name>
                                    <ce:surname>Johnson</ce:surname>
                                </author>
                                <author seq="4">
                                    <ce:initials>L.T.</ce:initials>
                                    <ce:indexed-name>Mariano L.T.</ce:indexed-name>
                                    <ce:surname>Mariano</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Educational and Behavioral Statistics.</ref-sourcetitle>
                            <ref-publicationyear first="2002"/>
                            <ref-volisspag>
                                <voliss issue="4" volume="27"/>
                                <pagerange first="341" last="384"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Patz RJ, Junker BW, Johnson MS, Mariano LT. The hierarchical rater model for rated test items and its application to large-scale educational assessment data. Journal of Educational and Behavioral Statistics. 2002; 27(4):341-384. https://doi.org/10.3102/10769986027004341</ref-fulltext>
                    </reference>
                    <reference id="24">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Why we (usually) don't have to worry about multiple comparisons</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1080/19345747.2011.618213</itemid>
                                <itemid idtype="SGR">84859606223</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>A.</ce:initials>
                                    <ce:indexed-name>Gelman A.</ce:indexed-name>
                                    <ce:surname>Gelman</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>J.</ce:initials>
                                    <ce:indexed-name>Hill J.</ce:indexed-name>
                                    <ce:surname>Hill</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>M.</ce:initials>
                                    <ce:indexed-name>Yajima M.</ce:indexed-name>
                                    <ce:surname>Yajima</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Research on Educational Effectiveness.</ref-sourcetitle>
                            <ref-publicationyear first="2012"/>
                            <ref-volisspag>
                                <voliss issue="2" volume="5"/>
                                <pagerange first="189" last="211"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Gelman A, Hill J, Yajima M. Why we (usually) don't have to worry about multiple comparisons. Journal of Research on Educational Effectiveness. 2012; 5(2):189-211. https://doi.org/10.1080/19345747. 2011.618213</ref-fulltext>
                    </reference>
                    <reference id="25">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Bayes inference in the Tobit censored regression model</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1016/0304-4076(92)90030-U</itemid>
                                <itemid idtype="SGR">44049123923</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>S.</ce:initials>
                                    <ce:indexed-name>Chib S.</ce:indexed-name>
                                    <ce:surname>Chib</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Econometrics.</ref-sourcetitle>
                            <ref-publicationyear first="1992"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="51"/>
                                <pagerange first="79" last="99"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Chib S. Bayes inference in the Tobit censored regression model. Journal of Econometrics. 1992; 51 (1):79-99. https://doi.org/10.1016/0304-4076(92)90030-U</ref-fulltext>
                    </reference>
                    <reference id="26">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Estimation of relationships for limited dependent variables</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.2307/1907382</itemid>
                                <itemid idtype="SGR">0000175291</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>J.</ce:initials>
                                    <ce:indexed-name>Tobin J.</ce:indexed-name>
                                    <ce:surname>Tobin</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Econometrica: Journal of the Econometric Society.</ref-sourcetitle>
                            <ref-publicationyear first="1958"/>
                            <ref-volisspag>
                                <pagerange first="24" last="36"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Tobin J. Estimation of relationships for limited dependent variables. Econometrica: Journal of the Econometric Society. 1958; p. 24-36. https://doi.org/10.2307/1907382</ref-fulltext>
                    </reference>
                    <reference id="27">
                        <ref-info>
                            <refd-itemidlist>
                                <itemid idtype="SGR">84863304598</itemid>
                            </refd-itemidlist>
                            <ref-sourcetitle>R: A Language and Environment for Statistical Computing</ref-sourcetitle>
                            <ref-publicationyear first="2014"/>
                            <ref-website>
                                <ce:e-address type="email">http://www.R-project.org/</ce:e-address>
                            </ref-website>
                            <ref-text>R Core Team.</ref-text>
                        </ref-info>
                        <ref-fulltext>R Core Team. R: A Language and Environment for Statistical Computing; 2014. Available from: Http:// www.R-project.org/.</ref-fulltext>
                    </reference>
                    <reference id="28">
                        <ref-info>
                            <refd-itemidlist>
                                <itemid idtype="SGR">84943640113</itemid>
                            </refd-itemidlist>
                            <ref-sourcetitle>Stan: A C++ Library for Probability and Sampling, Version 2.8.0</ref-sourcetitle>
                            <ref-publicationyear first="2015"/>
                            <ref-website>
                                <ce:e-address type="email">http://mc-stan.org/</ce:e-address>
                            </ref-website>
                            <ref-text>Stan Development Team.</ref-text>
                        </ref-info>
                        <ref-fulltext>Stan Development Team. Stan: A C++ Library for Probability and Sampling, Version 2.8.0; 2015. Available from: Http://mc-stan.org/.</ref-fulltext>
                    </reference>
                    <reference id="29">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="SGR">84901687683</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>M.D.</ce:initials>
                                    <ce:indexed-name>Hoffman M.D.</ce:indexed-name>
                                    <ce:surname>Hoffman</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>A.</ce:initials>
                                    <ce:indexed-name>Gelman A.</ce:indexed-name>
                                    <ce:surname>Gelman</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>The Journal of Machine Learning Research.</ref-sourcetitle>
                            <ref-publicationyear first="2014"/>
                            <ref-volisspag>
                                <voliss issue="1" volume="15"/>
                                <pagerange first="1593" last="1623"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Hoffman MD, Gelman A. The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research. 2014; 15(1):1593-1623.</ref-fulltext>
                    </reference>
                    <reference id="30">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Understanding predictive information criteria for Bayesian models</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1007/s11222-013-9416-2</itemid>
                                <itemid idtype="SGR">84916213666</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>A.</ce:initials>
                                    <ce:indexed-name>Gelman A.</ce:indexed-name>
                                    <ce:surname>Gelman</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>J.</ce:initials>
                                    <ce:indexed-name>Hwang J.</ce:indexed-name>
                                    <ce:surname>Hwang</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>A.</ce:initials>
                                    <ce:indexed-name>Vehtari A.</ce:indexed-name>
                                    <ce:surname>Vehtari</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Statistics and Computing.</ref-sourcetitle>
                            <ref-publicationyear first="2014"/>
                            <ref-volisspag>
                                <voliss issue="6" volume="24"/>
                                <pagerange first="997" last="1016"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Gelman A, Hwang J, Vehtari A. Understanding predictive information criteria for Bayesian models. Statistics and Computing. 2014; 24(6):997-1016. https://doi.org/10.1007/s11222-013-9416-2</ref-fulltext>
                    </reference>
                    <reference id="31">
                        <ref-info>
                            <refd-itemidlist>
                                <itemid idtype="SGR">84977556656</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>A.</ce:initials>
                                    <ce:indexed-name>Vehtari A.</ce:indexed-name>
                                    <ce:surname>Vehtari</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>A.</ce:initials>
                                    <ce:indexed-name>Gelman A.</ce:indexed-name>
                                    <ce:surname>Gelman</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>J.</ce:initials>
                                    <ce:indexed-name>Gabry J.</ce:indexed-name>
                                    <ce:surname>Gabry</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Loo: Efficient Leave-one-out Cross-validation and WAIC for Bayesian Models</ref-sourcetitle>
                            <ref-publicationyear first="2015"/>
                            <ref-website>
                                <ce:e-address type="email">https://github.com/jgabry/loo</ce:e-address>
                            </ref-website>
                        </ref-info>
                        <ref-fulltext>Vehtari A, Gelman A, Gabry J. loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models; 2015. Available from: Https://github.com/jgabry/loo.</ref-fulltext>
                    </reference>
                    <reference id="32">
                        <ref-info>
                            <ref-title>
                                <ref-titletext>Generating random correlation matrices based on vines and extended onion method</ref-titletext>
                            </ref-title>
                            <refd-itemidlist>
                                <itemid idtype="DOI">10.1016/j.jmva.2009.04.008</itemid>
                                <itemid idtype="SGR">68949200946</itemid>
                            </refd-itemidlist>
                            <ref-authors>
                                <author seq="1">
                                    <ce:initials>D.</ce:initials>
                                    <ce:indexed-name>Lewandowski D.</ce:indexed-name>
                                    <ce:surname>Lewandowski</ce:surname>
                                </author>
                                <author seq="2">
                                    <ce:initials>D.</ce:initials>
                                    <ce:indexed-name>Kurowicka D.</ce:indexed-name>
                                    <ce:surname>Kurowicka</ce:surname>
                                </author>
                                <author seq="3">
                                    <ce:initials>H.</ce:initials>
                                    <ce:indexed-name>Joe H.</ce:indexed-name>
                                    <ce:surname>Joe</ce:surname>
                                </author>
                            </ref-authors>
                            <ref-sourcetitle>Journal of Multivariate Analysis.</ref-sourcetitle>
                            <ref-publicationyear first="2009"/>
                            <ref-volisspag>
                                <voliss issue="9" volume="100"/>
                                <pagerange first="1989" last="2001"/>
                            </ref-volisspag>
                        </ref-info>
                        <ref-fulltext>Lewandowski D, Kurowicka D, Joe H. Generating random correlation matrices based on vines and extended onion method. Journal of multivariate analysis. 2009; 100(9):1989-2001. https://doi.org/10. 1016/j.jmva.2009.04.008</ref-fulltext>
                    </reference>
                </bibliography></tail></bibrecord></item></abstracts-retrieval-response>