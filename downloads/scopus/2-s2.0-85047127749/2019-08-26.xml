<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85047127749</prism:url><dc:identifier>SCOPUS_ID:85047127749</dc:identifier><eid>2-s2.0-85047127749</eid><prism:doi>10.1007/s00521-018-3530-1</prism:doi><dc:title>Evaluation and analysis of ear recognition models: performance, complexity and resource requirements</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ip</subtype><subtypeDescription>Article in Press</subtypeDescription><citedby-count>2</citedby-count><prism:publicationName>Neural Computing and Applications</prism:publicationName><dc:publisher> Springer London </dc:publisher><source-id>24800</source-id><prism:issn>09410643</prism:issn><prism:startingPage>1</prism:startingPage><prism:endingPage>16</prism:endingPage><prism:pageRange>1-16</prism:pageRange><prism:coverDate>2018-05-16</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="56097253100"><ce:initials>Ž.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emeršič</ce:surname><ce:given-name>Žiga</ce:given-name><preferred-name><ce:initials>Ž.</ce:initials><ce:indexed-name>Emeršič Ž.</ce:indexed-name><ce:surname>Emeršič</ce:surname><ce:given-name>Žiga</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56097253100</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"> <publishercopyright>© 2018 The Natural Computing Applications Forum</publishercopyright> <ce:para>Ear recognition technology has long been dominated by (local) descriptor-based techniques due to their formidable recognition performance and robustness to various sources of image variability. While deep-learning-based techniques have started to appear in this field only recently, they have already shown potential for further boosting the performance of ear recognition technology and dethroning descriptor-based methods as the current state of the art. However, while recognition performance is often the key factor when selecting recognition models for biometric technology, it is equally important that the behavior of the models is understood and their sensitivity to different covariates is known and well explored. Other factors, such as the train- and test-time complexity or resource requirements, are also paramount and need to be consider when designing recognition systems. To explore these issues, we present in this paper a comprehensive analysis of several descriptor- and deep-learning-based techniques for ear recognition. Our goal is to discover weak points of contemporary techniques, study the characteristics of the existing technology and identify open problems worth exploring in the future. We conduct our analysis through identification experiments on the challenging Annotated Web Ears (AWE) dataset and report our findings. The results of our analysis show that the presence of accessories and high degrees of head movement significantly impacts the identification performance of all types of recognition models, whereas mild degrees of the listed factors and other covariates such as gender and ethnicity impact the identification performance only to a limited extent. From a test-time-complexity point of view, the results suggest that lightweight deep models can be equally fast as descriptor-based methods given appropriate computing hardware, but require significantly more resources during training, where descriptor-based methods have a clear advantage. As an additional contribution, we also introduce a novel dataset of ear images, called AWE Extended (AWEx), which we collected from the web for the training of the deep models used in our experiments. AWEx contains 4104 images of 346 subjects and represents one of the largest and most challenging (publicly available) datasets of unconstrained ear images at the disposal of the research community.</ce:para> </abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85047127749" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85047127749&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85047127749&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="56097253100"><ce:initials>Ž.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emeršič</ce:surname><ce:given-name>Žiga</ce:given-name><preferred-name><ce:initials>Ž.</ce:initials><ce:indexed-name>Emeršič Ž.</ce:indexed-name><ce:surname>Emeršič</ce:surname><ce:given-name>Žiga</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56097253100</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="57191976811"><ce:initials>B.</ce:initials><ce:indexed-name>Meden B.</ce:indexed-name><ce:surname>Meden</ce:surname><ce:given-name>Blaž</ce:given-name><preferred-name><ce:initials>B.</ce:initials><ce:indexed-name>Meden B.</ce:indexed-name><ce:surname>Meden</ce:surname><ce:given-name>Blaž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57191976811</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="7003277146"><ce:initials>P.</ce:initials><ce:indexed-name>Peer P.</ce:indexed-name><ce:surname>Peer</ce:surname><ce:given-name>Peter</ce:given-name><preferred-name><ce:initials>P.</ce:initials><ce:indexed-name>Peer P.</ce:indexed-name><ce:surname>Peer</ce:surname><ce:given-name>Peter</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7003277146</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="4" auid="17347474600"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Štruc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/17347474600</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Convolutional neural networks</author-keyword><author-keyword>Covariate analysis</author-keyword><author-keyword>Ear recognition</author-keyword><author-keyword>Feature extraction</author-keyword></authkeywords><idxterms><mainterm weight="b" candidate="n">Comprehensive analysis</mainterm><mainterm weight="b" candidate="n">Contemporary techniques</mainterm><mainterm weight="b" candidate="n">Convolutional neural network</mainterm><mainterm weight="b" candidate="n">Covariates</mainterm><mainterm weight="b" candidate="n">Ear recognition</mainterm><mainterm weight="b" candidate="n">Evaluation and analysis</mainterm><mainterm weight="b" candidate="n">Research communities</mainterm><mainterm weight="b" candidate="n">Resource requirements</mainterm></idxterms><subject-areas><subject-area code="1712" abbrev="COMP">Software</subject-area><subject-area code="1702" abbrev="COMP">Artificial Intelligence</subject-area></subject-areas><item xmlns=""><ait:process-info><ait:date-delivered day="24" month="05" timestamp="2018-05-24T23:45:43.000043-04:00" year="2018"/><ait:date-sort day="16" month="05" year="2018"/><ait:status stage="S200" state="new" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2018 Elsevier B.V., All rights reserved.</copyright><itemidlist> <ce:doi>10.1007/s00521-018-3530-1</ce:doi> <itemid idtype="PUI">622223829</itemid> <itemid idtype="CAR-ID">910747809</itemid> <itemid idtype="CPX">20182105224007</itemid> <itemid idtype="SCOPUS">20181611277</itemid> <itemid idtype="SCP">85047127749</itemid> <itemid idtype="SGR">85047127749</itemid> </itemidlist><history> <date-created day="24" month="05" timestamp="BST 04:36:27" year="2018"/> </history><dbcollection>CPX</dbcollection><dbcollection>SCOPUS</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ip"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords> <author-keyword>Convolutional neural networks</author-keyword> <author-keyword>Covariate analysis</author-keyword> <author-keyword>Ear recognition</author-keyword> <author-keyword>Feature extraction</author-keyword> </author-keywords></citation-info><citation-title><titletext original="y" xml:lang="eng" language="English">Evaluation and analysis of ear recognition models: performance, complexity and resource requirements</titletext></citation-title><author-group><author auid="56097253100" orcid="0000-0002-3726-9404" seq="1" type="auth"><ce:initials>Ž.</ce:initials><ce:indexed-name>Emersic Z.</ce:indexed-name><ce:surname>Emeršič</ce:surname><ce:given-name>Žiga</ce:given-name><preferred-name> <ce:initials>Ž.</ce:initials> <ce:indexed-name>Emeršič Ž.</ce:indexed-name> <ce:surname>Emeršič</ce:surname> <ce:given-name>Žiga</ce:given-name> </preferred-name></author><author auid="57191976811" seq="2" type="auth"><ce:initials>B.</ce:initials><ce:indexed-name>Meden B.</ce:indexed-name><ce:surname>Meden</ce:surname><ce:given-name>Blaž</ce:given-name><preferred-name> <ce:initials>B.</ce:initials> <ce:indexed-name>Meden B.</ce:indexed-name> <ce:surname>Meden</ce:surname> <ce:given-name>Blaž</ce:given-name> </preferred-name></author><author auid="7003277146" seq="3" type="auth"><ce:initials>P.</ce:initials><ce:indexed-name>Peer P.</ce:indexed-name><ce:surname>Peer</ce:surname><ce:given-name>Peter</ce:given-name><preferred-name> <ce:initials>P.</ce:initials> <ce:indexed-name>Peer P.</ce:indexed-name> <ce:surname>Peer</ce:surname> <ce:given-name>Peter</ce:given-name> </preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Večna pot 113</address-part><city>Ljubljana</city><postal-code>1000</postal-code><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="17347474600" seq="4" type="auth"><ce:initials>V.</ce:initials><ce:indexed-name>Struc V.</ce:indexed-name><ce:surname>Štruc</ce:surname><ce:given-name>Vitomir</ce:given-name><preferred-name> <ce:initials>V.</ce:initials> <ce:indexed-name>Štruc V.</ce:indexed-name> <ce:surname>Štruc</ce:surname> <ce:given-name>Vitomir</ce:given-name> </preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Electrical Engineering</organization><organization>University of Ljubljana</organization><address-part>Tržaška 25</address-part><city>Ljubljana</city><postal-code>1000</postal-code><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person> <ce:initials>Ž.</ce:initials> <ce:indexed-name>Emersic Z.</ce:indexed-name> <ce:surname>Emeršič</ce:surname> <ce:given-name>Žiga</ce:given-name> </person><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Večna pot 113</address-part><city>Ljubljana</city><postal-code>1000</postal-code><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"> <publishercopyright>© 2018 The Natural Computing Applications Forum</publishercopyright> <ce:para>Ear recognition technology has long been dominated by (local) descriptor-based techniques due to their formidable recognition performance and robustness to various sources of image variability. While deep-learning-based techniques have started to appear in this field only recently, they have already shown potential for further boosting the performance of ear recognition technology and dethroning descriptor-based methods as the current state of the art. However, while recognition performance is often the key factor when selecting recognition models for biometric technology, it is equally important that the behavior of the models is understood and their sensitivity to different covariates is known and well explored. Other factors, such as the train- and test-time complexity or resource requirements, are also paramount and need to be consider when designing recognition systems. To explore these issues, we present in this paper a comprehensive analysis of several descriptor- and deep-learning-based techniques for ear recognition. Our goal is to discover weak points of contemporary techniques, study the characteristics of the existing technology and identify open problems worth exploring in the future. We conduct our analysis through identification experiments on the challenging Annotated Web Ears (AWE) dataset and report our findings. The results of our analysis show that the presence of accessories and high degrees of head movement significantly impacts the identification performance of all types of recognition models, whereas mild degrees of the listed factors and other covariates such as gender and ethnicity impact the identification performance only to a limited extent. From a test-time-complexity point of view, the results suggest that lightweight deep models can be equally fast as descriptor-based methods given appropriate computing hardware, but require significantly more resources during training, where descriptor-based methods have a clear advantage. As an additional contribution, we also introduce a novel dataset of ear images, called AWE Extended (AWEx), which we collected from the web for the training of the deep models used in our experiments. AWEx contains 4104 images of 346 subjects and represents one of the largest and most challenging (publicly available) datasets of unconstrained ear images at the disposal of the research community.</ce:para> </abstract></abstracts><source country="gbr" srcid="24800" type="j"><sourcetitle>Neural Computing and Applications</sourcetitle><sourcetitle-abbrev>Neural Comput. Appl.</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">Neural Computing and Applications</translated-sourcetitle><issn type="print">09410643</issn><volisspag> <pagerange first="1" last="16"/> </volisspag><publicationyear first="2018"/><publicationdate> <year>2018</year> <month>05</month> <day>16</day> <date-text xfab-added="true">16 May 2018</date-text></publicationdate><website> <ce:e-address type="email">http://link.springer.com/journal/521</ce:e-address> </website><publisher> <publishername>Springer London</publishername> </publisher></source><enhancement><classificationgroup><classifications type="CPXCLASS"> <classification> <classification-code>722</classification-code> <classification-description>Computer Hardware</classification-description> </classification> </classifications><classifications type="FLXCLASS"> <classification> <classification-code>902</classification-code> <classification-description>FLUIDEX; Related Topics</classification-description> </classification> </classifications><classifications type="ASJC"> <classification>1712</classification> <classification>1702</classification> </classifications><classifications type="SUBJABBR"><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail/></bibrecord></item></abstracts-retrieval-response>