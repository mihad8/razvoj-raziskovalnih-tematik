<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85050945099</prism:url><dc:identifier>SCOPUS_ID:85050945099</dc:identifier><eid>2-s2.0-85050945099</eid><prism:doi>10.1109/INFOTEH.2018.8345545</prism:doi><dc:title>Compression of convolutional neural networks: A short survey</dc:title><prism:aggregationType>Conference Proceeding</prism:aggregationType><srctype>p</srctype><subtype>cp</subtype><subtypeDescription>Conference Paper</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>2018 17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018 - Proceedings</prism:publicationName><dc:publisher>Institute of Electrical and Electronics Engineers Inc.</dc:publisher><source-id>21100871137</source-id><prism:isbn>9781538649077</prism:isbn><prism:volume>2018-January</prism:volume><prism:startingPage>1</prism:startingPage><prism:endingPage>6</prism:endingPage><prism:pageRange>1-6</prism:pageRange><prism:coverDate>2018-04-23</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="57195629109"><ce:initials>R.</ce:initials><ce:indexed-name>Pilipovic R.</ce:indexed-name><ce:surname>Pilipović</ce:surname><ce:given-name>Ratko</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Pilipović R.</ce:indexed-name><ce:surname>Pilipović</ce:surname><ce:given-name>Ratko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57195629109</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><publishercopyright>© 2018 IEEE.</publishercopyright><ce:para>Nowadays, convolutional neural networks (CNN) are considered as the state-of-the-art algorithms for various tasks, especially for image classification and recognition. Because of that, more and more attention is aimed towards implementation of CNNs on embedded systems. Main obstacles for implementing CNNs on embedded systems are their large model size and large number of operations needed for inference. In order to surpass these obstacles, algorithms for CNN compression tend to lower model size and number of operations needed for inference. In this paper we review the state-of-the-art in CNN compression. To this end we divided all approaches for CNN compression into three groups: precision reduction, network pruning and design of compact network architectures. After presenting the main approaches in each group we conclude that the future CNN compression algorithms should be co-designed with hardware which will process deep learning algorithms.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85050945099" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85050945099&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85050945099&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60104200" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104200"><affilname>University of Banja Luka</affilname><affiliation-city>Banja Luka</affiliation-city><affiliation-country>Bosnia and Herzegovina</affiliation-country></affiliation><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="57195629109"><ce:initials>R.</ce:initials><ce:indexed-name>Pilipovic R.</ce:indexed-name><ce:surname>Pilipović</ce:surname><ce:given-name>Ratko</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Pilipović R.</ce:indexed-name><ce:surname>Pilipović</ce:surname><ce:given-name>Ratko</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57195629109</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="6603205527"><ce:initials>P.</ce:initials><ce:indexed-name>Bulic P.</ce:indexed-name><ce:surname>Bulić</ce:surname><ce:given-name>Patricio</ce:given-name><preferred-name><ce:initials>P.</ce:initials><ce:indexed-name>Bulić P.</ce:indexed-name><ce:surname>Bulić</ce:surname><ce:given-name>Patricio</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603205527</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="6504241952"><ce:initials>V.</ce:initials><ce:indexed-name>Risojevic V.</ce:indexed-name><ce:surname>Risojević</ce:surname><ce:given-name>Vladimir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Risojević V.</ce:indexed-name><ce:surname>Risojević</ce:surname><ce:given-name>Vladimir</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6504241952</author-url><affiliation id="60104200" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104200"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>compact network architectures</author-keyword><author-keyword>Compression</author-keyword><author-keyword>Convolutional neural networks</author-keyword><author-keyword>precision</author-keyword><author-keyword>pruning</author-keyword></authkeywords><idxterms><mainterm weight="b" candidate="n">Classification and recognition</mainterm><mainterm weight="b" candidate="n">Compression algorithms</mainterm><mainterm weight="b" candidate="n">Convolutional neural network</mainterm><mainterm weight="b" candidate="n">Convolutional Neural Networks (CNN)</mainterm><mainterm weight="b" candidate="n">precision</mainterm><mainterm weight="b" candidate="n">pruning</mainterm><mainterm weight="b" candidate="n">State of the art</mainterm><mainterm weight="b" candidate="n">State-of-the-art algorithms</mainterm></idxterms><subject-areas><subject-area code="1705" abbrev="COMP">Computer Networks and Communications</subject-area><subject-area code="2102" abbrev="ENER">Energy Engineering and Power Technology</subject-area><subject-area code="2105" abbrev="ENER">Renewable Energy, Sustainability and the Environment</subject-area><subject-area code="1706" abbrev="COMP">Computer Science Applications</subject-area><subject-area code="1711" abbrev="COMP">Signal Processing</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2019-03-30T18:20:22Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/nlp</xocs:funding-addon-type><xocs:funding><xocs:funding-agency-matched-string>Ministry of Civil Affairs, Bosnia and Herzegovina</xocs:funding-agency-matched-string><xocs:funding-id>BI-BA/16-17-029</xocs:funding-id></xocs:funding><xocs:funding><xocs:funding-agency-matched-string>Slovenian Research Agency and Ministry of Civil Affairs</xocs:funding-agency-matched-string></xocs:funding><xocs:funding><xocs:funding-agency-matched-string>Slovenian Research Agency</xocs:funding-agency-matched-string><xocs:funding-id>P2-0359</xocs:funding-id><xocs:funding-agency>Javna Agencija za Raziskovalno Dejavnost RS</xocs:funding-agency><xocs:funding-agency-id>http://data.elsevier.com/vocabulary/SciValFunders/501100004329</xocs:funding-agency-id><xocs:funding-agency-country>http://sws.geonames.org/3190538/</xocs:funding-agency-country></xocs:funding><xocs:funding><xocs:funding-agency-matched-string>Ministry of Science and Technology of the Republic of Srpska</xocs:funding-agency-matched-string><xocs:funding-id>19/6-020/961-37/15</xocs:funding-id><xocs:funding-agency>Ministry of Science and Technology of the People's Republic of China</xocs:funding-agency><xocs:funding-agency-id>http://data.elsevier.com/vocabulary/SciValFunders/501100002855</xocs:funding-agency-id><xocs:funding-agency-country>http://sws.geonames.org/1814991/</xocs:funding-agency-country></xocs:funding><xocs:funding-text>ACKNOWLEDGEMENT This research was supported in part by the Ministry of Science and Technology of the Republic of Srpska under contract 19/6-020/961-37/15, by Slovenian Research Agency under Grants P2-0359 (National research program Pervasive computing), and by Slovenian Research Agency and Ministry of Civil Affairs, Bosnia and Herzegovina, under Grant BI-BA/16-17-029.</xocs:funding-text></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered year="2018" month="10" day="30" timestamp="2018-10-30T05:16:11.000011-04:00"/><ait:date-sort year="2018" month="04" day="23"/><ait:status type="core" state="update" stage="S300"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2018 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1109/INFOTEH.2018.8345545</ce:doi><itemid idtype="PUI">623230469</itemid><itemid idtype="CAR-ID">911932433</itemid><itemid idtype="CPX">20183205654943</itemid><itemid idtype="SCP">85050945099</itemid><itemid idtype="SGR">85050945099</itemid></itemidlist><history><date-created year="2018" month="08" day="07" timestamp="BST 06:01:41"/></history><dbcollection>CPX</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="cp"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">compact network architectures</author-keyword><author-keyword xml:lang="eng">Compression</author-keyword><author-keyword xml:lang="eng">Convolutional neural networks</author-keyword><author-keyword xml:lang="eng">precision</author-keyword><author-keyword xml:lang="eng">pruning</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Compression of convolutional neural networks: A short survey</titletext></citation-title><author-group><author auid="57195629109" seq="1" type="auth"><ce:initials>R.</ce:initials><ce:indexed-name>Pilipovic R.</ce:indexed-name><ce:surname>Pilipović</ce:surname><ce:given-name>Ratko</ce:given-name><preferred-name><ce:initials>R.</ce:initials><ce:indexed-name>Pilipović R.</ce:indexed-name><ce:surname>Pilipović</ce:surname><ce:given-name>Ratko</ce:given-name></preferred-name></author><author auid="6603205527" seq="2" type="auth"><ce:initials>P.</ce:initials><ce:indexed-name>Bulic P.</ce:indexed-name><ce:surname>Bulić</ce:surname><ce:given-name>Patricio</ce:given-name><preferred-name><ce:initials>P.</ce:initials><ce:indexed-name>Bulić P.</ce:indexed-name><ce:surname>Bulić</ce:surname><ce:given-name>Patricio</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University in Ljubljana</organization><city>Ljubljana</city><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="6504241952" seq="3" type="auth"><ce:initials>V.</ce:initials><ce:indexed-name>Risojevic V.</ce:indexed-name><ce:surname>Risojević</ce:surname><ce:given-name>Vladimir</ce:given-name><preferred-name><ce:initials>V.</ce:initials><ce:indexed-name>Risojević V.</ce:indexed-name><ce:surname>Risojević</ce:surname><ce:given-name>Vladimir</ce:given-name></preferred-name></author><affiliation afid="60104200" dptid="112983122" country="bih"><organization>Faculty of Electrical Engineering</organization><organization>University in Banja Luka</organization><city>Banja Luka</city><affiliation-id afid="60104200" dptid="112983122"/><country>Bosnia and Herzegovina</country></affiliation></author-group><grantlist complete="y"><grant><grant-id>19/6-020/961-37/15</grant-id><grant-acronym>MOST</grant-acronym><grant-agency iso-code="vnm">Ministry of Science and Technology</grant-agency><grant-agency-id>100007225</grant-agency-id></grant><grant-text xml:lang="eng">This research was supported in part by the Ministry of Science and Technology of the Republic of Srpska under contract 19/6-020/961-37/15, by Slovenian Research Agency under Grants P2-0359 (National research program Pervasive computing), and by Slovenian Research Agency and Ministry of Civil Affairs, Bosnia and Herzegovina, under Grant BIBA/ 16-17-029.</grant-text></grantlist><abstracts><abstract original="y" xml:lang="eng"><publishercopyright>© 2018 IEEE.</publishercopyright><ce:para>Nowadays, convolutional neural networks (CNN) are considered as the state-of-the-art algorithms for various tasks, especially for image classification and recognition. Because of that, more and more attention is aimed towards implementation of CNNs on embedded systems. Main obstacles for implementing CNNs on embedded systems are their large model size and large number of operations needed for inference. In order to surpass these obstacles, algorithms for CNN compression tend to lower model size and number of operations needed for inference. In this paper we review the state-of-the-art in CNN compression. To this end we divided all approaches for CNN compression into three groups: precision reduction, network pruning and design of compact network architectures. After presenting the main approaches in each group we conclude that the future CNN compression algorithms should be co-designed with hardware which will process deep learning algorithms.</ce:para></abstract></abstracts><source srcid="21100871137" type="p" country="usa"><sourcetitle>2018 17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018 - Proceedings</sourcetitle><sourcetitle-abbrev>Int. Symp. INFOTEH-JAHORINA, INFOTEH - Proc.</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">2018 17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018 - Proceedings</translated-sourcetitle><issuetitle>2018 17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018 - Proceedings</issuetitle><isbn type="electronic" length="13" level="volume">9781538649077</isbn><volisspag><voliss volume="2018-January"/><pagerange first="1" last="6"/></volisspag><publicationyear first="2018"/><publicationdate><year>2018</year><month>04</month><day>23</day><date-text xfab-added="true">23 April 2018</date-text></publicationdate><website><ce:e-address type="email">http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=8337878</ce:e-address></website><publisher><publishername>Institute of Electrical and Electronics Engineers Inc.</publishername></publisher><additional-srcinfo><conferenceinfo><confevent><confname>17th International Symposium on INFOTEH-JAHORINA, INFOTEH 2018</confname><confnumber>17</confnumber><confseriestitle>International Symposium on INFOTEH-JAHORINA</confseriestitle><conflocation country="bih"><city>East Sarajevo</city></conflocation><confdate><startdate year="2018" month="03" day="21"/><enddate year="2018" month="03" day="23"/></confdate><conforganization>University of East Sarajevo, Faculty of Electrical Engineering and University of Banja Luka, Faculty of Electrical Engineering</conforganization><confcatnumber>CFP18JAH-ART</confcatnumber><confcode>136150</confcode><confsponsors complete="n"><confsponsor>Eden Garden</confsponsor><confsponsor>Lanaco Informacione Technologije</confsponsor><confsponsor>LogoSoft</confsponsor></confsponsors></confevent><confpublication><procpartno>1 of 1</procpartno></confpublication></conferenceinfo></additional-srcinfo></source><enhancement><classificationgroup><classifications type="ASJC"><classification>1705</classification><classification>2102</classification><classification>2105</classification><classification>1706</classification><classification>1711</classification></classifications><classifications type="CPXCLASS"><classification>                                 <classification-code>716.1</classification-code>                                 <classification-description>Information and Communication Theory</classification-description>                             </classification><classification>                                 <classification-code>723.4.1</classification-code>                                 <classification-description>Expert Systems</classification-description>                             </classification></classifications><classifications type="FLXCLASS"><classification>                                 <classification-code>902</classification-code>                                 <classification-description>FLUIDEX; Related Topics</classification-description>                             </classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification><classification>ENER</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="31"><reference id="1"><ref-info><refd-itemidlist><itemid idtype="SGR">85028084102</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>V.</ce:initials><ce:indexed-name>Sze V.</ce:indexed-name><ce:surname>Sze</ce:surname></author><author seq="2"><ce:initials>Y.H.</ce:initials><ce:indexed-name>Chen Y.H.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="3"><ce:initials>T.J.</ce:initials><ce:indexed-name>Yang T.J.</ce:indexed-name><ce:surname>Yang</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Emer J.</ce:indexed-name><ce:surname>Emer</ce:surname></author></ref-authors><ref-sourcetitle>Efficient Processing of Deep Neural Networks: A Tutorial and Survey</ref-sourcetitle><ref-publicationyear first="2017"/></ref-info><ref-fulltext>Sze, V., Chen, Y.H., Yang, T.J. and Emer, J., 2017. Efficient processing of deep neural networks: A tutorial and survey. arXiv preprint arXiv:1703.09039.</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Distributed deep neural networks over the cloud, the edge and end devices</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85027287268</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Surat T.</ce:indexed-name><ce:surname>Surat</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>McDanel B.</ce:indexed-name><ce:surname>McDanel</ce:surname></author><author seq="3"><ce:initials>H.T.</ce:initials><ce:indexed-name>Kung H.T.</ce:indexed-name><ce:surname>Kung</ce:surname></author></ref-authors><ref-sourcetitle>Distributed Computing Systems (ICDCS), 2017 IEEE 37th International Conference on</ref-sourcetitle><ref-publicationyear first="2017"/><ref-volisspag><pagerange first="328" last="339"/></ref-volisspag><ref-text>IEEE</ref-text></ref-info><ref-fulltext>Teerapittayanon, Surat, Bradley McDanel, and H. T. Kung. "Distributed deep neural networks over the cloud, the edge and end devices." In Distributed Computing Systems (ICDCS), 2017 IEEE 37th International Conference on, pp. 328-339. IEEE, 2017.</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Understanding the difficulty of training deep feedforward neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84862277874</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>X.</ce:initials><ce:indexed-name>Glorot X.</ce:indexed-name><ce:surname>Glorot</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><pagerange first="249" last="256"/></ref-volisspag><ref-text>March</ref-text></ref-info><ref-fulltext>Glorot, X. and Bengio, Y., 2010, March. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 249-256).</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Imagenet classification with deep convolutional neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84876231242</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Krizhevsky A.</ce:indexed-name><ce:surname>Krizhevsky</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Sutskever I.</ce:indexed-name><ce:surname>Sutskever</ce:surname></author><author seq="3"><ce:initials>G.E.</ce:initials><ce:indexed-name>Hinton G.E.</ce:indexed-name><ce:surname>Hinton</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><pagerange first="1097" last="1105"/></ref-volisspag></ref-info><ref-fulltext>Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).</ref-fulltext></reference><reference id="5"><ref-info><refd-itemidlist><itemid idtype="SGR">84925410541</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Simonyan K.</ce:indexed-name><ce:surname>Simonyan</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Zisserman A.</ce:indexed-name><ce:surname>Zisserman</ce:surname></author></ref-authors><ref-sourcetitle>Very Deep Convolutional Networks for Large-scale Image Recognition</ref-sourcetitle><ref-publicationyear first="2014"/></ref-info><ref-fulltext>Simonyan, K. and Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>Going deeper with convolutions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84937522268</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.</ce:initials><ce:indexed-name>Szegedy C.</ce:indexed-name><ce:surname>Szegedy</ce:surname></author><author seq="2"><ce:initials>W.</ce:initials><ce:indexed-name>Liu W.</ce:indexed-name><ce:surname>Liu</ce:surname></author><author seq="3"><ce:initials>Y.</ce:initials><ce:indexed-name>Jia Y.</ce:indexed-name><ce:surname>Jia</ce:surname></author><author seq="4"><ce:initials>P.</ce:initials><ce:indexed-name>Sermanet P.</ce:indexed-name><ce:surname>Sermanet</ce:surname></author><author seq="5"><ce:initials>S.</ce:initials><ce:indexed-name>Reed S.</ce:indexed-name><ce:surname>Reed</ce:surname></author><author seq="6"><ce:initials>D.</ce:initials><ce:indexed-name>Anguelov D.</ce:indexed-name><ce:surname>Anguelov</ce:surname></author><author seq="7"><ce:initials>D.</ce:initials><ce:indexed-name>Erhan D.</ce:indexed-name><ce:surname>Erhan</ce:surname></author><author seq="8"><ce:initials>V.</ce:initials><ce:indexed-name>Vanhoucke V.</ce:indexed-name><ce:surname>Vanhoucke</ce:surname></author><author seq="9"><ce:initials>A.</ce:initials><ce:indexed-name>Rabinovich A.</ce:indexed-name><ce:surname>Rabinovich</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</ref-sourcetitle><ref-publicationyear first="2015"/><ref-volisspag><pagerange first="1" last="9"/></ref-volisspag></ref-info><ref-fulltext>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V. and Rabinovich, A., 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).</ref-fulltext></reference><reference id="7"><ref-info><ref-title><ref-titletext>Deep residual learning for image recognition</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84986274465</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>He K.</ce:indexed-name><ce:surname>He</ce:surname></author><author seq="2"><ce:initials>X.</ce:initials><ce:indexed-name>Zhang X.</ce:indexed-name><ce:surname>Zhang</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Ren S.</ce:indexed-name><ce:surname>Ren</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Sun J.</ce:indexed-name><ce:surname>Sun</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="770" last="778"/></ref-volisspag></ref-info><ref-fulltext>He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Imagenet: A large-scale hierarchical image database</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">72249100259</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Deng J.</ce:indexed-name><ce:surname>Deng</ce:surname></author><author seq="2"><ce:initials>W.</ce:initials><ce:indexed-name>Dong W.</ce:indexed-name><ce:surname>Dong</ce:surname></author><author seq="3"><ce:initials>R.</ce:initials><ce:indexed-name>Socher R.</ce:indexed-name><ce:surname>Socher</ce:surname></author><author seq="4"><ce:initials>L.J.</ce:initials><ce:indexed-name>Li L.J.</ce:indexed-name><ce:surname>Li</ce:surname></author><author seq="5"><ce:initials>K.</ce:initials><ce:indexed-name>Li K.</ce:indexed-name><ce:surname>Li</ce:surname></author><author seq="6"><ce:initials>L.</ce:initials><ce:indexed-name>Fei-Fei L.</ce:indexed-name><ce:surname>Fei-Fei</ce:surname></author></ref-authors><ref-sourcetitle>Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="248" last="255"/></ref-volisspag><ref-text>June, IEEE</ref-text></ref-info><ref-fulltext>Deng, J., Dong, W., Socher, R., Li, L.J., Li, K. and Fei-Fei, L., 2009, June. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on (pp. 248-255). IEEE.</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Applicability of approximate multipliers in hardware neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84864386338</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>U.</ce:initials><ce:indexed-name>Lotric U.</ce:indexed-name><ce:surname>Lotrič</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Bulic P.</ce:indexed-name><ce:surname>Bulić</ce:surname></author></ref-authors><ref-sourcetitle>Neurocomputing</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><voliss volume="96"/><pagerange first="57" last="65"/></ref-volisspag></ref-info><ref-fulltext>Lotrič, U. and Bulić, P., 2012. Applicability of approximate multipliers in hardware neural networks. Neurocomputing, 96, pp. 57-65.</ref-fulltext></reference><reference id="10"><ref-info><refd-itemidlist><itemid idtype="SGR">84988910518</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Gysel P.</ce:indexed-name><ce:surname>Gysel</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Motamedi M.</ce:indexed-name><ce:surname>Motamedi</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Ghiasi S.</ce:indexed-name><ce:surname>Ghiasi</ce:surname></author></ref-authors><ref-sourcetitle>Hardware-oriented Approximation of Convolutional Neural Networks</ref-sourcetitle><ref-publicationyear first="2016"/></ref-info><ref-fulltext>Gysel, P., Motamedi, M. and Ghiasi, S., 2016. Hardware-oriented approximation of convolutional neural networks. arXiv preprint arXiv:1604.03168.</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>Binaryconnect: Training deep neural networks with binary weights during propagations</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84965117606</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Courbariaux M.</ce:indexed-name><ce:surname>Courbariaux</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author><author seq="3"><ce:initials>J.P.</ce:initials><ce:indexed-name>David J.P.</ce:indexed-name><ce:surname>David</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="2015"/><ref-volisspag><pagerange first="3123" last="3131"/></ref-volisspag></ref-info><ref-fulltext>Courbariaux, M., Bengio, Y. and David, J.P., 2015. Binaryconnect: Training deep neural networks with binary weights during propagations. In Advances in Neural Information Processing Systems (pp. 3123-3131).</ref-fulltext></reference><reference id="12"><ref-info><refd-itemidlist><itemid idtype="SGR">84988920420</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Courbariaux M.</ce:indexed-name><ce:surname>Courbariaux</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Hubara I.</ce:indexed-name><ce:surname>Hubara</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Soudry D.</ce:indexed-name><ce:surname>Soudry</ce:surname></author><author seq="4"><ce:initials>R.</ce:initials><ce:indexed-name>El-Yaniv R.</ce:indexed-name><ce:surname>El-Yaniv</ce:surname></author><author seq="5"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author></ref-authors><ref-sourcetitle>Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or-1</ref-sourcetitle><ref-publicationyear first="2016"/></ref-info><ref-fulltext>Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R. and Bengio, Y., 2016. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or-1. arXiv preprint arXiv:1602.02830.</ref-fulltext></reference><reference id="13"><ref-info><ref-title><ref-titletext>Xnor-net: Imagenet classification using binary convolutional neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84986334939</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Rastegari M.</ce:indexed-name><ce:surname>Rastegari</ce:surname></author><author seq="2"><ce:initials>V.</ce:initials><ce:indexed-name>Ordonez V.</ce:indexed-name><ce:surname>Ordonez</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Redmon J.</ce:indexed-name><ce:surname>Redmon</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Farhadi A.</ce:indexed-name><ce:surname>Farhadi</ce:surname></author></ref-authors><ref-sourcetitle>CoRR</ref-sourcetitle><ref-publicationyear first="2016"/><ref-text>vol. abs/1603.05279</ref-text></ref-info><ref-fulltext>M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi, "Xnor-net: Imagenet classification using binary convolutional neural networks," CoRR, vol. abs/1603.05279, 2016.</ref-fulltext></reference><reference id="14"><ref-info><refd-itemidlist><itemid idtype="SGR">84944735469</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>I.</ce:initials><ce:indexed-name>Goodfellow I.</ce:indexed-name><ce:surname>Goodfellow</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Bengio Y.</ce:indexed-name><ce:surname>Bengio</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Courville A.</ce:indexed-name><ce:surname>Courville</ce:surname></author></ref-authors><ref-sourcetitle>Deep Learning</ref-sourcetitle><ref-publicationyear first="2016"/><ref-text>MIT press</ref-text></ref-info><ref-fulltext>Goodfellow, I., Bengio, Y. and Courville, A., 2016. Deep learning. MIT press.</ref-fulltext></reference><reference id="15"><ref-info><refd-itemidlist><itemid idtype="SGR">84995424242</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Miyashita D.</ce:indexed-name><ce:surname>Miyashita</ce:surname></author><author seq="2"><ce:initials>E.H.</ce:initials><ce:indexed-name>Lee E.H.</ce:indexed-name><ce:surname>Lee</ce:surname></author><author seq="3"><ce:initials>B.</ce:initials><ce:indexed-name>Murmann B.</ce:indexed-name><ce:surname>Murmann</ce:surname></author></ref-authors><ref-sourcetitle>Convolutional Neural Networks Using Logarithmic Data Representation</ref-sourcetitle><ref-publicationyear first="2016"/></ref-info><ref-fulltext>Miyashita, D., Lee, E.H. and Murmann, B., 2016. Convolutional neural networks using logarithmic data representation. arXiv preprint arXiv:1603.01025.</ref-fulltext></reference><reference id="16"><ref-info><refd-itemidlist><itemid idtype="SGR">85034742143</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Zhou A.</ce:indexed-name><ce:surname>Zhou</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Yao A.</ce:indexed-name><ce:surname>Yao</ce:surname></author><author seq="3"><ce:initials>Y.</ce:initials><ce:indexed-name>Guo Y.</ce:indexed-name><ce:surname>Guo</ce:surname></author><author seq="4"><ce:initials>L.</ce:initials><ce:indexed-name>Xu L.</ce:indexed-name><ce:surname>Xu</ce:surname></author><author seq="5"><ce:initials>Y.</ce:initials><ce:indexed-name>Chen Y.</ce:indexed-name><ce:surname>Chen</ce:surname></author></ref-authors><ref-sourcetitle>Incremental Network Quantization: Towards Lossless Cnns with Low-precision Weights</ref-sourcetitle><ref-publicationyear first="2017"/></ref-info><ref-fulltext>Zhou, A., Yao, A., Guo, Y., Xu, L. and Chen, Y., 2017. Incremental network quantization: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044.</ref-fulltext></reference><reference id="17"><ref-info><refd-itemidlist><itemid idtype="SGR">79956332226</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Rissanen J.</ce:indexed-name><ce:surname>Rissanen</ce:surname></author></ref-authors><ref-sourcetitle>An Introduction to the MDL Principle</ref-sourcetitle><ref-publicationyear first="2005"/><ref-website><ce:e-address type="email">www.mdl-research.org</ce:e-address></ref-website></ref-info><ref-fulltext>Rissanen, J., 2005. An introduction to the MDL principle. Available online at www. mdl-research. org.</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Optimal brain damage</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">0000494466</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>LeCun Y.</ce:indexed-name><ce:surname>LeCun</ce:surname></author><author seq="2"><ce:initials>J.S.</ce:initials><ce:indexed-name>Denker J.S.</ce:indexed-name><ce:surname>Denker</ce:surname></author><author seq="3"><ce:initials>S.A.</ce:initials><ce:indexed-name>Solla S.A.</ce:indexed-name><ce:surname>Solla</ce:surname></author><author seq="4"><ce:initials>R.E.</ce:initials><ce:indexed-name>Howard R.E.</ce:indexed-name><ce:surname>Howard</ce:surname></author><author seq="5"><ce:initials>L.D.</ce:initials><ce:indexed-name>Jackel L.D.</ce:indexed-name><ce:surname>Jackel</ce:surname></author></ref-authors><ref-sourcetitle>NIPs</ref-sourcetitle><ref-publicationyear first="1989"/><ref-volisspag><voliss volume="2"/><pagerange first="598" last="605"/></ref-volisspag></ref-info><ref-fulltext>Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D. Jackel, "Optimal brain damage.," in NIPs, vol. 2, pp. 598-605, 1989</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>2015. Learning both weights and connections for efficient neural network</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84965140688</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Han S.</ce:indexed-name><ce:surname>Han</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Pool J.</ce:indexed-name><ce:surname>Pool</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Tran J.</ce:indexed-name><ce:surname>Tran</ce:surname></author><author seq="4"><ce:initials>W.</ce:initials><ce:indexed-name>Dally W.</ce:indexed-name><ce:surname>Dally</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-volisspag><pagerange first="1135" last="1143"/></ref-volisspag></ref-info><ref-fulltext>Han, S., Pool, J., Tran, J. and Dally, W., 2015. Learning both weights and connections for efficient neural network. In Advances in Neural Information Processing Systems (pp. 1135-1143).</ref-fulltext></reference><reference id="20"><ref-info><refd-itemidlist><itemid idtype="SGR">84965175092</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Han S.</ce:indexed-name><ce:surname>Han</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Mao H.</ce:indexed-name><ce:surname>Mao</ce:surname></author><author seq="3"><ce:initials>W.J.</ce:initials><ce:indexed-name>Dally W.J.</ce:indexed-name><ce:surname>Dally</ce:surname></author></ref-authors><ref-sourcetitle>Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</ref-sourcetitle><ref-publicationyear first="2015"/></ref-info><ref-fulltext>Han, S., Mao, H. and Dally, W.J., 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149.</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>2016, June. EIE: Efficient inference engine on compressed deep neural network</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84988443578</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Han S.</ce:indexed-name><ce:surname>Han</ce:surname></author><author seq="2"><ce:initials>X.</ce:initials><ce:indexed-name>Liu X.</ce:indexed-name><ce:surname>Liu</ce:surname></author><author seq="3"><ce:initials>H.</ce:initials><ce:indexed-name>Mao H.</ce:indexed-name><ce:surname>Mao</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Pu J.</ce:indexed-name><ce:surname>Pu</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Pedram A.</ce:indexed-name><ce:surname>Pedram</ce:surname></author><author seq="6"><ce:initials>M.A.</ce:initials><ce:indexed-name>Horowitz M.A.</ce:indexed-name><ce:surname>Horowitz</ce:surname></author><author seq="7"><ce:initials>W.J.</ce:initials><ce:indexed-name>Dally W.J.</ce:indexed-name><ce:surname>Dally</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 43rd International Symposium on Computer Architecture</ref-sourcetitle><ref-volisspag><pagerange first="243" last="254"/></ref-volisspag><ref-text>IEEE Press</ref-text></ref-info><ref-fulltext>Han, S., Liu, X., Mao, H., Pu, J., Pedram, A., Horowitz, M.A. and Dally, W.J., 2016, June. EIE: efficient inference engine on compressed deep neural network. In Proceedings of the 43rd International Symposium on Computer Architecture (pp. 243-254). IEEE Press.</ref-fulltext></reference><reference id="22"><ref-info><ref-title><ref-titletext>Dynamic network surgery for efficient dnns</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85018895765</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Guo Y.</ce:indexed-name><ce:surname>Guo</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Yao A.</ce:indexed-name><ce:surname>Yao</ce:surname></author><author seq="3"><ce:initials>Y.</ce:initials><ce:indexed-name>Chen Y.</ce:indexed-name><ce:surname>Chen</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="1379" last="1387"/></ref-volisspag></ref-info><ref-fulltext>Guo, Y., Yao, A. and Chen, Y., 2016. Dynamic network surgery for efficient dnns. In Advances In Neural Information Processing Systems (pp. 1379-1387).</ref-fulltext></reference><reference id="23"><ref-info><ref-title><ref-titletext>An exploration of parameter redundancy in deep networks with circulant projections</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84973890879</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Cheng Y.</ce:indexed-name><ce:surname>Cheng</ce:surname></author><author seq="2"><ce:initials>F.X.</ce:initials><ce:indexed-name>Yu F.X.</ce:indexed-name><ce:surname>Yu</ce:surname></author><author seq="3"><ce:initials>R.S.</ce:initials><ce:indexed-name>Feris R.S.</ce:indexed-name><ce:surname>Feris</ce:surname></author><author seq="4"><ce:initials>S.</ce:initials><ce:indexed-name>Kumar S.</ce:indexed-name><ce:surname>Kumar</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Choudhary A.</ce:indexed-name><ce:surname>Choudhary</ce:surname></author><author seq="6"><ce:initials>S.F.</ce:initials><ce:indexed-name>Chang S.F.</ce:indexed-name><ce:surname>Chang</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the IEEE International Conference on Computer Vision</ref-sourcetitle><ref-publicationyear first="2015"/><ref-volisspag><pagerange first="2857" last="2865"/></ref-volisspag></ref-info><ref-fulltext>Cheng, Y., Yu, F.X., Feris, R.S., Kumar, S., Choudhary, A. and Chang, S.F., 2015. An exploration of parameter redundancy in deep networks with circulant projections. In Proceedings of the IEEE International Conference on Computer Vision (pp. 2857-2865).</ref-fulltext></reference><reference id="24"><ref-info><refd-itemidlist><itemid idtype="SGR">84965161214</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Y.</ce:initials><ce:indexed-name>Cheng Y.</ce:indexed-name><ce:surname>Cheng</ce:surname></author><author seq="2"><ce:initials>X.Y.</ce:initials><ce:indexed-name>Felix X.Y.</ce:indexed-name><ce:surname>Felix</ce:surname></author><author seq="3"><ce:initials>R.S.</ce:initials><ce:indexed-name>Feris R.S.</ce:indexed-name><ce:surname>Feris</ce:surname></author><author seq="4"><ce:initials>S.</ce:initials><ce:indexed-name>Kumar S.</ce:indexed-name><ce:surname>Kumar</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Choudhary A.</ce:indexed-name><ce:surname>Choudhary</ce:surname></author><author seq="6"><ce:initials>S.F.</ce:initials><ce:indexed-name>Chang S.F.</ce:indexed-name><ce:surname>Chang</ce:surname></author></ref-authors><ref-sourcetitle>Fast Neural Networks with Circulant Projections</ref-sourcetitle><ref-publicationyear first="2015"/></ref-info><ref-fulltext>Cheng, Y., Felix, X.Y., Feris, R.S., Kumar, S., Choudhary, A. and Chang, S.F., 2015. Fast neural networks with circulant projections. arXiv preprint arXiv:1502.03436.</ref-fulltext></reference><reference id="25"><ref-info><ref-title><ref-titletext>Deep fried convnets</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84973904224</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>Z.</ce:initials><ce:indexed-name>Yang Z.</ce:indexed-name><ce:surname>Yang</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Moczulski M.</ce:indexed-name><ce:surname>Moczulski</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Denil M.</ce:indexed-name><ce:surname>Denil</ce:surname></author><author seq="4"><ce:initials>N.</ce:initials><ce:indexed-name>De Freitas N.</ce:indexed-name><ce:surname>De Freitas</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Smola A.</ce:indexed-name><ce:surname>Smola</ce:surname></author><author seq="6"><ce:initials>L.</ce:initials><ce:indexed-name>Song L.</ce:indexed-name><ce:surname>Song</ce:surname></author><author seq="7"><ce:initials>Z.</ce:initials><ce:indexed-name>Wang Z.</ce:indexed-name><ce:surname>Wang</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the IEEE International Conference on Computer Vision</ref-sourcetitle><ref-publicationyear first="2015"/><ref-volisspag><pagerange first="1476" last="1483"/></ref-volisspag></ref-info><ref-fulltext>Yang, Z., Moczulski, M., Denil, M., de Freitas, N., Smola, A., Song, L. and Wang, Z., 2015. Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1476-1483).</ref-fulltext></reference><reference id="26"><ref-info><ref-title><ref-titletext>Group equivariant convolutional networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84998538835</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Cohen T.</ce:indexed-name><ce:surname>Cohen</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Welling M.</ce:indexed-name><ce:surname>Welling</ce:surname></author></ref-authors><ref-sourcetitle>International Conference on Machine Learning</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="2990" last="2999"/></ref-volisspag><ref-text>June</ref-text></ref-info><ref-fulltext>Cohen, T. and Welling, M., 2016, June. Group equivariant convolutional networks. In International Conference on Machine Learning (pp. 2990-2999).</ref-fulltext></reference><reference id="27"><ref-info><ref-title><ref-titletext>Multi-bias non-linear activation in deep neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85008258470</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Li H.</ce:indexed-name><ce:surname>Li</ce:surname></author><author seq="2"><ce:initials>W.</ce:initials><ce:indexed-name>Ouyang W.</ce:indexed-name><ce:surname>Ouyang</ce:surname></author><author seq="3"><ce:initials>X.</ce:initials><ce:indexed-name>Wang X.</ce:indexed-name><ce:surname>Wang</ce:surname></author></ref-authors><ref-sourcetitle>International Conference on Machine Learning</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="221" last="229"/></ref-volisspag><ref-text>June</ref-text></ref-info><ref-fulltext>Li, H., Ouyang, W. and Wang, X., 2016, June. Multi-bias non-linear activation in deep neural networks. In International conference on machine learning (pp. 221-229).</ref-fulltext></reference><reference id="28"><ref-info><refd-itemidlist><itemid idtype="SGR">85019156273</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Dieleman S.</ce:indexed-name><ce:surname>Dieleman</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>De Fauw J.</ce:indexed-name><ce:surname>De Fauw</ce:surname></author><author seq="3"><ce:initials>K.</ce:initials><ce:indexed-name>Kavukcuoglu K.</ce:indexed-name><ce:surname>Kavukcuoglu</ce:surname></author></ref-authors><ref-sourcetitle>Exploiting Cyclic Symmetry in Convolutional Neural Networks</ref-sourcetitle><ref-publicationyear first="2016"/></ref-info><ref-fulltext>Dieleman, S., De Fauw, J. and Kavukcuoglu, K., 2016. Exploiting cyclic symmetry in convolutional neural networks. arXiv preprint arXiv:1602.02660.</ref-fulltext></reference><reference id="29"><ref-info><refd-itemidlist><itemid idtype="SGR">84988340112</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.N.</ce:initials><ce:indexed-name>Iandola F.N.</ce:indexed-name><ce:surname>Iandola</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Han S.</ce:indexed-name><ce:surname>Han</ce:surname></author><author seq="3"><ce:initials>M.W.</ce:initials><ce:indexed-name>Moskewicz M.W.</ce:indexed-name><ce:surname>Moskewicz</ce:surname></author><author seq="4"><ce:initials>K.</ce:initials><ce:indexed-name>Ashraf K.</ce:indexed-name><ce:surname>Ashraf</ce:surname></author><author seq="5"><ce:initials>W.J.</ce:initials><ce:indexed-name>Dally W.J.</ce:indexed-name><ce:surname>Dally</ce:surname></author><author seq="6"><ce:initials>K.</ce:initials><ce:indexed-name>Keutzer K.</ce:indexed-name><ce:surname>Keutzer</ce:surname></author></ref-authors><ref-sourcetitle>SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and&lt; 0.5 MB Model Size</ref-sourcetitle><ref-publicationyear first="2016"/></ref-info><ref-fulltext>Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J. and Keutzer, K., 2016. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size. arXiv preprint arXiv:1602.07360.</ref-fulltext></reference><reference id="30"><ref-info><ref-title><ref-titletext>Doubly convolutional neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85019232787</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Zhai S.</ce:indexed-name><ce:surname>Zhai</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Cheng Y.</ce:indexed-name><ce:surname>Cheng</ce:surname></author><author seq="3"><ce:initials>Z.M.</ce:initials><ce:indexed-name>Zhang Z.M.</ce:indexed-name><ce:surname>Zhang</ce:surname></author><author seq="4"><ce:initials>W.</ce:initials><ce:indexed-name>Lu W.</ce:indexed-name><ce:surname>Lu</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="1082" last="1090"/></ref-volisspag></ref-info><ref-fulltext>Zhai, S., Cheng, Y., Zhang, Z.M. and Lu, W., 2016. Doubly convolutional neural networks. In Advances in neural information processing systems (pp. 1082-1090).</ref-fulltext></reference><reference id="31"><ref-info><refd-itemidlist><itemid idtype="SGR">85030212949</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.G.</ce:initials><ce:indexed-name>Howard A.G.</ce:indexed-name><ce:surname>Howard</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Zhu M.</ce:indexed-name><ce:surname>Zhu</ce:surname></author><author seq="3"><ce:initials>B.</ce:initials><ce:indexed-name>Chen B.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="4"><ce:initials>D.</ce:initials><ce:indexed-name>Kalenichenko D.</ce:indexed-name><ce:surname>Kalenichenko</ce:surname></author><author seq="5"><ce:initials>W.</ce:initials><ce:indexed-name>Wang W.</ce:indexed-name><ce:surname>Wang</ce:surname></author><author seq="6"><ce:initials>T.</ce:initials><ce:indexed-name>Weyand T.</ce:indexed-name><ce:surname>Weyand</ce:surname></author><author seq="7"><ce:initials>M.</ce:initials><ce:indexed-name>Andreetto M.</ce:indexed-name><ce:surname>Andreetto</ce:surname></author><author seq="8"><ce:initials>H.</ce:initials><ce:indexed-name>Adam H.</ce:indexed-name><ce:surname>Adam</ce:surname></author></ref-authors><ref-sourcetitle>Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications</ref-sourcetitle><ref-publicationyear first="2017"/></ref-info><ref-fulltext>Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M. and Adam, H., 2017. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861.</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>