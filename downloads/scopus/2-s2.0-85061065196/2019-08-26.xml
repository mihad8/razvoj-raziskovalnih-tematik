<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85061065196</prism:url><dc:identifier>SCOPUS_ID:85061065196</dc:identifier><eid>2-s2.0-85061065196</eid><prism:doi>10.3390/app9030439</prism:doi><article-number>439</article-number><dc:title>Automatic segmentation of ethnomusicological field recordings</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Applied Sciences (Switzerland)</prism:publicationName><dc:publisher>MDPI AGindexing@mdpi.com</dc:publisher><source-id>21100829268</source-id><prism:issn>20763417</prism:issn><prism:volume>9</prism:volume><prism:issueIdentifier>3</prism:issueIdentifier><prism:coverDate>2019-01-28</prism:coverDate><openaccess>1</openaccess><openaccessFlag>true</openaccessFlag><dc:creator><author seq="1" auid="6603601816"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603601816</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><publishercopyright>© 2019 by the authors.</publishercopyright><ce:para>The article presents a method for segmentation of ethnomusicological field recordings. Field recordings are integral documents of folk music performances captured in the field, and typically contain performances, intertwined with interviews and commentaries. As these are live recordings, captured in non-ideal conditions, they usually contain significant background noise. We present a segmentation method that segments field recordings into individual units labelled as speech, solo singing, choir singing, and instrumentals. Classification is based on convolutional deep networks, and is augmented with a probabilistic approach for segmentation. We describe the dataset gathered for the task and the tools developed for gathering the reference annotations. We outline a deep network architecture based on residual modules for labelling short audio segments and compare it to the more standard feature based approaches, where an improvement in classification accuracy of over 10% was obtained. We also present the SeFiRe segmentation tool that incorporates the presented segmentation method.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85061065196" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85061065196&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85061065196&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="6603601816"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6603601816</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="55582372100"><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name><preferred-name><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/55582372100</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="3" auid="7004603977"><ce:initials>A.</ce:initials><ce:indexed-name>Kavcic A.</ce:indexed-name><ce:surname>Kavčič</ce:surname><ce:given-name>Alenka</ce:given-name><preferred-name><ce:initials>A.</ce:initials><ce:indexed-name>Kavčič A.</ce:indexed-name><ce:surname>Kavčič</ce:surname><ce:given-name>Alenka</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/7004603977</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="4" auid="56258907000"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/56258907000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Audio segmentation</author-keyword><author-keyword>Deep learning</author-keyword><author-keyword>Field recordings</author-keyword><author-keyword>Music information retrieval</author-keyword></authkeywords><idxterms/><subject-areas><subject-area code="2500" abbrev="MATE">Materials Science (all)</subject-area><subject-area code="3105" abbrev="PHYS">Instrumentation</subject-area><subject-area code="2200" abbrev="ENGI">Engineering (all)</subject-area><subject-area code="1508" abbrev="CENG">Process Chemistry and Technology</subject-area><subject-area code="1706" abbrev="COMP">Computer Science Applications</subject-area><subject-area code="1507" abbrev="CENG">Fluid Flow and Transfer Processes</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2019-04-03T01:56:50.243Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/nlp</xocs:funding-addon-type><xocs:funding><xocs:funding-agency-matched-string>Slovenian Research Agency</xocs:funding-agency-matched-string><xocs:funding-id>J7-9426</xocs:funding-id><xocs:funding-agency>Javna Agencija za Raziskovalno Dejavnost RS</xocs:funding-agency><xocs:funding-agency-id>http://data.elsevier.com/vocabulary/SciValFunders/501100004329</xocs:funding-agency-id><xocs:funding-agency-country>http://sws.geonames.org/3190538/</xocs:funding-agency-country></xocs:funding><xocs:funding-text>Funding: This research was partly funded by the Slovenian Research Agency, within the project Thinking Folklore, grant number J7-9426.</xocs:funding-text></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered day="02" month="08" timestamp="2019-08-02T23:30:10.000010-04:00" year="2019"/><ait:date-sort day="28" month="01" year="2019"/><ait:status stage="S300" state="update" type="core"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2019 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.3390/app9030439</ce:doi><itemid idtype="PUI">626197657</itemid><itemid idtype="CAR-ID">915490228</itemid><itemid idtype="SCOPUS">20190368792</itemid><itemid idtype="SNPHYS">2019005826</itemid><itemid idtype="SCP">85061065196</itemid><itemid idtype="SGR">85061065196</itemid></itemidlist><history><date-created day="08" month="02" timestamp="BST 14:24:13" year="2019"/></history><dbcollection>SCOPUS</dbcollection><dbcollection>SNPHYS</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">Audio segmentation</author-keyword><author-keyword xml:lang="eng">Deep learning</author-keyword><author-keyword xml:lang="eng">Field recordings</author-keyword><author-keyword xml:lang="eng">Music information retrieval</author-keyword></author-keywords></citation-info><citation-title><titletext original="y" xml:lang="eng" language="English">Automatic segmentation of ethnomusicological field recordings</titletext></citation-title><author-group><author auid="6603601816" seq="1" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></preferred-name></author><author auid="55582372100" seq="2" type="auth"><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name><preferred-name><ce:initials>C.</ce:initials><ce:indexed-name>Bohak C.</ce:indexed-name><ce:surname>Bohak</ce:surname><ce:given-name>Ciril</ce:given-name></preferred-name></author><author auid="7004603977" seq="3" type="auth"><ce:initials>A.</ce:initials><ce:indexed-name>Kavcic A.</ce:indexed-name><ce:surname>Kavčič</ce:surname><ce:given-name>Alenka</ce:given-name><preferred-name><ce:initials>A.</ce:initials><ce:indexed-name>Kavčič A.</ce:indexed-name><ce:surname>Kavčič</ce:surname><ce:given-name>Alenka</ce:given-name></preferred-name></author><author auid="56258907000" seq="4" type="auth"><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name><preferred-name><ce:initials>M.</ce:initials><ce:indexed-name>Pesek M.</ce:indexed-name><ce:surname>Pesek</ce:surname><ce:given-name>Matevž</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Večna pot 113</address-part><city>Ljubljana</city><postal-code>1000</postal-code><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname><ce:given-name>Matija</ce:given-name></person><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Večna pot 113</address-part><city>Ljubljana</city><postal-code>1000</postal-code><country>Slovenia</country></affiliation></correspondence><grantlist complete="y"><grant-text xml:lang="eng">This research was partly funded by the Slovenian Research Agency, within the project Thinking Folklore, grant number J7-9426</grant-text></grantlist><abstracts><abstract original="y" xml:lang="eng"><publishercopyright>© 2019 by the authors.</publishercopyright><ce:para>The article presents a method for segmentation of ethnomusicological field recordings. Field recordings are integral documents of folk music performances captured in the field, and typically contain performances, intertwined with interviews and commentaries. As these are live recordings, captured in non-ideal conditions, they usually contain significant background noise. We present a segmentation method that segments field recordings into individual units labelled as speech, solo singing, choir singing, and instrumentals. Classification is based on convolutional deep networks, and is augmented with a probabilistic approach for segmentation. We describe the dataset gathered for the task and the tools developed for gathering the reference annotations. We outline a deep network architecture based on residual modules for labelling short audio segments and compare it to the more standard feature based approaches, where an improvement in classification accuracy of over 10% was obtained. We also present the SeFiRe segmentation tool that incorporates the presented segmentation method.</ce:para></abstract></abstracts><source country="che" srcid="21100829268" type="j"><sourcetitle>Applied Sciences (Switzerland)</sourcetitle><sourcetitle-abbrev>Appl. Sci.</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng"/><issn type="electronic">20763417</issn><volisspag><voliss issue="3" volume="9"/></volisspag><article-number>439</article-number><publicationyear first="2019"/><publicationdate><year>2019</year><month>01</month><day>28</day><date-text xfab-added="true">28 January 2019</date-text></publicationdate><website><ce:e-address type="email">https://www.mdpi.com/2076-3417/9/3/439/pdf</ce:e-address></website><publisher><publishername>MDPI AG</publishername><ce:e-address type="email">indexing@mdpi.com</ce:e-address></publisher></source><enhancement><classificationgroup><classifications type="ASJC"><classification>2500</classification><classification>3105</classification><classification>2200</classification><classification>1508</classification><classification>1706</classification><classification>1507</classification></classifications><classifications type="SUBJABBR"><classification>MATE</classification><classification>PHYS</classification><classification>ENGI</classification><classification>CENG</classification><classification>COMP</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="23"><reference id="1"><ref-info><ref-title><ref-titletext>Probabilistic Segmentation and Labeling of Ethnomusicological Field Recordings</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84873426490</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 10th International Society for Music Information Retrieval Conference</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="75" last="80"/></ref-volisspag><ref-text>Kobe, Japan, 26-30 October</ref-text></ref-info><ref-fulltext>Marolt, M. Probabilistic Segmentation and Labeling of Ethnomusicological Field Recordings. In Proceedings of the 10th International Society for Music Information Retrieval Conference, Kobe, Japan, 26-30 October 2009; pp. 75-80</ref-fulltext></reference><reference id="2"><ref-info><ref-title><ref-titletext>Music tonality features for speech/music discrimination</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84905265666</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Sell G.</ce:indexed-name><ce:surname>Sell</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Clark P.</ce:indexed-name><ce:surname>Clark</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 2014 IEEE International Conference on Acoustics</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><pagerange first="2489" last="2493"/></ref-volisspag><ref-text>Speech and Signal Processing (ICASSP), Florence, Italy, 4-9 May</ref-text></ref-info><ref-fulltext>Sell, G.; Clark, P. Music tonality features for speech/music discrimination. In Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 4-9 May 2014; pp. 2489-2493</ref-fulltext></reference><reference id="3"><ref-info><ref-title><ref-titletext>Noise robust features for speech/music discrimination in real-time telecommunication</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">70449585615</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Zhong-Hua F.</ce:indexed-name><ce:surname>Zhong-Hua</ce:surname></author><author seq="2"><ce:initials>W.</ce:initials><ce:indexed-name>Jhing-Fa W.</ce:indexed-name><ce:surname>Jhing-Fa</ce:surname></author><author seq="3"><ce:initials>X.</ce:initials><ce:indexed-name>Lei X.</ce:indexed-name><ce:surname>Lei</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the Multimedia and Expo, 2009. ICME 2009. IEEE International Conference on IEEE</ref-sourcetitle><ref-publicationyear first="2009"/><ref-volisspag><pagerange first="574" last="577"/></ref-volisspag><ref-text>New York, NY, USA, 28 June-3 July</ref-text></ref-info><ref-fulltext>Zhong-Hua, F.; Jhing-Fa, W.; Lei, X. Noise robust features for speech/music discrimination in real-time telecommunication. In Proceedings of the Multimedia and Expo, 2009. ICME 2009. IEEE International Conference on IEEE, New York, NY, USA, 28 June-3 July 2009; pp. 574-577</ref-fulltext></reference><reference id="4"><ref-info><ref-title><ref-titletext>Automatic music detection in television productions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84872745273</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>Seyerlehner K.</ce:indexed-name><ce:surname>Seyerlehner</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Pohle T.</ce:indexed-name><ce:surname>Pohle</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Schedl M.</ce:indexed-name><ce:surname>Schedl</ce:surname></author><author seq="4"><ce:initials>G.</ce:initials><ce:indexed-name>Widmer G.</ce:indexed-name><ce:surname>Widmer</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 10th International Conference on Digital Audio Effects</ref-sourcetitle><ref-publicationyear first="2007"/><ref-text>Bordeaux, France, 10-15 September</ref-text></ref-info><ref-fulltext>Seyerlehner, K.; Pohle, T.; Schedl, M.; Widmer, G. Automatic music detection in television productions. In Proceedings of the 10th International Conference on Digital Audio Effects, Bordeaux, France, 10-15 September 2007</ref-fulltext></reference><reference id="5"><ref-info><refd-itemidlist><itemid idtype="SGR">33646360883</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Ajmera J.</ce:indexed-name><ce:surname>Ajmera</ce:surname></author></ref-authors><ref-sourcetitle>Robust Audio Segmentation</ref-sourcetitle><ref-publicationyear first="2004"/><ref-text>Ecole Polytechnique Federale de Lausanne: Lausanne, Switzerland</ref-text></ref-info><ref-fulltext>Ajmera, J. Robust Audio Segmentation; Ecole Polytechnique Federale de Lausanne: Lausanne, Switzerland, 2004</ref-fulltext></reference><reference id="6"><ref-info><ref-title><ref-titletext>A Speech/Music Discriminator of Radio Recordings Based on Dynamic Programming and Bayesian Networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">47649121001</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Pikrakis A.</ce:indexed-name><ce:surname>Pikrakis</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Giannakopoulos T.</ce:indexed-name><ce:surname>Giannakopoulos</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Theodoridis S.</ce:indexed-name><ce:surname>Theodoridis</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Trans. Multimed</ref-sourcetitle><ref-publicationyear first="2008"/><ref-volisspag><voliss volume="10"/><pagerange first="846" last="857"/></ref-volisspag></ref-info><ref-fulltext>Pikrakis, A.; Giannakopoulos, T.; Theodoridis, S. A Speech/Music Discriminator of Radio Recordings Based on Dynamic Programming and Bayesian Networks. IEEE Trans. Multimed. 2008, 10, 846-857</ref-fulltext></reference><reference id="7"><ref-info><refd-itemidlist><itemid idtype="SGR">85061025778</itemid></refd-itemidlist><ref-authors><collaboration seq="1"><ce:indexed-name>Speech Classification and Detection Results</ce:indexed-name><ce:text>Speech Classification and Detection Results</ce:text></collaboration></ref-authors><ref-publicationyear first="2015"/><ref-website><ce:e-address type="email">https://www.music-ir.org/mirex/wiki/2015:Music/Speech_Classification_and_Detection_Results</ce:e-address></ref-website><ref-text>(accessed on 12 December 2018), Mirex</ref-text></ref-info><ref-fulltext>Speech Classification and Detection Results, Mirex 2015. Available online: https://www.music-ir.org/ mirex/wiki/2015:Music/Speech_Classification_and_Detection_Results (accessed on 12 December 2018)</ref-fulltext></reference><reference id="8"><ref-info><ref-title><ref-titletext>Spectral Convolutional Neural Network for Music Classification</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85061058986</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.</ce:initials><ce:indexed-name>Lidy T.</ce:indexed-name><ce:surname>Lidy</ce:surname></author></ref-authors><ref-sourcetitle>Mirex 2015</ref-sourcetitle><ref-publicationyear first="2015"/><ref-text>Malaga, Spain</ref-text></ref-info><ref-fulltext>Lidy, T. Spectral Convolutional Neural Network for Music Classification. In Mirex 2015; Malaga, Spain, 2015</ref-fulltext></reference><reference id="9"><ref-info><ref-title><ref-titletext>Automatic speech/music discrimination for broadcast signals</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85061060038</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Kruspe A.</ce:indexed-name><ce:surname>Kruspe</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Zapf D.</ce:indexed-name><ce:surname>Zapf</ce:surname></author><author seq="3"><ce:initials>H.</ce:initials><ce:indexed-name>Lukashevich H.</ce:indexed-name><ce:surname>Lukashevich</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the INFORMATIK 2017;Chemnitz:Germany</ref-sourcetitle><ref-publicationyear first="2017"/><ref-volisspag><pagerange first="151" last="162"/></ref-volisspag><ref-text>25-29 September</ref-text></ref-info><ref-fulltext>Kruspe, A.; Zapf, D.; Lukashevich, H. Automatic speech/music discrimination for broadcast signals. In Proceedings of the INFORMATIK 2017, Chemnitz, Germany, 25-29 September 2017; pp. 151-162</ref-fulltext></reference><reference id="10"><ref-info><refd-itemidlist><itemid idtype="SGR">85061060974</itemid></refd-itemidlist><ref-authors><collaboration seq="1"><ce:indexed-name>Music and/or Speech Detection Results</ce:indexed-name><ce:text>Music and/or Speech Detection Results</ce:text></collaboration></ref-authors><ref-publicationyear first="2018"/><ref-website><ce:e-address type="email">https://www.music-ir.org/mirex/wiki/2018:Music_and_or_Speech_Detection_Results</ce:e-address></ref-website><ref-text>(accessed on 12 December 2018), Mirex</ref-text></ref-info><ref-fulltext>Music and/or Speech Detection Results, Mirex 2018. Available online: https://www.music-ir.org/mirex/ wiki/2018:Music_and_or_Speech_Detection_Results (accessed on 12 December 2018)</ref-fulltext></reference><reference id="11"><ref-info><ref-title><ref-titletext>CNN Architectures for Large-Scale Audio Classification</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85023741194</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Hershey S.</ce:indexed-name><ce:surname>Hershey</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Chaudhuri S.</ce:indexed-name><ce:surname>Chaudhuri</ce:surname></author><author seq="3"><ce:initials>D.P.W.</ce:initials><ce:indexed-name>Ellis D.P.W.</ce:indexed-name><ce:surname>Ellis</ce:surname></author><author seq="4"><ce:initials>J.F.</ce:initials><ce:indexed-name>Gemmeke J.F.</ce:indexed-name><ce:surname>Gemmeke</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Jansen A.</ce:indexed-name><ce:surname>Jansen</ce:surname></author><author seq="6"><ce:initials>C.</ce:initials><ce:indexed-name>Moore C.</ce:indexed-name><ce:surname>Moore</ce:surname></author><author seq="7"><ce:initials>M.</ce:initials><ce:indexed-name>Plakal M.</ce:indexed-name><ce:surname>Plakal</ce:surname></author><author seq="8"><ce:initials>D.</ce:initials><ce:indexed-name>Platt D.</ce:indexed-name><ce:surname>Platt</ce:surname></author><author seq="9"><ce:initials>R.A.</ce:initials><ce:indexed-name>Saurous R.A.</ce:indexed-name><ce:surname>Saurous</ce:surname></author><author seq="10"><ce:initials>B.</ce:initials><ce:indexed-name>Seybold B.</ce:indexed-name><ce:surname>Seybold</ce:surname></author><et-al/></ref-authors><ref-sourcetitle>Proceedings of the 2017 IEEE International Conference on Acoustics</ref-sourcetitle><ref-publicationyear first="2017"/><ref-text>Speech and Signal Processing (ICASSP), New Orleans, LA, USA, 5-9 March</ref-text></ref-info><ref-fulltext>Hershey, S.; Chaudhuri, S.; Ellis, D.P.W.; Gemmeke, J.F.; Jansen, A.; Moore, C.; Plakal, M.; Platt, D.; Saurous, R.A.; Seybold, B.; et al. CNN Architectures for Large-Scale Audio Classification. In Proceedings of the 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), New Orleans, LA, USA, 5-9 March 2017</ref-fulltext></reference><reference id="12"><ref-info><ref-title><ref-titletext>Audio Set: An ontology and human-labeled dataset for audio events</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85023749902</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.F.</ce:initials><ce:indexed-name>Gemmeke J.F.</ce:indexed-name><ce:surname>Gemmeke</ce:surname></author><author seq="2"><ce:initials>D.P.W.</ce:initials><ce:indexed-name>Ellis D.P.W.</ce:indexed-name><ce:surname>Ellis</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Freedman D.</ce:indexed-name><ce:surname>Freedman</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Jansen A.</ce:indexed-name><ce:surname>Jansen</ce:surname></author><author seq="5"><ce:initials>W.</ce:initials><ce:indexed-name>Lawrence W.</ce:indexed-name><ce:surname>Lawrence</ce:surname></author><author seq="6"><ce:initials>R.C.</ce:initials><ce:indexed-name>Moore R.C.</ce:indexed-name><ce:surname>Moore</ce:surname></author><author seq="7"><ce:initials>M.</ce:initials><ce:indexed-name>Plakal M.</ce:indexed-name><ce:surname>Plakal</ce:surname></author><author seq="8"><ce:initials>M.</ce:initials><ce:indexed-name>Ritter M.</ce:indexed-name><ce:surname>Ritter</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 2017 IEEE International Conference on Acoustics</ref-sourcetitle><ref-publicationyear first="2017"/><ref-text>Speech and Signal Processing (ICASSP), New Orleans, LA, USA, 5-9 March</ref-text></ref-info><ref-fulltext>Gemmeke, J.F.; Ellis, D.P.W.; Freedman, D.; Jansen, A.; Lawrence, W.; Moore, R.C.; Plakal, M.; Ritter, M. Audio Set: An ontology and human-labeled dataset for audio events. In Proceedings of the 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), New Orleans, LA, USA, 5-9 March 2017</ref-fulltext></reference><reference id="13"><ref-info><refd-itemidlist><itemid idtype="SGR">85061047558</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>G.</ce:initials><ce:indexed-name>Tzanetakis G.</ce:indexed-name><ce:surname>Tzanetakis</ce:surname></author></ref-authors><ref-sourcetitle>GTZAN Music/Speech Collection</ref-sourcetitle><ref-website><ce:e-address type="email">http://marsyasweb.appspot.com/download/data_sets/</ce:e-address></ref-website><ref-text>(accessed on 12 December 2018)</ref-text></ref-info><ref-fulltext>Tzanetakis, G. GTZAN Music/Speech Collection. Available online: http://marsyasweb.appspot.com/ download/data_sets/ (accessed on 12 December 2018)</ref-fulltext></reference><reference id="14"><ref-info><refd-itemidlist><itemid idtype="ARXIV">1510.08484</itemid><itemid idtype="SGR">85023758526</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Snyder D.</ce:indexed-name><ce:surname>Snyder</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Chen G.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Povey D.</ce:indexed-name><ce:surname>Povey</ce:surname></author></ref-authors><ref-sourcetitle>Musan: A music, speech, and noise corpus</ref-sourcetitle><ref-publicationyear first="2015"/><ref-text>arXiv</ref-text></ref-info><ref-fulltext>Snyder, D.; Chen, G.; Povey, D. Musan: A music, speech, and noise corpus. arXiv 2015, arXiv:1510.08484</ref-fulltext></reference><reference id="15"><ref-info><ref-title><ref-titletext>Seeing Sound: Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85061277980</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Cartwright M.</ce:indexed-name><ce:surname>Cartwright</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Seals A.</ce:indexed-name><ce:surname>Seals</ce:surname></author><author seq="3"><ce:initials>J.</ce:initials><ce:indexed-name>Salamon J.</ce:indexed-name><ce:surname>Salamon</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Williams A.</ce:indexed-name><ce:surname>Williams</ce:surname></author><author seq="5"><ce:initials>S.</ce:initials><ce:indexed-name>Mikloska S.</ce:indexed-name><ce:surname>Mikloska</ce:surname></author><author seq="6"><ce:initials>D.</ce:initials><ce:indexed-name>MacConnell D.</ce:indexed-name><ce:surname>MacConnell</ce:surname></author><author seq="7"><ce:initials>E.</ce:initials><ce:indexed-name>Law E.</ce:indexed-name><ce:surname>Law</ce:surname></author><author seq="8"><ce:initials>J.P.</ce:initials><ce:indexed-name>Bello J.P.</ce:indexed-name><ce:surname>Bello</ce:surname></author><author seq="9"><ce:initials>O.</ce:initials><ce:indexed-name>Nov O.</ce:indexed-name><ce:surname>Nov</ce:surname></author></ref-authors><ref-sourcetitle>Proc. ACM Hum.-Comput. Interact</ref-sourcetitle><ref-publicationyear first="2017"/><ref-volisspag><voliss volume="1"/><pagerange first="1" last="21"/></ref-volisspag></ref-info><ref-fulltext>Cartwright, M.; Seals, A.; Salamon, J.; Williams, A.; Mikloska, S.; MacConnell, D.; Law, E.; Bello, J.P.; Nov, O. Seeing Sound: Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations. Proc. ACM Hum.-Comput. Interact. 2017, 1, 1-21</ref-fulltext></reference><reference id="16"><ref-info><ref-title><ref-titletext>Going Deep with Segmentation of Field Recordings</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85061052658</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 8th International Workshop on Folk Music Analysis</ref-sourcetitle><ref-publicationyear first="2018"/><ref-text>Thessaloniki, Greece, 26-29 June</ref-text></ref-info><ref-fulltext>Marolt, M. Going Deep with Segmentation of Field Recordings. In Proceedings of the 8th International Workshop on Folk Music Analysis, Thessaloniki, Greece, 26-29 June 2018</ref-fulltext></reference><reference id="17"><ref-info><refd-itemidlist><itemid idtype="ARXIV">1512.03385</itemid><itemid idtype="SGR">84958589374</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>K.</ce:initials><ce:indexed-name>He K.</ce:indexed-name><ce:surname>He</ce:surname></author><author seq="2"><ce:initials>X.</ce:initials><ce:indexed-name>Zhang X.</ce:indexed-name><ce:surname>Zhang</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Ren S.</ce:indexed-name><ce:surname>Ren</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Sun J.</ce:indexed-name><ce:surname>Sun</ce:surname></author></ref-authors><ref-sourcetitle>Deep Residual Learning for Image Recognition</ref-sourcetitle><ref-publicationyear first="2015"/><ref-text>arXiv</ref-text></ref-info><ref-fulltext>He, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. arXiv 2015, arXiv:1512.03385</ref-fulltext></reference><reference id="18"><ref-info><ref-title><ref-titletext>Experimenting with musically motivated convolutional neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">84978864793</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Pons J.</ce:indexed-name><ce:surname>Pons</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Lidy T.</ce:indexed-name><ce:surname>Lidy</ce:surname></author><author seq="3"><ce:initials>X.</ce:initials><ce:indexed-name>Serra X.</ce:indexed-name><ce:surname>Serra</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 14th International Workshop on Content-Based Multimedia Indexing</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="1" last="6"/></ref-volisspag><ref-text>Bucharest, Romania, 15-17 June</ref-text></ref-info><ref-fulltext>Pons, J.; Lidy, T.; Serra, X. Experimenting with musically motivated convolutional neural networks. In Proceedings of the 14th International Workshop on Content-Based Multimedia Indexing, Bucharest, Romania, 15-17 June 2016; pp. 1-6</ref-fulltext></reference><reference id="19"><ref-info><ref-title><ref-titletext>High-level music descriptor extraction algorithm based on combination of multi-channel cnns and lstm</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85061061065</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>N.</ce:initials><ce:indexed-name>Chen N.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Wang S.</ce:indexed-name><ce:surname>Wang</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 18th International Society for Music Information Retrieval Conference</ref-sourcetitle><ref-publicationyear first="2017"/><ref-volisspag><pagerange first="509" last="514"/></ref-volisspag><ref-text>Suzhou, China, 23-27 October</ref-text></ref-info><ref-fulltext>Chen, N.; Wang, S. High-level music descriptor extraction algorithm based on combination of multi-channel cnns and lstm. In Proceedings of the 18th International Society for Music Information Retrieval Conference, Suzhou, China, 23-27 October 2017; pp. 509-514</ref-fulltext></reference><reference id="20"><ref-info><ref-title><ref-titletext>Zero-mean convolutions for level-invariant singing voice detection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="SGR">85057115155</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Schluter J.</ce:indexed-name><ce:surname>Schlüter</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Lehner B.</ce:indexed-name><ce:surname>Lehner</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the 19th International Society for Music Information Retrieval Conference</ref-sourcetitle><ref-publicationyear first="2018"/><ref-volisspag><pagerange first="1" last="6"/></ref-volisspag><ref-text>Paris, France, 23-27 September</ref-text></ref-info><ref-fulltext>Schlüter, J.; Lehner, B. Zero-mean convolutions for level-invariant singing voice detection. In Proceedings of the 19th International Society for Music Information Retrieval Conference, Paris, France, 23-27 September 2018; pp. 1-6</ref-fulltext></reference><reference id="21"><ref-info><ref-title><ref-titletext>Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)</ref-titletext></ref-title><refd-itemidlist><itemid idtype="ARXIV">1511.07289</itemid><itemid idtype="SGR">85029355144</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.-A.</ce:initials><ce:indexed-name>Clevert D.-A.</ce:indexed-name><ce:surname>Clevert</ce:surname></author><author seq="2"><ce:initials>T.</ce:initials><ce:indexed-name>Unterthiner T.</ce:indexed-name><ce:surname>Unterthiner</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Hochreiter S.</ce:indexed-name><ce:surname>Hochreiter</ce:surname></author></ref-authors><ref-sourcetitle>CoRR</ref-sourcetitle><ref-publicationyear first="2015"/></ref-info><ref-fulltext>Clevert, D.-A.; Unterthiner, T.; Hochreiter, S. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). CoRR 2015, arXiv:1511.07289</ref-fulltext></reference><reference id="22"><ref-info><ref-title><ref-titletext>Unsupervised Learning of Semantic Audio Representations</ref-titletext></ref-title><refd-itemidlist><itemid idtype="ARXIV">1711.02209</itemid><itemid idtype="SGR">85061027943</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Jansen A.</ce:indexed-name><ce:surname>Jansen</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Plakal M.</ce:indexed-name><ce:surname>Plakal</ce:surname></author><author seq="3"><ce:initials>R.</ce:initials><ce:indexed-name>Pandya R.</ce:indexed-name><ce:surname>Pandya</ce:surname></author><author seq="4"><ce:initials>D.P.W.</ce:initials><ce:indexed-name>Ellis D.P.W.</ce:indexed-name><ce:surname>Ellis</ce:surname></author><author seq="5"><ce:initials>S.</ce:initials><ce:indexed-name>Hershey S.</ce:indexed-name><ce:surname>Hershey</ce:surname></author><author seq="6"><ce:initials>J.</ce:initials><ce:indexed-name>Liu J.</ce:indexed-name><ce:surname>Liu</ce:surname></author><author seq="7"><ce:initials>R.C.</ce:initials><ce:indexed-name>Moore R.C.</ce:indexed-name><ce:surname>Moore</ce:surname></author><author seq="8"><ce:initials>R.A.</ce:initials><ce:indexed-name>Saurous R.A.</ce:indexed-name><ce:surname>Saurous</ce:surname></author></ref-authors><ref-sourcetitle>CoRR</ref-sourcetitle><ref-publicationyear first="2017"/></ref-info><ref-fulltext>Jansen, A.; Plakal, M.; Pandya, R.; Ellis, D.P.W.; Hershey, S.; Liu, J.; Moore, R.C.; Saurous, R.A. Unsupervised Learning of Semantic Audio Representations. CoRR 2017, arXiv:1711.02209</ref-fulltext></reference><reference id="23"><ref-info><refd-itemidlist><itemid idtype="SGR">85038886638</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Marolt M.</ce:indexed-name><ce:surname>Marolt</ce:surname></author></ref-authors><ref-sourcetitle>Music/speech classification and detection submission for MIREX 2015</ref-sourcetitle><ref-publicationyear first="2015"/><ref-text>In Mirex 2015;</ref-text></ref-info><ref-fulltext>Marolt, M. Music/speech classification and detection submission for MIREX 2015. In Mirex 2015; 2015</ref-fulltext></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>