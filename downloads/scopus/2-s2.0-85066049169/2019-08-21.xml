<abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85066049169</prism:url><dc:identifier>SCOPUS_ID:85066049169</dc:identifier><eid>2-s2.0-85066049169</eid><prism:doi>10.1007/s10845-019-01476-x</prism:doi><dc:title>Segmentation-based deep-learning approach for surface-defect detection</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>Journal of Intelligent Manufacturing</prism:publicationName><dc:publisher>Springer New York LLCbarbara.b.bertram@gsk.com</dc:publisher><source-id>24363</source-id><prism:issn>15728145 09565515</prism:issn><prism:coverDate>2019-01-01</prism:coverDate><openaccess>0</openaccess><openaccessFlag>false</openaccessFlag><dc:creator><author seq="1" auid="55613308000"><ce:initials>D.</ce:initials><ce:indexed-name>Tabernik D.</ce:indexed-name><ce:surname>Tabernik</ce:surname><ce:given-name>Domen</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Tabernik D.</ce:indexed-name><ce:surname>Tabernik</ce:surname><ce:given-name>Domen</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/55613308000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><publishercopyright>© 2019, Springer Science+Business Media, LLC, part of Springer Nature.</publishercopyright><ce:para>Automated surface-anomaly detection using machine learning has become an interesting and promising area of research, with a very high and direct impact on the application domain of visual inspection. Deep-learning methods have become the most suitable approaches for this task. They allow the inspection system to learn to detect the surface anomaly by simply showing it a number of exemplar images. This paper presents a segmentation-based deep-learning architecture that is designed for the detection and segmentation of surface anomalies and is demonstrated on a specific domain of surface-crack detection. The design of the architecture enables the model to be trained using a small number of samples, which is an important requirement for practical applications. The proposed model is compared with the related deep-learning methods, including the state-of-the-art commercial software, showing that the proposed approach outperforms the related methods on the specific domain of surface-crack detection. The large number of experiments also shed light on the required precision of the annotation, the number of required training samples and on the required computational cost. Experiments are performed on a newly created dataset based on a real-world quality control case and demonstrates that the proposed approach is able to learn on a small number of defected surfaces, using only approximately 25–30 defective training samples, instead of hundreds or thousands, which is usually the case in deep-learning applications. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. The dataset is also made publicly available to encourage the development and evaluation of new methods for surface-defect detection.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85066049169" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85066049169&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85066049169&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="100719825" href="https://api.elsevier.com/content/affiliation/affiliation_id/100719825"><affilname>Kolektor Group d.o.o.</affilname><affiliation-city>Idrija</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"><affilname>University of Ljubljana</affilname><affiliation-city>Ljubljana</affiliation-city><affiliation-country>Slovenia</affiliation-country></affiliation><authors><author seq="1" auid="55613308000"><ce:initials>D.</ce:initials><ce:indexed-name>Tabernik D.</ce:indexed-name><ce:surname>Tabernik</ce:surname><ce:given-name>Domen</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Tabernik D.</ce:indexed-name><ce:surname>Tabernik</ce:surname><ce:given-name>Domen</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/55613308000</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author><author seq="2" auid="57208885039"><ce:initials>S.</ce:initials><ce:indexed-name>Sela S.</ce:indexed-name><ce:surname>Šela</ce:surname><ce:given-name>Samo</ce:given-name><preferred-name><ce:initials>S.</ce:initials><ce:indexed-name>Šela S.</ce:indexed-name><ce:surname>Šela</ce:surname><ce:given-name>Samo</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57208885039</author-url><affiliation id="100719825" href="https://api.elsevier.com/content/affiliation/affiliation_id/100719825"/></author><author seq="3" auid="57208959712"><ce:initials>J.</ce:initials><ce:indexed-name>Skvarc J.</ce:indexed-name><ce:surname>Skvarč</ce:surname><ce:given-name>Jure</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Skvarč J.</ce:indexed-name><ce:surname>Skvarč</ce:surname><ce:given-name>Jure</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57208959712</author-url><affiliation id="100719825" href="https://api.elsevier.com/content/affiliation/affiliation_id/100719825"/></author><author seq="4" auid="6508184644"><ce:initials>D.</ce:initials><ce:indexed-name>Skocaj D.</ce:indexed-name><ce:surname>Skočaj</ce:surname><ce:given-name>Danijel</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Skočaj D.</ce:indexed-name><ce:surname>Skočaj</ce:surname><ce:given-name>Danijel</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/6508184644</author-url><affiliation id="60031106" href="https://api.elsevier.com/content/affiliation/affiliation_id/60031106"/></author></authors><language xml:lang="eng"/><authkeywords><author-keyword>Computer vision</author-keyword><author-keyword>Deep learning</author-keyword><author-keyword>Industry 4.0</author-keyword><author-keyword>Quality control</author-keyword><author-keyword>Segmentation networks</author-keyword><author-keyword>Surface-defect detection</author-keyword><author-keyword>Visual inspection</author-keyword></authkeywords><idxterms><mainterm weight="b" candidate="n">Commercial software</mainterm><mainterm weight="b" candidate="n">Computational costs</mainterm><mainterm weight="b" candidate="n">Inspection system</mainterm><mainterm weight="b" candidate="n">Learning architectures</mainterm><mainterm weight="b" candidate="n">Required precision</mainterm><mainterm weight="b" candidate="n">Surface crack detection</mainterm><mainterm weight="b" candidate="n">Surface defect detections</mainterm><mainterm weight="b" candidate="n">Visual inspection</mainterm></idxterms><subject-areas><subject-area code="1712" abbrev="COMP">Software</subject-area><subject-area code="2209" abbrev="ENGI">Industrial and Manufacturing Engineering</subject-area><subject-area code="1702" abbrev="COMP">Artificial Intelligence</subject-area></subject-areas><item xmlns=""><xocs:meta><xocs:funding-list has-funding-info="1" pui-match="primary"><xocs:funding-addon-generated-timestamp>2019-05-28T21:32:32.173Z</xocs:funding-addon-generated-timestamp><xocs:funding-addon-type>http://vtw.elsevier.com/data/voc/AddOnTypes/50.7/car</xocs:funding-addon-type></xocs:funding-list></xocs:meta><ait:process-info><ait:date-delivered year="2019" month="06" day="02" timestamp="2019-06-02T08:58:09.000009-04:00"/><ait:date-sort year="2019" month="01" day="01"/><ait:status type="core" state="update" stage="S200"/></ait:process-info><bibrecord><item-info><copyright type="Elsevier">Copyright 2019 Elsevier B.V., All rights reserved.</copyright><itemidlist><ce:doi>10.1007/s10845-019-01476-x</ce:doi><itemid idtype="PUI">627809356</itemid><itemid idtype="CAR-ID">917640853</itemid><itemid idtype="CPX">20192206971890</itemid><itemid idtype="REAXYSCAR">20190977381</itemid><itemid idtype="SCOPUS">20191479780</itemid><itemid idtype="SCOPUS">20191479781</itemid><itemid idtype="SCP">85066049169</itemid><itemid idtype="SGR">85066049169</itemid></itemidlist><history><date-created year="2019" month="05" day="28" timestamp="BST 04:21:34"/></history><dbcollection>CPX</dbcollection><dbcollection>REAXYSCAR</dbcollection><dbcollection>SCOPUS</dbcollection><dbcollection>SCOPUS</dbcollection><dbcollection>Scopusbase</dbcollection></item-info><head><citation-info><citation-type code="ar"/><citation-language xml:lang="eng" language="English"/><abstract-language xml:lang="eng" language="English"/><author-keywords><author-keyword xml:lang="eng">Computer vision</author-keyword><author-keyword xml:lang="eng">Deep learning</author-keyword><author-keyword xml:lang="eng">Industry 4.0</author-keyword><author-keyword xml:lang="eng">Quality control</author-keyword><author-keyword xml:lang="eng">Segmentation networks</author-keyword><author-keyword xml:lang="eng">Surface-defect detection</author-keyword><author-keyword xml:lang="eng">Visual inspection</author-keyword></author-keywords></citation-info><citation-title><titletext xml:lang="eng" original="y" language="English">Segmentation-based deep-learning approach for surface-defect detection</titletext></citation-title><author-group><author auid="55613308000" seq="1" type="auth" orcid="0000-0002-5613-5882"><ce:initials>D.</ce:initials><ce:indexed-name>Tabernik D.</ce:indexed-name><ce:surname>Tabernik</ce:surname><ce:given-name>Domen</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Tabernik D.</ce:indexed-name><ce:surname>Tabernik</ce:surname><ce:given-name>Domen</ce:given-name></preferred-name></author><author auid="6508184644" seq="4" type="auth"><ce:initials>D.</ce:initials><ce:indexed-name>Skocaj D.</ce:indexed-name><ce:surname>Skočaj</ce:surname><ce:given-name>Danijel</ce:given-name><preferred-name><ce:initials>D.</ce:initials><ce:indexed-name>Skočaj D.</ce:indexed-name><ce:surname>Skočaj</ce:surname><ce:given-name>Danijel</ce:given-name></preferred-name></author><affiliation afid="60031106" country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Večna pot 113</address-part><city>Ljubljana</city><postal-code>1000</postal-code><ce:source-text>Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, 1000, Ljubljana, Slovenia</ce:source-text><affiliation-id afid="60031106"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="57208885039" seq="2" type="auth"><ce:initials>S.</ce:initials><ce:indexed-name>Sela S.</ce:indexed-name><ce:surname>Šela</ce:surname><ce:given-name>Samo</ce:given-name><preferred-name><ce:initials>S.</ce:initials><ce:indexed-name>Šela S.</ce:indexed-name><ce:surname>Šela</ce:surname><ce:given-name>Samo</ce:given-name></preferred-name></author><affiliation afid="100719825" country="svn"><organization>Kolektor Group d. o. o.</organization><address-part>Vojkova 10</address-part><city>Idrija</city><postal-code>5280</postal-code><ce:source-text>Kolektor Group d. o. o., Vojkova 10, 5280, Idrija, Slovenia</ce:source-text><affiliation-id afid="100719825"/><country>Slovenia</country></affiliation></author-group><author-group><author auid="57208959712" seq="3" type="auth"><ce:initials>J.</ce:initials><ce:indexed-name>Skvarc J.</ce:indexed-name><ce:surname>Skvarč</ce:surname><ce:given-name>Jure</ce:given-name><preferred-name><ce:initials>J.</ce:initials><ce:indexed-name>Skvarč J.</ce:indexed-name><ce:surname>Skvarč</ce:surname><ce:given-name>Jure</ce:given-name></preferred-name></author><affiliation afid="100719825" country="svn"><organization>Kolektor Orodjarna d. o. o.</organization><address-part>Vojkova 10</address-part><city>Idrija</city><postal-code>5280</postal-code><ce:source-text>Kolektor Orodjarna d. o. o., Vojkova 10, 5280, Idrija, Slovenia</ce:source-text><affiliation-id afid="100719825"/><country>Slovenia</country></affiliation></author-group><correspondence><person><ce:initials>D.</ce:initials><ce:indexed-name>Tabernik D.</ce:indexed-name><ce:surname>Tabernik</ce:surname><ce:given-name>Domen</ce:given-name></person><affiliation country="svn"><organization>Faculty of Computer and Information Science</organization><organization>University of Ljubljana</organization><address-part>Večna pot 113</address-part><city>Ljubljana</city><postal-code>1000</postal-code><ce:source-text>Faculty of Computer and Information Science, University of Ljubljana, Večna pot 113, 1000, Ljubljana, Slovenia</ce:source-text><country>Slovenia</country></affiliation></correspondence><abstracts><abstract original="y" xml:lang="eng"><publishercopyright>© 2019, Springer Science+Business Media, LLC, part of Springer Nature.</publishercopyright><ce:para>Automated surface-anomaly detection using machine learning has become an interesting and promising area of research, with a very high and direct impact on the application domain of visual inspection. Deep-learning methods have become the most suitable approaches for this task. They allow the inspection system to learn to detect the surface anomaly by simply showing it a number of exemplar images. This paper presents a segmentation-based deep-learning architecture that is designed for the detection and segmentation of surface anomalies and is demonstrated on a specific domain of surface-crack detection. The design of the architecture enables the model to be trained using a small number of samples, which is an important requirement for practical applications. The proposed model is compared with the related deep-learning methods, including the state-of-the-art commercial software, showing that the proposed approach outperforms the related methods on the specific domain of surface-crack detection. The large number of experiments also shed light on the required precision of the annotation, the number of required training samples and on the required computational cost. Experiments are performed on a newly created dataset based on a real-world quality control case and demonstrates that the proposed approach is able to learn on a small number of defected surfaces, using only approximately 25–30 defective training samples, instead of hundreds or thousands, which is usually the case in deep-learning applications. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. The dataset is also made publicly available to encourage the development and evaluation of new methods for surface-defect detection.</ce:para></abstract></abstracts><source srcid="24363" type="j" country="usa"><sourcetitle>Journal of Intelligent Manufacturing</sourcetitle><sourcetitle-abbrev>J Intell Manuf</sourcetitle-abbrev><translated-sourcetitle xml:lang="eng">Journal of Intelligent Manufacturing</translated-sourcetitle><issn type="electronic">15728145</issn><issn type="print">09565515</issn><codencode>JIMNE</codencode><publicationyear first="2019"/><publicationdate><year>2019</year><date-text xfab-added="true">2019</date-text></publicationdate><website><ce:e-address type="email">www.kluweronline.com/issn/0956-5515/</ce:e-address></website><publisher><publishername>Springer New York LLC</publishername><ce:e-address type="email">barbara.b.bertram@gsk.com</ce:e-address></publisher></source><enhancement><classificationgroup><classifications type="CPXCLASS"><classification>                                                                  <classification-code>723.5</classification-code>                                                                  <classification-description>Computer Applications</classification-description>                                                              </classification><classification>                                                                  <classification-code>913.3</classification-code>                                                                  <classification-description>Quality Assurance and Control</classification-description>                                                              </classification><classification>                                                                  <classification-code>951</classification-code>                                                                  <classification-description>Materials Science</classification-description>                                                              </classification></classifications><classifications type="FLXCLASS"><classification>                                                                  <classification-code>902</classification-code>                                                                  <classification-description>FLUIDEX; Related Topics</classification-description>                                                              </classification></classifications><classifications type="ASJC"><classification>1712</classification><classification>2209</classification><classification>1702</classification></classifications><classifications type="SUBJABBR"><classification>COMP</classification><classification>ENGI</classification></classifications></classificationgroup></enhancement></head><tail><bibliography refcount="23"><reference id="1"><ref-info><refd-itemidlist><itemid idtype="FRAGMENTID">CR1</itemid><itemid idtype="SGR">84958264664</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>M.</ce:initials><ce:indexed-name>Abadi M.</ce:indexed-name><ce:surname>Abadi</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Agarwal A.</ce:indexed-name><ce:surname>Agarwal</ce:surname></author><author seq="3"><ce:initials>P.</ce:initials><ce:indexed-name>Barham P.</ce:indexed-name><ce:surname>Barham</ce:surname></author><author seq="4"><ce:initials>E.</ce:initials><ce:indexed-name>Brevdo E.</ce:indexed-name><ce:surname>Brevdo</ce:surname></author><author seq="5"><ce:initials>Z.</ce:initials><ce:indexed-name>Chen Z.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="6"><ce:initials>C.</ce:initials><ce:indexed-name>Citro C.</ce:indexed-name><ce:surname>Citro</ce:surname></author><et-al/></ref-authors><ref-sourcetitle>Tensorflow: Large-Scale Machine Learning on Heterogeneous Systems</ref-sourcetitle><ref-publicationyear first="2015"/><ref-website><ce:e-address type="email">https://www.tensorflow.org/</ce:e-address></ref-website></ref-info><ref-fulltext>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., et al. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. https://www.tensorflow.org/.</ref-fulltext><ce:source-text>Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., et al. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. https://www.tensorflow.org/.</ce:source-text></reference><reference id="2"><ref-info><ref-title><ref-titletext>An efficient method for defect detection during the manufacturing of web materials</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1007/s10845-014-0876-9</itemid><itemid idtype="FRAGMENTID">CR2</itemid><itemid idtype="SGR">84961153745</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.G.</ce:initials><ce:indexed-name>Bulnes F.G.</ce:indexed-name><ce:surname>Bulnes</ce:surname></author><author seq="2"><ce:initials>R.</ce:initials><ce:indexed-name>Usamentiaga R.</ce:indexed-name><ce:surname>Usamentiaga</ce:surname></author><author seq="3"><ce:initials>D.F.</ce:initials><ce:indexed-name>Garcia D.F.</ce:indexed-name><ce:surname>Garcia</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Molleda J.</ce:indexed-name><ce:surname>Molleda</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Manufacturing</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><voliss volume="27" issue="2"/><pagerange first="431" last="445"/></ref-volisspag></ref-info><ref-fulltext>Bulnes, F. G., Usamentiaga, R., Garcia, D. F., &amp; Molleda, J. (2016). An efficient method for defect detection during the manufacturing of web materials. Journal of Intelligent Manufacturing, 27(2), 431–445. 10.1007/s10845-014-0876-9.</ref-fulltext><ce:source-text>Bulnes, F. G., Usamentiaga, R., Garcia, D. F., &amp; Molleda, J. (2016). An efficient method for defect detection during the manufacturing of web materials. Journal of Intelligent Manufacturing, 27(2), 431–445. 10.1007/s10845-014-0876-9.</ce:source-text></reference><reference id="3"><ref-info><ref-title><ref-titletext>Is overfeat useful for image-based surface defect classification tasks?</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR3</itemid><itemid idtype="SGR">85006701479</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.H.</ce:initials><ce:indexed-name>Chen P.H.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="2"><ce:initials>S.S.</ce:initials><ce:indexed-name>Ho S.S.</ce:indexed-name><ce:surname>Ho</ce:surname></author></ref-authors><ref-sourcetitle>In IEEE International Conference on Image Processing (ICIP)</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="749" last="753"/></ref-volisspag></ref-info><ref-fulltext>Chen, P. H., &amp; Ho, S. S. (2016). Is overfeat useful for image-based surface defect classification tasks? In IEEE international conference on image processing (ICIP) (pp. 749–753).</ref-fulltext><ce:source-text>Chen, P. H., &amp; Ho, S. S. (2016). Is overfeat useful for image-based surface defect classification tasks? In IEEE international conference on image processing (ICIP) (pp. 749–753).</ce:source-text></reference><reference id="4"><ref-info><refd-itemidlist><itemid idtype="FRAGMENTID">CR4</itemid><itemid idtype="SGR">85048372713</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>L.C.</ce:initials><ce:indexed-name>Chen L.C.</ce:indexed-name><ce:surname>Chen</ce:surname></author><author seq="2"><ce:initials>Y.</ce:initials><ce:indexed-name>Zhu Y.</ce:indexed-name><ce:surname>Zhu</ce:surname></author><author seq="3"><ce:initials>G.</ce:initials><ce:indexed-name>Papandreou G.</ce:indexed-name><ce:surname>Papandreou</ce:surname></author><author seq="4"><ce:initials>F.</ce:initials><ce:indexed-name>Schroff F.</ce:indexed-name><ce:surname>Schroff</ce:surname></author><author seq="5"><ce:initials>H.</ce:initials><ce:indexed-name>Adam H.</ce:indexed-name><ce:surname>Adam</ce:surname></author></ref-authors><ref-sourcetitle>Encoder–Decoder with Atrous Separable Convolution for Semantic Image Segmentation</ref-sourcetitle><ref-publicationyear first="2018"/><ref-text>Tech. rep</ref-text></ref-info><ref-fulltext>Chen, L. C., Zhu, Y., Papandreou, G., Schroff, F., &amp; Adam, H. (2018). Encoder–Decoder with atrous separable convolution for semantic image segmentation. Tech. rep.</ref-fulltext><ce:source-text>Chen, L. C., Zhu, Y., Papandreou, G., Schroff, F., &amp; Adam, H. (2018). Encoder–Decoder with atrous separable convolution for semantic image segmentation. Tech. rep.</ce:source-text></reference><reference id="5"><ref-info><ref-title><ref-titletext>Xception: Deep learning with depthwise separable convolutions</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1109/CVPR.2017.195</itemid><itemid idtype="FRAGMENTID">CR5</itemid><itemid idtype="SGR">85040604274</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>F.</ce:initials><ce:indexed-name>Chollet F.</ce:indexed-name><ce:surname>Chollet</ce:surname></author></ref-authors><ref-sourcetitle>Computer Vision and Pattern Recognition</ref-sourcetitle><ref-publicationyear first="2017"/><ref-volisspag><voliss volume="2017"/><pagerange first="1800" last="1807"/></ref-volisspag></ref-info><ref-fulltext>Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. Computer Vision and Pattern Recognition, 2017, 1800–1807. 10.1109/CVPR.2017.195.</ref-fulltext><ce:source-text>Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. Computer Vision and Pattern Recognition, 2017, 1800–1807. 10.1109/CVPR.2017.195.</ce:source-text></reference><reference id="6"><ref-info><refd-itemidlist><itemid idtype="FRAGMENTID">CR6</itemid><itemid idtype="SGR">85066024990</itemid></refd-itemidlist><ref-authors><collaboration seq="1"><ce:indexed-name>Cognex</ce:indexed-name><ce:text>Cognex</ce:text></collaboration></ref-authors><ref-sourcetitle>VISIONPRO VIDI: Deep Learning-Based Software for Industrial Image Analysis</ref-sourcetitle><ref-publicationyear first="2018"/><ref-website><ce:e-address type="email">https://www.cognex.com/products/machine-vision/vision-software/visionpro-vidi</ce:e-address></ref-website></ref-info><ref-fulltext>Cognex. (2018). VISIONPRO VIDI: Deep learning-based software for industrial image analysis. https://www.cognex.com/products/machine-vision/vision-software/visionpro-vidi</ref-fulltext><ce:source-text>Cognex. (2018). VISIONPRO VIDI: Deep learning-based software for industrial image analysis. https://www.cognex.com/products/machine-vision/vision-software/visionpro-vidi</ce:source-text></reference><reference id="7"><ref-info><ref-title><ref-titletext>Deep convolutional neural networks for detection of rail surface defects deep convolutional neural networks for detection of rail surface defects</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR7</itemid><itemid idtype="SGR">85007275325</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>S.</ce:initials><ce:indexed-name>Faghih-Roohi S.</ce:indexed-name><ce:surname>Faghih-Roohi</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Hajizadeh S.</ce:indexed-name><ce:surname>Hajizadeh</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Nunez A.</ce:indexed-name><ce:surname>Núñez</ce:surname></author><author seq="4"><ce:initials>R.</ce:initials><ce:indexed-name>Babuska R.</ce:indexed-name><ce:surname>Babuska</ce:surname></author><author seq="5"><ce:initials>B.D.</ce:initials><ce:indexed-name>Schutter B.D.</ce:indexed-name><ce:surname>Schutter</ce:surname></author></ref-authors><ref-sourcetitle>In International Joint Conference on Neural Networks</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><pagerange first="2584" last="2589"/></ref-volisspag></ref-info><ref-fulltext>Faghih-Roohi, S., Hajizadeh, S., Núñez, A., Babuska, R., &amp; Schutter, B. D. (2016). Deep convolutional neural networks for detection of rail surface defects deep convolutional neural networks for detection of rail surface defects. In International joint conference on neural networks (pp. 2584–2589).</ref-fulltext><ce:source-text>Faghih-Roohi, S., Hajizadeh, S., Núñez, A., Babuska, R., &amp; Schutter, B. D. (2016). Deep convolutional neural networks for detection of rail surface defects deep convolutional neural networks for detection of rail surface defects. In International joint conference on neural networks (pp. 2584–2589).</ce:source-text></reference><reference id="8"><ref-info><ref-title><ref-titletext>Mask R-CNN</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR8</itemid><itemid idtype="SGR">85050819941</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Kaiming H.</ce:indexed-name><ce:surname>Kaiming</ce:surname></author><author seq="2"><ce:initials>G.</ce:initials><ce:indexed-name>Gkioxara G.</ce:indexed-name><ce:surname>Gkioxara</ce:surname></author><author seq="3"><ce:initials>P.</ce:initials><ce:indexed-name>Dollar P.</ce:indexed-name><ce:surname>Dollar</ce:surname></author><author seq="4"><ce:initials>R.</ce:initials><ce:indexed-name>Girshick R.</ce:indexed-name><ce:surname>Girshick</ce:surname></author></ref-authors><ref-sourcetitle>ICCV</ref-sourcetitle><ref-publicationyear first="2017"/><ref-volisspag><pagerange first="2961" last="2969"/></ref-volisspag></ref-info><ref-fulltext>Kaiming, H., Gkioxara, G., Dollar, P., &amp; Girshick, R. (2017). Mask R-CNN. In ICCV (pp. 2961–2969).</ref-fulltext><ce:source-text>Kaiming, H., Gkioxara, G., Dollar, P., &amp; Girshick, R. (2017). Mask R-CNN. In ICCV (pp. 2961–2969).</ce:source-text></reference><reference id="9"><ref-info><ref-title><ref-titletext>ImageNet classification with deep convolutional neural networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR9</itemid><itemid idtype="SGR">84876231242</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>A.</ce:initials><ce:indexed-name>Krizhevsky A.</ce:indexed-name><ce:surname>Krizhevsky</ce:surname></author><author seq="2"><ce:initials>I.</ce:initials><ce:indexed-name>Sutskever I.</ce:indexed-name><ce:surname>Sutskever</ce:surname></author><author seq="3"><ce:initials>G.E.</ce:initials><ce:indexed-name>Hinton G.E.</ce:indexed-name><ce:surname>Hinton</ce:surname></author></ref-authors><ref-sourcetitle>Advances in Neural Information Processing Systems</ref-sourcetitle><ref-publicationyear first="2012"/><ref-volisspag><voliss volume="25"/><pagerange first="1097" last="1105"/></ref-volisspag></ref-info><ref-fulltext>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems (Vol. 25, pp. 1097–1105).</ref-fulltext><ce:source-text>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. In Advances in neural information processing systems (Vol. 25, pp. 1097–1105).</ce:source-text></reference><reference id="10"><ref-info><ref-title><ref-titletext>Automatic inspection system of LED chip using two-stages back-propagation neural network</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1007/s10845-012-0725-7</itemid><itemid idtype="FRAGMENTID">CR10</itemid><itemid idtype="SGR">84912032946</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>C.F.J.</ce:initials><ce:indexed-name>Kuo C.F.J.</ce:indexed-name><ce:surname>Kuo</ce:surname></author><author seq="2"><ce:initials>C.T.M.</ce:initials><ce:indexed-name>Hsu C.T.M.</ce:indexed-name><ce:surname>Hsu</ce:surname></author><author seq="3"><ce:initials>Z.X.</ce:initials><ce:indexed-name>Liu Z.X.</ce:indexed-name><ce:surname>Liu</ce:surname></author><author seq="4"><ce:initials>H.C.</ce:initials><ce:indexed-name>Wu H.C.</ce:indexed-name><ce:surname>Wu</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Manufacturing</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><voliss volume="25" issue="6"/><pagerange first="1235" last="1243"/></ref-volisspag></ref-info><ref-fulltext>Kuo, C. F. J., Hsu, C. T. M., Liu, Z. X., &amp; Wu, H. C. (2014). Automatic inspection system of LED chip using two-stages back-propagation neural network. Journal of Intelligent Manufacturing, 25(6), 1235–1243. 10.1007/s10845-012-0725-7.</ref-fulltext><ce:source-text>Kuo, C. F. J., Hsu, C. T. M., Liu, Z. X., &amp; Wu, H. C. (2014). Automatic inspection system of LED chip using two-stages back-propagation neural network. Journal of Intelligent Manufacturing, 25(6), 1235–1243. 10.1007/s10845-012-0725-7.</ce:source-text></reference><reference id="11"><ref-info><ref-title><ref-titletext>Automated defect inspection of LED chip using deep convolutional neural network</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR11</itemid><itemid idtype="SGR">85044531545</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>H.</ce:initials><ce:indexed-name>Lin H.</ce:indexed-name><ce:surname>Lin</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Li B.</ce:indexed-name><ce:surname>Li</ce:surname></author><author seq="3"><ce:initials>X.</ce:initials><ce:indexed-name>Wang X.</ce:indexed-name><ce:surname>Wang</ce:surname></author><author seq="4"><ce:initials>Y.</ce:initials><ce:indexed-name>Shu Y.</ce:indexed-name><ce:surname>Shu</ce:surname></author><author seq="5"><ce:initials>S.</ce:initials><ce:indexed-name>Niu S.</ce:indexed-name><ce:surname>Niu</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Manufacturing</ref-sourcetitle><ref-publicationyear first="2018"/><ref-volisspag><pagerange first="1" last="10"/></ref-volisspag><ref-website><ce:e-address type="email">https://doi.org/10.1007/s10845-018-1415-x</ce:e-address></ref-website></ref-info><ref-fulltext>Lin, H., Li, B., Wang, X., Shu, Y., &amp; Niu, S. (2018). Automated defect inspection of LED chip using deep convolutional neural network. Journal of Intelligent Manufacturing, 1–10. 10.1007/s10845-018-1415-x.</ref-fulltext><ce:source-text>Lin, H., Li, B., Wang, X., Shu, Y., &amp; Niu, S. (2018). Automated defect inspection of LED chip using deep convolutional neural network. Journal of Intelligent Manufacturing, 1–10. 10.1007/s10845-018-1415-x.</ce:source-text></reference><reference id="12"><ref-info><ref-title><ref-titletext>Microsoft COCO: Common objects in context</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR12</itemid><itemid idtype="SGR">84906493406</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>T.Y.</ce:initials><ce:indexed-name>Lin T.Y.</ce:indexed-name><ce:surname>Lin</ce:surname></author><author seq="2"><ce:initials>M.</ce:initials><ce:indexed-name>Maire M.</ce:indexed-name><ce:surname>Maire</ce:surname></author><author seq="3"><ce:initials>S.</ce:initials><ce:indexed-name>Belongie S.</ce:indexed-name><ce:surname>Belongie</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Hays J.</ce:indexed-name><ce:surname>Hays</ce:surname></author><author seq="5"><ce:initials>P.</ce:initials><ce:indexed-name>Perona P.</ce:indexed-name><ce:surname>Perona</ce:surname></author><author seq="6"><ce:initials>D.</ce:initials><ce:indexed-name>Ramanan D.</ce:indexed-name><ce:surname>Ramanan</ce:surname></author><author seq="7"><ce:initials>P.</ce:initials><ce:indexed-name>Dollar P.</ce:indexed-name><ce:surname>Dollár</ce:surname></author><author seq="8"><ce:initials>C.L.</ce:initials><ce:indexed-name>Zitnick C.L.</ce:indexed-name><ce:surname>Zitnick</ce:surname></author></ref-authors><ref-sourcetitle>LNCS 8693 LNCS</ref-sourcetitle><ref-publicationyear first="2014"/><ref-volisspag><pagerange first="740" last="755"/></ref-volisspag></ref-info><ref-fulltext>Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., &amp; Zitnick, C. L. (2014). Microsoft COCO: Common objects in context. LNCS 8693 LNCS(PART 5):740–755.</ref-fulltext><ce:source-text>Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., &amp; Zitnick, C. L. (2014). Microsoft COCO: Common objects in context. LNCS 8693 LNCS(PART 5):740–755.</ce:source-text></reference><reference id="13"><ref-info><ref-title><ref-titletext>Fully convolutional networks for semantic segmentation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR13</itemid><itemid idtype="SGR">84959205572</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Long J.</ce:indexed-name><ce:surname>Long</ce:surname></author><author seq="2"><ce:initials>E.</ce:initials><ce:indexed-name>Shelhamer E.</ce:indexed-name><ce:surname>Shelhamer</ce:surname></author><author seq="3"><ce:initials>T.</ce:initials><ce:indexed-name>Darrell T.</ce:indexed-name><ce:surname>Darrell</ce:surname></author></ref-authors><ref-sourcetitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</ref-sourcetitle><ref-publicationyear first="2015"/><ref-volisspag><voliss volume="8828"/><pagerange first="3431" last="3440"/></ref-volisspag><ref-website><ce:e-address type="email">https://doi.org/10.1109/CVPR.2015.7298965</ce:e-address></ref-website></ref-info><ref-fulltext>Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (Vol. 8828, pp. 3431–3440). 10.1109/CVPR.2015.7298965.</ref-fulltext><ce:source-text>Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (Vol. 8828, pp. 3431–3440). 10.1109/CVPR.2015.7298965.</ce:source-text></reference><reference id="14"><ref-info><ref-title><ref-titletext>Steel defect classification with Max-Pooling Convolutional Neural Networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR14</itemid><itemid idtype="SGR">84865073494</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>J.</ce:initials><ce:indexed-name>Masci J.</ce:indexed-name><ce:surname>Masci</ce:surname></author><author seq="2"><ce:initials>U.</ce:initials><ce:indexed-name>Meier U.</ce:indexed-name><ce:surname>Meier</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Ciresan D.</ce:indexed-name><ce:surname>Ciresan</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Schmidhuber J.</ce:indexed-name><ce:surname>Schmidhuber</ce:surname></author><author seq="5"><ce:initials>G.</ce:initials><ce:indexed-name>Fricout G.</ce:indexed-name><ce:surname>Fricout</ce:surname></author></ref-authors><ref-sourcetitle>Proceedings of the International Joint Conference on Neural Networks</ref-sourcetitle><ref-publicationyear first="2012"/><ref-website><ce:e-address type="email">https://doi.org/10.1109/IJCNN.2012.6252468</ce:e-address></ref-website></ref-info><ref-fulltext>Masci, J., Meier, U., Ciresan, D., Schmidhuber, J., &amp; Fricout, G. (2012). Steel defect classification with Max-Pooling Convolutional Neural Networks. In Proceedings of the international joint conference on neural networks. 10.1109/IJCNN.2012.6252468.</ref-fulltext><ce:source-text>Masci, J., Meier, U., Ciresan, D., Schmidhuber, J., &amp; Fricout, G. (2012). Steel defect classification with Max-Pooling Convolutional Neural Networks. In Proceedings of the international joint conference on neural networks. 10.1109/IJCNN.2012.6252468.</ce:source-text></reference><reference id="15"><ref-info><ref-title><ref-titletext>Literature review of Industry 4.0 and related technologies</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1007/s10845-018-1433-8</itemid><itemid idtype="FRAGMENTID">CR15</itemid><itemid idtype="SGR">85050613632</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>E.</ce:initials><ce:indexed-name>Oztemel E.</ce:indexed-name><ce:surname>Oztemel</ce:surname></author><author seq="2"><ce:initials>S.</ce:initials><ce:indexed-name>Gursev S.</ce:indexed-name><ce:surname>Gursev</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Manufacturing</ref-sourcetitle><ref-publicationyear first="2018"/><ref-text>&amp;, (,).,.,., https://doi.org/10.1007/s10845-018-1433-8</ref-text></ref-info><ref-fulltext>Oztemel, E., &amp; Gursev, S. (2018). Literature review of Industry 4.0 and related technologies. Journal of Intelligent Manufacturing. 10.1007/s10845-018-1433-8.</ref-fulltext><ce:source-text>Oztemel, E., &amp; Gursev, S. (2018). Literature review of Industry 4.0 and related technologies. Journal of Intelligent Manufacturing. 10.1007/s10845-018-1433-8.</ce:source-text></reference><reference id="16"><ref-info><ref-title><ref-titletext>Improving the industrial classification of cork stoppers by using image processing and Neuro-Fuzzy computing</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1007/s10845-009-0251-4</itemid><itemid idtype="FRAGMENTID">CR16</itemid><itemid idtype="SGR">78650179211</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Paniagua B.</ce:indexed-name><ce:surname>Paniagua</ce:surname></author><author seq="2"><ce:initials>M.A.</ce:initials><ce:indexed-name>Vega-Rodriguez M.A.</ce:indexed-name><ce:surname>Vega-Rodríguez</ce:surname></author><author seq="3"><ce:initials>J.A.</ce:initials><ce:indexed-name>Gomez-Pulido J.A.</ce:indexed-name><ce:surname>Gomez-Pulido</ce:surname></author><author seq="4"><ce:initials>J.M.</ce:initials><ce:indexed-name>Sanchez-Perez J.M.</ce:indexed-name><ce:surname>Sanchez-Perez</ce:surname></author></ref-authors><ref-sourcetitle>Journal of Intelligent Manufacturing</ref-sourcetitle><ref-publicationyear first="2010"/><ref-volisspag><voliss volume="21" issue="6"/><pagerange first="745" last="760"/></ref-volisspag></ref-info><ref-fulltext>Paniagua, B., Vega-Rodríguez, M. A., Gomez-Pulido, J. A., &amp; Sanchez-Perez, J. M. (2010). Improving the industrial classification of cork stoppers by using image processing and Neuro-Fuzzy computing. Journal of Intelligent Manufacturing, 21(6), 745–760. 10.1007/s10845-009-0251-4.</ref-fulltext><ce:source-text>Paniagua, B., Vega-Rodríguez, M. A., Gomez-Pulido, J. A., &amp; Sanchez-Perez, J. M. (2010). Improving the industrial classification of cork stoppers by using image processing and Neuro-Fuzzy computing. Journal of Intelligent Manufacturing, 21(6), 745–760. 10.1007/s10845-009-0251-4.</ce:source-text></reference><reference id="17"><ref-info><ref-title><ref-titletext>A compact convolutional neural network for textured surface anomaly detection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR17</itemid><itemid idtype="SGR">85050962059</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Racki D.</ce:indexed-name><ce:surname>Rački</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Tomazevic D.</ce:indexed-name><ce:surname>Tomaževič</ce:surname></author><author seq="3"><ce:initials>D.</ce:initials><ce:indexed-name>Skocaj D.</ce:indexed-name><ce:surname>Skočaj</ce:surname></author></ref-authors><ref-sourcetitle>IEEE Winter Conference on Applications of Computer Vision</ref-sourcetitle><ref-publicationyear first="2018"/><ref-volisspag><pagerange first="1331" last="1339"/></ref-volisspag><ref-website><ce:e-address type="email">https://doi.org/10.1109/WACV.2018.00150</ce:e-address></ref-website></ref-info><ref-fulltext>Rački, D., Tomaževič, D., &amp; Skočaj, D. (2018). A compact convolutional neural network for textured surface anomaly detection. In IEEE winter conference on applications of computer vision (pp. 1331–1339). 10.1109/WACV.2018.00150.</ref-fulltext><ce:source-text>Rački, D., Tomaževič, D., &amp; Skočaj, D. (2018). A compact convolutional neural network for textured surface anomaly detection. In IEEE winter conference on applications of computer vision (pp. 1331–1339). 10.1109/WACV.2018.00150.</ce:source-text></reference><reference id="18"><ref-info><ref-title><ref-titletext>U-Net: Convolutional networks for biomedical image segmentation</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR18</itemid><itemid idtype="SGR">84951834022</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Ronneberger O.</ce:indexed-name><ce:surname>Ronneberger</ce:surname></author><author seq="2"><ce:initials>P.</ce:initials><ce:indexed-name>Fischer P.</ce:indexed-name><ce:surname>Fischer</ce:surname></author><author seq="3"><ce:initials>T.</ce:initials><ce:indexed-name>Brox T.</ce:indexed-name><ce:surname>Brox</ce:surname></author></ref-authors><ref-sourcetitle>In Medical Image Computing and Computer-Assisted intervention—MICCAI 2015</ref-sourcetitle><ref-publicationyear first="2015"/><ref-volisspag><pagerange first="234" last="241"/></ref-volisspag></ref-info><ref-fulltext>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention—MICCAI 2015 (pp. 234–241).</ref-fulltext><ce:source-text>Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-Net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention—MICCAI 2015 (pp. 234–241).</ce:source-text></reference><reference id="19"><ref-info><ref-title><ref-titletext>ImageNet large scale visual recognition challenge</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1007/s11263-015-0816-y</itemid><itemid idtype="FRAGMENTID">CR19</itemid><itemid idtype="SGR">84947041871</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>O.</ce:initials><ce:indexed-name>Russakovsky O.</ce:indexed-name><ce:surname>Russakovsky</ce:surname></author><author seq="2"><ce:initials>J.</ce:initials><ce:indexed-name>Deng J.</ce:indexed-name><ce:surname>Deng</ce:surname></author><author seq="3"><ce:initials>H.</ce:initials><ce:indexed-name>Su H.</ce:indexed-name><ce:surname>Su</ce:surname></author><author seq="4"><ce:initials>J.</ce:initials><ce:indexed-name>Krause J.</ce:indexed-name><ce:surname>Krause</ce:surname></author><author seq="5"><ce:initials>S.</ce:initials><ce:indexed-name>Satheesh S.</ce:indexed-name><ce:surname>Satheesh</ce:surname></author><author seq="6"><ce:initials>S.</ce:initials><ce:indexed-name>Ma S.</ce:indexed-name><ce:surname>Ma</ce:surname></author><author seq="7"><ce:initials>Z.</ce:initials><ce:indexed-name>Huang Z.</ce:indexed-name><ce:surname>Huang</ce:surname></author><author seq="8"><ce:initials>A.</ce:initials><ce:indexed-name>Karpathy A.</ce:indexed-name><ce:surname>Karpathy</ce:surname></author><author seq="9"><ce:initials>A.</ce:initials><ce:indexed-name>Khosla A.</ce:indexed-name><ce:surname>Khosla</ce:surname></author><author seq="10"><ce:initials>M.</ce:initials><ce:indexed-name>Bernstein M.</ce:indexed-name><ce:surname>Bernstein</ce:surname></author><author seq="11"><ce:initials>A.C.</ce:initials><ce:indexed-name>Berg A.C.</ce:indexed-name><ce:surname>Berg</ce:surname></author><author seq="12"><ce:initials>L.</ce:initials><ce:indexed-name>Fei-Fei L.</ce:indexed-name><ce:surname>Fei-Fei</ce:surname></author></ref-authors><ref-sourcetitle>International Journal of Computer Vision</ref-sourcetitle><ref-publicationyear first="2015"/><ref-volisspag><voliss volume="115" issue="3"/><pagerange first="211" last="252"/></ref-volisspag></ref-info><ref-fulltext>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015). ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3), 211–252. 10.1007/s11263-015-0816-y.</ref-fulltext><ce:source-text>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015). ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3), 211–252. 10.1007/s11263-015-0816-y.</ce:source-text></reference><reference id="20"><ref-info><ref-title><ref-titletext>OverFeat: Integrated recognition, localization and detection using convolutional networks</ref-titletext></ref-title><refd-itemidlist><itemid idtype="FRAGMENTID">CR20</itemid><itemid idtype="SGR">84906347546</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>P.</ce:initials><ce:indexed-name>Sermanet P.</ce:indexed-name><ce:surname>Sermanet</ce:surname></author><author seq="2"><ce:initials>D.</ce:initials><ce:indexed-name>Eigen D.</ce:indexed-name><ce:surname>Eigen</ce:surname></author></ref-authors><ref-sourcetitle>International Conference on Learning Representations (ICLR2014), CBLS</ref-sourcetitle><ref-publicationyear first="2014"/></ref-info><ref-fulltext>Sermanet, P., &amp; Eigen, D. (2014). OverFeat: Integrated recognition, localization and detection using convolutional networks. In International conference on learning representations (ICLR2014), CBLS.</ref-fulltext><ce:source-text>Sermanet, P., &amp; Eigen, D. (2014). OverFeat: Integrated recognition, localization and detection using convolutional networks. In International conference on learning representations (ICLR2014), CBLS.</ce:source-text></reference><reference id="21"><ref-info><ref-title><ref-titletext>Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1016/j.cirp.2016.04.072</itemid><itemid idtype="FRAGMENTID">CR21</itemid><itemid idtype="SGR">84975136179</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Weimer D.</ce:indexed-name><ce:surname>Weimer</ce:surname></author><author seq="2"><ce:initials>B.</ce:initials><ce:indexed-name>Scholz-Reiter B.</ce:indexed-name><ce:surname>Scholz-Reiter</ce:surname></author><author seq="3"><ce:initials>M.</ce:initials><ce:indexed-name>Shpitalni M.</ce:indexed-name><ce:surname>Shpitalni</ce:surname></author></ref-authors><ref-sourcetitle>CIRP Annals-Manufacturing Technology</ref-sourcetitle><ref-publicationyear first="2016"/><ref-volisspag><voliss volume="65" issue="1"/><pagerange first="417" last="420"/></ref-volisspag></ref-info><ref-fulltext>Weimer, D., Scholz-Reiter, B., &amp; Shpitalni, M. (2016). Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection. CIRP Annals-Manufacturing Technology, 65(1), 417–420. 10.1016/j.cirp.2016.04.072.</ref-fulltext><ce:source-text>Weimer, D., Scholz-Reiter, B., &amp; Shpitalni, M. (2016). Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection. CIRP Annals-Manufacturing Technology, 65(1), 417–420. 10.1016/j.cirp.2016.04.072.</ce:source-text></reference><reference id="22"><ref-info><ref-title><ref-titletext>Learning defect classifiers for textured surfaces using neural networks and statistical feature representations</ref-titletext></ref-title><refd-itemidlist><itemid idtype="DOI">10.1016/j.procir.2013.05.059</itemid><itemid idtype="FRAGMENTID">CR22</itemid><itemid idtype="SGR">84883877802</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>D.</ce:initials><ce:indexed-name>Weimer D.</ce:indexed-name><ce:surname>Weimer</ce:surname></author><author seq="2"><ce:initials>H.</ce:initials><ce:indexed-name>Thamer H.</ce:indexed-name><ce:surname>Thamer</ce:surname></author><author seq="3"><ce:initials>B.</ce:initials><ce:indexed-name>Scholz-Reiter B.</ce:indexed-name><ce:surname>Scholz-Reiter</ce:surname></author></ref-authors><ref-sourcetitle>Procedia CIRP</ref-sourcetitle><ref-publicationyear first="2013"/><ref-volisspag><voliss volume="7"/><pagerange first="347" last="352"/></ref-volisspag></ref-info><ref-fulltext>Weimer, D., Thamer, H., &amp; Scholz-Reiter, B. (2013). Learning defect classifiers for textured surfaces using neural networks and statistical feature representations. Procedia CIRP, 7, 347–352. 10.1016/j.procir.2013.05.059.</ref-fulltext><ce:source-text>Weimer, D., Thamer, H., &amp; Scholz-Reiter, B. (2013). Learning defect classifiers for textured surfaces using neural networks and statistical feature representations. Procedia CIRP, 7, 347–352. 10.1016/j.procir.2013.05.059.</ce:source-text></reference><reference id="23"><ref-info><refd-itemidlist><itemid idtype="FRAGMENTID">CR23</itemid><itemid idtype="SGR">84990054197</itemid></refd-itemidlist><ref-authors><author seq="1"><ce:initials>B.</ce:initials><ce:indexed-name>Zhou B.</ce:indexed-name><ce:surname>Zhou</ce:surname></author><author seq="2"><ce:initials>A.</ce:initials><ce:indexed-name>Khosla A.</ce:indexed-name><ce:surname>Khosla</ce:surname></author><author seq="3"><ce:initials>A.</ce:initials><ce:indexed-name>Lapedriza A.</ce:indexed-name><ce:surname>Lapedriza</ce:surname></author><author seq="4"><ce:initials>A.</ce:initials><ce:indexed-name>Oliva A.</ce:indexed-name><ce:surname>Oliva</ce:surname></author><author seq="5"><ce:initials>A.</ce:initials><ce:indexed-name>Torralba A.</ce:indexed-name><ce:surname>Torralba</ce:surname></author></ref-authors><ref-sourcetitle>Learning Deep Features for Discriminative Localization</ref-sourcetitle><ref-publicationyear first="2016"/><ref-text>In Computer vision and pattern recognition</ref-text></ref-info><ref-fulltext>Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp; Torralba, A. (2016). Learning deep features for discriminative localization. In Computer vision and pattern recognition.</ref-fulltext><ce:source-text>Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., &amp; Torralba, A. (2016). Learning deep features for discriminative localization. In Computer vision and pattern recognition.</ce:source-text></reference></bibliography></tail></bibrecord></item></abstracts-retrieval-response>