{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 04-preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words = [\n",
    "    \"finding\", \"research\", \"purpose\", \"study\", \"methodology\", \"result\", \n",
    "    \"analysis\", \"method\", \"paper\", \"literature\", \"innovation\",\n",
    "    \"also\", \"within\", \"whereas\", \"would\", \"br\", \"elsevier\", \"data\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(open('data/scopus_data_2019-09-05-093052.json'), encoding='utf-8')\n",
    "\n",
    "documents_tokens, data_preprocessed, bigram_lexicon = preprocess(data, to_ignore=ignore_words, save_to_file='tmp_preprocessed.csv')\n",
    "print('Documents kept after preprocessing: {}'.format(len(documents_tokens)))\n",
    "\n",
    "with open('Preprocess/tokens.pkl', 'wb') as handle:\n",
    "    pickle.dump(documents_tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequencies\n",
    "\n",
    "Find most frequent words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = word_frequencies(documents_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change `show_top_n` to adjust the number of words to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_top_n = 30\n",
    "\n",
    "# calc\n",
    "mc = fdist.most_common(show_top_n)\n",
    "\n",
    "# plot\n",
    "mc = mc[::-1] ; ws, fs = zip(*mc) ; ws = ['{} ({})'.format(w, f) for w, f in mc]\n",
    "plt.figure(figsize=(3, 6*show_top_n/30))\n",
    "plt.box(False); plt.tick_params(top=False, bottom=True, left=False, right=False, labelleft=True, labelbottom=True)\n",
    "plt.barh(range(len(ws)), fs) ; plt.yticks(range(len(ws)), ws); plt.ylim(-1, len(ws));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context of words\n",
    "\n",
    "Find common contexts (co-occurring words) where the words from the list appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['hotel', 'technology']\n",
    "\n",
    "context = word_contexts(documents_tokens, word_list, num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Topic Modeling/pyldavis.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show topics and most frequently used words in each topic\n",
    "Set `num_topics` to the expected number of topics in the corpus.\n",
    "\n",
    "Set `num_words` to control the number of ***most frequent*** words listed for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_topics = 7\n",
    "num_words = 10\n",
    "\n",
    "corpus, dictionary, ldamodel = pyldavis_prep(documents_tokens, num_topics=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive visualization of topics\n",
    "\n",
    "The visualization shows topics as circles in a 2D plot. This is an approximation of topic similarity. The more similar two topics are, the closer they will be in the plot. The size of the circle corresponds to the presence of the topic in the corpus.\n",
    "\n",
    "The visualization also shows the top 30 ***most relevant*** terms (words) for each topic. If a word is frequent in a topic, but also in the entire corpus, it will get a lower relevance score than a word that is frequent in a topic alone. \n",
    "\n",
    "***Relevance*** of a word in a topic is a weighted measure of the word probability within the topic and the word lift (the ratio of the word probability within the topic to its probability in the entire corpus). \n",
    "\n",
    "***Saliency*** refers to the importance of each word for a topic.\n",
    "\n",
    "\n",
    "How to interact with the visualization:\n",
    "1. Select a topic by clicking on a circle in the plot or by selecting a topic number in the control area at the top.\n",
    "\n",
    "2. On the right, you see the most relevant terms for the selected topic. Adjust the relevance slider. For `lambda = 0` it is equal to the lift of the word, for `lambda = 1` it is equal to probability of the word within the topic.\n",
    "\n",
    "3. If you click on a word in the histogram on the right, topic circles will resize according to the ***saliency*** of the term in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyldavis_vis(corpus, dictionary, ldamodel, save_to_html='tmp.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster documents and topics\n",
    "\n",
    "Add topic vectors and generate a clustering of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Topic Modeling/visualization.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vectors = add_topics_vector(corpus, ldamodel)\n",
    "data_preprocessed_vectors = pd.concat([data_preprocessed, topic_vectors], axis=1)\n",
    "data_preprocessed_vectors.to_csv('tmp_preprocessed_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize with heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hm, cm = visualize(topic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize by time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic = get_dominant_topic(ldamodel, corpus, data)\n",
    "dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_docs = get_representative_doc(dominant_topic)\n",
    "representative_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = get_topic_distribution(dominant_topic, representative_docs)\n",
    "topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-based visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing which topics were the most popular throughout the years. We first plot the distribution through time of the topics generated on the whole database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = []\n",
    "means = []\n",
    "x = []\n",
    "\n",
    "start = int(data['Date'].min())\n",
    "end = int(data['Date'].max())\n",
    "\n",
    "for year in range(start, end):\n",
    "    indices = data.index[data['Date'] == year].tolist()\n",
    "    \n",
    "    total = len(indices)\n",
    "\n",
    "    year_dominant_topic = dominant_topic[dominant_topic.index.isin(indices)]\n",
    "    topic_count = year_dominant_topic['Dominant_Topic'].value_counts()\n",
    "    \n",
    "    t = []\n",
    "    m = []\n",
    "    \n",
    "    for i in range(num_topics):\n",
    "        try:\n",
    "            t.append(topic_count[i])\n",
    "            m.append(topic_count[i] / total)\n",
    "        except:\n",
    "            t.append(0)\n",
    "            m.append(0)\n",
    "        \n",
    "    totals.append(t)\n",
    "    means.append(m)\n",
    "    x.append(year)\n",
    "    \n",
    "totals = np.array(totals).transpose()\n",
    "means = np.array(means).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list containing topic keywords for the vizualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_text = []\n",
    "\n",
    "for i in range(num_topics):\n",
    "    words = []\n",
    "    topic = ldamodel.show_topic(i)\n",
    "    for j in range(len(topic)):\n",
    "        words.append(topic[j][0])\n",
    "        \n",
    "    topic_text.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(totals)):\n",
    "    fig.add_trace(go.Scatter(x = x, y=totals[i],\n",
    "                    mode='lines',\n",
    "                    name=str(topic_text[i])))\n",
    "    \n",
    "#fig.update_layout(width=1200, height=500)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"Total number of documents by topic\",\n",
    "        font=dict(size=22)\n",
    "    ),\n",
    "    xaxis=go.layout.XAxis(\n",
    "        title=go.layout.xaxis.Title(\n",
    "            text=\"Year\",\n",
    "        )\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text=\"Number of documents\",\n",
    "        )\n",
    "    ),\n",
    "    legend=go.layout.Legend(\n",
    "        x=0,\n",
    "        y=-0.8,\n",
    "        font=dict(\n",
    "            family=\"sans-serif\",\n",
    "            size=12,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(totals)):\n",
    "    fig.add_trace(go.Scatter(x = x, y=means[i],\n",
    "                    mode='lines',\n",
    "                    name=str(topic_text[i])))\n",
    "    \n",
    "#fig.update_layout(width=1200, height=500)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"Percentage of documents representing a topic in a year\",\n",
    "        font=dict(size=22)\n",
    "    ),\n",
    "    xaxis=go.layout.XAxis(\n",
    "        title=go.layout.xaxis.Title(\n",
    "            text=\"Year\",\n",
    "        )\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text=\"Percentage\",\n",
    "        )\n",
    "    ),\n",
    "    legend=go.layout.Legend(\n",
    "        x=0,\n",
    "        y=-0.8,\n",
    "        font=dict(\n",
    "            family=\"sans-serif\",\n",
    "            size=12,\n",
    "            color=\"black\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be more interesting to see is, how are topics generated in a smaller time period represented in the whole database. For that we need a function that finds topics for an unseen document or a group of them.\n",
    "\n",
    "`find_doc_topic` calculates the weights of the topics for every document in the dataframe `preprocessed_data`. We can change the number of topics (`no_outputs`) we want to return, the default value is only one topic, which is the one the model evaluated as the best. If we want the function to return only topics, which have a score higher than some value, we can set that value with the `threshold` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doc_topic(ldamodel, dictionary, preprocessed_data, no_outputs=1, threshold=None):\n",
    "    \n",
    "    new_df = preprocessed_data.filter(['Title', 'Abstract', 'Date'], axis=1)\n",
    "    \n",
    "    topic_vectors = []\n",
    "    dominant_topics = []\n",
    "    \n",
    "    # Create a set of tokens used in the smaller model. Using intersection is faster than comparing lists\n",
    "    topic_tokens = set()\n",
    "    for i in range(len(dictionary)):\n",
    "        topic_tokens.add(dictionary[i])\n",
    "        \n",
    "    # Remove words which are not included in the topic_model corpus\n",
    "    for i, row in preprocessed_data.iterrows():\n",
    "        row['tokens'] = topic_tokens.intersection(set(row['tokens']))\n",
    "        \n",
    "    # create corpus for new documents with the ldamodel dictionary\n",
    "    corpus = [dictionary.doc2bow(text) for text in preprocessed_data['tokens']]\n",
    "    \n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        arr = []\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j in range(min(no_outputs, len(row))):\n",
    "            arr.append(row[j])\n",
    "            \n",
    "        topic_vectors.append(arr)\n",
    "        dominant_topics.append(arr[0])\n",
    "        \n",
    "    if no_outputs == 1 and threshold is not None:\n",
    "        for i in range(len(dominant_topics)):\n",
    "            if dominant_topics[i][1] < threshold:\n",
    "                dominant_topics[i] = (None, None)                \n",
    "\n",
    "    df = pd.DataFrame((dominant_topics), columns =['Topic', 'Topic_Weigth'])\n",
    "    topic_df = pd.DataFrame([topic_vectors])\n",
    "    topic_df = topic_df.T\n",
    "    topic_df.columns = ['Topic_Vectors']\n",
    "    df = pd.concat([df, new_df, topic_df], axis=1)\n",
    "    \n",
    "    \n",
    "    return df, topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to select the time period which will be used to generate topics. Then we select the data from the database, which falls into the selected time period and generate the topics with the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startYear = 1995\n",
    "endYear = 2000\n",
    "\n",
    "topic_data = data.loc[(data['Date'] >= startYear) & (data['Date'] <= endYear)]\n",
    "\n",
    "topic_documents_tokens, topic_data_preprocessed, topic_bigram_lexicon = preprocess(topic_data, to_ignore=ignore_words)\n",
    "\n",
    "topic_corpus, topic_dictionary, topic_ldamodel = pyldavis_prep(topic_documents_tokens, num_topics=8, num_words=num_words)\n",
    "\n",
    "topic_vectors = add_topics_vector(topic_corpus, topic_ldamodel)\n",
    "data_preprocessed_vectors = pd.concat([data_preprocessed, topic_vectors], axis=1)\n",
    "\n",
    "with open('Preprocess/tokens_period.pkl', 'wb') as handle:\n",
    "    pickle.dump(topic_documents_tokens, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame, topics = find_doc_topic(topic_ldamodel, topic_dictionary, data_preprocessed, no_outputs=num_topics)\n",
    "year_groups = dataFrame.groupby(['Date', 'Topic']).size().reset_index(name='counts')\n",
    "count = dict(data.groupby('Date').size())\n",
    "\n",
    "totals = []\n",
    "means = []\n",
    "\n",
    "start = 1995\n",
    "stop = 2018\n",
    "\n",
    "for year in range(start, stop + 1):\n",
    "    year_group = year_groups[year_groups['Date'] == year]\n",
    "    \n",
    "    t = []\n",
    "    m = []\n",
    "    \n",
    "    for i in range(8):\n",
    "        if i in list(year_group['Topic']):\n",
    "            t.append(int(year_group[year_group['Topic'] == i]['counts']))\n",
    "            m.append(int(year_group[year_group['Topic'] == i]['counts']) / count[year])\n",
    "        else:\n",
    "            t.append(0)\n",
    "            m.append(0)\n",
    "            \n",
    "    totals.append(t)\n",
    "    means.append(m)\n",
    "\n",
    "totals = np.array(totals).transpose()\n",
    "means = np.array(means).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get descriptions of the topic, for the graph labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_text_small = []\n",
    "\n",
    "for i in range(8):\n",
    "    words = []\n",
    "    topic = topic_ldamodel.show_topic(i)\n",
    "    for j in range(len(topic)):\n",
    "        words.append(topic[j][0])\n",
    "        \n",
    "    topic_text_small.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highlited zone on the plot shows the time period the data was taken from to generate the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(totals)):\n",
    "    fig.add_trace(go.Scatter(x = list(range(start, stop+1)), y=means[i],\n",
    "                    mode='lines',\n",
    "                    name=str(topic_text_small[i])))\n",
    "    \n",
    "fig.update_layout(legend=dict(x=0, y=-0.7))\n",
    "#fig.update_layout(showlegend=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"Distribution of a topic over the documents through years\",\n",
    "    ),\n",
    "    xaxis=go.layout.XAxis(\n",
    "        title=go.layout.xaxis.Title(\n",
    "            text=\"Year\",\n",
    "        )\n",
    "    ),\n",
    "    yaxis=go.layout.YAxis(\n",
    "        title=go.layout.yaxis.Title(\n",
    "            text=\"Percentage of documents\",\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    shapes=[\n",
    "        go.layout.Shape(\n",
    "            type=\"rect\",\n",
    "            # x-reference is assigned to the x-values\n",
    "            xref=\"x\",\n",
    "            # y-reference is assigned to the plot paper [0,1]\n",
    "            yref=\"paper\",\n",
    "            x0=startYear,\n",
    "            y0=0,\n",
    "            x1=endYear,\n",
    "            y1=1,\n",
    "            fillcolor=\"LightSalmon\",\n",
    "            opacity=0.5,\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "#fig.update_layout(width=1200, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "We have now discovered the research topics. We would like to know which authors are publishing the articles with certain topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def get_topic_authors(data, dominant_topic):\n",
    "    df = pd.concat([dominant_topic['Dominant_Topic'], data['Authors']], axis=1)\n",
    "    \n",
    "    topic_authors = []\n",
    "    \n",
    "    for i in range(num_topics):\n",
    "        group = df[df['Dominant_Topic'] == i]\n",
    "        group = group['Authors'].tolist()\n",
    "        authors = dict()\n",
    "        for author_list in group:\n",
    "            for author in author_list:\n",
    "                if author in authors.keys():\n",
    "                    authors[author] += 1\n",
    "                else:\n",
    "                    authors[author] = 1\n",
    "                    \n",
    "        authors = dict((k, v) for k, v in authors.items() if v >= 10)            \n",
    "        sorted_authors = sorted(authors.items(), key=operator.itemgetter(1), reverse=True)\n",
    "                    \n",
    "        topic_authors.append(sorted_authors)\n",
    "    \n",
    "    \n",
    "    return topic_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominant_topic = get_dominant_topic(ldamodel, corpus, data)\n",
    "\n",
    "topic_text = []\n",
    "\n",
    "for i in range(num_topics):\n",
    "    words = []\n",
    "    topic = ldamodel.show_topic(i)\n",
    "    for j in range(len(topic)):\n",
    "        words.append(topic[j][0])\n",
    "        \n",
    "    topic_text.append(words)\n",
    "\n",
    "topic_authors = get_topic_authors(data, dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 0\n",
    "\n",
    "# calc\n",
    "mc = topic_authors[topic]\n",
    "\n",
    "# plot\n",
    "mc = mc[::-1] ; ws, fs = zip(*mc) ; ws = ['{} ({})'.format(w, f) for w, f in mc]\n",
    "fig = plt.figure(figsize=(3, len(mc)/2))\n",
    "plt.box(False); plt.tick_params(top=False, bottom=True, left=False, right=False, labelleft=True, labelbottom=True)\n",
    "plt.barh(range(len(ws)), fs) ; plt.yticks(range(len(ws)), ws); plt.ylim(-1, len(ws));\n",
    "fig.suptitle(\"Number of articles published by author for theme:\\n\" + str(topic_text[topic]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
